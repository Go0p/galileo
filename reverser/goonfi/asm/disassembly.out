entrypoint:
    ldxdw r9, [r1+0x0]                      
    jne r9, 3, lbb_43                               if r9 != (3 as i32 as i64 as u64) { pc += 41 }
    ldxdw r9, [r1+0x8]                      
    jne r9, 66047, lbb_43                           if r9 != (66047 as i32 as i64 as u64) { pc += 39 }
    ldxdw r9, [r1+0x10]                     
    lddw r8, 0x9704a87bee0e880d                     r8 load str located at -7564736223847217139
    jne r9, r8, lbb_43                              if r9 != r8 { pc += 35 }
    ldxdw r9, [r1+0x18]                     
    lddw r8, 0x6e7c683a88af1227                     r8 load str located at 7961352841894760999
    jne r9, r8, lbb_43                              if r9 != r8 { pc += 31 }
    ldxdw r9, [r1+0x20]                     
    lddw r8, 0x872b2854e8d350bb                     r8 load str located at -8706821109442195269
    jne r9, r8, lbb_43                              if r9 != r8 { pc += 27 }
    ldxdw r9, [r1+0x28]                     
    lddw r8, 0xbad9de01fa77caac                     r8 load str located at -4982707412630254932
    jne r9, r8, lbb_43                              if r9 != r8 { pc += 23 }
    ldxdw r9, [r1+0x5478]                   
    ldxdw r8, [r1+0x7cb1]                   
    jge r9, r8, lbb_26                              if r9 >= r8 { pc += 3 }
    lddw r0, 0x1                                    r0 load str located at 1
    exit                                    
lbb_26:
    stxdw [r1+0x28e8], r8                   
    ldxdw r9, [r1+0x7cd1]                   
    ldxdw r8, [r1+0x2bf8]                   
    jge r9, r8, lbb_33                              if r9 >= r8 { pc += 3 }
    lddw r0, 0x2                                    r0 load str located at 2
    exit                                    
lbb_33:
    stxdw [r1+0x2bf8], r9                   
    ldxdw r9, [r1+0x7cb9]                   
    stxdw [r1+0x28d0], r9                   
    ldxdw r9, [r1+0x7cc1]                   
    stxdw [r1+0x28d8], r9                   
    ldxdw r9, [r1+0x7cc9]                   
    stxdw [r1+0x28e0], r9                   
    lddw r0, 0x0                                    r0 load str located at 0
    exit                                    
lbb_43:
    call e                                  
    exit                                    

function_45:
    ldxdw r0, [r5-0xff8]                    
    jlt r3, r2, lbb_54                              if r3 < r2 { pc += 7 }
    ldxdw r5, [r5-0x1000]                   
    jgt r3, r5, lbb_58                              if r3 > r5 { pc += 9 }
    sub64 r3, r2                                    r3 -= r2   ///  r3 = r3.wrapping_sub(r2)
    stxdw [r1+0x8], r3                      
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    stxdw [r1+0x0], r4                      
    exit                                    
lbb_54:
    mov64 r1, r2                                    r1 = r2
    mov64 r2, r3                                    r2 = r3
    mov64 r3, r0                                    r3 = r0
    call function_8841                      
lbb_58:
    mov64 r1, r3                                    r1 = r3
    mov64 r2, r5                                    r2 = r5
    mov64 r3, r0                                    r3 = r0
    call function_8840                      

function_62:
    ldxdw r3, [r2+0x0]                      
    ldxdw r4, [r3+0x50]                     
    jne r4, 82, lbb_84                              if r4 != (82 as i32 as i64 as u64) { pc += 19 }
    ldxdw r4, [r3+0x28]                     
    lddw r5, 0x93a165d7e1f6dd06                     r5 load str located at -7808848301000303354
    jne r4, r5, lbb_81                              if r4 != r5 { pc += 12 }
    ldxdw r4, [r3+0x30]                     
    lddw r5, 0xac79ebce46e1cbd9                     r5 load str located at -6018520155818964007
    jne r4, r5, lbb_81                              if r4 != r5 { pc += 8 }
    ldxdw r4, [r3+0x38]                     
    lddw r5, 0x91375b5fed85b41c                     r5 load str located at -7982811346925931492
    jne r4, r5, lbb_81                              if r4 != r5 { pc += 4 }
    ldxdw r3, [r3+0x40]                     
    lddw r4, 0xa900ff7e85f58c3a                     r4 load str located at -6268729762421306310
    jeq r3, r4, lbb_87                              if r3 == r4 { pc += 6 }
lbb_81:
    stdw [r1+0x0], 0                        
    stw [r1+0x8], 22                        
    ja lbb_86                                       if true { pc += 2 }
lbb_84:
    stdw [r1+0x0], 0                        
    stw [r1+0x8], 3                         
lbb_86:
    exit                                    
lbb_87:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    call function_7233                      
    ldxdw r1, [r10-0x20]                    
    jeq r1, 0, lbb_99                               if r1 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r2, [r10-0x10]                    
    ldxb r3, [r10-0x8]                      
    stxb [r6+0x10], r3                      
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    ja lbb_86                                       if true { pc += -13 }
lbb_99:
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x8], r1                      
    stdw [r6+0x0], 0                        
    ja lbb_86                                       if true { pc += -17 }

function_103:
    ldxdw r3, [r2+0x0]                      
    ldxdw r4, [r3+0x50]                     
    jne r4, 165, lbb_122                            if r4 != (165 as i32 as i64 as u64) { pc += 16 }
    ldxdw r4, [r3+0x28]                     
    lddw r5, 0x93a165d7e1f6dd06                     r5 load str located at -7808848301000303354
    jne r4, r5, lbb_122                             if r4 != r5 { pc += 12 }
    ldxdw r4, [r3+0x30]                     
    lddw r5, 0xac79ebce46e1cbd9                     r5 load str located at -6018520155818964007
    jne r4, r5, lbb_122                             if r4 != r5 { pc += 8 }
    ldxdw r4, [r3+0x38]                     
    lddw r5, 0x91375b5fed85b41c                     r5 load str located at -7982811346925931492
    jne r4, r5, lbb_122                             if r4 != r5 { pc += 4 }
    ldxdw r3, [r3+0x40]                     
    lddw r4, 0xa900ff7e85f58c3a                     r4 load str located at -6268729762421306310
    jeq r3, r4, lbb_125                             if r3 == r4 { pc += 3 }
lbb_122:
    stdw [r1+0x0], 0                        
    stw [r1+0x8], 3                         
lbb_124:
    exit                                    
lbb_125:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    call function_7233                      
    ldxdw r1, [r10-0x20]                    
    jeq r1, 0, lbb_137                              if r1 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r2, [r10-0x10]                    
    ldxb r3, [r10-0x8]                      
    stxb [r6+0x10], r3                      
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    ja lbb_124                                      if true { pc += -13 }
lbb_137:
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x8], r1                      
    stdw [r6+0x0], 0                        
    ja lbb_124                                      if true { pc += -17 }

function_141:
    lddw r1, 0x1000176c0 --> b"\x00\x00\x00\x00\xfbf\x01\x00\x12\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295063232
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    lddw r2, 0x1000176d0 --> b"\x00\x00\x00\x00\x0dg\x01\x00\x15\x00\x00\x00\x00\x00\x00\x00\xaa\x02\x00…        r2 load str located at 4295063248
    call function_7789                      
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r1+0x8]                      
    ldxdw r1, [r1+0x0]                      
    call function_8751                      
    exit                                    

function_158:
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jgt r2, 7, lbb_171                              if r2 > (7 as i32 as i64 as u64) { pc += 11 }
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    lsh64 r3, r2                                    r3 <<= r2   ///  r3 = r3.wrapping_shl(r2 as u32)
    and64 r3, 255                                   r3 &= 255   ///  r3 = r3.and(255)
    ldxb r2, [r1+0x0]                       
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    mov64 r3, r2                                    r3 = r2
    and64 r3, 255                                   r3 &= 255   ///  r3 = r3.and(255)
    jne r3, r2, lbb_168                             if r3 != r2 { pc += 0 }
lbb_168:
    jne r3, r2, lbb_174                             if r3 != r2 { pc += 5 }
    stxb [r1+0x0], r2                       
    exit                                    
lbb_171:
    lddw r1, 0x100017690 --> b"\x00\x00\x00\x00\xe8f\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xe1\x02\x00…        r1 load str located at 4295063184
    call function_9597                      
lbb_174:
    lddw r1, 0x1000176a8 --> b"\x00\x00\x00\x00\xe8f\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xe1\x02\x00…        r1 load str located at 4295063208
    call function_9564                      

function_177:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r7+0x0]                      
    ldxdw r2, [r7+0x8]                      
    mov64 r3, r2                                    r3 = r2
    mov64 r8, r1                                    r8 = r1
    jlt r2, 8, lbb_263                              if r2 < (8 as i32 as i64 as u64) { pc += 79 }
    mov64 r8, r1                                    r8 = r1
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    add64 r3, -8                                    r3 += -8   ///  r3 = r3.wrapping_add(-8 as i32 as i64 as u64)
    jlt r3, 8, lbb_263                              if r3 < (8 as i32 as i64 as u64) { pc += 74 }
    mov64 r8, r1                                    r8 = r1
    add64 r8, 16                                    r8 += 16   ///  r8 = r8.wrapping_add(16 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    add64 r3, -16                                   r3 += -16   ///  r3 = r3.wrapping_add(-16 as i32 as i64 as u64)
    jlt r3, 8, lbb_263                              if r3 < (8 as i32 as i64 as u64) { pc += 69 }
    mov64 r8, r1                                    r8 = r1
    add64 r8, 24                                    r8 += 24   ///  r8 = r8.wrapping_add(24 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    add64 r3, -24                                   r3 += -24   ///  r3 = r3.wrapping_add(-24 as i32 as i64 as u64)
    jlt r3, 8, lbb_263                              if r3 < (8 as i32 as i64 as u64) { pc += 64 }
    mov64 r8, r1                                    r8 = r1
    add64 r8, 32                                    r8 += 32   ///  r8 = r8.wrapping_add(32 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    add64 r3, -32                                   r3 += -32   ///  r3 = r3.wrapping_add(-32 as i32 as i64 as u64)
    jlt r3, 8, lbb_263                              if r3 < (8 as i32 as i64 as u64) { pc += 59 }
    mov64 r8, r1                                    r8 = r1
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    add64 r3, -40                                   r3 += -40   ///  r3 = r3.wrapping_add(-40 as i32 as i64 as u64)
    jlt r3, 8, lbb_263                              if r3 < (8 as i32 as i64 as u64) { pc += 54 }
    mov64 r8, r1                                    r8 = r1
    add64 r8, 48                                    r8 += 48   ///  r8 = r8.wrapping_add(48 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    add64 r3, -48                                   r3 += -48   ///  r3 = r3.wrapping_add(-48 as i32 as i64 as u64)
    jlt r3, 8, lbb_263                              if r3 < (8 as i32 as i64 as u64) { pc += 49 }
    mov64 r8, r1                                    r8 = r1
    add64 r8, 56                                    r8 += 56   ///  r8 = r8.wrapping_add(56 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    add64 r3, -56                                   r3 += -56   ///  r3 = r3.wrapping_add(-56 as i32 as i64 as u64)
    jlt r3, 8, lbb_263                              if r3 < (8 as i32 as i64 as u64) { pc += 44 }
    mov64 r8, r1                                    r8 = r1
    add64 r8, 64                                    r8 += 64   ///  r8 = r8.wrapping_add(64 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    add64 r3, -64                                   r3 += -64   ///  r3 = r3.wrapping_add(-64 as i32 as i64 as u64)
    jlt r3, 8, lbb_263                              if r3 < (8 as i32 as i64 as u64) { pc += 39 }
    mov64 r8, r1                                    r8 = r1
    add64 r8, 72                                    r8 += 72   ///  r8 = r8.wrapping_add(72 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    add64 r3, -72                                   r3 += -72   ///  r3 = r3.wrapping_add(-72 as i32 as i64 as u64)
    jlt r3, 8, lbb_263                              if r3 < (8 as i32 as i64 as u64) { pc += 34 }
    ldxdw r3, [r1+0x0]                      
    stxdw [r10-0x8], r3                     
    ldxdw r3, [r1+0x8]                      
    stxdw [r10-0x10], r3                    
    ldxdw r3, [r1+0x10]                     
    stxdw [r10-0x18], r3                    
    ldxdw r3, [r1+0x18]                     
    stxdw [r10-0x20], r3                    
    ldxdw r8, [r1+0x20]                     
    ldxdw r9, [r1+0x28]                     
    ldxdw r0, [r1+0x30]                     
    ldxdw r5, [r1+0x38]                     
    ldxdw r4, [r1+0x40]                     
    add64 r2, -80                                   r2 += -80   ///  r2 = r2.wrapping_add(-80 as i32 as i64 as u64)
    ldxdw r3, [r1+0x48]                     
    stxdw [r7+0x8], r2                      
    add64 r1, 80                                    r1 += 80   ///  r1 = r1.wrapping_add(80 as i32 as i64 as u64)
    stxdw [r7+0x0], r1                      
    stxdw [r6+0x50], r3                     
    stxdw [r6+0x48], r4                     
    stxdw [r6+0x40], r5                     
    stxdw [r6+0x38], r0                     
    stxdw [r6+0x30], r9                     
    stxdw [r6+0x28], r8                     
    ldxdw r1, [r10-0x20]                    
    stxdw [r6+0x20], r1                     
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x10]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x8]                     
    stxdw [r6+0x8], r1                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_271                                      if true { pc += 8 }
lbb_263:
    add64 r8, r3                                    r8 += r3   ///  r8 = r8.wrapping_add(r3)
    lddw r1, 0x100017668 --> b"\x00\x00\x00\x00\xb0f\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295063144
    call function_7172                      
    stxdw [r7+0x0], r8                      
    stdw [r7+0x8], 0                        
    stxdw [r6+0x8], r0                      
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_271:
    stxdw [r6+0x0], r1                      
    exit                                    

function_273:
    jne r2, 848, lbb_280                            if r2 != (848 as i32 as i64 as u64) { pc += 6 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    jeq r2, 0, lbb_278                              if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_285                                      if true { pc += 7 }
lbb_278:
    mov64 r0, r1                                    r0 = r1
    exit                                    
lbb_280:
    lddw r1, 0x10001672c --> b"from_bytes_mut"        r1 load str located at 4295059244
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_290                       
lbb_285:
    lddw r1, 0x10001672c --> b"from_bytes_mut"        r1 load str located at 4295059244
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_290                       

function_290:
    stxdw [r10-0x60], r2                    
    stxdw [r10-0x68], r1                    
    stxb [r10-0x51], r3                     
    lddw r1, 0x1000176e8 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295063272
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    lddw r1, 0x10000caa0 --> b"{\x1a\xb8\xff\x00\x00\x00\x00\x18\x01\x00\x00\xc0h\x01\x00\x00\x00\x00\x0…        r1 load str located at 4295019168
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -81                                   r1 += -81   ///  r1 = r1.wrapping_add(-81 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x1000005e8 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x12\x08\x00\x00\x00\x00\x00y\x11\x00\x00\x…        r1 load str located at 4294968808
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    lddw r2, 0x100017708 --> b"\x00\x00\x00\x00;g\x01\x00\x0f\x00\x00\x00\x00\x00\x00\x00!\x00\x00\x00\x…        r2 load str located at 4295063304
    call function_7789                      

function_319:
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r2+0x0]                      
    ldxdw r3, [r1+0x8]                      
    lddw r4, 0x515c2c1917d5a706                     r4 load str located at 5862609301215225606
    jne r3, r4, lbb_352                             if r3 != r4 { pc += 27 }
    ldxdw r3, [r1+0x10]                     
    lddw r4, 0x7ff14a3d4cc98c21                     r4 load str located at 9219231539345853473
    jne r3, r4, lbb_352                             if r3 != r4 { pc += 23 }
    ldxdw r3, [r1+0x18]                     
    lddw r4, 0x44fda19b08eeda58                     r4 load str located at 4971307250928769624
    jne r3, r4, lbb_352                             if r3 != r4 { pc += 19 }
    ldxdw r1, [r1+0x20]                     
    lddw r3, 0x8ad9dbe3                             r3 load str located at 2329533411
    jne r1, r3, lbb_352                             if r1 != r3 { pc += 15 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    call function_7233                      
    ldxdw r1, [r10-0x20]                    
    jeq r1, 0, lbb_348                              if r1 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r2, [r10-0x10]                    
    ldxb r3, [r10-0x8]                      
    stxb [r6+0x10], r3                      
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    ja lbb_351                                      if true { pc += 3 }
lbb_348:
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x8], r1                      
    stdw [r6+0x0], 0                        
lbb_351:
    exit                                    
lbb_352:
    stdw [r6+0x0], 0                        
    stw [r6+0x8], 1                         
    ja lbb_351                                      if true { pc += -4 }

function_355:
    mov64 r8, r5                                    r8 = r5
    stxdw [r10-0xa0], r4                    
    mov64 r4, r1                                    r4 = r1
    mov64 r7, 10                                    r7 = 10 as i32 as i64 as u64
    jlt r3, 4, lbb_375                              if r3 < (4 as i32 as i64 as u64) { pc += 15 }
    ldxdw r9, [r2+0x0]                      
    mov64 r6, r2                                    r6 = r2
    mov64 r2, r9                                    r2 = r9
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    lddw r1, 0x1000165b0 --> b"\x0d\x88\x0e\xee{\xa8\x04\x97'\x12\xaf\x88:h|n\xbbP\xd3\xe8T(+\x87\xac\xc…        r1 load str located at 4295058864
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    stxdw [r10-0x98], r4                    
    call function_9989                      
    ldxdw r4, [r10-0x98]                    
    mov64 r3, r6                                    r3 = r6
    mov64 r7, 25                                    r7 = 25 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_378                              if r0 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_375:
    stxw [r4+0x4], r1                       
    stxw [r4+0x0], r7                       
    exit                                    
lbb_378:
    ldxb r2, [r9+0x1]                       
    jeq r2, 0, lbb_375                              if r2 == (0 as i32 as i64 as u64) { pc += -5 }
    ldxdw r1, [r3+0x10]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x100016650 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r2 load str located at 4295059024
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    ldxdw r4, [r10-0x98]                    
    mov64 r9, r6                                    r9 = r6
    mov64 r7, 6                                     r7 = 6 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_375                              if r0 != (0 as i32 as i64 as u64) { pc += -17 }
    jlt r8, 32, lbb_401                             if r8 < (32 as i32 as i64 as u64) { pc += 8 }
    ldxdw r5, [r10-0xa0]                    
    ldxdw r1, [r5+0x6]                      
    ldxb r2, [r5+0xe]                       
    stxb [r10-0x48], r2                     
    stxdw [r10-0x50], r1                    
    jeq r8, 32, lbb_401                             if r8 == (32 as i32 as i64 as u64) { pc += 2 }
    jeq r8, 33, lbb_416                             if r8 == (33 as i32 as i64 as u64) { pc += 16 }
    call function_141                       
lbb_401:
    lddw r1, 0x100017668 --> b"\x00\x00\x00\x00\xb0f\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295063144
    call function_7172                      
    mov64 r7, 2                                     r7 = 2 as i32 as i64 as u64
    mov64 r2, r0                                    r2 = r0
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    ldxdw r4, [r10-0x98]                    
    jne r2, 1, lbb_375                              if r2 != (1 as i32 as i64 as u64) { pc += -34 }
    ldxdw r1, [r0+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_375                              if r2 == (0 as i32 as i64 as u64) { pc += -37 }
    ldxdw r1, [r0-0x1]                      
    callx r2                                
    ldxdw r4, [r10-0x98]                    
    ja lbb_375                                      if true { pc += -41 }
lbb_416:
    mov64 r6, r9                                    r6 = r9
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r2, [r10-0x4f]                    
    ldxb r8, [r5+0x20]                      
    ldxb r3, [r5+0x2]                       
    stxb [r10-0x70], r3                     
    ldxh r3, [r5+0x0]                       
    stxh [r10-0x72], r3                     
    ldxh r3, [r5+0x3]                       
    ldxb r4, [r5+0x5]                       
    stxw [r10-0x6b], r2                     
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    stxw [r10-0x67], r2                     
    lsh64 r4, 16                                    r4 <<= 16   ///  r4 = r4.wrapping_shl(16)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    lsh64 r1, 24                                    r1 <<= 24   ///  r1 = r1.wrapping_shl(24)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    stxw [r10-0x6f], r1                     
    ldxdw r1, [r5+0xf]                      
    stxdw [r10-0x63], r1                    
    ldxdw r1, [r5+0x17]                     
    stxdw [r10-0x5b], r1                    
    ldxb r1, [r5+0x1f]                      
    stxb [r10-0x53], r1                     
    stxb [r10-0x52], r8                     
    mov64 r2, r10                                   r2 = r10
    add64 r2, -114                                  r2 += -114   ///  r2 = r2.wrapping_add(-114 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    call function_5464                      
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r1, 27                                    r1 = 27 as i32 as i64 as u64
    and64 r0, 253                                   r0 &= 253   ///  r0 = r0.and(253)
    or64 r0, 2                                      r0 |= 2   ///  r0 = r0.or(2)
    ldxdw r4, [r10-0x98]                    
    jeq r0, 3, lbb_375                              if r0 == (3 as i32 as i64 as u64) { pc += -76 }
    mov64 r2, r9                                    r2 = r9
    add64 r2, 24                                    r2 += 24   ///  r2 = r2.wrapping_add(24 as i32 as i64 as u64)
    stxb [r10-0x51], r8                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -81                                   r1 += -81   ///  r1 = r1.wrapping_add(-81 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -114                                  r1 += -114   ///  r1 = r1.wrapping_add(-114 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    lddw r1, 0x1000167b6 --> b"blacklistprogram/src/instructions/blacklist_trader"        r1 load str located at 4295059382
    stxdw [r10-0x50], r1                    
    stdw [r10-0x28], 1                      
    stdw [r10-0x38], 32                     
    stdw [r10-0x48], 9                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    call function_319                       
    ldxw r1, [r10-0x14]                     
    ldxw r7, [r10-0x18]                     
    ldxdw r3, [r10-0x20]                    
    jeq r3, 0, lbb_538                              if r3 == (0 as i32 as i64 as u64) { pc += 65 }
    ldxdw r2, [r10-0x10]                    
    ldxdw r4, [r3+0x10]                     
    stxdw [r10-0x8], r4                     
    ldxdw r4, [r3+0x8]                      
    stxdw [r10-0x10], r4                    
    ldxdw r3, [r3+0x0]                      
    stxdw [r10-0x18], r3                    
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    or64 r1, r7                                     r1 |= r7   ///  r1 = r1.or(r7)
    call function_158                       
    stdw [r10-0x20], 1                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0xfe8], r1                   
    stxdw [r10-0xff0], r9                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    stdw [r10-0xff8], 3                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r6                                    r2 = r6
    lddw r3, 0x100016690 --> b"\x0a"                r3 load str located at 4295059088
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_4844                      
    ldxw r1, [r10-0x7c]                     
    ldxw r7, [r10-0x80]                     
    jeq r7, 26, lbb_504                             if r7 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_538                                      if true { pc += 34 }
lbb_504:
    mov64 r7, 11                                    r7 = 11 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r8, [r6+0x0]                      
    ldxb r2, [r8+0x0]                       
    mov64 r3, r2                                    r3 = r2
    and64 r3, 15                                    r3 &= 15   ///  r3 = r3.and(15)
    jne r3, 15, lbb_538                             if r3 != (15 as i32 as i64 as u64) { pc += 27 }
    and64 r2, 247                                   r2 &= 247   ///  r2 = r2.and(247)
    stxb [r8+0x0], r2                       
    ldxdw r1, [r8+0x50]                     
    lddw r2, 0x1000178c8 --> b"\x00\x00\x00\x00\xbfg\x01\x00,\x00\x00\x00\x00\x00\x00\x00@\x00\x00\x00\x…        r2 load str located at 4295063752
    stxdw [r10-0xff8], r2                   
    stxdw [r10-0x1000], r1                  
    mov64 r4, r8                                    r4 = r8
    add64 r4, 88                                    r4 += 88   ///  r4 = r4.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_45                        
    ldxdw r1, [r10-0x88]                    
    jeq r1, 1, lbb_532                              if r1 == (1 as i32 as i64 as u64) { pc += 4 }
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x1000178e0 --> b"\x00\x00\x00\x00\xbfg\x01\x00,\x00\x00\x00\x00\x00\x00\x00@\x00\x00\x00#\…        r3 load str located at 4295063776
    call function_8842                      
lbb_532:
    ldxdw r1, [r10-0x90]                    
    stb [r1+0x0], 1                         
    ldxb r1, [r8+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r8+0x0], r1                       
    mov64 r7, 26                                    r7 = 26 as i32 as i64 as u64
lbb_538:
    ldxdw r4, [r10-0x98]                    
    ja lbb_375                                      if true { pc += -165 }

function_540:
    mov64 r6, r1                                    r6 = r1
    mov64 r7, 10                                    r7 = 10 as i32 as i64 as u64
    jlt r3, 2, lbb_563                              if r3 < (2 as i32 as i64 as u64) { pc += 20 }
    ldxdw r1, [r2+0x0]                      
    mov64 r7, r2                                    r7 = r2
    stxdw [r10-0x48], r1                    
    mov64 r2, r1                                    r2 = r1
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    lddw r1, 0x1000165b0 --> b"\x0d\x88\x0e\xee{\xa8\x04\x97'\x12\xaf\x88:h|n\xbbP\xd3\xe8T(+\x87\xac\xc…        r1 load str located at 4295058864
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    mov64 r8, r5                                    r8 = r5
    mov64 r9, r6                                    r9 = r6
    mov64 r6, r4                                    r6 = r4
    call function_9989                      
    mov64 r5, r6                                    r5 = r6
    mov64 r6, r9                                    r6 = r9
    mov64 r3, r8                                    r3 = r8
    mov64 r8, r7                                    r8 = r7
    mov64 r7, 25                                    r7 = 25 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_566                              if r0 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_563:
    stxw [r6+0x4], r1                       
    stxw [r6+0x0], r7                       
    exit                                    
lbb_566:
    ldxdw r1, [r10-0x48]                    
    ldxb r2, [r1+0x1]                       
    jeq r2, 0, lbb_563                              if r2 == (0 as i32 as i64 as u64) { pc += -6 }
    jlt r3, 32, lbb_577                             if r3 < (32 as i32 as i64 as u64) { pc += 7 }
    ldxdw r1, [r5+0x6]                      
    ldxb r2, [r5+0xe]                       
    stxb [r10-0x8], r2                      
    stxdw [r10-0x10], r1                    
    jeq r3, 32, lbb_577                             if r3 == (32 as i32 as i64 as u64) { pc += 2 }
    jeq r3, 33, lbb_590                             if r3 == (33 as i32 as i64 as u64) { pc += 14 }
    call function_141                       
lbb_577:
    lddw r1, 0x100017668 --> b"\x00\x00\x00\x00\xb0f\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295063144
    call function_7172                      
    mov64 r7, 2                                     r7 = 2 as i32 as i64 as u64
    mov64 r2, r0                                    r2 = r0
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    jne r2, 1, lbb_563                              if r2 != (1 as i32 as i64 as u64) { pc += -21 }
    ldxdw r1, [r0+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_563                              if r2 == (0 as i32 as i64 as u64) { pc += -24 }
    ldxdw r1, [r0-0x1]                      
    callx r2                                
    ja lbb_563                                      if true { pc += -27 }
lbb_590:
    mov64 r9, r8                                    r9 = r8
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r3, [r10-0xf]                     
    ldxb r2, [r5+0x20]                      
    ldxb r4, [r5+0x2]                       
    stxb [r10-0x30], r4                     
    ldxh r4, [r5+0x0]                       
    stxh [r10-0x32], r4                     
    ldxh r4, [r5+0x3]                       
    mov64 r0, r5                                    r0 = r5
    ldxb r5, [r0+0x5]                       
    stxw [r10-0x2b], r3                     
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    stxw [r10-0x27], r3                     
    lsh64 r5, 16                                    r5 <<= 16   ///  r5 = r5.wrapping_shl(16)
    or64 r4, r5                                     r4 |= r5   ///  r4 = r4.or(r5)
    lsh64 r1, 24                                    r1 <<= 24   ///  r1 = r1.wrapping_shl(24)
    or64 r1, r4                                     r1 |= r4   ///  r1 = r1.or(r4)
    stxw [r10-0x2f], r1                     
    ldxdw r1, [r0+0xf]                      
    stxdw [r10-0x23], r1                    
    ldxdw r1, [r0+0x17]                     
    stxdw [r10-0x1b], r1                    
    ldxb r1, [r0+0x1f]                      
    stxb [r10-0x13], r1                     
    stxb [r10-0x12], r2                     
    mov64 r2, r10                                   r2 = r10
    add64 r2, -50                                   r2 += -50   ///  r2 = r2.wrapping_add(-50 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    call function_5464                      
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r1, 28                                    r1 = 28 as i32 as i64 as u64
    and64 r0, 255                                   r0 &= 255   ///  r0 = r0.and(255)
    jeq r0, 2, lbb_563                              if r0 == (2 as i32 as i64 as u64) { pc += -61 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, r8                                    r3 = r8
    call function_5392                      
    ldxw r7, [r10-0x40]                     
    jne r7, 26, lbb_633                             if r7 != (26 as i32 as i64 as u64) { pc += 2 }
    mov64 r7, 26                                    r7 = 26 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += -70 }
lbb_633:
    ldxw r1, [r10-0x3c]                     
    ja lbb_563                                      if true { pc += -72 }

function_635:
    mov64 r6, r1                                    r6 = r1
    mov64 r9, 10                                    r9 = 10 as i32 as i64 as u64
    jlt r3, 6, lbb_652                              if r3 < (6 as i32 as i64 as u64) { pc += 14 }
    ldxdw r7, [r2+0x0]                      
    mov64 r4, r2                                    r4 = r2
    mov64 r2, r7                                    r2 = r7
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    lddw r1, 0x1000165b0 --> b"\x0d\x88\x0e\xee{\xa8\x04\x97'\x12\xaf\x88:h|n\xbbP\xd3\xe8T(+\x87\xac\xc…        r1 load str located at 4295058864
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    stxdw [r10-0x50], r4                    
    call function_9989                      
    ldxdw r2, [r10-0x50]                    
    mov64 r9, 25                                    r9 = 25 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_655                              if r0 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_652:
    stxw [r6+0x4], r8                       
    stxw [r6+0x0], r9                       
    exit                                    
lbb_655:
    ldxb r1, [r7+0x1]                       
    jeq r1, 0, lbb_652                              if r1 == (0 as i32 as i64 as u64) { pc += -5 }
    ldxdw r1, [r2+0x20]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x100016630 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295058992
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    ldxdw r1, [r10-0x50]                    
    mov64 r9, 6                                     r9 = 6 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_652                              if r0 != (0 as i32 as i64 as u64) { pc += -16 }
    ldxdw r1, [r1+0x28]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x100016650 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r2 load str located at 4295059024
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    ldxdw r7, [r10-0x50]                    
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_652                              if r0 != (0 as i32 as i64 as u64) { pc += -26 }
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r8, [r7+0x0]                      
    ldxdw r1, [r8+0x50]                     
    mov64 r9, 3                                     r9 = 3 as i32 as i64 as u64
    jne r1, 856, lbb_735                            if r1 != (856 as i32 as i64 as u64) { pc += 52 }
    mov64 r9, 22                                    r9 = 22 as i32 as i64 as u64
    ldxdw r1, [r8+0x28]                     
    lddw r2, 0xd8a6fe616d93320a                     r2 load str located at -2835299220979502582
    jne r1, r2, lbb_735                             if r1 != r2 { pc += 47 }
    ldxdw r1, [r8+0x30]                     
    lddw r2, 0xb174a1c206e706b6                     r2 load str located at -5659720976986339658
    jne r1, r2, lbb_735                             if r1 != r2 { pc += 43 }
    ldxdw r1, [r8+0x38]                     
    lddw r2, 0xadd40f4b82a5676b                     r2 load str located at -5921090793096517781
    jne r1, r2, lbb_735                             if r1 != r2 { pc += 39 }
    ldxdw r1, [r8+0x40]                     
    lddw r2, 0x7e6598cdd46af8a9                     r2 load str located at 9107853831226194089
    jne r1, r2, lbb_735                             if r1 != r2 { pc += 35 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_7233                      
    ldxw r9, [r10-0x14]                     
    ldxw r1, [r10-0x18]                     
    ldxdw r0, [r10-0x20]                    
    jeq r0, 0, lbb_730                              if r0 == (0 as i32 as i64 as u64) { pc += 22 }
    lsh64 r9, 32                                    r9 <<= 32   ///  r9 = r9.wrapping_shl(32)
    or64 r9, r1                                     r9 |= r1   ///  r9 = r9.or(r1)
    jgt r9, 7, lbb_716                              if r9 > (7 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, r9                                    r2 = r9
    lddw r3, 0x1000178f8 --> b"\x00\x00\x00\x00\xf1g\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x00\xbc\x00\x00…        r3 load str located at 4295063800
    call function_8840                      
lbb_716:
    ldxb r2, [r10-0x8]                      
    ldxdw r1, [r10-0x10]                    
    ldxdw r5, [r0+0x0]                      
    lddw r4, 0xe643d503bdd93852                     r4 load str located at -1854404409499764654
    ldxdw r3, [r10-0x50]                    
    jeq r5, r4, lbb_724                             if r5 == r4 { pc += 1 }
    ja lbb_733                                      if true { pc += 9 }
lbb_724:
    jgt r9, 855, lbb_738                            if r9 > (855 as i32 as i64 as u64) { pc += 13 }
    mov64 r1, 856                                   r1 = 856 as i32 as i64 as u64
    mov64 r2, r9                                    r2 = r9
    lddw r3, 0x100017910 --> b"\x00\x00\x00\x00\xf1g\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x00\xc1\x00\x00…        r3 load str located at 4295063824
    call function_8840                      
lbb_730:
    lsh64 r9, 32                                    r9 <<= 32   ///  r9 = r9.wrapping_shl(32)
    or64 r9, r1                                     r9 |= r1   ///  r9 = r9.or(r1)
    ja lbb_735                                      if true { pc += 2 }
lbb_733:
    call function_158                       
    mov64 r9, 3                                     r9 = 3 as i32 as i64 as u64
lbb_735:
    mov64 r8, r9                                    r8 = r9
    rsh64 r8, 32                                    r8 >>= 32   ///  r8 = r8.wrapping_shr(32)
    ja lbb_652                                      if true { pc += -86 }
lbb_738:
    stxdw [r10-0x68], r8                    
    stxdw [r10-0x60], r1                    
    stxdw [r10-0x58], r2                    
    mov64 r1, r0                                    r1 = r0
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x80], r1                    
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jeq r1, 0, lbb_751                              if r1 == (0 as i32 as i64 as u64) { pc += 5 }
    lddw r1, 0x100016722 --> b"from_bytes"          r1 load str located at 4295059234
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_290                       
lbb_751:
    add64 r3, 16                                    r3 += 16   ///  r3 = r3.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x78], r3                    
    ldxdw r1, [r3+0x0]                      
    mov64 r2, r0                                    r2 = r0
    add64 r2, 320                                   r2 += 320   ///  r2 = r2.wrapping_add(320 as i32 as i64 as u64)
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    stxdw [r10-0x70], r0                    
    call function_9989                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r9, 3                                     r9 = 3 as i32 as i64 as u64
    jne r0, 0, lbb_832                              if r0 != (0 as i32 as i64 as u64) { pc += 68 }
    ldxdw r4, [r10-0x68]                    
    add64 r4, 8                                     r4 += 8   ///  r4 = r4.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r3, [r10-0x70]                    
    add64 r3, 256                                   r3 += 256   ///  r3 = r3.wrapping_add(256 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    ldxdw r2, [r10-0x78]                    
    stxdw [r10-0x68], r4                    
    call function_4705                      
    ldxw r9, [r10-0x28]                     
    jne r9, 26, lbb_832                             if r9 != (26 as i32 as i64 as u64) { pc += 57 }
    ldxdw r1, [r10-0x50]                    
    add64 r1, 24                                    r1 += 24   ///  r1 = r1.wrapping_add(24 as i32 as i64 as u64)
    stxdw [r10-0x88], r1                    
    ldxdw r1, [r1+0x0]                      
    ldxdw r2, [r10-0x70]                    
    add64 r2, 352                                   r2 += 352   ///  r2 = r2.wrapping_add(352 as i32 as i64 as u64)
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r9, 3                                     r9 = 3 as i32 as i64 as u64
    jne r0, 0, lbb_832                              if r0 != (0 as i32 as i64 as u64) { pc += 44 }
    ldxdw r3, [r10-0x70]                    
    add64 r3, 288                                   r3 += 288   ///  r3 = r3.wrapping_add(288 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    ldxdw r2, [r10-0x88]                    
    ldxdw r4, [r10-0x68]                    
    call function_4705                      
    ldxw r9, [r10-0x30]                     
    jeq r9, 26, lbb_798                             if r9 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_832                                      if true { pc += 34 }
lbb_798:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -56                                   r1 += -56   ///  r1 = r1.wrapping_add(-56 as i32 as i64 as u64)
    ldxdw r2, [r10-0x80]                    
    mov64 r3, r7                                    r3 = r7
    ldxdw r4, [r10-0x78]                    
    ldxdw r5, [r10-0x50]                    
    call function_4532                      
    ldxw r8, [r10-0x34]                     
    ldxw r9, [r10-0x38]                     
    jne r9, 26, lbb_832                             if r9 != (26 as i32 as i64 as u64) { pc += 24 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    ldxdw r2, [r10-0x80]                    
    mov64 r3, r7                                    r3 = r7
    ldxdw r4, [r10-0x88]                    
    ldxdw r5, [r10-0x50]                    
    call function_4532                      
    ldxw r8, [r10-0x3c]                     
    ldxw r9, [r10-0x40]                     
    jne r9, 26, lbb_832                             if r9 != (26 as i32 as i64 as u64) { pc += 14 }
    ldxdw r1, [r10-0x60]                    
    ldxdw r2, [r10-0x58]                    
    call function_158                       
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    ldxdw r3, [r10-0x50]                    
    call function_5392                      
    mov64 r9, 26                                    r9 = 26 as i32 as i64 as u64
    ldxw r1, [r10-0x48]                     
    jeq r1, 26, lbb_652                             if r1 == (26 as i32 as i64 as u64) { pc += -177 }
    ldxw r8, [r10-0x44]                     
    mov64 r9, r1                                    r9 = r1
    ja lbb_652                                      if true { pc += -180 }
lbb_832:
    ldxdw r1, [r10-0x60]                    
    ldxdw r2, [r10-0x58]                    
    call function_158                       
    ja lbb_652                                      if true { pc += -184 }

function_836:
    mov64 r7, r5                                    r7 = r5
    mov64 r9, r4                                    r9 = r4
    mov64 r4, r1                                    r4 = r1
    mov64 r6, 10                                    r6 = 10 as i32 as i64 as u64
    jlt r3, 8, lbb_858                              if r3 < (8 as i32 as i64 as u64) { pc += 17 }
    ldxdw r8, [r2+0x0]                      
    mov64 r3, r8                                    r3 = r8
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    lddw r1, 0x1000165b0 --> b"\x0d\x88\x0e\xee{\xa8\x04\x97'\x12\xaf\x88:h|n\xbbP\xd3\xe8T(+\x87\xac\xc…        r1 load str located at 4295058864
    stxdw [r10-0x670], r2                   
    stxdw [r10-0x678], r3                   
    mov64 r2, r3                                    r2 = r3
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    stxdw [r10-0x668], r4                   
    call function_9989                      
    ldxdw r3, [r10-0x670]                   
    ldxdw r4, [r10-0x668]                   
    mov64 r6, 25                                    r6 = 25 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_861                              if r0 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_858:
    stxw [r4+0x4], r1                       
    stxw [r4+0x0], r6                       
    exit                                    
lbb_861:
    ldxb r2, [r8+0x1]                       
    jeq r2, 0, lbb_858                              if r2 == (0 as i32 as i64 as u64) { pc += -5 }
    ldxdw r1, [r3+0x30]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x100016630 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295058992
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    ldxdw r2, [r10-0x670]                   
    ldxdw r4, [r10-0x668]                   
    mov64 r6, 6                                     r6 = 6 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_858                              if r0 != (0 as i32 as i64 as u64) { pc += -17 }
    ldxdw r1, [r2+0x38]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x100016650 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r2 load str located at 4295059024
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    ldxdw r4, [r10-0x668]                   
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_858                              if r0 != (0 as i32 as i64 as u64) { pc += -27 }
    jlt r7, 8, lbb_1065                             if r7 < (8 as i32 as i64 as u64) { pc += 179 }
    jeq r7, 8, lbb_1065                             if r7 == (8 as i32 as i64 as u64) { pc += 178 }
    jeq r7, 9, lbb_1065                             if r7 == (9 as i32 as i64 as u64) { pc += 177 }
    jeq r7, 10, lbb_1065                            if r7 == (10 as i32 as i64 as u64) { pc += 176 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -11                                   r1 += -11   ///  r1 = r1.wrapping_add(-11 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 173 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -19                                   r1 += -19   ///  r1 = r1.wrapping_add(-19 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 170 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -27                                   r1 += -27   ///  r1 = r1.wrapping_add(-27 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 167 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -35                                   r1 += -35   ///  r1 = r1.wrapping_add(-35 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 164 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -43                                   r1 += -43   ///  r1 = r1.wrapping_add(-43 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 161 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -51                                   r1 += -51   ///  r1 = r1.wrapping_add(-51 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 158 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -59                                   r1 += -59   ///  r1 = r1.wrapping_add(-59 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 155 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -67                                   r1 += -67   ///  r1 = r1.wrapping_add(-67 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 152 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -75                                   r1 += -75   ///  r1 = r1.wrapping_add(-75 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 149 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -83                                   r1 += -83   ///  r1 = r1.wrapping_add(-83 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 146 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -91                                   r1 += -91   ///  r1 = r1.wrapping_add(-91 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 143 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -99                                   r1 += -99   ///  r1 = r1.wrapping_add(-99 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 140 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -107                                  r1 += -107   ///  r1 = r1.wrapping_add(-107 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 137 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -115                                  r1 += -115   ///  r1 = r1.wrapping_add(-115 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 134 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -123                                  r1 += -123   ///  r1 = r1.wrapping_add(-123 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 131 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -131                                  r1 += -131   ///  r1 = r1.wrapping_add(-131 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 128 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -139                                  r1 += -139   ///  r1 = r1.wrapping_add(-139 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 125 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -147                                  r1 += -147   ///  r1 = r1.wrapping_add(-147 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 122 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -155                                  r1 += -155   ///  r1 = r1.wrapping_add(-155 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 119 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -163                                  r1 += -163   ///  r1 = r1.wrapping_add(-163 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 116 }
    ldxdw r1, [r9+0x0]                      
    stxdw [r10-0x718], r1                   
    ldxb r1, [r9+0x8]                       
    stxdw [r10-0x710], r1                   
    ldxb r1, [r9+0x9]                       
    stxdw [r10-0x728], r1                   
    ldxb r1, [r9+0xa]                       
    stxdw [r10-0x720], r1                   
    ldxdw r8, [r9+0xb]                      
    ldxdw r1, [r9+0x13]                     
    stxdw [r10-0x680], r1                   
    ldxdw r1, [r9+0x1b]                     
    stxdw [r10-0x688], r1                   
    ldxdw r1, [r9+0x23]                     
    stxdw [r10-0x690], r1                   
    ldxdw r1, [r9+0x2b]                     
    stxdw [r10-0x698], r1                   
    ldxdw r1, [r9+0x33]                     
    stxdw [r10-0x6a0], r1                   
    ldxdw r1, [r9+0x3b]                     
    stxdw [r10-0x6a8], r1                   
    ldxdw r1, [r9+0x43]                     
    stxdw [r10-0x6b0], r1                   
    ldxdw r1, [r9+0x4b]                     
    stxdw [r10-0x6b8], r1                   
    ldxdw r1, [r9+0x53]                     
    stxdw [r10-0x6c0], r1                   
    ldxdw r1, [r9+0x5b]                     
    stxdw [r10-0x6c8], r1                   
    ldxdw r1, [r9+0x63]                     
    stxdw [r10-0x6d0], r1                   
    ldxdw r1, [r9+0x6b]                     
    stxdw [r10-0x6d8], r1                   
    ldxdw r1, [r9+0x73]                     
    stxdw [r10-0x6e0], r1                   
    ldxdw r1, [r9+0x7b]                     
    stxdw [r10-0x6e8], r1                   
    ldxdw r1, [r9+0x83]                     
    stxdw [r10-0x6f0], r1                   
    ldxdw r1, [r9+0x8b]                     
    stxdw [r10-0x6f8], r1                   
    ldxdw r1, [r9+0x93]                     
    stxdw [r10-0x700], r1                   
    ldxdw r1, [r9+0x9b]                     
    stxdw [r10-0x708], r1                   
    add64 r7, -171                                  r7 += -171   ///  r7 = r7.wrapping_add(-171 as i32 as i64 as u64)
    ldxdw r6, [r9+0xa3]                     
    stxdw [r10-0x60], r7                    
    add64 r9, 171                                   r9 += 171   ///  r9 = r9.wrapping_add(171 as i32 as i64 as u64)
    stxdw [r10-0x68], r9                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1112                                 r1 += -1112   ///  r1 = r1.wrapping_add(-1112 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -104                                  r2 += -104   ///  r2 = r2.wrapping_add(-104 as i32 as i64 as u64)
    call function_177                       
    ldxdw r9, [r10-0x450]                   
    ldxdw r1, [r10-0x458]                   
    jne r1, 0, lbb_1069                             if r1 != (0 as i32 as i64 as u64) { pc += 62 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -424                                  r1 += -424   ///  r1 = r1.wrapping_add(-424 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1096                                 r2 += -1096   ///  r2 = r2.wrapping_add(-1096 as i32 as i64 as u64)
    mov64 r3, 72                                    r3 = 72 as i32 as i64 as u64
    call function_9980                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1112                                 r1 += -1112   ///  r1 = r1.wrapping_add(-1112 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -104                                  r2 += -104   ///  r2 = r2.wrapping_add(-104 as i32 as i64 as u64)
    call function_177                       
    ldxdw r1, [r10-0x450]                   
    stxdw [r10-0x730], r1                   
    ldxdw r1, [r10-0x458]                   
    jne r1, 0, lbb_1081                             if r1 != (0 as i32 as i64 as u64) { pc += 59 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1568                                 r1 += -1568   ///  r1 = r1.wrapping_add(-1568 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1096                                 r2 += -1096   ///  r2 = r2.wrapping_add(-1096 as i32 as i64 as u64)
    mov64 r3, 72                                    r3 = 72 as i32 as i64 as u64
    call function_9980                      
    ldxdw r7, [r10-0x60]                    
    jlt r7, 8, lbb_1065                             if r7 < (8 as i32 as i64 as u64) { pc += 35 }
    mov64 r1, r7                                    r1 = r7
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    jeq r1, 8, lbb_1065                             if r1 == (8 as i32 as i64 as u64) { pc += 32 }
    jeq r7, 16, lbb_1065                            if r7 == (16 as i32 as i64 as u64) { pc += 31 }
    jeq r7, 17, lbb_1065                            if r7 == (17 as i32 as i64 as u64) { pc += 30 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -18                                   r1 += -18   ///  r1 = r1.wrapping_add(-18 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 27 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -26                                   r1 += -26   ///  r1 = r1.wrapping_add(-26 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 24 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -34                                   r1 += -34   ///  r1 = r1.wrapping_add(-34 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 21 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -42                                   r1 += -42   ///  r1 = r1.wrapping_add(-42 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 18 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -50                                   r1 += -50   ///  r1 = r1.wrapping_add(-50 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 15 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -58                                   r1 += -58   ///  r1 = r1.wrapping_add(-58 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 12 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -66                                   r1 += -66   ///  r1 = r1.wrapping_add(-66 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 9 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -74                                   r1 += -74   ///  r1 = r1.wrapping_add(-74 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 6 }
    jeq r7, 82, lbb_1065                            if r7 == (82 as i32 as i64 as u64) { pc += 5 }
    jeq r7, 83, lbb_1065                            if r7 == (83 as i32 as i64 as u64) { pc += 4 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, -84                                   r1 += -84   ///  r1 = r1.wrapping_add(-84 as i32 as i64 as u64)
    jlt r1, 8, lbb_1065                             if r1 < (8 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1083                                     if true { pc += 18 }
lbb_1065:
    lddw r1, 0x100017668 --> b"\x00\x00\x00\x00\xb0f\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295063144
    call function_7172                      
    mov64 r9, r0                                    r9 = r0
lbb_1069:
    mov64 r6, 2                                     r6 = 2 as i32 as i64 as u64
    mov64 r2, r9                                    r2 = r9
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    ldxdw r4, [r10-0x668]                   
    jne r2, 1, lbb_858                              if r2 != (1 as i32 as i64 as u64) { pc += -216 }
    ldxdw r1, [r9+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_858                              if r2 == (0 as i32 as i64 as u64) { pc += -219 }
    ldxdw r1, [r9-0x1]                      
    callx r2                                
    ldxdw r4, [r10-0x668]                   
    ja lbb_858                                      if true { pc += -223 }
lbb_1081:
    ldxdw r9, [r10-0x730]                   
    ja lbb_1069                                     if true { pc += -14 }
lbb_1083:
    ldxdw r1, [r10-0x68]                    
    ldxdw r2, [r1+0x0]                      
    stxdw [r10-0x768], r2                   
    ldxdw r2, [r1+0x8]                      
    stxdw [r10-0x770], r2                   
    ldxb r2, [r1+0x10]                      
    stxdw [r10-0x788], r2                   
    ldxb r2, [r1+0x11]                      
    stxdw [r10-0x758], r2                   
    ldxdw r2, [r1+0x12]                     
    stxdw [r10-0x778], r2                   
    ldxdw r2, [r1+0x1a]                     
    stxdw [r10-0x750], r2                   
    ldxdw r2, [r1+0x22]                     
    stxdw [r10-0x780], r2                   
    ldxdw r2, [r1+0x2a]                     
    stxdw [r10-0x790], r2                   
    ldxdw r2, [r1+0x32]                     
    stxdw [r10-0x798], r2                   
    ldxdw r2, [r1+0x3a]                     
    stxdw [r10-0x7a0], r2                   
    ldxdw r2, [r1+0x42]                     
    stxdw [r10-0x7a8], r2                   
    ldxdw r2, [r1+0x4a]                     
    stxdw [r10-0x7b0], r2                   
    ldxb r2, [r1+0x52]                      
    stxdw [r10-0x7b8], r2                   
    ldxb r2, [r1+0x53]                      
    stxdw [r10-0x7c0], r2                   
    ldxdw r1, [r1+0x54]                     
    stxdw [r10-0x760], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -296                                  r1 += -296   ///  r1 = r1.wrapping_add(-296 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -424                                  r2 += -424   ///  r2 = r2.wrapping_add(-424 as i32 as i64 as u64)
    mov64 r3, 72                                    r3 = 72 as i32 as i64 as u64
    call function_9980                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -504                                  r1 += -504   ///  r1 = r1.wrapping_add(-504 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1568                                 r2 += -1568   ///  r2 = r2.wrapping_add(-1568 as i32 as i64 as u64)
    mov64 r3, 72                                    r3 = 72 as i32 as i64 as u64
    call function_9980                      
    jeq r7, 92, lbb_1128                            if r7 == (92 as i32 as i64 as u64) { pc += 1 }
    call function_141                       
lbb_1128:
    ldxdw r1, [r10-0x670]                   
    mov64 r2, r1                                    r2 = r1
    add64 r2, 24                                    r2 += 24   ///  r2 = r2.wrapping_add(24 as i32 as i64 as u64)
    stxdw [r10-0x740], r2                   
    mov64 r2, r1                                    r2 = r1
    add64 r2, 16                                    r2 += 16   ///  r2 = r2.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x738], r2                   
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x748], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1400                                 r1 += -1400   ///  r1 = r1.wrapping_add(-1400 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -296                                  r2 += -296   ///  r2 = r2.wrapping_add(-296 as i32 as i64 as u64)
    mov64 r3, 72                                    r3 = 72 as i32 as i64 as u64
    call function_9980                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1320                                 r1 += -1320   ///  r1 = r1.wrapping_add(-1320 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -504                                  r2 += -504   ///  r2 = r2.wrapping_add(-504 as i32 as i64 as u64)
    mov64 r3, 72                                    r3 = 72 as i32 as i64 as u64
    call function_9980                      
    ldxdw r1, [r10-0x788]                   
    stxb [r10-0x498], r1                    
    ldxdw r1, [r10-0x7b0]                   
    stxdw [r10-0x4a0], r1                   
    ldxdw r1, [r10-0x7a8]                   
    stxdw [r10-0x4a8], r1                   
    ldxdw r1, [r10-0x7a0]                   
    stxdw [r10-0x4b0], r1                   
    ldxdw r1, [r10-0x798]                   
    stxdw [r10-0x4b8], r1                   
    ldxdw r1, [r10-0x790]                   
    stxdw [r10-0x4c0], r1                   
    ldxdw r1, [r10-0x780]                   
    stxdw [r10-0x4c8], r1                   
    ldxdw r1, [r10-0x778]                   
    stxdw [r10-0x4d0], r1                   
    ldxdw r1, [r10-0x770]                   
    stxdw [r10-0x4d8], r1                   
    ldxdw r1, [r10-0x768]                   
    stxdw [r10-0x4e0], r1                   
    ldxdw r1, [r10-0x730]                   
    stxdw [r10-0x530], r1                   
    stxdw [r10-0x580], r9                   
    stxdw [r10-0x588], r6                   
    ldxdw r1, [r10-0x708]                   
    stxdw [r10-0x590], r1                   
    ldxdw r1, [r10-0x700]                   
    stxdw [r10-0x598], r1                   
    ldxdw r1, [r10-0x6f8]                   
    stxdw [r10-0x5a0], r1                   
    ldxdw r1, [r10-0x6f0]                   
    stxdw [r10-0x5a8], r1                   
    ldxdw r1, [r10-0x6e8]                   
    stxdw [r10-0x5b0], r1                   
    ldxdw r1, [r10-0x6e0]                   
    stxdw [r10-0x5b8], r1                   
    ldxdw r1, [r10-0x6d8]                   
    stxdw [r10-0x5c0], r1                   
    ldxdw r1, [r10-0x6d0]                   
    stxdw [r10-0x5c8], r1                   
    ldxdw r1, [r10-0x6c8]                   
    stxdw [r10-0x5d0], r1                   
    ldxdw r1, [r10-0x6c0]                   
    stxdw [r10-0x5d8], r1                   
    ldxdw r1, [r10-0x6b8]                   
    stxdw [r10-0x5e0], r1                   
    ldxdw r1, [r10-0x6b0]                   
    stxdw [r10-0x5e8], r1                   
    ldxdw r1, [r10-0x6a8]                   
    stxdw [r10-0x5f0], r1                   
    ldxdw r1, [r10-0x6a0]                   
    stxdw [r10-0x5f8], r1                   
    ldxdw r1, [r10-0x698]                   
    stxdw [r10-0x600], r1                   
    ldxdw r1, [r10-0x690]                   
    stxdw [r10-0x608], r1                   
    ldxdw r1, [r10-0x688]                   
    stxdw [r10-0x610], r1                   
    ldxdw r1, [r10-0x680]                   
    stxdw [r10-0x618], r1                   
    mov64 r1, r8                                    r1 = r8
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    stxw [r10-0x61c], r1                    
    stxw [r10-0x620], r8                    
    ldxdw r1, [r10-0x758]                   
    jlt r1, 15, lbb_1216                            if r1 < (15 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 15                                    r1 = 15 as i32 as i64 as u64
lbb_1216:
    stw [r10-0x487], 0                      
    stw [r10-0x484], 0                      
    ldxdw r2, [r10-0x750]                   
    stxdw [r10-0x490], r2                   
    ldxdw r2, [r10-0x760]                   
    stxdw [r10-0x480], r2                   
    stxb [r10-0x488], r1                    
    stw [r10-0x494], 0                      
    stw [r10-0x497], 0                      
    stdw [r10-0x478], 0                     
    ldxdw r1, [r10-0x748]                   
    ldxdw r1, [r1+0x0]                      
    stxdw [r10-0x680], r1                   
    ldxdw r1, [r10-0x740]                   
    ldxdw r2, [r1+0x0]                      
    ldxdw r1, [r10-0x738]                   
    ldxdw r1, [r1+0x0]                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x688], r1                   
    stxdw [r10-0x690], r2                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_1648                             if r0 == (0 as i32 as i64 as u64) { pc += 404 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1112                                 r1 += -1112   ///  r1 = r1.wrapping_add(-1112 as i32 as i64 as u64)
    ldxdw r2, [r10-0x738]                   
    call function_62                        
    ldxw r1, [r10-0x44c]                    
    ldxw r2, [r10-0x450]                    
    ldxdw r3, [r10-0x458]                   
    jeq r3, 0, lbb_1414                             if r3 == (0 as i32 as i64 as u64) { pc += 162 }
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    ldxdw r2, [r10-0x448]                   
    ldxb r7, [r3+0x2c]                      
    call function_158                       
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1112                                 r1 += -1112   ///  r1 = r1.wrapping_add(-1112 as i32 as i64 as u64)
    ldxdw r2, [r10-0x740]                   
    call function_62                        
    ldxw r1, [r10-0x44c]                    
    ldxw r2, [r10-0x450]                    
    ldxdw r3, [r10-0x458]                   
    jeq r3, 0, lbb_1414                             if r3 == (0 as i32 as i64 as u64) { pc += 149 }
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    ldxdw r2, [r10-0x448]                   
    ldxb r9, [r3+0x2c]                      
    call function_158                       
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x7b8]                   
    mov64 r3, r7                                    r3 = r7
    jne r3, r2, lbb_1648                            if r3 != r2 { pc += 374 }
    ldxdw r2, [r10-0x7c0]                   
    mov64 r3, r9                                    r3 = r9
    jeq r3, r2, lbb_1278                            if r3 == r2 { pc += 1 }
    ja lbb_1648                                     if true { pc += 370 }
lbb_1278:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1576                                 r1 += -1576   ///  r1 = r1.wrapping_add(-1576 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1568                                 r2 += -1568   ///  r2 = r2.wrapping_add(-1568 as i32 as i64 as u64)
    call function_3274                      
    ldxw r1, [r10-0x624]                    
    ldxw r6, [r10-0x628]                    
    jne r6, 26, lbb_1648                            if r6 != (26 as i32 as i64 as u64) { pc += 362 }
    ldxdw r2, [r10-0x678]                   
    ldxdw r1, [r2+0x18]                     
    stxdw [r10-0x210], r1                   
    ldxdw r1, [r2+0x10]                     
    stxdw [r10-0x218], r1                   
    ldxdw r1, [r2+0x8]                      
    stxdw [r10-0x220], r1                   
    ldxdw r1, [r2+0x0]                      
    stxdw [r10-0x228], r1                   
    ldxdw r9, [r10-0x688]                   
    ldxdw r1, [r9+0x18]                     
    stxdw [r10-0x230], r1                   
    ldxdw r1, [r9+0x10]                     
    stxdw [r10-0x238], r1                   
    ldxdw r1, [r9+0x8]                      
    stxdw [r10-0x240], r1                   
    ldxdw r1, [r9+0x0]                      
    stxdw [r10-0x248], r1                   
    ldxdw r7, [r10-0x690]                   
    ldxdw r1, [r7+0x0]                      
    stxdw [r10-0x268], r1                   
    ldxdw r1, [r7+0x8]                      
    stxdw [r10-0x260], r1                   
    ldxdw r1, [r7+0x10]                     
    stxdw [r10-0x258], r1                   
    ldxdw r1, [r7+0x18]                     
    stxdw [r10-0x250], r1                   
    ldxdw r6, [r10-0x670]                   
    ldxdw r1, [r6+0x20]                     
    ldxdw r2, [r1+0x20]                     
    stxdw [r10-0x270], r2                   
    ldxdw r2, [r1+0x18]                     
    stxdw [r10-0x278], r2                   
    ldxdw r2, [r1+0x10]                     
    stxdw [r10-0x280], r2                   
    ldxdw r1, [r1+0x8]                      
    stxdw [r10-0x288], r1                   
    ldxdw r1, [r6+0x28]                     
    ldxdw r2, [r1+0x8]                      
    stxdw [r10-0x2a8], r2                   
    ldxdw r2, [r1+0x10]                     
    stxdw [r10-0x2a0], r2                   
    ldxdw r2, [r1+0x18]                     
    stxdw [r10-0x298], r2                   
    ldxdw r1, [r1+0x20]                     
    stxdw [r10-0x290], r1                   
    stdw [r10-0x1f8], 0                     
    stdw [r10-0x1f0], 0                     
    stdw [r10-0x1e8], 0                     
    stdw [r10-0x1e0], 0                     
    stdw [r10-0x1d8], 0                     
    stdw [r10-0x1d0], 0                     
    stdw [r10-0x1c8], 0                     
    stdw [r10-0x1c0], 0                     
    stdw [r10-0x1b8], 0                     
    stdw [r10-0x1b0], 0                     
    stdw [r10-0x470], 0                     
    stdw [r10-0x468], 0                     
    stdw [r10-0x460], 0                     
    stdw [r10-0x200], 0                     
    stdw [r10-0x208], 0                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -424                                  r1 += -424   ///  r1 = r1.wrapping_add(-424 as i32 as i64 as u64)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r3, 112                                   r3 = 112 as i32 as i64 as u64
    call function_9984                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1112                                 r1 += -1112   ///  r1 = r1.wrapping_add(-1112 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1568                                 r2 += -1568   ///  r2 = r2.wrapping_add(-1568 as i32 as i64 as u64)
    mov64 r3, 432                                   r3 = 432 as i32 as i64 as u64
    call function_9980                      
    ldxdw r1, [r10-0x710]                   
    stxb [r10-0x131], r1                    
    ldxdw r1, [r10-0x718]                   
    stxdw [r10-0x130], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -305                                  r1 += -305   ///  r1 = r1.wrapping_add(-305 as i32 as i64 as u64)
    stxdw [r10-0xe8], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -304                                  r1 += -304   ///  r1 = r1.wrapping_add(-304 as i32 as i64 as u64)
    stxdw [r10-0xf8], r1                    
    stxdw [r10-0x108], r7                   
    stxdw [r10-0x118], r9                   
    lddw r1, 0x1000167eb --> b"marketprogram/src/state/market.rsvault\x04y\xd5[\xf21\xc0n\xeet\xc5n"        r1 load str located at 4295059435
    stxdw [r10-0x128], r1                   
    stdw [r10-0xd8], 0                      
    stdw [r10-0xe0], 1                      
    stdw [r10-0xf0], 8                      
    stdw [r10-0x100], 32                    
    stdw [r10-0x110], 32                    
    stdw [r10-0x120], 6                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -216                                  r1 += -216   ///  r1 = r1.wrapping_add(-216 as i32 as i64 as u64)
    stxdw [r10-0xfe8], r1                   
    stxdw [r10-0xff0], r6                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -296                                  r1 += -296   ///  r1 = r1.wrapping_add(-296 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    stdw [r10-0xff8], 5                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1584                                 r1 += -1584   ///  r1 = r1.wrapping_add(-1584 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0x748]                   
    lddw r3, 0x100016690 --> b"\x0a2\x93ma\xfe\xa6\xd8\xb6\x06\xe7\x06\xc2\xa1t\xb1kg\xa5\x82K\x0f\xd4\x…        r3 load str located at 4295059088
    mov64 r4, 856                                   r4 = 856 as i32 as i64 as u64
    call function_4844                      
    ldxw r1, [r10-0x62c]                    
    ldxw r6, [r10-0x630]                    
    jne r6, 26, lbb_1648                            if r6 != (26 as i32 as i64 as u64) { pc += 250 }
    mov64 r6, 11                                    r6 = 11 as i32 as i64 as u64
    ldxdw r1, [r10-0x680]                   
    ldxb r2, [r1+0x0]                       
    mov64 r3, r2                                    r3 = r2
    and64 r3, 15                                    r3 &= 15   ///  r3 = r3.and(15)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jne r3, 15, lbb_1648                            if r3 != (15 as i32 as i64 as u64) { pc += 243 }
    and64 r2, 247                                   r2 &= 247   ///  r2 = r2.and(247)
    ldxdw r1, [r10-0x680]                   
    stxb [r1+0x0], r2                       
    ldxdw r2, [r1+0x50]                     
    jgt r2, 7, lbb_1419                             if r2 > (7 as i32 as i64 as u64) { pc += 9 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    lddw r3, 0x100017928 --> b"\x00\x00\x00\x00\xf1g\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x00/\x01\x00\x0…        r3 load str located at 4295063848
    call function_8840                      
lbb_1414:
    mov64 r6, 26                                    r6 = 26 as i32 as i64 as u64
    ldxdw r4, [r10-0x668]                   
    jeq r2, 26, lbb_858                             if r2 == (26 as i32 as i64 as u64) { pc += -559 }
    mov64 r6, r2                                    r6 = r2
    ja lbb_858                                      if true { pc += -561 }
lbb_1419:
    lddw r1, 0xe643d503bdd93852                     r1 load str located at -1854404409499764654
    ldxdw r4, [r10-0x680]                   
    stxdw [r4+0x58], r1                     
    lddw r1, 0x100017940 --> b"\x00\x00\x00\x00\xf1g\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x001\x01\x00\x0…        r1 load str located at 4295063872
    stxdw [r10-0xff8], r1                   
    stxdw [r10-0x1000], r2                  
    add64 r4, 88                                    r4 += 88   ///  r4 = r4.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1600                                 r1 += -1600   ///  r1 = r1.wrapping_add(-1600 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    mov64 r3, 856                                   r3 = 856 as i32 as i64 as u64
    call function_45                        
    ldxdw r1, [r10-0x638]                   
    jeq r1, 848, lbb_1440                           if r1 == (848 as i32 as i64 as u64) { pc += 4 }
    mov64 r2, 848                                   r2 = 848 as i32 as i64 as u64
    lddw r3, 0x100017958 --> b"\x00\x00\x00\x00\xf1g\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x001\x01\x00\x0…        r3 load str located at 4295063896
    call function_8842                      
lbb_1440:
    ldxdw r7, [r10-0x670]                   
    add64 r7, 32                                    r7 += 32   ///  r7 = r7.wrapping_add(32 as i32 as i64 as u64)
    ldxdw r9, [r10-0x680]                   
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r6, [r10-0x640]                   
    ldxdw r1, [r10-0x718]                   
    stxdw [r6+0x0], r1                      
    mov64 r1, r6                                    r1 = r6
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -424                                  r2 += -424   ///  r2 = r2.wrapping_add(-424 as i32 as i64 as u64)
    mov64 r3, 112                                   r3 = 112 as i32 as i64 as u64
    call function_9980                      
    mov64 r1, r6                                    r1 = r6
    add64 r1, 120                                   r1 += 120   ///  r1 = r1.wrapping_add(120 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -504                                  r2 += -504   ///  r2 = r2.wrapping_add(-504 as i32 as i64 as u64)
    mov64 r3, 80                                    r3 = 80 as i32 as i64 as u64
    call function_9980                      
    ldxdw r1, [r10-0x200]                   
    stxdw [r6+0xd0], r1                     
    ldxdw r1, [r10-0x208]                   
    stxdw [r6+0xc8], r1                     
    ldxdw r1, [r10-0x210]                   
    stxdw [r6+0xf0], r1                     
    ldxdw r1, [r10-0x218]                   
    stxdw [r6+0xe8], r1                     
    ldxdw r1, [r10-0x220]                   
    stxdw [r6+0xe0], r1                     
    ldxdw r1, [r10-0x228]                   
    stxdw [r6+0xd8], r1                     
    ldxdw r1, [r10-0x230]                   
    stxdw [r6+0x110], r1                    
    ldxdw r1, [r10-0x238]                   
    stxdw [r6+0x108], r1                    
    ldxdw r1, [r10-0x240]                   
    stxdw [r6+0x100], r1                    
    ldxdw r1, [r10-0x248]                   
    stxdw [r6+0xf8], r1                     
    ldxdw r1, [r10-0x268]                   
    stxdw [r6+0x118], r1                    
    ldxdw r1, [r10-0x260]                   
    stxdw [r6+0x120], r1                    
    ldxdw r1, [r10-0x258]                   
    stxdw [r6+0x128], r1                    
    ldxdw r1, [r10-0x250]                   
    stxdw [r6+0x130], r1                    
    ldxdw r1, [r10-0x288]                   
    stxdw [r6+0x138], r1                    
    ldxdw r1, [r10-0x280]                   
    stxdw [r6+0x140], r1                    
    ldxdw r1, [r10-0x278]                   
    stxdw [r6+0x148], r1                    
    ldxdw r1, [r10-0x270]                   
    stxdw [r6+0x150], r1                    
    ldxdw r1, [r10-0x290]                   
    stxdw [r6+0x170], r1                    
    ldxdw r1, [r10-0x298]                   
    stxdw [r6+0x168], r1                    
    ldxdw r1, [r10-0x2a0]                   
    stxdw [r6+0x160], r1                    
    ldxdw r1, [r10-0x2a8]                   
    stxdw [r6+0x158], r1                    
    mov64 r1, r6                                    r1 = r6
    add64 r1, 376                                   r1 += 376   ///  r1 = r1.wrapping_add(376 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1112                                 r2 += -1112   ///  r2 = r2.wrapping_add(-1112 as i32 as i64 as u64)
    mov64 r3, 432                                   r3 = 432 as i32 as i64 as u64
    call function_9980                      
    ldxdw r1, [r10-0x7c0]                   
    stxb [r6+0x32d], r1                     
    ldxdw r1, [r10-0x7b8]                   
    stxb [r6+0x32c], r1                     
    ldxdw r1, [r10-0x720]                   
    stxb [r6+0x329], r1                     
    ldxdw r1, [r10-0x710]                   
    stxb [r6+0x328], r1                     
    ldxdw r8, [r10-0x728]                   
    stxb [r6+0x32a], r8                     
    stdw [r6+0x330], 0                      
    sth [r6+0x32e], 1                       
    stb [r6+0x32b], 0                       
    ldxdw r1, [r10-0x470]                   
    stxdw [r6+0x338], r1                    
    ldxdw r1, [r10-0x468]                   
    stxdw [r6+0x340], r1                    
    ldxdw r1, [r10-0x460]                   
    stxdw [r6+0x348], r1                    
    stxb [r10-0xb1], r8                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -177                                  r1 += -177   ///  r1 = r1.wrapping_add(-177 as i32 as i64 as u64)
    stxdw [r10-0x80], r1                    
    ldxdw r1, [r10-0x688]                   
    stxdw [r10-0x90], r1                    
    stxdw [r10-0xa0], r9                    
    lddw r1, 0x10001680c --> b"vault\x04y\xd5[\xf21\xc0n\xeet\xc5n\xceh\x15\x07\xfd\xb1\xb2\xde\xa3\xf4\…        r1 load str located at 4295059468
    stxdw [r10-0xb0], r1                    
    stdw [r10-0x78], 1                      
    stdw [r10-0x88], 32                     
    stdw [r10-0x98], 32                     
    stdw [r10-0xa8], 5                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -216                                  r1 += -216   ///  r1 = r1.wrapping_add(-216 as i32 as i64 as u64)
    stxdw [r10-0xfe8], r1                   
    ldxdw r1, [r10-0x670]                   
    stxdw [r10-0xff0], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    stdw [r10-0xff8], 4                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1608                                 r1 += -1608   ///  r1 = r1.wrapping_add(-1608 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r7                                    r2 = r7
    lddw r3, 0x100016630 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r3 load str located at 4295058992
    mov64 r4, 165                                   r4 = 165 as i32 as i64 as u64
    call function_4844                      
    ldxw r1, [r10-0x644]                    
    ldxw r6, [r10-0x648]                    
    jne r6, 26, lbb_1644                            if r6 != (26 as i32 as i64 as u64) { pc += 82 }
    stxdw [r10-0x58], r9                    
    ldxdw r1, [r10-0x738]                   
    stxdw [r10-0x60], r1                    
    stxdw [r10-0x68], r7                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    stxdw [r10-0x28], r1                    
    stdw [r10-0x20], 4                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1616                                 r1 += -1616   ///  r1 = r1.wrapping_add(-1616 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -104                                  r2 += -104   ///  r2 = r2.wrapping_add(-104 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -40                                   r3 += -40   ///  r3 = r3.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_6608                      
    ldxw r1, [r10-0x64c]                    
    ldxw r6, [r10-0x650]                    
    jne r6, 26, lbb_1644                            if r6 != (26 as i32 as i64 as u64) { pc += 63 }
    ldxdw r2, [r10-0x670]                   
    mov64 r7, r2                                    r7 = r2
    add64 r7, 40                                    r7 += 40   ///  r7 = r7.wrapping_add(40 as i32 as i64 as u64)
    ldxdw r1, [r10-0x720]                   
    stxb [r10-0x69], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -105                                  r1 += -105   ///  r1 = r1.wrapping_add(-105 as i32 as i64 as u64)
    stxdw [r10-0x38], r1                    
    ldxdw r1, [r10-0x690]                   
    stxdw [r10-0x48], r1                    
    stxdw [r10-0x58], r9                    
    lddw r1, 0x10001680c --> b"vault\x04y\xd5[\xf21\xc0n\xeet\xc5n\xceh\x15\x07\xfd\xb1\xb2\xde\xa3\xf4\…        r1 load str located at 4295059468
    stxdw [r10-0x68], r1                    
    stdw [r10-0x30], 1                      
    stdw [r10-0x40], 32                     
    stdw [r10-0x50], 32                     
    stdw [r10-0x60], 5                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -216                                  r1 += -216   ///  r1 = r1.wrapping_add(-216 as i32 as i64 as u64)
    stxdw [r10-0xfe8], r1                   
    stxdw [r10-0xff0], r2                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    stdw [r10-0xff8], 4                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1624                                 r1 += -1624   ///  r1 = r1.wrapping_add(-1624 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r7                                    r2 = r7
    lddw r3, 0x100016630 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r3 load str located at 4295058992
    mov64 r4, 165                                   r4 = 165 as i32 as i64 as u64
    call function_4844                      
    ldxw r1, [r10-0x654]                    
    ldxw r6, [r10-0x658]                    
    jne r6, 26, lbb_1644                            if r6 != (26 as i32 as i64 as u64) { pc += 26 }
    stxdw [r10-0x18], r9                    
    ldxdw r1, [r10-0x740]                   
    stxdw [r10-0x20], r1                    
    stxdw [r10-0x28], r7                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x8], 4                       
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1632                                 r1 += -1632   ///  r1 = r1.wrapping_add(-1632 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -40                                   r2 += -40   ///  r2 = r2.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -16                                   r3 += -16   ///  r3 = r3.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_6608                      
    ldxw r1, [r10-0x65c]                    
    ldxw r6, [r10-0x660]                    
    jne r6, 26, lbb_1644                            if r6 != (26 as i32 as i64 as u64) { pc += 7 }
    ldxdw r2, [r10-0x680]                   
    ldxb r1, [r2+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r2+0x0], r1                       
    mov64 r6, 26                                    r6 = 26 as i32 as i64 as u64
    ldxdw r4, [r10-0x668]                   
    ja lbb_858                                      if true { pc += -786 }
lbb_1644:
    ldxdw r3, [r10-0x680]                   
    ldxb r2, [r3+0x0]                       
    or64 r2, 8                                      r2 |= 8   ///  r2 = r2.or(8)
    stxb [r3+0x0], r2                       
lbb_1648:
    ldxdw r4, [r10-0x668]                   
    ja lbb_858                                      if true { pc += -792 }

function_1650:
    mov64 r6, r5                                    r6 = r5
    stxdw [r10-0x58], r4                    
    mov64 r4, r1                                    r4 = r1
    mov64 r8, 10                                    r8 = 10 as i32 as i64 as u64
    jlt r3, 8, lbb_1671                             if r3 < (8 as i32 as i64 as u64) { pc += 16 }
    ldxdw r9, [r2+0x0]                      
    mov64 r7, r9                                    r7 = r9
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    lddw r1, 0x1000165b0 --> b"\x0d\x88\x0e\xee{\xa8\x04\x97'\x12\xaf\x88:h|n\xbbP\xd3\xe8T(+\x87\xac\xc…        r1 load str located at 4295058864
    stxdw [r10-0x50], r2                    
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    stxdw [r10-0x48], r4                    
    call function_9989                      
    ldxdw r2, [r10-0x50]                    
    ldxdw r4, [r10-0x48]                    
    mov64 r8, 25                                    r8 = 25 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_1674                             if r0 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_1671:
    stxw [r4+0x4], r5                       
    stxw [r4+0x0], r8                       
    exit                                    
lbb_1674:
    ldxb r1, [r9+0x1]                       
    jeq r1, 0, lbb_1671                             if r1 == (0 as i32 as i64 as u64) { pc += -5 }
    ldxdw r1, [r2+0x30]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x100016630 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295058992
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    ldxdw r1, [r10-0x50]                    
    ldxdw r4, [r10-0x48]                    
    mov64 r8, 6                                     r8 = 6 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_1671                             if r0 != (0 as i32 as i64 as u64) { pc += -17 }
    ldxdw r1, [r1+0x38]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x100016650 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r2 load str located at 4295059024
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    ldxdw r2, [r10-0x50]                    
    ldxdw r4, [r10-0x48]                    
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_1671                             if r0 != (0 as i32 as i64 as u64) { pc += -28 }
    jlt r6, 8, lbb_1705                             if r6 < (8 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, r6                                    r1 = r6
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    jeq r1, 8, lbb_1705                             if r1 == (8 as i32 as i64 as u64) { pc += 2 }
    jeq r6, 16, lbb_1722                            if r6 == (16 as i32 as i64 as u64) { pc += 18 }
    call function_141                       
lbb_1705:
    lddw r1, 0x100017668 --> b"\x00\x00\x00\x00\xb0f\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295063144
    call function_7172                      
    ldxdw r4, [r10-0x48]                    
    mov64 r8, 2                                     r8 = 2 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r1, r0                                    r1 = r0
    and64 r1, 3                                     r1 &= 3   ///  r1 = r1.and(3)
    jne r1, 1, lbb_1671                             if r1 != (1 as i32 as i64 as u64) { pc += -43 }
    ldxdw r1, [r0+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_1671                             if r2 == (0 as i32 as i64 as u64) { pc += -46 }
    ldxdw r1, [r0-0x1]                      
    callx r2                                
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0x48]                    
    ja lbb_1671                                     if true { pc += -51 }
lbb_1722:
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r6, [r2+0x0]                      
    ldxdw r1, [r6+0x50]                     
    mov64 r8, 3                                     r8 = 3 as i32 as i64 as u64
    jne r1, 856, lbb_1788                           if r1 != (856 as i32 as i64 as u64) { pc += 61 }
    mov64 r8, 22                                    r8 = 22 as i32 as i64 as u64
    ldxdw r1, [r6+0x28]                     
    lddw r3, 0xd8a6fe616d93320a                     r3 load str located at -2835299220979502582
    jne r1, r3, lbb_1788                            if r1 != r3 { pc += 56 }
    ldxdw r1, [r6+0x30]                     
    lddw r3, 0xb174a1c206e706b6                     r3 load str located at -5659720976986339658
    jne r1, r3, lbb_1788                            if r1 != r3 { pc += 52 }
    ldxdw r1, [r6+0x38]                     
    lddw r3, 0xadd40f4b82a5676b                     r3 load str located at -5921090793096517781
    jne r1, r3, lbb_1788                            if r1 != r3 { pc += 48 }
    ldxdw r1, [r6+0x40]                     
    lddw r3, 0x7e6598cdd46af8a9                     r3 load str located at 9107853831226194089
    jne r1, r3, lbb_1788                            if r1 != r3 { pc += 44 }
    ldxdw r1, [r10-0x58]                    
    ldxdw r3, [r1+0x0]                      
    stxdw [r10-0x68], r3                    
    ldxdw r9, [r1+0x8]                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    call function_7233                      
    ldxw r8, [r10-0x14]                     
    ldxw r1, [r10-0x18]                     
    ldxdw r2, [r10-0x20]                    
    jeq r2, 0, lbb_1780                             if r2 == (0 as i32 as i64 as u64) { pc += 25 }
    stxdw [r10-0x78], r9                    
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    or64 r8, r1                                     r8 |= r1   ///  r8 = r8.or(r1)
    jgt r8, 7, lbb_1764                             if r8 > (7 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, r8                                    r2 = r8
    lddw r3, 0x1000178f8 --> b"\x00\x00\x00\x00\xf1g\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x00\xbc\x00\x00…        r3 load str located at 4295063800
    call function_8840                      
lbb_1764:
    ldxb r1, [r10-0x8]                      
    stxdw [r10-0x58], r1                    
    ldxdw r1, [r10-0x10]                    
    stxdw [r10-0x60], r1                    
    mov64 r3, r2                                    r3 = r2
    ldxdw r1, [r2+0x0]                      
    lddw r2, 0xe643d503bdd93852                     r2 load str located at -1854404409499764654
    jeq r1, r2, lbb_1774                            if r1 == r2 { pc += 1 }
    ja lbb_1783                                     if true { pc += 9 }
lbb_1774:
    jgt r8, 855, lbb_1791                           if r8 > (855 as i32 as i64 as u64) { pc += 16 }
    mov64 r1, 856                                   r1 = 856 as i32 as i64 as u64
    mov64 r2, r8                                    r2 = r8
    lddw r3, 0x100017910 --> b"\x00\x00\x00\x00\xf1g\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x00\xc1\x00\x00…        r3 load str located at 4295063824
    call function_8840                      
lbb_1780:
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    or64 r8, r1                                     r8 |= r1   ///  r8 = r8.or(r1)
    ja lbb_1787                                     if true { pc += 4 }
lbb_1783:
    ldxdw r1, [r10-0x60]                    
    ldxdw r2, [r10-0x58]                    
    call function_158                       
    mov64 r8, 3                                     r8 = 3 as i32 as i64 as u64
lbb_1787:
    ldxdw r4, [r10-0x48]                    
lbb_1788:
    mov64 r5, r8                                    r5 = r8
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    ja lbb_1671                                     if true { pc += -120 }
lbb_1791:
    mov64 r1, r3                                    r1 = r3
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x70], r1                    
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jeq r1, 0, lbb_1801                             if r1 == (0 as i32 as i64 as u64) { pc += 5 }
    lddw r1, 0x100016722 --> b"from_bytes"          r1 load str located at 4295059234
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_290                       
lbb_1801:
    ldxdw r1, [r10-0x50]                    
    mov64 r5, r1                                    r5 = r1
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    mov64 r4, r1                                    r4 = r1
    add64 r4, 32                                    r4 += 32   ///  r4 = r4.wrapping_add(32 as i32 as i64 as u64)
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    ldxdw r2, [r10-0x70]                    
    mov64 r3, r6                                    r3 = r6
    stxdw [r10-0x80], r4                    
    stxdw [r10-0x88], r5                    
    call function_4638                      
    ldxw r8, [r10-0x28]                     
    jeq r8, 26, lbb_1817                            if r8 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1870                                     if true { pc += 53 }
lbb_1817:
    ldxdw r9, [r10-0x50]                    
    mov64 r5, r9                                    r5 = r9
    add64 r5, 24                                    r5 += 24   ///  r5 = r5.wrapping_add(24 as i32 as i64 as u64)
    add64 r9, 16                                    r9 += 16   ///  r9 = r9.wrapping_add(16 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    ldxdw r2, [r10-0x70]                    
    mov64 r3, r7                                    r3 = r7
    mov64 r4, r9                                    r4 = r9
    mov64 r7, r5                                    r7 = r5
    call function_4682                      
    ldxw r8, [r10-0x30]                     
    jne r8, 26, lbb_1870                            if r8 != (26 as i32 as i64 as u64) { pc += 40 }
    ldxdw r1, [r10-0x68]                    
    stxdw [r10-0x8], r1                     
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0x10], r1                    
    ldxdw r1, [r10-0x80]                    
    stxdw [r10-0x18], r1                    
    stxdw [r10-0x20], r9                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -56                                   r1 += -56   ///  r1 = r1.wrapping_add(-56 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -32                                   r2 += -32   ///  r2 = r2.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_6816                      
    ldxw r6, [r10-0x34]                     
    ldxw r8, [r10-0x38]                     
    jne r8, 26, lbb_1870                            if r8 != (26 as i32 as i64 as u64) { pc += 23 }
    ldxdw r1, [r10-0x78]                    
    stxdw [r10-0x8], r1                     
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0x10], r1                    
    ldxdw r1, [r10-0x88]                    
    stxdw [r10-0x18], r1                    
    stxdw [r10-0x20], r7                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -32                                   r2 += -32   ///  r2 = r2.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_6816                      
    ldxw r6, [r10-0x3c]                     
    ldxw r8, [r10-0x40]                     
    jne r8, 26, lbb_1870                            if r8 != (26 as i32 as i64 as u64) { pc += 6 }
    ldxdw r1, [r10-0x60]                    
    ldxdw r2, [r10-0x58]                    
    call function_158                       
    mov64 r8, 26                                    r8 = 26 as i32 as i64 as u64
    ldxdw r4, [r10-0x48]                    
    ja lbb_1671                                     if true { pc += -199 }
lbb_1870:
    ldxdw r1, [r10-0x60]                    
    ldxdw r2, [r10-0x58]                    
    call function_158                       
    ldxdw r4, [r10-0x48]                    
    mov64 r5, r6                                    r5 = r6
    ja lbb_1671                                     if true { pc += -205 }

function_1876:
    mov64 r6, r1                                    r6 = r1
    mov64 r7, 10                                    r7 = 10 as i32 as i64 as u64
    jlt r3, 7, lbb_1898                             if r3 < (7 as i32 as i64 as u64) { pc += 19 }
    jlt r5, 8, lbb_1885                             if r5 < (8 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, r5                                    r1 = r5
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    jeq r1, 8, lbb_1885                             if r1 == (8 as i32 as i64 as u64) { pc += 2 }
    jeq r5, 16, lbb_1901                            if r5 == (16 as i32 as i64 as u64) { pc += 17 }
    call function_141                       
lbb_1885:
    lddw r1, 0x100017668 --> b"\x00\x00\x00\x00\xb0f\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295063144
    call function_7172                      
    mov64 r7, 2                                     r7 = 2 as i32 as i64 as u64
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, r0                                    r1 = r0
    and64 r1, 3                                     r1 &= 3   ///  r1 = r1.and(3)
    jne r1, 1, lbb_1898                             if r1 != (1 as i32 as i64 as u64) { pc += 5 }
    ldxdw r1, [r0+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_1898                             if r2 == (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r0-0x1]                      
    callx r2                                
lbb_1898:
    stxw [r6+0x4], r8                       
    stxw [r6+0x0], r7                       
    exit                                    
lbb_1901:
    ldxdw r1, [r4+0x0]                      
    stxdw [r10-0xd0], r1                    
    ldxdw r1, [r4+0x8]                      
    stxdw [r10-0xd8], r1                    
    ldxdw r9, [r2+0x0]                      
    mov64 r3, r9                                    r3 = r9
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    lddw r1, 0x1000165b0 --> b"\x0d\x88\x0e\xee{\xa8\x04\x97'\x12\xaf\x88:h|n\xbbP\xd3\xe8T(+\x87\xac\xc…        r1 load str located at 4295058864
    stxdw [r10-0xc0], r2                    
    stxdw [r10-0xc8], r3                    
    mov64 r2, r3                                    r2 = r3
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    ldxdw r1, [r10-0xc0]                    
    mov64 r7, 25                                    r7 = 25 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_1898                             if r0 != (0 as i32 as i64 as u64) { pc += -22 }
    ldxb r2, [r9+0x1]                       
    jeq r2, 0, lbb_1898                             if r2 == (0 as i32 as i64 as u64) { pc += -24 }
    ldxdw r1, [r1+0x30]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x100016630 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295058992
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    ldxdw r1, [r10-0xc0]                    
    mov64 r7, 6                                     r7 = 6 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_1898                             if r0 != (0 as i32 as i64 as u64) { pc += -35 }
    mov64 r8, r1                                    r8 = r1
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r9, [r8+0x0]                      
    ldxdw r1, [r9+0x50]                     
    mov64 r7, 3                                     r7 = 3 as i32 as i64 as u64
    jne r1, 856, lbb_1995                           if r1 != (856 as i32 as i64 as u64) { pc += 56 }
    mov64 r7, 22                                    r7 = 22 as i32 as i64 as u64
    ldxdw r1, [r9+0x28]                     
    lddw r2, 0xd8a6fe616d93320a                     r2 load str located at -2835299220979502582
    jne r1, r2, lbb_1995                            if r1 != r2 { pc += 51 }
    ldxdw r1, [r9+0x30]                     
    lddw r2, 0xb174a1c206e706b6                     r2 load str located at -5659720976986339658
    jne r1, r2, lbb_1995                            if r1 != r2 { pc += 47 }
    ldxdw r1, [r9+0x38]                     
    lddw r2, 0xadd40f4b82a5676b                     r2 load str located at -5921090793096517781
    jne r1, r2, lbb_1995                            if r1 != r2 { pc += 43 }
    ldxdw r1, [r9+0x40]                     
    lddw r2, 0x7e6598cdd46af8a9                     r2 load str located at 9107853831226194089
    jne r1, r2, lbb_1995                            if r1 != r2 { pc += 39 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    call function_7233                      
    ldxw r7, [r10-0x74]                     
    ldxw r1, [r10-0x78]                     
    ldxdw r4, [r10-0x80]                    
    jeq r4, 0, lbb_1989                             if r4 == (0 as i32 as i64 as u64) { pc += 25 }
    stxdw [r10-0xe8], r9                    
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    or64 r7, r1                                     r7 |= r1   ///  r7 = r7.or(r1)
    jgt r7, 7, lbb_1973                             if r7 > (7 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, r7                                    r2 = r7
    lddw r3, 0x1000178f8 --> b"\x00\x00\x00\x00\xf1g\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x00\xbc\x00\x00…        r3 load str located at 4295063800
    call function_8840                      
lbb_1973:
    ldxb r2, [r10-0x68]                     
    ldxdw r9, [r10-0x70]                    
    ldxdw r1, [r4+0x0]                      
    lddw r3, 0xe643d503bdd93852                     r3 load str located at -1854404409499764654
    jeq r1, r3, lbb_1980                            if r1 == r3 { pc += 1 }
    ja lbb_1992                                     if true { pc += 12 }
lbb_1980:
    stxdw [r10-0xf0], r2                    
    stxdw [r10-0x108], r8                   
    stxdw [r10-0xe0], r9                    
    jgt r7, 855, lbb_1998                           if r7 > (855 as i32 as i64 as u64) { pc += 14 }
    mov64 r1, 856                                   r1 = 856 as i32 as i64 as u64
    mov64 r2, r7                                    r2 = r7
    lddw r3, 0x100017910 --> b"\x00\x00\x00\x00\xf1g\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x00\xc1\x00\x00…        r3 load str located at 4295063824
    call function_8840                      
lbb_1989:
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    or64 r7, r1                                     r7 |= r1   ///  r7 = r7.or(r1)
    ja lbb_1995                                     if true { pc += 3 }
lbb_1992:
    mov64 r1, r9                                    r1 = r9
    call function_158                       
    mov64 r7, 3                                     r7 = 3 as i32 as i64 as u64
lbb_1995:
    mov64 r8, r7                                    r8 = r7
    rsh64 r8, 32                                    r8 >>= 32   ///  r8 = r8.wrapping_shr(32)
    ja lbb_1898                                     if true { pc += -100 }
lbb_1998:
    mov64 r1, r4                                    r1 = r4
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x100], r1                   
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jeq r1, 0, lbb_2008                             if r1 == (0 as i32 as i64 as u64) { pc += 5 }
    lddw r1, 0x100016722 --> b"from_bytes"          r1 load str located at 4295059234
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_290                       
lbb_2008:
    ldxdw r1, [r10-0xc0]                    
    add64 r1, 32                                    r1 += 32   ///  r1 = r1.wrapping_add(32 as i32 as i64 as u64)
    stxdw [r10-0xf8], r1                    
    ldxdw r1, [r1+0x0]                      
    mov64 r2, r4                                    r2 = r4
    add64 r2, 320                                   r2 += 320   ///  r2 = r2.wrapping_add(320 as i32 as i64 as u64)
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    mov64 r9, r4                                    r9 = r4
    call function_9989                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r7, 3                                     r7 = 3 as i32 as i64 as u64
    jne r0, 0, lbb_2143                             if r0 != (0 as i32 as i64 as u64) { pc += 121 }
    ldxdw r4, [r10-0xe8]                    
    add64 r4, 8                                     r4 += 8   ///  r4 = r4.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, r9                                    r3 = r9
    add64 r3, 256                                   r3 += 256   ///  r3 = r3.wrapping_add(256 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -152                                  r1 += -152   ///  r1 = r1.wrapping_add(-152 as i32 as i64 as u64)
    ldxdw r2, [r10-0xf8]                    
    stxdw [r10-0x110], r3                   
    stxdw [r10-0xe8], r4                    
    call function_4705                      
    ldxw r7, [r10-0x98]                     
    jne r7, 26, lbb_2143                            if r7 != (26 as i32 as i64 as u64) { pc += 109 }
    ldxdw r1, [r10-0xc0]                    
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x118], r1                   
    ldxdw r1, [r1+0x0]                      
    mov64 r2, r9                                    r2 = r9
    add64 r2, 352                                   r2 += 352   ///  r2 = r2.wrapping_add(352 as i32 as i64 as u64)
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r7, 3                                     r7 = 3 as i32 as i64 as u64
    jne r0, 0, lbb_2143                             if r0 != (0 as i32 as i64 as u64) { pc += 96 }
    mov64 r3, r9                                    r3 = r9
    add64 r3, 288                                   r3 += 288   ///  r3 = r3.wrapping_add(288 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -160                                  r1 += -160   ///  r1 = r1.wrapping_add(-160 as i32 as i64 as u64)
    ldxdw r2, [r10-0x118]                   
    stxdw [r10-0x120], r3                   
    ldxdw r4, [r10-0xe8]                    
    call function_4705                      
    ldxw r7, [r10-0xa0]                     
    jne r7, 26, lbb_2143                            if r7 != (26 as i32 as i64 as u64) { pc += 86 }
    ldxdw r4, [r10-0xc0]                    
    mov64 r5, r4                                    r5 = r4
    add64 r5, 24                                    r5 += 24   ///  r5 = r5.wrapping_add(24 as i32 as i64 as u64)
    add64 r4, 16                                    r4 += 16   ///  r4 = r4.wrapping_add(16 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -168                                  r1 += -168   ///  r1 = r1.wrapping_add(-168 as i32 as i64 as u64)
    ldxdw r2, [r10-0x100]                   
    ldxdw r3, [r10-0xc8]                    
    stxdw [r10-0xc0], r4                    
    stxdw [r10-0xc8], r5                    
    call function_4682                      
    ldxw r7, [r10-0xa8]                     
    jne r7, 26, lbb_2143                            if r7 != (26 as i32 as i64 as u64) { pc += 73 }
    ldxb r1, [r9+0x330]                     
    stxb [r10-0x89], r1                     
    ldxdw r1, [r9+0x8]                      
    stxdw [r10-0x88], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -137                                  r1 += -137   ///  r1 = r1.wrapping_add(-137 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -136                                  r1 += -136   ///  r1 = r1.wrapping_add(-136 as i32 as i64 as u64)
    stxdw [r10-0x50], r1                    
    ldxdw r1, [r10-0x120]                   
    stxdw [r10-0x60], r1                    
    ldxdw r1, [r10-0x110]                   
    stxdw [r10-0x70], r1                    
    lddw r1, 0x1000167eb --> b"marketprogram/src/state/market.rsvault\x04y\xd5[\xf21\xc0n\xeet\xc5n"        r1 load str located at 4295059435
    stxdw [r10-0x80], r1                    
    stdw [r10-0x38], 1                      
    stdw [r10-0x48], 8                      
    stdw [r10-0x58], 32                     
    stdw [r10-0x68], 32                     
    stdw [r10-0x78], 6                      
    ldxdw r1, [r10-0xd0]                    
    stxdw [r10-0x18], r1                    
    ldxdw r1, [r10-0x108]                   
    stxdw [r10-0x20], r1                    
    ldxdw r1, [r10-0xc0]                    
    stxdw [r10-0x28], r1                    
    ldxdw r1, [r10-0xf8]                    
    stxdw [r10-0x30], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x8], 5                       
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -48                                   r2 += -48   ///  r2 = r2.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -16                                   r3 += -16   ///  r3 = r3.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_6816                      
    ldxw r8, [r10-0xac]                     
    ldxw r7, [r10-0xb0]                     
    jne r7, 26, lbb_2143                            if r7 != (26 as i32 as i64 as u64) { pc += 28 }
    ldxdw r1, [r10-0xd8]                    
    stxdw [r10-0x18], r1                    
    ldxdw r1, [r10-0x108]                   
    stxdw [r10-0x20], r1                    
    ldxdw r1, [r10-0xc8]                    
    stxdw [r10-0x28], r1                    
    ldxdw r1, [r10-0x118]                   
    stxdw [r10-0x30], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x8], 5                       
    mov64 r1, r10                                   r1 = r10
    add64 r1, -184                                  r1 += -184   ///  r1 = r1.wrapping_add(-184 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -48                                   r2 += -48   ///  r2 = r2.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -16                                   r3 += -16   ///  r3 = r3.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_6816                      
    ldxw r8, [r10-0xb4]                     
    ldxw r7, [r10-0xb8]                     
    jne r7, 26, lbb_2143                            if r7 != (26 as i32 as i64 as u64) { pc += 5 }
    ldxdw r1, [r10-0xe0]                    
    ldxdw r2, [r10-0xf0]                    
    call function_158                       
    mov64 r7, 26                                    r7 = 26 as i32 as i64 as u64
    ja lbb_1898                                     if true { pc += -245 }
lbb_2143:
    ldxdw r1, [r10-0xf0]                    
    jgt r1, 7, lbb_2158                             if r1 > (7 as i32 as i64 as u64) { pc += 13 }
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lsh64 r2, r1                                    r2 <<= r1   ///  r2 = r2.wrapping_shl(r1 as u32)
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    ldxdw r1, [r10-0xe0]                    
    ldxb r1, [r1+0x0]                       
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, r1                                    r2 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jne r2, r1, lbb_2154                            if r2 != r1 { pc += 0 }
lbb_2154:
    jne r2, r1, lbb_2161                            if r2 != r1 { pc += 6 }
    ldxdw r2, [r10-0xe0]                    
    stxb [r2+0x0], r1                       
    ja lbb_1898                                     if true { pc += -260 }
lbb_2158:
    lddw r1, 0x100017690 --> b"\x00\x00\x00\x00\xe8f\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xe1\x02\x00…        r1 load str located at 4295063184
    call function_9597                      
lbb_2161:
    lddw r1, 0x1000176a8 --> b"\x00\x00\x00\x00\xe8f\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xe1\x02\x00…        r1 load str located at 4295063208
    call function_9564                      

function_2164:
    mov64 r6, r1                                    r6 = r1
    jeq r3, 0, lbb_2213                             if r3 == (0 as i32 as i64 as u64) { pc += 47 }
    ldxb r1, [r2+0x0]                       
    stxb [r10-0x59], r1                     
    jlt r1, 2, lbb_2190                             if r1 < (2 as i32 as i64 as u64) { pc += 21 }
lbb_2169:
    lddw r1, 0x100017680 --> b"\x00\x00\x00\x00\xcbf\x01\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295063168
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100012f90 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295045008
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    call function_7524                      
    call function_141                       
lbb_2190:
    jeq r3, 1, lbb_2213                             if r3 == (1 as i32 as i64 as u64) { pc += 22 }
    mov64 r4, r3                                    r4 = r3
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    jlt r4, 8, lbb_2213                             if r4 < (8 as i32 as i64 as u64) { pc += 19 }
    mov64 r4, r3                                    r4 = r3
    add64 r4, -10                                   r4 += -10   ///  r4 = r4.wrapping_add(-10 as i32 as i64 as u64)
    jlt r4, 8, lbb_2213                             if r4 < (8 as i32 as i64 as u64) { pc += 16 }
    ldxb r5, [r2+0x1]                       
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r7, [r2+0xa]                      
    ldxdw r0, [r2+0x2]                      
    jeq r3, 18, lbb_2203                            if r3 == (18 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2227                                     if true { pc += 24 }
lbb_2203:
    mov64 r2, r0                                    r2 = r0
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    stxb [r6+0x11], r1                      
    stxb [r6+0x10], r5                      
    stxdw [r6+0x8], r7                      
    stxw [r6+0x4], r2                       
    stxw [r6+0x0], r0                       
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    stxb [r6+0x18], r4                      
    ja lbb_2226                                     if true { pc += 13 }
lbb_2213:
    lddw r1, 0x100017668 --> b"\x00\x00\x00\x00\xb0f\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295063144
    call function_7172                      
    mov64 r1, r0                                    r1 = r0
    and64 r1, 3                                     r1 &= 3   ///  r1 = r1.and(3)
    jne r1, 1, lbb_2224                             if r1 != (1 as i32 as i64 as u64) { pc += 5 }
    ldxdw r1, [r0+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_2224                             if r2 == (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r0-0x1]                      
    callx r2                                
lbb_2224:
    stb [r6+0x11], 2                        
    stw [r6+0x0], 2                         
lbb_2226:
    exit                                    
lbb_2227:
    ldxb r4, [r2+0x12]                      
    stxb [r10-0x59], r4                     
    jlt r4, 2, lbb_2203                             if r4 < (2 as i32 as i64 as u64) { pc += -27 }
    ja lbb_2169                                     if true { pc += -62 }

function_2231:
    stxdw [r10-0x168], r5                   
    mov64 r9, r4                                    r9 = r4
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r8, 10                                    r8 = 10 as i32 as i64 as u64
    jlt r3, 9, lbb_2247                             if r3 < (9 as i32 as i64 as u64) { pc += 10 }
    ldxdw r1, [r7+0x40]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x100016630 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295058992
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    mov64 r8, 6                                     r8 = 6 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_2250                             if r0 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_2247:
    stxw [r6+0x4], r1                       
    stxw [r6+0x0], r8                       
    exit                                    
lbb_2250:
    mov64 r1, r7                                    r1 = r7
    add64 r1, 56                                    r1 += 56   ///  r1 = r1.wrapping_add(56 as i32 as i64 as u64)
    stxdw [r10-0x170], r1                   
    ldxdw r1, [r1+0x0]                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x1000165d0 --> b"\x06\xa7\xd5\x17\x18{\xd1f5\xda\xd4\x04U\xfd\xc2\xc0\xc1$\xc6\x8f!Vu\xa5\…        r2 load str located at 4295058896
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2247                             if r0 != (0 as i32 as i64 as u64) { pc += -15 }
    mov64 r8, 25                                    r8 = 25 as i32 as i64 as u64
    ldxdw r4, [r7+0x0]                      
    ldxb r2, [r4+0x1]                       
    jeq r2, 0, lbb_2247                             if r2 == (0 as i32 as i64 as u64) { pc += -19 }
    mov64 r5, r7                                    r5 = r7
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r0, [r5+0x0]                      
    ldxdw r2, [r0+0x50]                     
    mov64 r8, 3                                     r8 = 3 as i32 as i64 as u64
    jne r2, 856, lbb_2247                           if r2 != (856 as i32 as i64 as u64) { pc += -26 }
    mov64 r8, 22                                    r8 = 22 as i32 as i64 as u64
    ldxdw r2, [r0+0x28]                     
    lddw r3, 0xd8a6fe616d93320a                     r3 load str located at -2835299220979502582
    jne r2, r3, lbb_2247                            if r2 != r3 { pc += -31 }
    ldxdw r2, [r0+0x30]                     
    lddw r3, 0xb174a1c206e706b6                     r3 load str located at -5659720976986339658
    jne r2, r3, lbb_2247                            if r2 != r3 { pc += -35 }
    ldxdw r2, [r0+0x38]                     
    lddw r3, 0xadd40f4b82a5676b                     r3 load str located at -5921090793096517781
    jne r2, r3, lbb_2247                            if r2 != r3 { pc += -39 }
    ldxdw r2, [r0+0x40]                     
    lddw r3, 0x7e6598cdd46af8a9                     r3 load str located at 9107853831226194089
    jne r2, r3, lbb_2247                            if r2 != r3 { pc += -43 }
    mov64 r8, 11                                    r8 = 11 as i32 as i64 as u64
    ldxb r2, [r0+0x0]                       
    mov64 r3, r2                                    r3 = r2
    and64 r3, 15                                    r3 &= 15   ///  r3 = r3.and(15)
    jne r3, 15, lbb_2247                            if r3 != (15 as i32 as i64 as u64) { pc += -48 }
    stxdw [r10-0x180], r4                   
    mov64 r3, r2                                    r3 = r2
    and64 r3, 247                                   r3 &= 247   ///  r3 = r3.and(247)
    stxb [r0+0x0], r3                       
    ldxdw r3, [r0+0x58]                     
    lddw r4, 0xe643d503bdd93852                     r4 load str located at -1854404409499764654
    jne r3, r4, lbb_2370                            if r3 != r4 { pc += 67 }
    stxdw [r10-0x198], r5                   
    mov64 r8, r7                                    r8 = r7
    add64 r8, 32                                    r8 += 32   ///  r8 = r8.wrapping_add(32 as i32 as i64 as u64)
    mov64 r1, r0                                    r1 = r0
    add64 r1, 96                                    r1 += 96   ///  r1 = r1.wrapping_add(96 as i32 as i64 as u64)
    stxdw [r10-0x190], r1                   
    mov64 r2, 848                                   r2 = 848 as i32 as i64 as u64
    stxdw [r10-0x178], r0                   
    call function_273                       
    stxdw [r10-0x188], r8                   
    ldxdw r1, [r8+0x0]                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r2, [r10-0x178]                   
    add64 r2, 408                                   r2 += 408   ///  r2 = r2.wrapping_add(408 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r8, 3                                     r8 = 3 as i32 as i64 as u64
    jeq r0, 0, lbb_2324                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2365                                     if true { pc += 41 }
lbb_2324:
    ldxdw r1, [r10-0x178]                   
    mov64 r4, r1                                    r4 = r1
    add64 r4, 8                                     r4 += 8   ///  r4 = r4.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, r1                                    r3 = r1
    add64 r3, 344                                   r3 += 344   ///  r3 = r3.wrapping_add(344 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -320                                  r1 += -320   ///  r1 = r1.wrapping_add(-320 as i32 as i64 as u64)
    ldxdw r2, [r10-0x188]                   
    stxdw [r10-0x1a8], r3                   
    stxdw [r10-0x1a0], r4                   
    call function_4705                      
    ldxw r8, [r10-0x140]                    
    jne r8, 26, lbb_2365                            if r8 != (26 as i32 as i64 as u64) { pc += 28 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x1b0], r1                   
    ldxdw r1, [r1+0x0]                      
    ldxdw r2, [r10-0x178]                   
    add64 r2, 440                                   r2 += 440   ///  r2 = r2.wrapping_add(440 as i32 as i64 as u64)
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r8, 3                                     r8 = 3 as i32 as i64 as u64
    jne r0, 0, lbb_2365                             if r0 != (0 as i32 as i64 as u64) { pc += 15 }
    ldxdw r3, [r10-0x178]                   
    add64 r3, 376                                   r3 += 376   ///  r3 = r3.wrapping_add(376 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -328                                  r1 += -328   ///  r1 = r1.wrapping_add(-328 as i32 as i64 as u64)
    ldxdw r2, [r10-0x1b0]                   
    stxdw [r10-0x1b8], r3                   
    ldxdw r4, [r10-0x1a0]                   
    call function_4705                      
    ldxw r8, [r10-0x148]                    
    jne r8, 26, lbb_2365                            if r8 != (26 as i32 as i64 as u64) { pc += 5 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, 14                                    r1 = 14 as i32 as i64 as u64
    ldxdw r2, [r10-0x178]                   
    ldxb r2, [r2+0x38e]                     
    jeq r2, 0, lbb_2373                             if r2 == (0 as i32 as i64 as u64) { pc += 8 }
lbb_2365:
    ldxdw r3, [r10-0x178]                   
    ldxb r2, [r3+0x0]                       
    or64 r2, 8                                      r2 |= 8   ///  r2 = r2.or(8)
    stxb [r3+0x0], r2                       
    ja lbb_2247                                     if true { pc += -123 }
lbb_2370:
    stxb [r0+0x0], r2                       
    mov64 r8, 3                                     r8 = 3 as i32 as i64 as u64
    ja lbb_2247                                     if true { pc += -126 }
lbb_2373:
    ldxdw r1, [r10-0x188]                   
    ldxdw r1, [r10-0x178]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -232                                  r1 += -232   ///  r1 = r1.wrapping_add(-232 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    ldxdw r3, [r10-0x168]                   
    call function_2164                      
    ldxb r1, [r10-0xd7]                     
    stxdw [r10-0x168], r1                   
    jne r1, 2, lbb_2387                             if r1 != (2 as i32 as i64 as u64) { pc += 4 }
    ldxw r1, [r10-0xe4]                     
    ldxw r8, [r10-0xe8]                     
    ldxdw r2, [r10-0x178]                   
    ja lbb_2365                                     if true { pc += -22 }
lbb_2387:
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    ldxdw r2, [r10-0xe8]                    
    ldxdw r3, [r10-0x178]                   
    ldxdw r3, [r10-0x188]                   
    stxdw [r10-0x1c0], r2                   
    jeq r2, 0, lbb_2365                             if r2 == (0 as i32 as i64 as u64) { pc += -28 }
    mov64 r2, r7                                    r2 = r7
    add64 r2, 48                                    r2 += 48   ///  r2 = r2.wrapping_add(48 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    add64 r1, 24                                    r1 += 24   ///  r1 = r1.wrapping_add(24 as i32 as i64 as u64)
    stxdw [r10-0x1a0], r1                   
    mov64 r8, r7                                    r8 = r7
    add64 r8, 16                                    r8 += 16   ///  r8 = r8.wrapping_add(16 as i32 as i64 as u64)
    ldxb r1, [r10-0xd0]                     
    stxdw [r10-0x1d0], r1                   
    ldxdw r1, [r10-0xe0]                    
    stxdw [r10-0x1c8], r1                   
    ldxdw r9, [r10-0x180]                   
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -232                                  r1 += -232   ///  r1 = r1.wrapping_add(-232 as i32 as i64 as u64)
    ldxdw r3, [r10-0x170]                   
    mov64 r4, r9                                    r4 = r9
    call function_4794                      
    ldxw r1, [r10-0xe4]                     
    stxdw [r10-0x180], r1                   
    ldxw r1, [r10-0xe8]                     
    stxdw [r10-0x170], r1                   
    ldxb r1, [r10-0xe0]                     
    stxdw [r10-0x1d8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -336                                  r1 += -336   ///  r1 = r1.wrapping_add(-336 as i32 as i64 as u64)
    ldxdw r2, [r10-0x190]                   
    mov64 r3, r9                                    r3 = r9
    stxdw [r10-0x1e0], r8                   
    mov64 r4, r8                                    r4 = r8
    ldxdw r5, [r10-0x1a0]                   
    call function_4682                      
    ldxw r8, [r10-0x150]                    
    jne r8, 26, lbb_2365                            if r8 != (26 as i32 as i64 as u64) { pc += -62 }
    ldxdw r2, [r10-0x178]                   
    ldxb r1, [r2+0x388]                     
    stxb [r10-0x131], r1                    
    ldxdw r1, [r2+0x60]                     
    stxdw [r10-0x130], r1                   
    ldxdw r2, [r10-0x1a8]                   
    ldxdw r1, [r2+0x18]                     
    stxdw [r10-0x110], r1                   
    ldxdw r1, [r2+0x10]                     
    stxdw [r10-0x118], r1                   
    ldxdw r1, [r2+0x8]                      
    stxdw [r10-0x120], r1                   
    ldxdw r1, [r2+0x0]                      
    stxdw [r10-0x128], r1                   
    ldxdw r2, [r10-0x1b8]                   
    ldxdw r1, [r2+0x18]                     
    stxdw [r10-0xf0], r1                    
    ldxdw r1, [r2+0x10]                     
    stxdw [r10-0xf8], r1                    
    ldxdw r1, [r2+0x8]                      
    stxdw [r10-0x100], r1                   
    ldxdw r1, [r2+0x0]                      
    stxdw [r10-0x108], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -232                                  r1 += -232   ///  r1 = r1.wrapping_add(-232 as i32 as i64 as u64)
    ldxdw r2, [r10-0x188]                   
    call function_103                       
    ldxw r1, [r10-0xdc]                     
    ldxw r8, [r10-0xe0]                     
    ldxdw r3, [r10-0xe8]                    
    jeq r3, 0, lbb_2501                             if r3 == (0 as i32 as i64 as u64) { pc += 43 }
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    or64 r1, r8                                     r1 |= r8   ///  r1 = r1.or(r8)
    ldxdw r2, [r10-0xd8]                    
    ldxdw r9, [r3+0x40]                     
    call function_158                       
    mov64 r1, r10                                   r1 = r10
    add64 r1, -232                                  r1 += -232   ///  r1 = r1.wrapping_add(-232 as i32 as i64 as u64)
    call function_7253                      
    ldxw r1, [r10-0xe0]                     
    ldxw r8, [r10-0xe4]                     
    ldxw r2, [r10-0xe8]                     
    jne r2, 0, lbb_2501                             if r2 != (0 as i32 as i64 as u64) { pc += 31 }
    ldxdw r1, [r10-0x170]                   
    jne r1, 26, lbb_2499                            if r1 != (26 as i32 as i64 as u64) { pc += 27 }
    ldxdw r2, [r10-0x1d0]                   
    ldxdw r1, [r10-0x1d8]                   
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    ldxdw r1, [r10-0xe0]                    
    ldxdw r3, [r10-0x180]                   
    stxdw [r10-0xfe8], r3                   
    stxdw [r10-0xff0], r9                   
    stxdw [r10-0x1000], r1                  
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxdw [r10-0xfe0], r2                   
    ldxdw r8, [r10-0x168]                   
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    stxdw [r10-0xff8], r8                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -232                                  r1 += -232   ///  r1 = r1.wrapping_add(-232 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0x190]                   
    ldxdw r3, [r10-0x1c0]                   
    ldxdw r4, [r10-0x1c8]                   
    call function_3867                      
    ldxw r1, [r10-0xe8]                     
    jne r1, 0, lbb_2495                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2503                                     if true { pc += 8 }
lbb_2495:
    ldxw r1, [r10-0xe0]                     
    ldxw r8, [r10-0xe4]                     
    ldxdw r2, [r10-0x178]                   
    ja lbb_2365                                     if true { pc += -134 }
lbb_2499:
    ldxdw r1, [r10-0x180]                   
    ldxdw r8, [r10-0x170]                   
lbb_2501:
    ldxdw r2, [r10-0x178]                   
    ja lbb_2365                                     if true { pc += -138 }
lbb_2503:
    ldxdw r1, [r10-0xe0]                    
    stxdw [r10-0x170], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -216                                  r2 += -216   ///  r2 = r2.wrapping_add(-216 as i32 as i64 as u64)
    mov64 r9, r1                                    r9 = r1
    mov64 r3, 80                                    r3 = 80 as i32 as i64 as u64
    call function_9980                      
    mov64 r1, 304                                   r1 = 304 as i32 as i64 as u64
    jne r8, 0, lbb_2515                             if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 296                                   r1 = 296 as i32 as i64 as u64
lbb_2515:
    ldxdw r2, [r10-0x178]                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxdw r1, [r10-0x88]                    
    stxdw [r2+0x0], r1                      
    mov64 r2, 216                                   r2 = 216 as i32 as i64 as u64
    jne r8, 0, lbb_2522                             if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 136                                   r2 = 136 as i32 as i64 as u64
lbb_2522:
    ldxdw r8, [r10-0x178]                   
    mov64 r1, r8                                    r1 = r8
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 80                                    r3 = 80 as i32 as i64 as u64
    call function_9980                      
    ldxb r1, [r8+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r8+0x0], r1                       
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, 15                                    r1 = 15 as i32 as i64 as u64
    ldxdw r2, [r10-0x1c8]                   
    ldxdw r3, [r10-0x170]                   
    jlt r3, r2, lbb_2247                            if r3 < r2 { pc += -289 }
    ldxdw r2, [r10-0x168]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    ldxdw r1, [r10-0x1a0]                   
    jne r2, 0, lbb_2541                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    ldxdw r1, [r10-0x1e0]                   
lbb_2541:
    ldxdw r3, [r10-0x1b0]                   
    jne r2, 0, lbb_2544                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    ldxdw r3, [r10-0x188]                   
lbb_2544:
    ldxdw r2, [r10-0x1c0]                   
    stxdw [r10-0xd0], r2                    
    stxdw [r10-0xd8], r7                    
    stxdw [r10-0xe0], r3                    
    stxdw [r10-0xe8], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -344                                  r1 += -344   ///  r1 = r1.wrapping_add(-344 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -232                                  r2 += -232   ///  r2 = r2.wrapping_add(-232 as i32 as i64 as u64)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_6816                      
    ldxw r1, [r10-0x154]                    
    ldxw r8, [r10-0x158]                    
    jne r8, 26, lbb_2247                            if r8 != (26 as i32 as i64 as u64) { pc += -312 }
    ldxdw r1, [r10-0x168]                   
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    mov64 r2, r1                                    r2 = r1
    jne r1, 0, lbb_2565                             if r1 != (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r10-0x1b0]                   
    stxdw [r10-0x188], r1                   
lbb_2565:
    jne r2, 0, lbb_2568                             if r2 != (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r10-0x1a0]                   
    stxdw [r10-0x1e0], r1                   
lbb_2568:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -305                                  r1 += -305   ///  r1 = r1.wrapping_add(-305 as i32 as i64 as u64)
    stxdw [r10-0xa8], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -304                                  r1 += -304   ///  r1 = r1.wrapping_add(-304 as i32 as i64 as u64)
    stxdw [r10-0xb8], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -264                                  r1 += -264   ///  r1 = r1.wrapping_add(-264 as i32 as i64 as u64)
    stxdw [r10-0xc8], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -296                                  r1 += -296   ///  r1 = r1.wrapping_add(-296 as i32 as i64 as u64)
    stxdw [r10-0xd8], r1                    
    lddw r1, 0x1000167eb --> b"marketprogram/src/state/market.rsvault\x04y\xd5[\xf21\xc0n\xeet\xc5n"        r1 load str located at 4295059435
    stxdw [r10-0xe8], r1                    
    stdw [r10-0xa0], 1                      
    stdw [r10-0xb0], 8                      
    stdw [r10-0xc0], 32                     
    stdw [r10-0xd0], 32                     
    stdw [r10-0xe0], 6                      
    ldxdw r1, [r10-0x170]                   
    stxdw [r10-0x68], r1                    
    ldxdw r1, [r10-0x198]                   
    stxdw [r10-0x70], r1                    
    ldxdw r1, [r10-0x1e0]                   
    stxdw [r10-0x78], r1                    
    ldxdw r1, [r10-0x188]                   
    stxdw [r10-0x80], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -232                                  r1 += -232   ///  r1 = r1.wrapping_add(-232 as i32 as i64 as u64)
    stxdw [r10-0x60], r1                    
    stdw [r10-0x58], 5                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -352                                  r1 += -352   ///  r1 = r1.wrapping_add(-352 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -128                                  r2 += -128   ///  r2 = r2.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -96                                   r3 += -96   ///  r3 = r3.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_6816                      
    ldxw r1, [r10-0x15c]                    
    ldxw r8, [r10-0x160]                    
    jne r8, 26, lbb_2247                            if r8 != (26 as i32 as i64 as u64) { pc += -364 }
    mov64 r8, 26                                    r8 = 26 as i32 as i64 as u64
    ja lbb_2247                                     if true { pc += -366 }

function_2613:
    stxdw [r10-0x1d0], r5                   
    stxdw [r10-0x1d8], r4                   
    mov64 r9, r2                                    r9 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r7, 10                                    r7 = 10 as i32 as i64 as u64
    jlt r3, 4, lbb_2630                             if r3 < (4 as i32 as i64 as u64) { pc += 11 }
    ldxdw r8, [r9+0x0]                      
    mov64 r2, r8                                    r2 = r8
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    lddw r1, 0x1000165b0 --> b"\x0d\x88\x0e\xee{\xa8\x04\x97'\x12\xaf\x88:h|n\xbbP\xd3\xe8T(+\x87\xac\xc…        r1 load str located at 4295058864
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    mov64 r7, 25                                    r7 = 25 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_2633                             if r0 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_2630:
    stxw [r6+0x4], r1                       
    stxw [r6+0x0], r7                       
    exit                                    
lbb_2633:
    ldxb r2, [r8+0x1]                       
    jeq r2, 0, lbb_2630                             if r2 == (0 as i32 as i64 as u64) { pc += -5 }
    ldxdw r1, [r9+0x10]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x100016650 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r2 load str located at 4295059024
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    mov64 r7, 6                                     r7 = 6 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2630                             if r0 != (0 as i32 as i64 as u64) { pc += -15 }
    mov64 r7, 3                                     r7 = 3 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r9, [r9+0x8]                      
    ldxdw r2, [r9+0x50]                     
    jne r2, 856, lbb_2630                           if r2 != (856 as i32 as i64 as u64) { pc += -20 }
    mov64 r7, 22                                    r7 = 22 as i32 as i64 as u64
    ldxdw r2, [r9+0x28]                     
    lddw r3, 0xd8a6fe616d93320a                     r3 load str located at -2835299220979502582
    jne r2, r3, lbb_2630                            if r2 != r3 { pc += -25 }
    ldxdw r2, [r9+0x30]                     
    lddw r3, 0xb174a1c206e706b6                     r3 load str located at -5659720976986339658
    jne r2, r3, lbb_2630                            if r2 != r3 { pc += -29 }
    ldxdw r2, [r9+0x38]                     
    lddw r3, 0xadd40f4b82a5676b                     r3 load str located at -5921090793096517781
    jne r2, r3, lbb_2630                            if r2 != r3 { pc += -33 }
    ldxdw r2, [r9+0x40]                     
    lddw r3, 0x7e6598cdd46af8a9                     r3 load str located at 9107853831226194089
    jne r2, r3, lbb_2630                            if r2 != r3 { pc += -37 }
    mov64 r7, 11                                    r7 = 11 as i32 as i64 as u64
    ldxb r2, [r9+0x0]                       
    mov64 r3, r2                                    r3 = r2
    and64 r3, 15                                    r3 &= 15   ///  r3 = r3.and(15)
    jne r3, 15, lbb_2630                            if r3 != (15 as i32 as i64 as u64) { pc += -42 }
    mov64 r1, r2                                    r1 = r2
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r9+0x0], r1                       
    mov64 r7, 3                                     r7 = 3 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r3, [r9+0x58]                     
    lddw r4, 0xe643d503bdd93852                     r4 load str located at -1854404409499764654
    jne r3, r4, lbb_2876                            if r3 != r4 { pc += 195 }
    mov64 r1, r9                                    r1 = r9
    add64 r1, 96                                    r1 += 96   ///  r1 = r1.wrapping_add(96 as i32 as i64 as u64)
    mov64 r2, 848                                   r2 = 848 as i32 as i64 as u64
    call function_273                       
    ldxdw r1, [r10-0x1d0]                   
    jlt r1, 8, lbb_2862                             if r1 < (8 as i32 as i64 as u64) { pc += 175 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    jeq r1, 8, lbb_2862                             if r1 == (8 as i32 as i64 as u64) { pc += 173 }
    jeq r1, 16, lbb_2862                            if r1 == (16 as i32 as i64 as u64) { pc += 172 }
    jeq r1, 24, lbb_2862                            if r1 == (24 as i32 as i64 as u64) { pc += 171 }
    jeq r1, 32, lbb_2862                            if r1 == (32 as i32 as i64 as u64) { pc += 170 }
    jeq r1, 40, lbb_2862                            if r1 == (40 as i32 as i64 as u64) { pc += 169 }
    jeq r1, 48, lbb_2862                            if r1 == (48 as i32 as i64 as u64) { pc += 168 }
    jeq r1, 56, lbb_2862                            if r1 == (56 as i32 as i64 as u64) { pc += 167 }
    jeq r1, 64, lbb_2862                            if r1 == (64 as i32 as i64 as u64) { pc += 166 }
    jeq r1, 72, lbb_2862                            if r1 == (72 as i32 as i64 as u64) { pc += 165 }
    jeq r1, 80, lbb_2862                            if r1 == (80 as i32 as i64 as u64) { pc += 164 }
    jeq r1, 88, lbb_2862                            if r1 == (88 as i32 as i64 as u64) { pc += 163 }
    jeq r1, 96, lbb_2862                            if r1 == (96 as i32 as i64 as u64) { pc += 162 }
    jeq r1, 104, lbb_2862                           if r1 == (104 as i32 as i64 as u64) { pc += 161 }
    jeq r1, 112, lbb_2862                           if r1 == (112 as i32 as i64 as u64) { pc += 160 }
    jeq r1, 120, lbb_2862                           if r1 == (120 as i32 as i64 as u64) { pc += 159 }
    jeq r1, 128, lbb_2862                           if r1 == (128 as i32 as i64 as u64) { pc += 158 }
    jeq r1, 136, lbb_2862                           if r1 == (136 as i32 as i64 as u64) { pc += 157 }
    jeq r1, 144, lbb_2862                           if r1 == (144 as i32 as i64 as u64) { pc += 156 }
    jeq r1, 152, lbb_2862                           if r1 == (152 as i32 as i64 as u64) { pc += 155 }
    jeq r1, 160, lbb_2862                           if r1 == (160 as i32 as i64 as u64) { pc += 154 }
    jeq r1, 168, lbb_2862                           if r1 == (168 as i32 as i64 as u64) { pc += 153 }
    jeq r1, 176, lbb_2862                           if r1 == (176 as i32 as i64 as u64) { pc += 152 }
    jeq r1, 184, lbb_2862                           if r1 == (184 as i32 as i64 as u64) { pc += 151 }
    jeq r1, 192, lbb_2862                           if r1 == (192 as i32 as i64 as u64) { pc += 150 }
    jeq r1, 200, lbb_2862                           if r1 == (200 as i32 as i64 as u64) { pc += 149 }
    jeq r1, 208, lbb_2862                           if r1 == (208 as i32 as i64 as u64) { pc += 148 }
    jeq r1, 216, lbb_2862                           if r1 == (216 as i32 as i64 as u64) { pc += 147 }
    jeq r1, 224, lbb_2862                           if r1 == (224 as i32 as i64 as u64) { pc += 146 }
    jeq r1, 232, lbb_2862                           if r1 == (232 as i32 as i64 as u64) { pc += 145 }
    jeq r1, 240, lbb_2862                           if r1 == (240 as i32 as i64 as u64) { pc += 144 }
    jeq r1, 248, lbb_2862                           if r1 == (248 as i32 as i64 as u64) { pc += 143 }
    jeq r1, 256, lbb_2862                           if r1 == (256 as i32 as i64 as u64) { pc += 142 }
    jeq r1, 264, lbb_2862                           if r1 == (264 as i32 as i64 as u64) { pc += 141 }
    jeq r1, 272, lbb_2862                           if r1 == (272 as i32 as i64 as u64) { pc += 140 }
    jeq r1, 280, lbb_2862                           if r1 == (280 as i32 as i64 as u64) { pc += 139 }
    jeq r1, 288, lbb_2862                           if r1 == (288 as i32 as i64 as u64) { pc += 138 }
    jeq r1, 296, lbb_2862                           if r1 == (296 as i32 as i64 as u64) { pc += 137 }
    jeq r1, 304, lbb_2862                           if r1 == (304 as i32 as i64 as u64) { pc += 136 }
    jeq r1, 312, lbb_2862                           if r1 == (312 as i32 as i64 as u64) { pc += 135 }
    jeq r1, 320, lbb_2862                           if r1 == (320 as i32 as i64 as u64) { pc += 134 }
    jeq r1, 328, lbb_2862                           if r1 == (328 as i32 as i64 as u64) { pc += 133 }
    jeq r1, 336, lbb_2862                           if r1 == (336 as i32 as i64 as u64) { pc += 132 }
    jeq r1, 344, lbb_2862                           if r1 == (344 as i32 as i64 as u64) { pc += 131 }
    jeq r1, 352, lbb_2862                           if r1 == (352 as i32 as i64 as u64) { pc += 130 }
    jeq r1, 360, lbb_2862                           if r1 == (360 as i32 as i64 as u64) { pc += 129 }
    jeq r1, 368, lbb_2862                           if r1 == (368 as i32 as i64 as u64) { pc += 128 }
    jeq r1, 376, lbb_2862                           if r1 == (376 as i32 as i64 as u64) { pc += 127 }
    jeq r1, 384, lbb_2862                           if r1 == (384 as i32 as i64 as u64) { pc += 126 }
    ldxdw r2, [r10-0x1d0]                   
    jeq r2, 392, lbb_2862                           if r2 == (392 as i32 as i64 as u64) { pc += 124 }
    ldxdw r2, [r10-0x1d0]                   
    add64 r2, -393                                  r2 += -393   ///  r2 = r2.wrapping_add(-393 as i32 as i64 as u64)
    jlt r2, 7, lbb_2862                             if r2 < (7 as i32 as i64 as u64) { pc += 121 }
    jeq r1, 400, lbb_2862                           if r1 == (400 as i32 as i64 as u64) { pc += 120 }
    ldxdw r2, [r10-0x1d0]                   
    jeq r2, 408, lbb_2862                           if r2 == (408 as i32 as i64 as u64) { pc += 118 }
    ldxdw r2, [r10-0x1d0]                   
    add64 r2, -409                                  r2 += -409   ///  r2 = r2.wrapping_add(-409 as i32 as i64 as u64)
    jlt r2, 7, lbb_2862                             if r2 < (7 as i32 as i64 as u64) { pc += 115 }
    jeq r1, 416, lbb_2862                           if r1 == (416 as i32 as i64 as u64) { pc += 114 }
    jeq r1, 424, lbb_2862                           if r1 == (424 as i32 as i64 as u64) { pc += 113 }
    ldxdw r4, [r10-0x1d8]                   
    ldxdw r1, [r4+0x0]                      
    stxdw [r10-0x1e0], r1                   
    ldxdw r1, [r4+0x8]                      
    stxdw [r10-0x1e8], r1                   
    ldxdw r1, [r4+0x10]                     
    stxdw [r10-0x1f0], r1                   
    ldxdw r1, [r4+0x18]                     
    stxdw [r10-0x1f8], r1                   
    ldxdw r1, [r4+0x20]                     
    stxdw [r10-0x200], r1                   
    ldxdw r1, [r4+0x28]                     
    stxdw [r10-0x208], r1                   
    ldxdw r1, [r4+0x30]                     
    stxdw [r10-0x210], r1                   
    ldxdw r1, [r4+0x38]                     
    stxdw [r10-0x218], r1                   
    ldxdw r1, [r4+0x40]                     
    stxdw [r10-0x220], r1                   
    ldxdw r1, [r4+0x48]                     
    stxdw [r10-0x228], r1                   
    ldxdw r1, [r4+0x50]                     
    stxdw [r10-0x230], r1                   
    ldxdw r1, [r4+0x58]                     
    stxdw [r10-0x238], r1                   
    ldxdw r1, [r4+0x60]                     
    stxdw [r10-0x240], r1                   
    ldxdw r1, [r4+0x68]                     
    stxdw [r10-0x248], r1                   
    ldxdw r1, [r4+0x70]                     
    stxdw [r10-0x250], r1                   
    ldxdw r1, [r4+0x78]                     
    stxdw [r10-0x258], r1                   
    ldxdw r1, [r4+0x80]                     
    stxdw [r10-0x260], r1                   
    ldxdw r1, [r4+0x88]                     
    stxdw [r10-0x268], r1                   
    ldxdw r1, [r4+0x90]                     
    stxdw [r10-0x270], r1                   
    ldxdw r1, [r4+0x98]                     
    stxdw [r10-0x278], r1                   
    ldxdw r1, [r4+0xa0]                     
    stxdw [r10-0x280], r1                   
    ldxdw r1, [r4+0xa8]                     
    stxdw [r10-0x288], r1                   
    ldxdw r1, [r4+0xb0]                     
    stxdw [r10-0x290], r1                   
    ldxdw r1, [r4+0xb8]                     
    stxdw [r10-0x298], r1                   
    ldxdw r1, [r4+0xc0]                     
    stxdw [r10-0x2a0], r1                   
    ldxdw r1, [r4+0xc8]                     
    stxdw [r10-0x2a8], r1                   
    ldxdw r1, [r4+0xd0]                     
    stxdw [r10-0x2b0], r1                   
    ldxdw r1, [r4+0xd8]                     
    stxdw [r10-0x2b8], r1                   
    ldxdw r1, [r4+0xe0]                     
    stxdw [r10-0x2c0], r1                   
    ldxdw r1, [r4+0xe8]                     
    stxdw [r10-0x2c8], r1                   
    ldxdw r1, [r4+0xf0]                     
    stxdw [r10-0x2d0], r1                   
    ldxdw r1, [r4+0xf8]                     
    stxdw [r10-0x2d8], r1                   
    ldxdw r1, [r4+0x100]                    
    stxdw [r10-0x2e0], r1                   
    ldxdw r1, [r4+0x108]                    
    stxdw [r10-0x2e8], r1                   
    ldxdw r1, [r4+0x110]                    
    stxdw [r10-0x2f0], r1                   
    ldxdw r1, [r4+0x118]                    
    stxdw [r10-0x2f8], r1                   
    ldxdw r1, [r4+0x120]                    
    stxdw [r10-0x300], r1                   
    ldxdw r1, [r4+0x128]                    
    stxdw [r10-0x308], r1                   
    ldxdw r1, [r4+0x130]                    
    stxdw [r10-0x310], r1                   
    ldxdw r1, [r4+0x138]                    
    stxdw [r10-0x318], r1                   
    ldxdw r1, [r4+0x140]                    
    stxdw [r10-0x320], r1                   
    ldxdw r1, [r4+0x148]                    
    stxdw [r10-0x328], r1                   
    ldxdw r1, [r4+0x150]                    
    stxdw [r10-0x330], r1                   
    ldxdw r1, [r4+0x158]                    
    stxdw [r10-0x338], r1                   
    ldxdw r1, [r4+0x160]                    
    stxdw [r10-0x340], r1                   
    ldxdw r1, [r4+0x168]                    
    stxdw [r10-0x348], r1                   
    ldxdw r1, [r4+0x170]                    
    stxdw [r10-0x350], r1                   
    ldxdw r1, [r4+0x178]                    
    stxdw [r10-0x358], r1                   
    ldxdw r1, [r4+0x180]                    
    stxdw [r10-0x360], r1                   
    ldxb r1, [r4+0x188]                     
    stxdw [r10-0x368], r1                   
    ldxb r1, [r4+0x18f]                     
    stxdw [r10-0x370], r1                   
    ldxdw r1, [r4+0x190]                    
    stxdw [r10-0x378], r1                   
    ldxb r2, [r4+0x198]                     
    ldxb r3, [r4+0x19f]                     
    ldxdw r1, [r4+0x1a0]                    
    ldxdw r8, [r4+0x1a8]                    
    stxdw [r10-0x10], r8                    
    ldxdw r4, [r10-0x1d0]                   
    jeq r4, 432, lbb_2878                           if r4 == (432 as i32 as i64 as u64) { pc += 17 }
    call function_141                       
lbb_2862:
    lddw r1, 0x100017668 --> b"\x00\x00\x00\x00\xb0f\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295063144
    call function_7172                      
    mov64 r7, 2                                     r7 = 2 as i32 as i64 as u64
    mov64 r2, r0                                    r2 = r0
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    jne r2, 1, lbb_2874                             if r2 != (1 as i32 as i64 as u64) { pc += 5 }
    ldxdw r1, [r0+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_2874                             if r2 == (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r0-0x1]                      
    callx r2                                
lbb_2874:
    ldxb r2, [r9+0x0]                       
    or64 r2, 8                                      r2 |= 8   ///  r2 = r2.or(8)
lbb_2876:
    stxb [r9+0x0], r2                       
    ja lbb_2630                                     if true { pc += -248 }
lbb_2878:
    ldxdw r7, [r10-0x1d8]                   
    mov64 r4, r7                                    r4 = r7
    add64 r4, 393                                   r4 += 393   ///  r4 = r4.wrapping_add(393 as i32 as i64 as u64)
    add64 r7, 409                                   r7 += 409   ///  r7 = r7.wrapping_add(409 as i32 as i64 as u64)
    ldxdw r5, [r10-0x9]                     
    ldxh r0, [r4+0x4]                       
    stxh [r10-0x33], r0                     
    ldxw r4, [r4+0x0]                       
    stxw [r10-0x37], r4                     
    lddw r4, 0xffffffffffffff                       r4 load str located at 72057594037927935
    and64 r8, r4                                    r8 &= r4   ///  r8 = r8.and(r4)
    lsh64 r5, 56                                    r5 <<= 56   ///  r5 = r5.wrapping_shl(56)
    or64 r5, r8                                     r5 |= r8   ///  r5 = r5.or(r8)
    ldxw r4, [r7+0x0]                       
    stxw [r10-0x27], r4                     
    ldxh r4, [r7+0x4]                       
    stxh [r10-0x23], r4                     
    stxdw [r10-0x18], r5                    
    stxdw [r10-0x20], r1                    
    stxb [r10-0x21], r3                     
    stxb [r10-0x28], r2                     
    ldxdw r1, [r10-0x378]                   
    stxdw [r10-0x30], r1                    
    ldxdw r1, [r10-0x370]                   
    stxb [r10-0x31], r1                     
    ldxdw r1, [r10-0x368]                   
    stxb [r10-0x38], r1                     
    ldxdw r1, [r10-0x360]                   
    stxdw [r10-0x40], r1                    
    ldxdw r1, [r10-0x358]                   
    stxdw [r10-0x48], r1                    
    ldxdw r1, [r10-0x350]                   
    stxdw [r10-0x50], r1                    
    ldxdw r1, [r10-0x348]                   
    stxdw [r10-0x58], r1                    
    ldxdw r1, [r10-0x340]                   
    stxdw [r10-0x60], r1                    
    ldxdw r1, [r10-0x338]                   
    stxdw [r10-0x68], r1                    
    ldxdw r1, [r10-0x330]                   
    stxdw [r10-0x70], r1                    
    ldxdw r1, [r10-0x328]                   
    stxdw [r10-0x78], r1                    
    ldxdw r1, [r10-0x320]                   
    stxdw [r10-0x80], r1                    
    ldxdw r1, [r10-0x318]                   
    stxdw [r10-0x88], r1                    
    ldxdw r1, [r10-0x310]                   
    stxdw [r10-0x90], r1                    
    ldxdw r1, [r10-0x308]                   
    stxdw [r10-0x98], r1                    
    ldxdw r1, [r10-0x300]                   
    stxdw [r10-0xa0], r1                    
    ldxdw r1, [r10-0x2f8]                   
    stxdw [r10-0xa8], r1                    
    ldxdw r1, [r10-0x2f0]                   
    stxdw [r10-0xb0], r1                    
    ldxdw r1, [r10-0x2e8]                   
    stxdw [r10-0xb8], r1                    
    ldxdw r1, [r10-0x2e0]                   
    stxdw [r10-0xc0], r1                    
    ldxdw r1, [r10-0x2d8]                   
    stxdw [r10-0xc8], r1                    
    ldxdw r1, [r10-0x2d0]                   
    stxdw [r10-0xd0], r1                    
    ldxdw r1, [r10-0x2c8]                   
    stxdw [r10-0xd8], r1                    
    ldxdw r1, [r10-0x2c0]                   
    stxdw [r10-0xe0], r1                    
    ldxdw r1, [r10-0x2b8]                   
    stxdw [r10-0xe8], r1                    
    ldxdw r1, [r10-0x2b0]                   
    stxdw [r10-0xf0], r1                    
    ldxdw r1, [r10-0x2a8]                   
    stxdw [r10-0xf8], r1                    
    ldxdw r1, [r10-0x2a0]                   
    stxdw [r10-0x100], r1                   
    ldxdw r1, [r10-0x298]                   
    stxdw [r10-0x108], r1                   
    ldxdw r1, [r10-0x290]                   
    stxdw [r10-0x110], r1                   
    ldxdw r1, [r10-0x288]                   
    stxdw [r10-0x118], r1                   
    ldxdw r1, [r10-0x280]                   
    stxdw [r10-0x120], r1                   
    ldxdw r1, [r10-0x278]                   
    stxdw [r10-0x128], r1                   
    ldxdw r1, [r10-0x270]                   
    stxdw [r10-0x130], r1                   
    ldxdw r1, [r10-0x268]                   
    stxdw [r10-0x138], r1                   
    ldxdw r1, [r10-0x260]                   
    stxdw [r10-0x140], r1                   
    ldxdw r1, [r10-0x258]                   
    stxdw [r10-0x148], r1                   
    ldxdw r1, [r10-0x250]                   
    stxdw [r10-0x150], r1                   
    ldxdw r1, [r10-0x248]                   
    stxdw [r10-0x158], r1                   
    ldxdw r1, [r10-0x240]                   
    stxdw [r10-0x160], r1                   
    ldxdw r1, [r10-0x238]                   
    stxdw [r10-0x168], r1                   
    ldxdw r1, [r10-0x230]                   
    stxdw [r10-0x170], r1                   
    ldxdw r1, [r10-0x228]                   
    stxdw [r10-0x178], r1                   
    ldxdw r1, [r10-0x220]                   
    stxdw [r10-0x180], r1                   
    ldxdw r1, [r10-0x218]                   
    stxdw [r10-0x188], r1                   
    ldxdw r1, [r10-0x210]                   
    stxdw [r10-0x190], r1                   
    ldxdw r1, [r10-0x208]                   
    stxdw [r10-0x198], r1                   
    ldxdw r1, [r10-0x200]                   
    stxdw [r10-0x1a0], r1                   
    ldxdw r1, [r10-0x1f8]                   
    stxdw [r10-0x1a8], r1                   
    ldxdw r1, [r10-0x1f0]                   
    stxdw [r10-0x1b0], r1                   
    ldxdw r1, [r10-0x1e8]                   
    stxdw [r10-0x1b8], r1                   
    ldxdw r2, [r10-0x1e0]                   
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    stxw [r10-0x1bc], r1                    
    stxw [r10-0x1c0], r2                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -456                                  r1 += -456   ///  r1 = r1.wrapping_add(-456 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -448                                  r2 += -448   ///  r2 = r2.wrapping_add(-448 as i32 as i64 as u64)
    call function_3274                      
    ldxw r7, [r10-0x1c8]                    
    jeq r7, 26, lbb_3015                            if r7 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3023                                     if true { pc += 8 }
lbb_3015:
    mov64 r1, r9                                    r1 = r9
    add64 r1, 472                                   r1 += 472   ///  r1 = r1.wrapping_add(472 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -448                                  r2 += -448   ///  r2 = r2.wrapping_add(-448 as i32 as i64 as u64)
    mov64 r3, 432                                   r3 = 432 as i32 as i64 as u64
    call function_9980                      
    mov64 r7, 26                                    r7 = 26 as i32 as i64 as u64
    ja lbb_2874                                     if true { pc += -149 }
lbb_3023:
    ldxw r1, [r10-0x1c4]                    
    ja lbb_2874                                     if true { pc += -151 }

function_3025:
    mov64 r7, r5                                    r7 = r5
    mov64 r0, r4                                    r0 = r4
    mov64 r6, r1                                    r6 = r1
    ldxdw r5, [r7-0xff8]                    
    ldxdw r4, [r7-0x1000]                   
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jsgt r2, 4, lbb_3074                            if (r2 as i64) > (4 as i32 as i64) { pc += 42 }
    jsgt r2, 1, lbb_3085                            if (r2 as i64) > (1 as i32 as i64) { pc += 52 }
    jeq r2, 0, lbb_3129                             if r2 == (0 as i32 as i64 as u64) { pc += 95 }
    mov64 r1, 10                                    r1 = 10 as i32 as i64 as u64
    jne r0, 3, lbb_3161                             if r0 != (3 as i32 as i64 as u64) { pc += 125 }
    ldxdw r7, [r3+0x0]                      
    mov64 r2, r7                                    r2 = r7
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    lddw r1, 0x1000165b0 --> b"\x0d\x88\x0e\xee{\xa8\x04\x97'\x12\xaf\x88:h|n\xbbP\xd3\xe8T(+\x87\xac\xc…        r1 load str located at 4295058864
    mov64 r8, r3                                    r8 = r3
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    mov64 r9, r4                                    r9 = r4
    call function_9989                      
    mov64 r3, r9                                    r3 = r9
    mov64 r4, r8                                    r4 = r8
    mov64 r1, 25                                    r1 = 25 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_3161                             if r0 != (0 as i32 as i64 as u64) { pc += 110 }
    ldxb r5, [r7+0x1]                       
    jeq r5, 0, lbb_3161                             if r5 == (0 as i32 as i64 as u64) { pc += 108 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 30                                    r2 = 30 as i32 as i64 as u64
    ldxdw r0, [r3+0x20]                     
    ldxdw r5, [r4+0x8]                      
    ldxdw r7, [r5+0x390]                    
    jlt r0, r7, lbb_3161                            if r0 < r7 { pc += 102 }
    mov64 r2, 22                                    r2 = 22 as i32 as i64 as u64
    ldxdw r7, [r3+0x0]                      
    ldxdw r4, [r4+0x10]                     
    ldxdw r4, [r4+0x58]                     
    jlt r4, r7, lbb_3161                            if r4 < r7 { pc += 97 }
    ldxdw r1, [r3+0x8]                      
    stxdw [r5+0x68], r1                     
    ldxdw r1, [r3+0x10]                     
    stxdw [r5+0x70], r1                     
    ldxdw r1, [r3+0x18]                     
    stxdw [r5+0x390], r0                    
    stxdw [r5+0x80], r7                     
    stxdw [r5+0x78], r1                     
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ja lbb_3161                                     if true { pc += 87 }
lbb_3074:
    jsgt r2, 7, lbb_3095                            if (r2 as i64) > (7 as i32 as i64) { pc += 20 }
    jeq r2, 5, lbb_3145                             if r2 == (5 as i32 as i64 as u64) { pc += 69 }
    jeq r2, 6, lbb_3164                             if r2 == (6 as i32 as i64 as u64) { pc += 87 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    mov64 r3, r0                                    r3 = r0
    call function_2613                      
    ldxw r2, [r10-0x1c]                     
    ldxw r1, [r10-0x20]                     
    ja lbb_3161                                     if true { pc += 76 }
lbb_3085:
    jeq r2, 2, lbb_3105                             if r2 == (2 as i32 as i64 as u64) { pc += 19 }
    jeq r2, 3, lbb_3121                             if r2 == (3 as i32 as i64 as u64) { pc += 34 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    mov64 r3, r0                                    r3 = r0
    call function_1876                      
    ldxw r2, [r10-0x24]                     
    ldxw r1, [r10-0x28]                     
    ja lbb_3161                                     if true { pc += 66 }
lbb_3095:
    jeq r2, 8, lbb_3113                             if r2 == (8 as i32 as i64 as u64) { pc += 17 }
    jeq r2, 9, lbb_3137                             if r2 == (9 as i32 as i64 as u64) { pc += 40 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -8                                    r1 += -8   ///  r1 = r1.wrapping_add(-8 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    mov64 r3, r0                                    r3 = r0
    call function_540                       
    ldxw r2, [r10-0x4]                      
    ldxw r1, [r10-0x8]                      
    ja lbb_3161                                     if true { pc += 56 }
lbb_3105:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -56                                   r1 += -56   ///  r1 = r1.wrapping_add(-56 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    mov64 r3, r0                                    r3 = r0
    call function_2231                      
    ldxw r2, [r10-0x34]                     
    ldxw r1, [r10-0x38]                     
    ja lbb_3161                                     if true { pc += 48 }
lbb_3113:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    mov64 r3, r0                                    r3 = r0
    call function_355                       
    ldxw r2, [r10-0x14]                     
    ldxw r1, [r10-0x18]                     
    ja lbb_3161                                     if true { pc += 40 }
lbb_3121:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    mov64 r3, r0                                    r3 = r0
    call function_1650                      
    ldxw r2, [r10-0x2c]                     
    ldxw r1, [r10-0x30]                     
    ja lbb_3161                                     if true { pc += 32 }
lbb_3129:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    mov64 r3, r0                                    r3 = r0
    call function_836                       
    ldxw r2, [r10-0x3c]                     
    ldxw r1, [r10-0x40]                     
    ja lbb_3161                                     if true { pc += 24 }
lbb_3137:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    mov64 r3, r0                                    r3 = r0
    call function_635                       
    ldxw r2, [r10-0xc]                      
    ldxw r1, [r10-0x10]                     
    ja lbb_3161                                     if true { pc += 16 }
lbb_3145:
    mov64 r1, 10                                    r1 = 10 as i32 as i64 as u64
    jlt r0, 2, lbb_3160                             if r0 < (2 as i32 as i64 as u64) { pc += 13 }
    ldxdw r7, [r3+0x0]                      
    mov64 r2, r7                                    r2 = r7
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    lddw r1, 0x1000165b0 --> b"\x0d\x88\x0e\xee{\xa8\x04\x97'\x12\xaf\x88:h|n\xbbP\xd3\xe8T(+\x87\xac\xc…        r1 load str located at 4295058864
    mov64 r8, r3                                    r8 = r3
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    mov64 r2, r8                                    r2 = r8
    mov64 r1, 25                                    r1 = 25 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_3221                             if r0 == (0 as i32 as i64 as u64) { pc += 61 }
lbb_3160:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_3161:
    stxw [r6+0x0], r1                       
    stxw [r6+0x4], r2                       
    exit                                    
lbb_3164:
    mov64 r1, 10                                    r1 = 10 as i32 as i64 as u64
    jlt r0, 2, lbb_3179                             if r0 < (2 as i32 as i64 as u64) { pc += 13 }
    ldxdw r7, [r3+0x0]                      
    mov64 r2, r7                                    r2 = r7
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    lddw r1, 0x1000165b0 --> b"\x0d\x88\x0e\xee{\xa8\x04\x97'\x12\xaf\x88:h|n\xbbP\xd3\xe8T(+\x87\xac\xc…        r1 load str located at 4295058864
    mov64 r8, r3                                    r8 = r3
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    mov64 r2, r8                                    r2 = r8
    mov64 r1, 25                                    r1 = 25 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_3180                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
lbb_3179:
    ja lbb_3160                                     if true { pc += -20 }
lbb_3180:
    ldxb r3, [r7+0x1]                       
    jeq r3, 0, lbb_3179                             if r3 == (0 as i32 as i64 as u64) { pc += -3 }
    mov64 r1, 3                                     r1 = 3 as i32 as i64 as u64
    ldxdw r2, [r2+0x8]                      
    ldxdw r3, [r2+0x50]                     
    jne r3, 856, lbb_3179                           if r3 != (856 as i32 as i64 as u64) { pc += -7 }
    mov64 r1, 22                                    r1 = 22 as i32 as i64 as u64
    ldxdw r3, [r2+0x28]                     
    lddw r4, 0xd8a6fe616d93320a                     r4 load str located at -2835299220979502582
    jne r3, r4, lbb_3179                            if r3 != r4 { pc += -12 }
    ldxdw r3, [r2+0x30]                     
    lddw r4, 0xb174a1c206e706b6                     r4 load str located at -5659720976986339658
    jne r3, r4, lbb_3179                            if r3 != r4 { pc += -16 }
    ldxdw r3, [r2+0x38]                     
    lddw r4, 0xadd40f4b82a5676b                     r4 load str located at -5921090793096517781
    jne r3, r4, lbb_3179                            if r3 != r4 { pc += -20 }
    ldxdw r3, [r2+0x40]                     
    lddw r4, 0x7e6598cdd46af8a9                     r4 load str located at 9107853831226194089
    jne r3, r4, lbb_3179                            if r3 != r4 { pc += -24 }
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    ldxb r3, [r2+0x0]                       
    mov64 r4, r3                                    r4 = r3
    and64 r4, 15                                    r4 &= 15   ///  r4 = r4.and(15)
    jne r4, 15, lbb_3179                            if r4 != (15 as i32 as i64 as u64) { pc += -29 }
    mov64 r1, r3                                    r1 = r3
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r2+0x0], r1                       
    mov64 r1, 3                                     r1 = 3 as i32 as i64 as u64
    ldxdw r4, [r2+0x58]                     
    lddw r5, 0xe643d503bdd93852                     r5 load str located at -1854404409499764654
    jne r4, r5, lbb_3272                            if r4 != r5 { pc += 56 }
    mov64 r1, r2                                    r1 = r2
    add64 r1, 96                                    r1 += 96   ///  r1 = r1.wrapping_add(96 as i32 as i64 as u64)
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jeq r1, 0, lbb_3270                             if r1 == (0 as i32 as i64 as u64) { pc += 50 }
    ja lbb_3261                                     if true { pc += 40 }
lbb_3221:
    ldxb r3, [r7+0x1]                       
    jeq r3, 0, lbb_3160                             if r3 == (0 as i32 as i64 as u64) { pc += -63 }
    mov64 r1, 3                                     r1 = 3 as i32 as i64 as u64
    ldxdw r2, [r2+0x8]                      
    ldxdw r3, [r2+0x50]                     
    jne r3, 856, lbb_3160                           if r3 != (856 as i32 as i64 as u64) { pc += -67 }
    mov64 r1, 22                                    r1 = 22 as i32 as i64 as u64
    ldxdw r3, [r2+0x28]                     
    lddw r4, 0xd8a6fe616d93320a                     r4 load str located at -2835299220979502582
    jne r3, r4, lbb_3160                            if r3 != r4 { pc += -72 }
    ldxdw r3, [r2+0x30]                     
    lddw r4, 0xb174a1c206e706b6                     r4 load str located at -5659720976986339658
    jne r3, r4, lbb_3160                            if r3 != r4 { pc += -76 }
    ldxdw r3, [r2+0x38]                     
    lddw r4, 0xadd40f4b82a5676b                     r4 load str located at -5921090793096517781
    jne r3, r4, lbb_3160                            if r3 != r4 { pc += -80 }
    ldxdw r3, [r2+0x40]                     
    lddw r4, 0x7e6598cdd46af8a9                     r4 load str located at 9107853831226194089
    jne r3, r4, lbb_3160                            if r3 != r4 { pc += -84 }
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    ldxb r3, [r2+0x0]                       
    mov64 r4, r3                                    r4 = r3
    and64 r4, 15                                    r4 &= 15   ///  r4 = r4.and(15)
    jne r4, 15, lbb_3160                            if r4 != (15 as i32 as i64 as u64) { pc += -89 }
    mov64 r1, r3                                    r1 = r3
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r2+0x0], r1                       
    mov64 r1, 3                                     r1 = 3 as i32 as i64 as u64
    ldxdw r4, [r2+0x58]                     
    lddw r5, 0xe643d503bdd93852                     r5 load str located at -1854404409499764654
    jne r4, r5, lbb_3268                            if r4 != r5 { pc += 11 }
    mov64 r1, r2                                    r1 = r2
    add64 r1, 96                                    r1 += 96   ///  r1 = r1.wrapping_add(96 as i32 as i64 as u64)
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jeq r1, 0, lbb_3266                             if r1 == (0 as i32 as i64 as u64) { pc += 5 }
lbb_3261:
    lddw r1, 0x10001672c --> b"from_bytes_mut"        r1 load str located at 4295059244
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_290                       
lbb_3266:
    stb [r2+0x38e], 1                       
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
lbb_3268:
    stxb [r2+0x0], r3                       
    ja lbb_3160                                     if true { pc += -110 }
lbb_3270:
    stb [r2+0x38e], 0                       
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
lbb_3272:
    stxb [r2+0x0], r3                       
    ja lbb_3179                                     if true { pc += -95 }

function_3274:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, 17                                    r4 = 17 as i32 as i64 as u64
    ldxdw r5, [r2+0x140]                    
    jgt r5, 100, lbb_3369                           if r5 > (100 as i32 as i64 as u64) { pc += 91 }
    mov64 r4, 18                                    r4 = 18 as i32 as i64 as u64
    ldxdw r5, [r2+0x160]                    
    ldxdw r0, [r2+0x158]                    
    jgt r0, r5, lbb_3369                            if r0 > r5 { pc += 87 }
    ldxdw r5, [r2+0x178]                    
    ldxdw r0, [r2+0x170]                    
    jgt r0, r5, lbb_3369                            if r0 > r5 { pc += 84 }
    mov64 r4, 19                                    r4 = 19 as i32 as i64 as u64
    ldxdw r5, [r2+0x168]                    
    ldxdw r0, [r2+0x180]                    
    jle r0, r5, lbb_3369                            if r0 <= r5 { pc += 80 }
    mov64 r4, 4                                     r4 = 4 as i32 as i64 as u64
    ldxb r5, [r2+0x188]                     
    jgt r5, 200, lbb_3369                           if r5 > (200 as i32 as i64 as u64) { pc += 77 }
    mov64 r4, 23                                    r4 = 23 as i32 as i64 as u64
    ldxdw r5, [r2+0x148]                    
    jgt r5, 1000000, lbb_3369                       if r5 > (1000000 as i32 as i64 as u64) { pc += 74 }
    ldxdw r5, [r2+0x150]                    
    jgt r5, 1000000, lbb_3369                       if r5 > (1000000 as i32 as i64 as u64) { pc += 72 }
    ldxdw r5, [r2+0x190]                    
    jgt r5, 1000000, lbb_3369                       if r5 > (1000000 as i32 as i64 as u64) { pc += 70 }
    ldxdw r5, [r2+0x1a0]                    
    jgt r5, 1000000, lbb_3369                       if r5 > (1000000 as i32 as i64 as u64) { pc += 68 }
    ldxb r5, [r2+0x198]                     
    jgt r5, 15, lbb_3369                            if r5 > (15 as i32 as i64 as u64) { pc += 66 }
    ldxdw r4, [r2+0x0]                      
    mov64 r3, r4                                    r3 = r4
    add64 r3, -1000001                              r3 += -1000001   ///  r3 = r3.wrapping_add(-1000001 as i32 as i64 as u64)
    jlt r3, -1000000, lbb_3338                      if r3 < (-1000000 as i32 as i64 as u64) { pc += 31 }
    ldxdw r3, [r2+0x8]                      
    jle r3, r4, lbb_3338                            if r3 <= r4 { pc += 29 }
    jgt r3, 1000000, lbb_3338                       if r3 > (1000000 as i32 as i64 as u64) { pc += 28 }
    ldxdw r4, [r2+0x10]                     
    jle r4, r3, lbb_3338                            if r4 <= r3 { pc += 26 }
    jgt r4, 1000000, lbb_3338                       if r4 > (1000000 as i32 as i64 as u64) { pc += 25 }
    ldxdw r3, [r2+0x18]                     
    jle r3, r4, lbb_3338                            if r3 <= r4 { pc += 23 }
    jgt r3, 1000000, lbb_3338                       if r3 > (1000000 as i32 as i64 as u64) { pc += 22 }
    ldxdw r4, [r2+0x20]                     
    jle r4, r3, lbb_3338                            if r4 <= r3 { pc += 20 }
    jgt r4, 1000000, lbb_3338                       if r4 > (1000000 as i32 as i64 as u64) { pc += 19 }
    ldxdw r3, [r2+0x28]                     
    jle r3, r4, lbb_3338                            if r3 <= r4 { pc += 17 }
    jgt r3, 1000000, lbb_3338                       if r3 > (1000000 as i32 as i64 as u64) { pc += 16 }
    ldxdw r4, [r2+0x30]                     
    jle r4, r3, lbb_3338                            if r4 <= r3 { pc += 14 }
    jgt r4, 1000000, lbb_3338                       if r4 > (1000000 as i32 as i64 as u64) { pc += 13 }
    ldxdw r3, [r2+0x38]                     
    jle r3, r4, lbb_3338                            if r3 <= r4 { pc += 11 }
    jgt r3, 1000000, lbb_3338                       if r3 > (1000000 as i32 as i64 as u64) { pc += 10 }
    ldxdw r4, [r2+0x40]                     
    jle r4, r3, lbb_3338                            if r4 <= r3 { pc += 8 }
    jgt r4, 1000000, lbb_3338                       if r4 > (1000000 as i32 as i64 as u64) { pc += 7 }
    ldxdw r3, [r2+0x48]                     
    jle r3, r4, lbb_3338                            if r3 <= r4 { pc += 5 }
    jgt r3, 1000000, lbb_3338                       if r3 > (1000000 as i32 as i64 as u64) { pc += 4 }
    ldxdw r4, [r2+0x50]                     
    mov64 r3, r4                                    r3 = r4
    add64 r3, -1000001                              r3 += -1000001   ///  r3 = r3.wrapping_add(-1000001 as i32 as i64 as u64)
    jgt r3, -1000001, lbb_3341                      if r3 > (-1000001 as i32 as i64 as u64) { pc += 3 }
lbb_3338:
    mov64 r4, 13                                    r4 = 13 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ja lbb_3369                                     if true { pc += 28 }
lbb_3341:
    ldxdw r3, [r2+0x58]                     
    jle r3, r4, lbb_3338                            if r3 <= r4 { pc += -5 }
    jgt r3, 1000000, lbb_3338                       if r3 > (1000000 as i32 as i64 as u64) { pc += -6 }
    ldxdw r4, [r2+0x60]                     
    jle r4, r3, lbb_3338                            if r4 <= r3 { pc += -8 }
    jgt r4, 1000000, lbb_3338                       if r4 > (1000000 as i32 as i64 as u64) { pc += -9 }
    ldxdw r3, [r2+0x68]                     
    jle r3, r4, lbb_3338                            if r3 <= r4 { pc += -11 }
    jgt r3, 1000000, lbb_3338                       if r3 > (1000000 as i32 as i64 as u64) { pc += -12 }
    ldxdw r4, [r2+0x70]                     
    jle r4, r3, lbb_3338                            if r4 <= r3 { pc += -14 }
    jgt r4, 1000000, lbb_3338                       if r4 > (1000000 as i32 as i64 as u64) { pc += -15 }
    ldxdw r3, [r2+0x78]                     
    jle r3, r4, lbb_3338                            if r3 <= r4 { pc += -17 }
    jgt r3, 1000000, lbb_3338                       if r3 > (1000000 as i32 as i64 as u64) { pc += -18 }
    ldxdw r4, [r2+0x80]                     
    jle r4, r3, lbb_3338                            if r4 <= r3 { pc += -20 }
    jgt r4, 1000000, lbb_3338                       if r4 > (1000000 as i32 as i64 as u64) { pc += -21 }
    ldxdw r3, [r2+0x88]                     
    jle r3, r4, lbb_3338                            if r3 <= r4 { pc += -23 }
    jgt r3, 1000000, lbb_3338                       if r3 > (1000000 as i32 as i64 as u64) { pc += -24 }
    ldxdw r4, [r2+0x90]                     
    jle r4, r3, lbb_3338                            if r4 <= r3 { pc += -26 }
    jgt r4, 1000000, lbb_3338                       if r4 > (1000000 as i32 as i64 as u64) { pc += -27 }
    ldxdw r2, [r2+0x98]                     
    jle r2, r4, lbb_3338                            if r2 <= r4 { pc += -29 }
    mov64 r3, 26                                    r3 = 26 as i32 as i64 as u64
    jgt r2, 1000000, lbb_3338                       if r2 > (1000000 as i32 as i64 as u64) { pc += -31 }
lbb_3369:
    stxw [r1+0x4], r4                       
    stxw [r1+0x0], r3                       
    exit                                    

function_3372:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r9, [r2+0x2f8]                    
    jslt r9, 0, lbb_3404                            if (r9 as i64) < (0 as i32 as i64) { pc += 29 }
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jsgt r9, 0, lbb_3379                            if (r9 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_3379:
    mov64 r6, r5                                    r6 = r5
    sub64 r6, r9                                    r6 -= r9   ///  r6 = r6.wrapping_sub(r9)
    jslt r6, r5, lbb_3383                           if (r6 as i64) < (r5 as i64) { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_3383:
    xor64 r0, r8                                    r0 ^= r8   ///  r0 = r0.xor(r8)
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    jne r0, 0, lbb_3404                             if r0 != (0 as i32 as i64 as u64) { pc += 18 }
    ldxdw r8, [r2+0x2e0]                    
    jslt r8, 0, lbb_3404                            if (r8 as i64) < (0 as i32 as i64) { pc += 16 }
    ldxdw r9, [r2+0x2d0]                    
    jslt r9, 0, lbb_3404                            if (r9 as i64) < (0 as i32 as i64) { pc += 14 }
    ldxdw r5, [r2+0x2d8]                    
    mov64 r0, r5                                    r0 = r5
    jslt r5, 0, lbb_3404                            if (r5 as i64) < (0 as i32 as i64) { pc += 11 }
    ldxdw r5, [r2+0x2e8]                    
    jslt r5, 0, lbb_3404                            if (r5 as i64) < (0 as i32 as i64) { pc += 9 }
    ldxdw r2, [r2+0x2f0]                    
    stxdw [r10-0xa8], r2                    
    jslt r2, 0, lbb_3404                            if (r2 as i64) < (0 as i32 as i64) { pc += 6 }
    stxdw [r10-0xb0], r1                    
    jsgt r6, 0, lbb_3406                            if (r6 as i64) > (0 as i32 as i64) { pc += 6 }
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    jeq r6, r2, lbb_3404                            if r6 == r2 { pc += 1 }
    ja lbb_3512                                     if true { pc += 108 }
lbb_3404:
    stxdw [r1+0x0], r7                      
    exit                                    
lbb_3406:
    jne r4, 0, lbb_3408                             if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    stxdw [r10-0xa8], r5                    
lbb_3408:
    jne r4, 0, lbb_3410                             if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, r9                                    r0 = r9
lbb_3410:
    mov64 r9, r0                                    r9 = r0
    jeq r0, 0, lbb_3404                             if r0 == (0 as i32 as i64 as u64) { pc += -8 }
    ldxdw r1, [r10-0xa8]                    
    stxdw [r10-0xa8], r1                    
    stxdw [r10-0xb8], r3                    
    mov64 r2, r6                                    r2 = r6
    jlt r6, r8, lbb_3418                            if r6 < r8 { pc += 1 }
    mov64 r2, r8                                    r2 = r8
lbb_3418:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, 1000000                               r4 = 1000000 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    mov64 r4, r9                                    r4 = r9
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
    ldxdw r3, [r10-0x50]                    
    mov64 r2, r3                                    r2 = r3
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r2, r3, lbb_3434                            if r2 < r3 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_3434:
    ldxdw r3, [r10-0x48]                    
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_10694                     
    ldxdw r2, [r10-0x58]                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_3444                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_3444:
    ldxdw r5, [r10-0x60]                    
    ldxdw r1, [r10-0xb0]                    
    ldxdw r4, [r10-0xa8]                    
    jslt r5, 0, lbb_3449                            if (r5 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_3449:
    jeq r2, 0, lbb_3451                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r9, r3                                    r9 = r3
lbb_3451:
    and64 r9, 1                                     r9 &= 1   ///  r9 = r9.and(1)
    jne r9, 0, lbb_3404                             if r9 != (0 as i32 as i64 as u64) { pc += -49 }
    jeq r4, 0, lbb_3404                             if r4 == (0 as i32 as i64 as u64) { pc += -50 }
    mov64 r9, r5                                    r9 = r5
    sub64 r6, r8                                    r6 -= r8   ///  r6 = r6.wrapping_sub(r8)
    jsgt r6, 0, lbb_3458                            if (r6 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_3458:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, 1000000                               r4 = 1000000 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    ldxdw r4, [r10-0xa8]                    
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
    ldxdw r3, [r10-0x70]                    
    mov64 r2, r3                                    r2 = r3
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r2, r3, lbb_3475                            if r2 < r3 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_3475:
    ldxdw r3, [r10-0x68]                    
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_10694                     
    ldxdw r3, [r10-0x78]                    
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_3485                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_3485:
    ldxdw r2, [r10-0x80]                    
    ldxdw r1, [r10-0xb0]                    
    ldxdw r4, [r10-0xb8]                    
    jslt r2, 0, lbb_3490                            if (r2 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_3490:
    jeq r3, 0, lbb_3492                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r5                                    r6 = r5
lbb_3492:
    and64 r6, 1                                     r6 &= 1   ///  r6 = r6.and(1)
    jne r6, 0, lbb_3404                             if r6 != (0 as i32 as i64 as u64) { pc += -90 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jslt r2, 0, lbb_3498                            if (r2 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_3498:
    mov64 r6, r9                                    r6 = r9
    mov64 r5, r6                                    r5 = r6
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    jslt r5, r6, lbb_3503                           if (r5 as i64) < (r6 as i64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_3503:
    xor64 r3, r0                                    r3 ^= r0   ///  r3 = r3.xor(r0)
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_3404                             if r3 != (0 as i32 as i64 as u64) { pc += -102 }
    mov64 r3, r5                                    r3 = r5
    neg64 r3                                        r3 = -r3   ///  r3 = (r3 as i64).wrapping_neg() as u64
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    jeq r5, r2, lbb_3404                            if r5 == r2 { pc += -107 }
    ja lbb_3609                                     if true { pc += 97 }
lbb_3512:
    jne r4, 0, lbb_3514                             if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    ldxdw r5, [r10-0xa8]                    
lbb_3514:
    jne r4, 0, lbb_3516                             if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r9, r0                                    r9 = r0
lbb_3516:
    jeq r9, 0, lbb_3404                             if r9 == (0 as i32 as i64 as u64) { pc += -113 }
    stxdw [r10-0xa8], r5                    
    stxdw [r10-0xb8], r3                    
    neg64 r6                                        r6 = -r6   ///  r6 = (r6 as i64).wrapping_neg() as u64
    mov64 r2, r8                                    r2 = r8
    jlt r8, r6, lbb_3523                            if r8 < r6 { pc += 1 }
    mov64 r2, r6                                    r2 = r6
lbb_3523:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, 1000000                               r4 = 1000000 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    mov64 r1, r9                                    r1 = r9
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
    ldxdw r3, [r10-0x10]                    
    mov64 r2, r3                                    r2 = r3
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r2, r3, lbb_3537                            if r2 < r3 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_3537:
    ldxdw r3, [r10-0x8]                     
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r4, r9                                    r4 = r9
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_10694                     
    ldxdw r2, [r10-0x18]                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_3548                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_3548:
    ldxdw r9, [r10-0x20]                    
    ldxdw r1, [r10-0xb0]                    
    ldxdw r4, [r10-0xa8]                    
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jslt r9, 0, lbb_3554                            if (r9 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_3554:
    jeq r2, 0, lbb_3556                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, r3                                    r5 = r3
lbb_3556:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_3404                             if r5 != (0 as i32 as i64 as u64) { pc += -154 }
    jeq r4, 0, lbb_3404                             if r4 == (0 as i32 as i64 as u64) { pc += -155 }
    sub64 r6, r8                                    r6 -= r8   ///  r6 = r6.wrapping_sub(r8)
    jsgt r6, 0, lbb_3562                            if (r6 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_3562:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, 1000000                               r4 = 1000000 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    ldxdw r4, [r10-0xa8]                    
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
    ldxdw r3, [r10-0x30]                    
    mov64 r2, r3                                    r2 = r3
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r2, r3, lbb_3579                            if r2 < r3 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_3579:
    ldxdw r3, [r10-0x28]                    
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_10694                     
    ldxdw r3, [r10-0x38]                    
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_3589                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_3589:
    ldxdw r2, [r10-0x40]                    
    ldxdw r1, [r10-0xb0]                    
    ldxdw r4, [r10-0xb8]                    
    jslt r2, 0, lbb_3594                            if (r2 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_3594:
    jeq r3, 0, lbb_3596                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r5                                    r6 = r5
lbb_3596:
    and64 r6, 1                                     r6 &= 1   ///  r6 = r6.and(1)
    jne r6, 0, lbb_3404                             if r6 != (0 as i32 as i64 as u64) { pc += -194 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jslt r2, 0, lbb_3602                            if (r2 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_3602:
    mov64 r3, r9                                    r3 = r9
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    jslt r3, r9, lbb_3606                           if (r3 as i64) < (r9 as i64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_3606:
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_3404                             if r5 != (0 as i32 as i64 as u64) { pc += -205 }
lbb_3609:
    mov64 r2, r3                                    r2 = r3
    add64 r2, 1000000                               r2 += 1000000   ///  r2 = r2.wrapping_add(1000000 as i32 as i64 as u64)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jslt r2, r3, lbb_3614                           if (r2 as i64) < (r3 as i64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_3614:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_3404                             if r5 != (0 as i32 as i64 as u64) { pc += -212 }
    jslt r3, -999999, lbb_3404                      if (r3 as i64) < (-999999 as i32 as i64) { pc += -213 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -160                                  r1 += -160   ///  r1 = r1.wrapping_add(-160 as i32 as i64 as u64)
    ldxdw r2, [r10-0x90]                    
    ldxdw r3, [r10-0x88]                    
    mov64 r4, 1000000                               r4 = 1000000 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_10694                     
    ldxdw r1, [r10-0xb0]                    
    ldxdw r2, [r10-0x98]                    
    stxdw [r1+0x10], r2                     
    ldxdw r2, [r10-0xa0]                    
    stxdw [r1+0x8], r2                      
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ja lbb_3404                                     if true { pc += -232 }

function_3636:
    mov64 r7, r3                                    r7 = r3
    mov64 r0, r1                                    r0 = r1
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jlt r7, r4, lbb_3865                            if r7 < r4 { pc += 225 }
    ldxdw r1, [r5-0xff0]                    
    ldxdw r3, [r5-0xff8]                    
    sub64 r7, r4                                    r7 -= r4   ///  r7 = r7.wrapping_sub(r4)
    jlt r7, r3, lbb_3645                            if r7 < r3 { pc += 1 }
    mov64 r7, r3                                    r7 = r3
lbb_3645:
    ldxdw r3, [r5-0x1000]                   
    stxdw [r10-0xd8], r3                    
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    stxdw [r10-0xc8], r3                    
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r6, r1                                    r6 = r1
    and64 r6, 255                                   r6 &= 255   ///  r6 = r6.and(255)
    stxdw [r10-0xd0], r0                    
    jeq r6, 0, lbb_3797                             if r6 == (0 as i32 as i64 as u64) { pc += 143 }
    stxdw [r10-0xc8], r3                    
    mov64 r3, r6                                    r3 = r6
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jeq r3, 0, lbb_3663                             if r3 == (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    stxdw [r10-0xc8], r3                    
    mov64 r8, 10                                    r8 = 10 as i32 as i64 as u64
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    jeq r1, 1, lbb_3797                             if r1 == (1 as i32 as i64 as u64) { pc += 134 }
lbb_3663:
    mov64 r1, r6                                    r1 = r6
    and64 r1, 2                                     r1 &= 2   ///  r1 = r1.and(2)
    stxdw [r10-0xe0], r2                    
    jeq r1, 0, lbb_3682                             if r1 == (0 as i32 as i64 as u64) { pc += 15 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    ldxdw r3, [r10-0xc8]                    
    mov64 r4, 100                                   r4 = 100 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    ldxdw r0, [r10-0xd0]                    
    ldxdw r2, [r10-0xe0]                    
    ldxdw r1, [r10-0x8]                     
    stxdw [r10-0xc8], r1                    
    ldxdw r8, [r10-0x10]                    
    mov64 r1, r6                                    r1 = r6
    and64 r1, 254                                   r1 &= 254   ///  r1 = r1.and(254)
    jeq r1, 2, lbb_3797                             if r1 == (2 as i32 as i64 as u64) { pc += 115 }
lbb_3682:
    mov64 r1, r6                                    r1 = r6
    and64 r1, 4                                     r1 &= 4   ///  r1 = r1.and(4)
    jeq r1, 0, lbb_3700                             if r1 == (0 as i32 as i64 as u64) { pc += 15 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    ldxdw r3, [r10-0xc8]                    
    mov64 r4, 10000                                 r4 = 10000 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    ldxdw r0, [r10-0xd0]                    
    ldxdw r2, [r10-0xe0]                    
    ldxdw r1, [r10-0x18]                    
    stxdw [r10-0xc8], r1                    
    ldxdw r8, [r10-0x20]                    
    mov64 r1, r6                                    r1 = r6
    and64 r1, 252                                   r1 &= 252   ///  r1 = r1.and(252)
    jeq r1, 4, lbb_3797                             if r1 == (4 as i32 as i64 as u64) { pc += 97 }
lbb_3700:
    mov64 r1, r6                                    r1 = r6
    and64 r1, 8                                     r1 &= 8   ///  r1 = r1.and(8)
    jeq r1, 0, lbb_3718                             if r1 == (0 as i32 as i64 as u64) { pc += 15 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    ldxdw r3, [r10-0xc8]                    
    mov64 r4, 100000000                             r4 = 100000000 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    ldxdw r0, [r10-0xd0]                    
    ldxdw r2, [r10-0xe0]                    
    ldxdw r1, [r10-0x28]                    
    stxdw [r10-0xc8], r1                    
    ldxdw r8, [r10-0x30]                    
    mov64 r1, r6                                    r1 = r6
    and64 r1, 248                                   r1 &= 248   ///  r1 = r1.and(248)
    jeq r1, 8, lbb_3797                             if r1 == (8 as i32 as i64 as u64) { pc += 79 }
lbb_3718:
    mov64 r1, r6                                    r1 = r6
    and64 r1, 16                                    r1 &= 16   ///  r1 = r1.and(16)
    jeq r1, 0, lbb_3737                             if r1 == (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    ldxdw r3, [r10-0xc8]                    
    lddw r4, 0x2386f26fc10000                       r4 load str located at 10000000000000000
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    ldxdw r0, [r10-0xd0]                    
    ldxdw r2, [r10-0xe0]                    
    ldxdw r1, [r10-0x38]                    
    stxdw [r10-0xc8], r1                    
    ldxdw r8, [r10-0x40]                    
    mov64 r1, r6                                    r1 = r6
    and64 r1, 240                                   r1 &= 240   ///  r1 = r1.and(240)
    jeq r1, 16, lbb_3797                            if r1 == (16 as i32 as i64 as u64) { pc += 60 }
lbb_3737:
    mov64 r1, r6                                    r1 = r6
    and64 r1, 32                                    r1 &= 32   ///  r1 = r1.and(32)
    jeq r1, 0, lbb_3865                             if r1 == (0 as i32 as i64 as u64) { pc += 125 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    ldxdw r2, [r10-0xc8]                    
    stxdw [r10-0xc8], r2                    
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    lddw r4, 0x85acef8100000000                     r4 load str located at -8814407033341083648
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    lddw r4, 0x4ee2d6d415b                          r4 load str located at 5421010862427
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    lddw r4, 0x85acef8100000000                     r4 load str located at -8814407033341083648
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0xc8]                    
    jne r2, 0, lbb_3769                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_3769:
    ldxdw r2, [r10-0x58]                    
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_3773                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_3773:
    ldxdw r0, [r10-0x68]                    
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0xe0]                    
    jne r0, 0, lbb_3778                             if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_3778:
    ldxdw r0, [r10-0x70]                    
    ldxdw r8, [r10-0x60]                    
    add64 r8, r0                                    r8 += r0   ///  r8 = r8.wrapping_add(r0)
    ldxdw r3, [r10-0x48]                    
    mov64 r0, r3                                    r0 = r3
    add64 r0, r8                                    r0 += r8   ///  r0 = r0.wrapping_add(r8)
    stxdw [r10-0xc8], r0                    
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jlt r0, r3, lbb_3788                            if r0 < r3 { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_3788:
    and64 r6, 224                                   r6 &= 224   ///  r6 = r6.and(224)
    ldxdw r0, [r10-0xd0]                    
    jne r6, 32, lbb_3865                            if r6 != (32 as i32 as i64 as u64) { pc += 74 }
    or64 r1, r4                                     r1 |= r4   ///  r1 = r1.or(r4)
    or64 r1, r5                                     r1 |= r5   ///  r1 = r1.or(r5)
    or64 r1, r8                                     r1 |= r8   ///  r1 = r1.or(r8)
    ldxdw r8, [r10-0x50]                    
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_3865                             if r1 != (0 as i32 as i64 as u64) { pc += 68 }
lbb_3797:
    jne r2, 0, lbb_3824                             if r2 != (0 as i32 as i64 as u64) { pc += 26 }
    mov64 r1, r8                                    r1 = r8
    ldxdw r2, [r10-0xc8]                    
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    jeq r1, 0, lbb_3865                             if r1 == (0 as i32 as i64 as u64) { pc += 63 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0xd8]                    
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    ldxdw r2, [r10-0x80]                    
    ldxdw r3, [r10-0x78]                    
    mov64 r4, r8                                    r4 = r8
    ldxdw r5, [r10-0xc8]                    
    call function_10694                     
    ldxdw r0, [r10-0xd0]                    
    ldxdw r1, [r10-0x90]                    
    ldxdw r2, [r10-0x88]                    
    jne r2, 0, lbb_3865                             if r2 != (0 as i32 as i64 as u64) { pc += 45 }
lbb_3820:
    stxdw [r0+0x10], r1                     
    stxdw [r0+0x8], r7                      
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    ja lbb_3865                                     if true { pc += 41 }
lbb_3824:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    ldxdw r2, [r10-0xc8]                    
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r7                                    r4 = r7
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -160                                  r1 += -160   ///  r1 = r1.wrapping_add(-160 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r8                                    r4 = r8
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0xa8]                    
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_3843                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_3843:
    ldxdw r4, [r10-0x98]                    
    ldxdw r5, [r10-0xb0]                    
    mov64 r3, r4                                    r3 = r4
    add64 r3, r5                                    r3 += r5   ///  r3 = r3.wrapping_add(r5)
    ldxdw r0, [r10-0xd0]                    
    jlt r3, r4, lbb_3850                            if r3 < r4 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_3850:
    ldxdw r4, [r10-0xd8]                    
    jeq r4, 0, lbb_3865                             if r4 == (0 as i32 as i64 as u64) { pc += 13 }
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_3865                             if r1 != (0 as i32 as i64 as u64) { pc += 10 }
    ldxdw r2, [r10-0xa0]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -192                                  r1 += -192   ///  r1 = r1.wrapping_add(-192 as i32 as i64 as u64)
    ldxdw r4, [r10-0xd8]                    
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_10694                     
    ldxdw r0, [r10-0xd0]                    
    ldxdw r1, [r10-0xc0]                    
    ldxdw r2, [r10-0xb8]                    
    jeq r2, 0, lbb_3820                             if r2 == (0 as i32 as i64 as u64) { pc += -45 }
lbb_3865:
    stxdw [r0+0x0], r9                      
    exit                                    

function_3867:
    mov64 r8, r1                                    r8 = r1
    ldxb r0, [r2+0x300]                     
    ldxdw r7, [r2+0x20]                     
    mov64 r1, r7                                    r1 = r7
    add64 r1, r0                                    r1 += r0   ///  r1 = r1.wrapping_add(r0)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r1, r7, lbb_3875                            if r1 < r7 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_3875:
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    jne r0, 0, lbb_4529                             if r0 != (0 as i32 as i64 as u64) { pc += 652 }
    ldxdw r6, [r5-0x1000]                   
    jlt r1, r6, lbb_3880                            if r1 < r6 { pc += 1 }
    ja lbb_3883                                     if true { pc += 3 }
lbb_3880:
    stw [r8+0x8], 21                        
    stdw [r8+0x0], 1                        
    ja lbb_4516                                     if true { pc += 633 }
lbb_3883:
    ldxdw r0, [r5-0xff0]                    
    jsgt r0, -1, lbb_3886                           if (r0 as i64) > (-1 as i32 as i64) { pc += 1 }
    ja lbb_3915                                     if true { pc += 29 }
lbb_3886:
    stxdw [r10-0x180], r6                   
    stxdw [r10-0x1a8], r3                   
    stxdw [r10-0x198], r4                   
    ldxdw r1, [r5-0xfe0]                    
    stxdw [r10-0x1a0], r1                   
    ldxdw r1, [r5-0xfe8]                    
    stxdw [r10-0x1b8], r1                   
    ldxdw r1, [r5-0xff8]                    
    stxdw [r10-0x1d0], r1                   
    jne r1, 0, lbb_3897                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3918                                     if true { pc += 21 }
lbb_3897:
    ldxdw r3, [r2+0x10]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    stxdw [r10-0x1c8], r4                   
    mov64 r9, r2                                    r9 = r2
    mov64 r5, r0                                    r5 = r0
    call function_3372                      
    mov64 r6, 120                                   r6 = 120 as i32 as i64 as u64
    mov64 r2, 208                                   r2 = 208 as i32 as i64 as u64
    mov64 r1, 616                                   r1 = 616 as i32 as i64 as u64
    ldxdw r5, [r10-0x58]                    
    ldxdw r0, [r10-0x60]                    
    ldxdw r3, [r10-0x68]                    
    jne r3, 0, lbb_3935                             if r3 != (0 as i32 as i64 as u64) { pc += 23 }
lbb_3912:
    stw [r8+0x8], 5                         
    stdw [r8+0x0], 1                        
    ja lbb_4516                                     if true { pc += 601 }
lbb_3915:
    stw [r8+0x8], 10                        
    stdw [r8+0x0], 1                        
    ja lbb_4516                                     if true { pc += 598 }
lbb_3918:
    ldxdw r3, [r2+0x8]                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    mov64 r9, r2                                    r9 = r2
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r5, r0                                    r5 = r0
    call function_3372                      
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
    stxdw [r10-0x1c8], r1                   
    mov64 r6, 40                                    r6 = 40 as i32 as i64 as u64
    mov64 r2, 200                                   r2 = 200 as i32 as i64 as u64
    mov64 r1, 536                                   r1 = 536 as i32 as i64 as u64
    ldxdw r5, [r10-0x58]                    
    ldxdw r0, [r10-0x60]                    
    ldxdw r3, [r10-0x68]                    
    jne r3, 0, lbb_3935                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3912                                     if true { pc += -23 }
lbb_3935:
    mov64 r3, 376                                   r3 = 376 as i32 as i64 as u64
    ldxdw r4, [r10-0x1a0]                   
    jne r4, 0, lbb_3939                             if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 456                                   r3 = 456 as i32 as i64 as u64
lbb_3939:
    stxdw [r10-0x1d8], r3                   
    ldxdw r3, [r10-0x1b8]                   
    and64 r3, 255                                   r3 &= 255   ///  r3 = r3.and(255)
    jgt r3, 1, lbb_3945                             if r3 > (1 as i32 as i64 as u64) { pc += 2 }
    mov64 r3, 456                                   r3 = 456 as i32 as i64 as u64
    stxdw [r10-0x1d8], r3                   
lbb_3945:
    mov64 r3, r9                                    r3 = r9
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    ldxdw r2, [r3+0x0]                      
    ldxdw r3, [r10-0x180]                   
    jlt r3, r2, lbb_4291                            if r3 < r2 { pc += 341 }
    stxdw [r10-0x1e8], r0                   
    stxdw [r10-0x1e0], r5                   
    stxdw [r10-0x200], r8                   
    sub64 r3, r2                                    r3 -= r2   ///  r3 = r3.wrapping_sub(r2)
    stxdw [r10-0x1b0], r9                   
    ldxdw r8, [r9+0x2b8]                    
    mov64 r9, r3                                    r9 = r3
    jlt r3, r8, lbb_3959                            if r3 < r8 { pc += 1 }
    mov64 r9, r8                                    r9 = r8
lbb_3959:
    mov64 r2, 7                                     r2 = 7 as i32 as i64 as u64
    stxdw [r10-0x188], r2                   
    jeq r8, 0, lbb_4512                             if r8 == (0 as i32 as i64 as u64) { pc += 550 }
    ldxdw r2, [r10-0x1b0]                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    stxdw [r10-0x190], r2                   
    ldxdw r4, [r2+0x0]                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -120                                  r1 += -120   ///  r1 = r1.wrapping_add(-120 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x70]                    
    jne r2, 0, lbb_3976                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_3976:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    mov64 r3, r8                                    r3 = r8
    jne r1, 0, lbb_4512                             if r1 != (0 as i32 as i64 as u64) { pc += 533 }
    ldxdw r1, [r10-0x1b0]                   
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    ldxdw r2, [r10-0x78]                    
    div64 r2, r3                                    r2 /= r3   ///  r2 = r2 / r3
    stxdw [r10-0x1c0], r1                   
    ldxdw r3, [r1+0x0]                      
    mov64 r1, r3                                    r1 = r3
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jgt r1, r3, lbb_3991                            if r1 > r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_3991:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jne r2, 0, lbb_3994                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r1                                    r3 = r1
lbb_3994:
    stxdw [r10-0x68], r3                    
    ldxdw r1, [r10-0x190]                   
    ldxdw r4, [r1+0x8]                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -136                                  r1 += -136   ///  r1 = r1.wrapping_add(-136 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    ldxdw r1, [r10-0x80]                    
    jne r1, 0, lbb_4006                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_4006:
    and64 r6, 1                                     r6 &= 1   ///  r6 = r6.and(1)
    mov64 r1, r8                                    r1 = r8
    jne r6, 0, lbb_4512                             if r6 != (0 as i32 as i64 as u64) { pc += 503 }
    ldxdw r2, [r10-0x88]                    
    div64 r2, r1                                    r2 /= r1   ///  r2 = r2 / r1
    ldxdw r1, [r10-0x1c0]                   
    ldxdw r3, [r1+0x8]                      
    mov64 r1, r3                                    r1 = r3
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jgt r1, r3, lbb_4019                            if r1 > r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_4019:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jne r2, 0, lbb_4022                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r1                                    r3 = r1
lbb_4022:
    stxdw [r10-0x60], r3                    
    ldxdw r1, [r10-0x190]                   
    ldxdw r4, [r1+0x10]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -152                                  r1 += -152   ///  r1 = r1.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    ldxdw r1, [r10-0x90]                    
    jne r1, 0, lbb_4034                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_4034:
    and64 r6, 1                                     r6 &= 1   ///  r6 = r6.and(1)
    mov64 r1, r8                                    r1 = r8
    jne r6, 0, lbb_4512                             if r6 != (0 as i32 as i64 as u64) { pc += 475 }
    ldxdw r2, [r10-0x98]                    
    div64 r2, r1                                    r2 /= r1   ///  r2 = r2 / r1
    ldxdw r1, [r10-0x1c0]                   
    ldxdw r3, [r1+0x10]                     
    mov64 r1, r3                                    r1 = r3
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jgt r1, r3, lbb_4047                            if r1 > r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_4047:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jne r2, 0, lbb_4050                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r1                                    r3 = r1
lbb_4050:
    stxdw [r10-0x58], r3                    
    ldxdw r1, [r10-0x190]                   
    ldxdw r4, [r1+0x18]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -168                                  r1 += -168   ///  r1 = r1.wrapping_add(-168 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    ldxdw r1, [r10-0xa0]                    
    jne r1, 0, lbb_4062                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_4062:
    and64 r6, 1                                     r6 &= 1   ///  r6 = r6.and(1)
    mov64 r1, r8                                    r1 = r8
    jne r6, 0, lbb_4512                             if r6 != (0 as i32 as i64 as u64) { pc += 447 }
    ldxdw r2, [r10-0xa8]                    
    div64 r2, r1                                    r2 /= r1   ///  r2 = r2 / r1
    ldxdw r1, [r10-0x1c0]                   
    ldxdw r3, [r1+0x18]                     
    mov64 r1, r3                                    r1 = r3
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jgt r1, r3, lbb_4075                            if r1 > r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_4075:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jne r2, 0, lbb_4078                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r1                                    r3 = r1
lbb_4078:
    stxdw [r10-0x50], r3                    
    ldxdw r1, [r10-0x190]                   
    ldxdw r4, [r1+0x20]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -184                                  r1 += -184   ///  r1 = r1.wrapping_add(-184 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    ldxdw r1, [r10-0xb0]                    
    jne r1, 0, lbb_4090                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_4090:
    and64 r6, 1                                     r6 &= 1   ///  r6 = r6.and(1)
    mov64 r1, r8                                    r1 = r8
    jne r6, 0, lbb_4512                             if r6 != (0 as i32 as i64 as u64) { pc += 419 }
    ldxdw r2, [r10-0xb8]                    
    div64 r2, r1                                    r2 /= r1   ///  r2 = r2 / r1
    ldxdw r1, [r10-0x1c0]                   
    ldxdw r3, [r1+0x20]                     
    mov64 r1, r3                                    r1 = r3
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jgt r1, r3, lbb_4103                            if r1 > r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_4103:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jne r2, 0, lbb_4106                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r1                                    r3 = r1
lbb_4106:
    stxdw [r10-0x48], r3                    
    ldxdw r1, [r10-0x190]                   
    ldxdw r4, [r1+0x28]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -200                                  r1 += -200   ///  r1 = r1.wrapping_add(-200 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    ldxdw r1, [r10-0xc0]                    
    jne r1, 0, lbb_4118                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_4118:
    and64 r6, 1                                     r6 &= 1   ///  r6 = r6.and(1)
    mov64 r1, r8                                    r1 = r8
    jne r6, 0, lbb_4512                             if r6 != (0 as i32 as i64 as u64) { pc += 391 }
    ldxdw r2, [r10-0xc8]                    
    div64 r2, r1                                    r2 /= r1   ///  r2 = r2 / r1
    ldxdw r1, [r10-0x1c0]                   
    ldxdw r3, [r1+0x28]                     
    mov64 r1, r3                                    r1 = r3
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jgt r1, r3, lbb_4131                            if r1 > r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_4131:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jne r2, 0, lbb_4134                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r1                                    r3 = r1
lbb_4134:
    stxdw [r10-0x40], r3                    
    ldxdw r1, [r10-0x190]                   
    ldxdw r4, [r1+0x30]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -216                                  r1 += -216   ///  r1 = r1.wrapping_add(-216 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    ldxdw r1, [r10-0xd0]                    
    jne r1, 0, lbb_4146                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_4146:
    and64 r6, 1                                     r6 &= 1   ///  r6 = r6.and(1)
    mov64 r1, r8                                    r1 = r8
    jne r6, 0, lbb_4512                             if r6 != (0 as i32 as i64 as u64) { pc += 363 }
    ldxdw r2, [r10-0xd8]                    
    div64 r2, r1                                    r2 /= r1   ///  r2 = r2 / r1
    ldxdw r1, [r10-0x1c0]                   
    ldxdw r3, [r1+0x30]                     
    mov64 r1, r3                                    r1 = r3
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jgt r1, r3, lbb_4159                            if r1 > r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_4159:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jne r2, 0, lbb_4162                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r1                                    r3 = r1
lbb_4162:
    stxdw [r10-0x38], r3                    
    ldxdw r1, [r10-0x190]                   
    ldxdw r4, [r1+0x38]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -232                                  r1 += -232   ///  r1 = r1.wrapping_add(-232 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    ldxdw r1, [r10-0xe0]                    
    jne r1, 0, lbb_4174                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_4174:
    and64 r6, 1                                     r6 &= 1   ///  r6 = r6.and(1)
    mov64 r1, r8                                    r1 = r8
    jne r6, 0, lbb_4512                             if r6 != (0 as i32 as i64 as u64) { pc += 335 }
    ldxdw r2, [r10-0xe8]                    
    div64 r2, r1                                    r2 /= r1   ///  r2 = r2 / r1
    ldxdw r1, [r10-0x1c0]                   
    ldxdw r3, [r1+0x38]                     
    mov64 r1, r3                                    r1 = r3
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jgt r1, r3, lbb_4187                            if r1 > r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_4187:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jne r2, 0, lbb_4190                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r1                                    r3 = r1
lbb_4190:
    stxdw [r10-0x30], r3                    
    ldxdw r1, [r10-0x190]                   
    ldxdw r4, [r1+0x40]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -248                                  r1 += -248   ///  r1 = r1.wrapping_add(-248 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    ldxdw r1, [r10-0xf0]                    
    jne r1, 0, lbb_4202                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_4202:
    and64 r6, 1                                     r6 &= 1   ///  r6 = r6.and(1)
    mov64 r1, r8                                    r1 = r8
    jne r6, 0, lbb_4512                             if r6 != (0 as i32 as i64 as u64) { pc += 307 }
    ldxdw r2, [r10-0xf8]                    
    div64 r2, r1                                    r2 /= r1   ///  r2 = r2 / r1
    ldxdw r1, [r10-0x1c0]                   
    ldxdw r3, [r1+0x40]                     
    mov64 r1, r3                                    r1 = r3
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jgt r1, r3, lbb_4215                            if r1 > r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_4215:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jne r2, 0, lbb_4218                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r1                                    r3 = r1
lbb_4218:
    stxdw [r10-0x28], r3                    
    ldxdw r1, [r10-0x190]                   
    ldxdw r4, [r1+0x48]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -264                                  r1 += -264   ///  r1 = r1.wrapping_add(-264 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    ldxdw r1, [r10-0x100]                   
    jne r1, 0, lbb_4230                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_4230:
    and64 r6, 1                                     r6 &= 1   ///  r6 = r6.and(1)
    jne r6, 0, lbb_4512                             if r6 != (0 as i32 as i64 as u64) { pc += 280 }
    ldxdw r2, [r10-0x108]                   
    div64 r2, r8                                    r2 /= r8   ///  r2 = r2 / r8
    ldxdw r1, [r10-0x1c0]                   
    ldxdw r3, [r1+0x48]                     
    mov64 r1, r3                                    r1 = r3
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jgt r1, r3, lbb_4242                            if r1 > r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_4242:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jne r2, 0, lbb_4245                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r1                                    r3 = r1
lbb_4245:
    stxdw [r10-0x20], r3                    
    ldxdw r1, [r10-0x1b0]                   
    ldxb r1, [r1+0x310]                     
    mov64 r2, r7                                    r2 = r7
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r2, r7, lbb_4253                            if r2 < r7 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_4253:
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
    ldxdw r5, [r10-0x180]                   
    jne r3, 0, lbb_4257                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r2                                    r1 = r2
lbb_4257:
    ldxdw r3, [r10-0x1b0]                   
    ldxdw r2, [r3+0x2c0]                    
    ldxdw r4, [r3+0x318]                    
    jne r4, 0, lbb_4262                             if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r2                                    r4 = r2
lbb_4262:
    ldxdw r3, [r10-0x1a0]                   
    jne r3, 0, lbb_4265                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r2                                    r4 = r2
lbb_4265:
    mov64 r3, r5                                    r3 = r5
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jgt r3, r5, lbb_4270                            if r3 > r5 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_4270:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jne r1, 0, lbb_4273                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, r3                                    r2 = r3
lbb_4273:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -280                                  r1 += -280   ///  r1 = r1.wrapping_add(-280 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    ldxdw r1, [r10-0x110]                   
    jne r1, 0, lbb_4281                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_4281:
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    stxdw [r10-0x188], r1                   
    and64 r6, 1                                     r6 &= 1   ///  r6 = r6.and(1)
    jne r6, 0, lbb_4512                             if r6 != (0 as i32 as i64 as u64) { pc += 227 }
    ldxdw r9, [r10-0x118]                   
    ldxdw r1, [r10-0x1b8]                   
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    jeq r1, 0, lbb_4502                             if r1 == (0 as i32 as i64 as u64) { pc += 213 }
    jeq r1, 1, lbb_4294                             if r1 == (1 as i32 as i64 as u64) { pc += 4 }
    ja lbb_4305                                     if true { pc += 14 }
lbb_4291:
    stw [r8+0x8], 6                         
    stdw [r8+0x0], 1                        
    ja lbb_4516                                     if true { pc += 222 }
lbb_4294:
    ldxdw r1, [r10-0x1b0]                   
    ldxdw r1, [r1+0x2c8]                    
    mov64 r3, r9                                    r3 = r9
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r3, r9, lbb_4301                            if r3 < r9 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_4301:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_4512                             if r2 != (0 as i32 as i64 as u64) { pc += 209 }
lbb_4303:
    add64 r1, r9                                    r1 += r9   ///  r1 = r1.wrapping_add(r9)
    mov64 r9, r1                                    r9 = r1
lbb_4305:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxdw r8, [r10-0x1b0]                   
    ldxb r1, [r8+0x32c]                     
    stxdw [r10-0x1f8], r1                   
    ldxdw r1, [r8+0x18]                     
    stxdw [r10-0x1c0], r1                   
    ldxdw r1, [r10-0x1d8]                   
    add64 r8, r1                                    r8 += r1   ///  r8 = r8.wrapping_add(r1)
    stxdw [r10-0x1b0], r8                   
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x1a0], r1                   
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    stxdw [r10-0x1f0], r9                   
    ja lbb_4329                                     if true { pc += 9 }
lbb_4320:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_4512                             if r3 != (0 as i32 as i64 as u64) { pc += 190 }
    ldxdw r3, [r10-0x1a0]                   
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    mov64 r6, r8                                    r6 = r8
    stxdw [r10-0x1a0], r1                   
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r10-0x1a8]                   
    jeq r1, r2, lbb_4331                            if r1 == r2 { pc += 2 }
lbb_4329:
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    jne r8, 88, lbb_4337                            if r8 != (88 as i32 as i64 as u64) { pc += 6 }
lbb_4331:
    mov64 r1, 15                                    r1 = 15 as i32 as i64 as u64
    stxdw [r10-0x188], r1                   
    ldxdw r1, [r10-0x198]                   
    mov64 r7, r3                                    r7 = r3
    jlt r3, r1, lbb_4512                            if r3 < r1 { pc += 176 }
    ja lbb_4517                                     if true { pc += 180 }
lbb_4337:
    ldxdw r1, [r10-0x1b0]                   
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_4331                             if r2 == (0 as i32 as i64 as u64) { pc += -10 }
    stxdw [r10-0x1b8], r3                   
    mov64 r3, r2                                    r3 = r2
    add64 r3, r9                                    r3 += r9   ///  r3 = r3.wrapping_add(r9)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r3, r2, lbb_4347                            if r3 < r2 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_4347:
    mov64 r3, 12                                    r3 = 12 as i32 as i64 as u64
    stxdw [r10-0x188], r3                   
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_4512                             if r1 != (0 as i32 as i64 as u64) { pc += 161 }
    add64 r2, r9                                    r2 += r9   ///  r2 = r2.wrapping_add(r9)
    ldxdw r4, [r10-0x1c8]                   
    mov64 r5, r4                                    r5 = r4
    arsh64 r5, 63                                   r5 >>= 63 (signed)   ///  r5 = (r5 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -296                                  r1 += -296   ///  r1 = r1.wrapping_add(-296 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_11391                     
    stdw [r10-0x130], 0                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -304                                  r1 += -304   ///  r1 = r1.wrapping_add(-304 as i32 as i64 as u64)
    stxdw [r10-0xff8], r1                   
    stdw [r10-0x1000], 0                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -320                                  r1 += -320   ///  r1 = r1.wrapping_add(-320 as i32 as i64 as u64)
    ldxdw r2, [r10-0x128]                   
    ldxdw r3, [r10-0x120]                   
    mov64 r5, r10                                   r5 = r10
    ldxdw r4, [r10-0x1c0]                   
    call function_11004                     
    ldxdw r1, [r10-0x130]                   
    jne r1, 0, lbb_4373                             if r1 != (0 as i32 as i64 as u64) { pc += 0 }
lbb_4373:
    jne r1, 0, lbb_4512                             if r1 != (0 as i32 as i64 as u64) { pc += 138 }
    ldxdw r2, [r10-0x140]                   
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    lddw r3, 0xffffff172b4badc1                     r3 load str located at -1000000999999
    jlt r2, r3, lbb_4381                            if r2 < r3 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_4381:
    ldxdw r3, [r10-0x138]                   
    jslt r3, -1, lbb_4384                           if (r3 as i64) < (-1 as i32 as i64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_4384:
    jeq r3, -1, lbb_4386                            if r3 == (-1 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r4                                    r1 = r4
lbb_4386:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_4512                             if r1 != (0 as i32 as i64 as u64) { pc += 124 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -336                                  r1 += -336   ///  r1 = r1.wrapping_add(-336 as i32 as i64 as u64)
    mov64 r4, 1000000                               r4 = 1000000 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_10946                     
    stdw [r10-0x158], 0                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -344                                  r1 += -344   ///  r1 = r1.wrapping_add(-344 as i32 as i64 as u64)
    stxdw [r10-0xff8], r1                   
    ldxdw r2, [r10-0x150]                   
    mov64 r4, r2                                    r4 = r2
    add64 r4, 1000000                               r4 += 1000000   ///  r4 = r4.wrapping_add(1000000 as i32 as i64 as u64)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r4, r2, lbb_4403                            if r4 < r2 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_4403:
    ldxdw r2, [r10-0x148]                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    stxdw [r10-0x1000], r2                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -360                                  r1 += -360   ///  r1 = r1.wrapping_add(-360 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0x1e8]                   
    ldxdw r3, [r10-0x1e0]                   
    call function_11004                     
    ldxdw r1, [r10-0x158]                   
    jne r1, 0, lbb_4414                             if r1 != (0 as i32 as i64 as u64) { pc += 0 }
lbb_4414:
    jne r1, 0, lbb_4512                             if r1 != (0 as i32 as i64 as u64) { pc += 97 }
    ldxdw r2, [r10-0x168]                   
    mov64 r3, r2                                    r3 = r2
    add64 r3, 999999                                r3 += 999999   ///  r3 = r3.wrapping_add(999999 as i32 as i64 as u64)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r3, r2, lbb_4422                            if r3 < r2 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_4422:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jgt r3, 999998, lbb_4425                        if r3 > (999998 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_4425:
    ldxdw r3, [r10-0x160]                   
    mov64 r0, r3                                    r0 = r3
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    jgt r0, 1000000, lbb_4430                       if r0 > (1000000 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_4430:
    jeq r0, 1000000, lbb_4432                       if r0 == (1000000 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r1                                    r4 = r1
lbb_4432:
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jne r4, 0, lbb_4512                             if r4 != (0 as i32 as i64 as u64) { pc += 78 }
    ldxdw r1, [r10-0x198]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -376                                  r1 += -376   ///  r1 = r1.wrapping_add(-376 as i32 as i64 as u64)
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r4, 1000000                               r4 = 1000000 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_10946                     
    mov64 r7, r10                                   r7 = r10
    add64 r7, -104                                  r7 += -104   ///  r7 = r7.wrapping_add(-104 as i32 as i64 as u64)
    add64 r7, r6                                    r7 += r6   ///  r7 = r7.wrapping_add(r6)
    ldxdw r1, [r10-0x190]                   
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    ldxdw r3, [r1+0x0]                      
    ldxdw r6, [r7+0x0]                      
    mov64 r1, r3                                    r1 = r3
    sub64 r1, r6                                    r1 -= r6   ///  r1 = r1.wrapping_sub(r6)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jgt r1, r3, lbb_4453                            if r1 > r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_4453:
    jne r2, 0, lbb_4455                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r9, r1                                    r9 = r1
lbb_4455:
    ldxdw r1, [r10-0x1f8]                   
    stxdw [r10-0xff0], r1                   
    ldxdw r1, [r10-0x178]                   
    stxdw [r10-0x1000], r1                  
    stxdw [r10-0xff8], r9                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0x1d0]                   
    ldxdw r3, [r10-0x1a8]                   
    ldxdw r4, [r10-0x1a0]                   
    call function_3636                      
    mov64 r1, 9                                     r1 = 9 as i32 as i64 as u64
    stxdw [r10-0x188], r1                   
    ldxdw r1, [r10-0x18]                    
    ldxdw r2, [r10-0x198]                   
    ldxdw r2, [r10-0x180]                   
    ldxdw r9, [r10-0x1f0]                   
    jne r1, 1, lbb_4512                             if r1 != (1 as i32 as i64 as u64) { pc += 38 }
    ldxdw r1, [r10-0x10]                    
    mov64 r3, r6                                    r3 = r6
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r3, r6, lbb_4480                            if r3 < r6 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_4480:
    mov64 r3, 16                                    r3 = 16 as i32 as i64 as u64
    stxdw [r10-0x188], r3                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_4512                             if r2 != (0 as i32 as i64 as u64) { pc += 28 }
    ldxdw r2, [r10-0x8]                     
    mov64 r3, r1                                    r3 = r1
    add64 r3, r6                                    r3 += r6   ///  r3 = r3.wrapping_add(r6)
    stxdw [r7+0x0], r3                      
    ldxdw r4, [r10-0x1b8]                   
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r2, r4, lbb_4493                            if r2 < r4 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_4493:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_4512                             if r3 != (0 as i32 as i64 as u64) { pc += 17 }
    ldxdw r5, [r10-0x1a0]                   
    mov64 r4, r5                                    r4 = r5
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r4, r5, lbb_4320                            if r4 < r5 { pc += -180 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ja lbb_4320                                     if true { pc += -182 }
lbb_4502:
    ldxdw r1, [r10-0x1b0]                   
    ldxdw r1, [r1+0x308]                    
    mov64 r3, r9                                    r3 = r9
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r3, r9, lbb_4509                            if r3 < r9 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_4509:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_4512                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4303                                     if true { pc += -209 }
lbb_4512:
    ldxdw r1, [r10-0x200]                   
    ldxdw r2, [r10-0x188]                   
    stxw [r1+0x8], r2                       
    stdw [r1+0x0], 1                        
lbb_4516:
    exit                                    
lbb_4517:
    ldxdw r6, [r10-0x200]                   
    mov64 r1, r6                                    r1 = r6
    add64 r1, 16                                    r1 += 16   ///  r1 = r1.wrapping_add(16 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -104                                  r2 += -104   ///  r2 = r2.wrapping_add(-104 as i32 as i64 as u64)
    mov64 r3, 80                                    r3 = 80 as i32 as i64 as u64
    call function_9980                      
    ldxdw r1, [r10-0x180]                   
    stxdw [r6+0x60], r1                     
    stxdw [r6+0x8], r7                      
    stw [r6+0x0], 0                         
    ja lbb_4516                                     if true { pc += -13 }
lbb_4529:
    lddw r1, 0x100017970 --> b"\x00\x00\x00\x00\xf1g\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x00\xb5\x02\x00…        r1 load str located at 4295063920
    call function_9564                      

function_4532:
    mov64 r0, 3                                     r0 = 3 as i32 as i64 as u64
    ldxdw r6, [r4+0x0]                      
    ldxdw r7, [r6+0x50]                     
    jne r7, 165, lbb_4565                           if r7 != (165 as i32 as i64 as u64) { pc += 29 }
    ldxdw r7, [r6+0x28]                     
    lddw r8, 0x93a165d7e1f6dd06                     r8 load str located at -7808848301000303354
    jne r7, r8, lbb_4565                            if r7 != r8 { pc += 25 }
    ldxdw r7, [r6+0x30]                     
    lddw r8, 0xac79ebce46e1cbd9                     r8 load str located at -6018520155818964007
    jne r7, r8, lbb_4565                            if r7 != r8 { pc += 21 }
    ldxdw r7, [r6+0x38]                     
    lddw r8, 0x91375b5fed85b41c                     r8 load str located at -7982811346925931492
    jne r7, r8, lbb_4565                            if r7 != r8 { pc += 17 }
    ldxdw r6, [r6+0x40]                     
    lddw r7, 0xa900ff7e85f58c3a                     r7 load str located at -6268729762421306310
    jne r6, r7, lbb_4565                            if r6 != r7 { pc += 13 }
    mov64 r7, r3                                    r7 = r3
    mov64 r8, r5                                    r8 = r5
    mov64 r9, r2                                    r9 = r2
    stxdw [r10-0x98], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r6, r4                                    r6 = r4
    mov64 r2, r4                                    r2 = r4
    call function_7233                      
    ldxdw r3, [r10-0x60]                    
    jne r3, 0, lbb_4570                             if r3 != (0 as i32 as i64 as u64) { pc += 7 }
    ldxdw r0, [r10-0x58]                    
    ldxdw r1, [r10-0x98]                    
lbb_4565:
    mov64 r3, r0                                    r3 = r0
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
lbb_4567:
    stxw [r1+0x4], r3                       
    stxw [r1+0x0], r0                       
    exit                                    
lbb_4570:
    ldxb r1, [r10-0x48]                     
    jgt r1, 7, lbb_4632                             if r1 > (7 as i32 as i64 as u64) { pc += 60 }
    ldxdw r4, [r10-0x50]                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lsh64 r2, r1                                    r2 <<= r1   ///  r2 = r2.wrapping_shl(r1 as u32)
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    ldxb r5, [r4+0x0]                       
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    mov64 r0, r5                                    r0 = r5
    and64 r0, 255                                   r0 &= 255   ///  r0 = r0.and(255)
    ldxdw r1, [r10-0x98]                    
    mov64 r2, r9                                    r2 = r9
    jne r0, r5, lbb_4583                            if r0 != r5 { pc += 0 }
lbb_4583:
    jne r0, r5, lbb_4635                            if r0 != r5 { pc += 51 }
    ldxdw r9, [r3+0x40]                     
    stxb [r4+0x0], r5                       
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 26                                    r3 = 26 as i32 as i64 as u64
    jeq r9, 0, lbb_4590                             if r9 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4567                                     if true { pc += -23 }
lbb_4590:
    stxdw [r10-0x78], r7                    
    stxdw [r10-0x80], r8                    
    stxdw [r10-0x88], r6                    
    ldxdw r1, [r2+0x0]                      
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r3, [r2+0x328]                     
    stxdw [r10-0x20], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    mov64 r1, r2                                    r1 = r2
    add64 r1, 280                                   r1 += 280   ///  r1 = r1.wrapping_add(280 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    add64 r2, 248                                   r2 += 248   ///  r2 = r2.wrapping_add(248 as i32 as i64 as u64)
    stxdw [r10-0x50], r2                    
    lddw r1, 0x1000167eb --> b"marketprogram/src/state/market.rsvault\x04y\xd5[\xf21\xc0n\xeet\xc5n"        r1 load str located at 4295059435
    stxdw [r10-0x60], r1                    
    stxb [r10-0x1], r3                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x70], r1                    
    stdw [r10-0x18], 1                      
    stdw [r10-0x28], 8                      
    stdw [r10-0x38], 32                     
    stdw [r10-0x48], 32                     
    stdw [r10-0x58], 6                      
    stdw [r10-0x68], 5                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -136                                  r2 += -136   ///  r2 = r2.wrapping_add(-136 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -112                                  r3 += -112   ///  r3 = r3.wrapping_add(-112 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_6471                      
    ldxdw r1, [r10-0x98]                    
    ldxw r3, [r10-0x8c]                     
    ldxw r0, [r10-0x90]                     
    ja lbb_4567                                     if true { pc += -65 }
lbb_4632:
    lddw r1, 0x100017690 --> b"\x00\x00\x00\x00\xe8f\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xe1\x02\x00…        r1 load str located at 4295063184
    call function_9597                      
lbb_4635:
    lddw r1, 0x1000176a8 --> b"\x00\x00\x00\x00\xe8f\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xe1\x02\x00…        r1 load str located at 4295063208
    call function_9564                      

function_4638:
    stxdw [r10-0x18], r5                    
    mov64 r8, r4                                    r8 = r4
    mov64 r7, r3                                    r7 = r3
    mov64 r9, r2                                    r9 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r8+0x0]                      
    add64 r2, 312                                   r2 += 312   ///  r2 = r2.wrapping_add(312 as i32 as i64 as u64)
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    mov64 r1, 3                                     r1 = 3 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_4680                             if r0 != (0 as i32 as i64 as u64) { pc += 28 }
    mov64 r3, r9                                    r3 = r9
    add64 r3, 248                                   r3 += 248   ///  r3 = r3.wrapping_add(248 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -8                                    r1 += -8   ///  r1 = r1.wrapping_add(-8 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r4, r7                                    r4 = r7
    call function_4705                      
    ldxw r1, [r10-0x8]                      
    jne r1, 26, lbb_4680                            if r1 != (26 as i32 as i64 as u64) { pc += 19 }
    ldxdw r1, [r10-0x18]                    
    ldxdw r1, [r1+0x0]                      
    mov64 r2, r9                                    r2 = r9
    add64 r2, 344                                   r2 += 344   ///  r2 = r2.wrapping_add(344 as i32 as i64 as u64)
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r1, 3                                     r1 = 3 as i32 as i64 as u64
    jne r0, 0, lbb_4680                             if r0 != (0 as i32 as i64 as u64) { pc += 8 }
    add64 r9, 280                                   r9 += 280   ///  r9 = r9.wrapping_add(280 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    ldxdw r2, [r10-0x18]                    
    mov64 r3, r9                                    r3 = r9
    mov64 r4, r7                                    r4 = r7
    call function_4705                      
    ldxw r1, [r10-0x10]                     
lbb_4680:
    stxw [r6+0x0], r1                       
    exit                                    

function_4682:
    mov64 r8, r5                                    r8 = r5
    mov64 r7, r3                                    r7 = r3
    mov64 r9, r2                                    r9 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r3, r9                                    r3 = r9
    add64 r3, 248                                   r3 += 248   ///  r3 = r3.wrapping_add(248 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -8                                    r1 += -8   ///  r1 = r1.wrapping_add(-8 as i32 as i64 as u64)
    mov64 r2, r4                                    r2 = r4
    mov64 r4, r7                                    r4 = r7
    call function_4705                      
    ldxw r1, [r10-0x8]                      
    jne r1, 26, lbb_4703                            if r1 != (26 as i32 as i64 as u64) { pc += 8 }
    add64 r9, 280                                   r9 += 280   ///  r9 = r9.wrapping_add(280 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r9                                    r3 = r9
    mov64 r4, r7                                    r4 = r7
    call function_4705                      
    ldxw r1, [r10-0x10]                     
lbb_4703:
    stxw [r6+0x0], r1                       
    exit                                    

function_4705:
    mov64 r9, 3                                     r9 = 3 as i32 as i64 as u64
    ldxdw r5, [r2+0x0]                      
    ldxdw r0, [r5+0x50]                     
    jne r0, 165, lbb_4786                           if r0 != (165 as i32 as i64 as u64) { pc += 77 }
    ldxdw r0, [r5+0x28]                     
    lddw r6, 0x93a165d7e1f6dd06                     r6 load str located at -7808848301000303354
    jne r0, r6, lbb_4786                            if r0 != r6 { pc += 73 }
    ldxdw r0, [r5+0x30]                     
    lddw r6, 0xac79ebce46e1cbd9                     r6 load str located at -6018520155818964007
    jne r0, r6, lbb_4786                            if r0 != r6 { pc += 69 }
    ldxdw r0, [r5+0x38]                     
    lddw r6, 0x91375b5fed85b41c                     r6 load str located at -7982811346925931492
    jne r0, r6, lbb_4786                            if r0 != r6 { pc += 65 }
    ldxdw r5, [r5+0x40]                     
    lddw r0, 0xa900ff7e85f58c3a                     r0 load str located at -6268729762421306310
    jne r5, r0, lbb_4786                            if r5 != r0 { pc += 61 }
    mov64 r8, r4                                    r8 = r4
    mov64 r6, r3                                    r6 = r3
    stxdw [r10-0x28], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    call function_7233                      
    ldxdw r7, [r10-0x20]                    
    jne r7, 0, lbb_4735                             if r7 != (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r10-0x28]                    
    ja lbb_4786                                     if true { pc += 51 }
lbb_4735:
    ldxb r1, [r10-0x8]                      
    stxdw [r10-0x30], r1                    
    ldxdw r1, [r10-0x10]                    
    stxdw [r10-0x38], r1                    
    mov64 r1, r7                                    r1 = r7
    add64 r1, 32                                    r1 += 32   ///  r1 = r1.wrapping_add(32 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    mov64 r9, 22                                    r9 = 22 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_4772                             if r0 != (0 as i32 as i64 as u64) { pc += 24 }
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    mov64 r9, 3                                     r9 = 3 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_4772                             if r0 != (0 as i32 as i64 as u64) { pc += 16 }
    ldxdw r1, [r10-0x30]                    
    jgt r1, 7, lbb_4788                             if r1 > (7 as i32 as i64 as u64) { pc += 30 }
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    lsh64 r3, r1                                    r3 <<= r1   ///  r3 = r3.wrapping_shl(r1 as u32)
    and64 r3, 255                                   r3 &= 255   ///  r3 = r3.and(255)
    ldxdw r4, [r10-0x38]                    
    ldxb r2, [r4+0x0]                       
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    mov64 r3, r2                                    r3 = r2
    and64 r3, 255                                   r3 &= 255   ///  r3 = r3.and(255)
    ldxdw r1, [r10-0x28]                    
    jne r3, r2, lbb_4768                            if r3 != r2 { pc += 0 }
lbb_4768:
    jne r3, r2, lbb_4791                            if r3 != r2 { pc += 22 }
    stxb [r4+0x0], r2                       
    mov64 r9, 26                                    r9 = 26 as i32 as i64 as u64
    ja lbb_4786                                     if true { pc += 14 }
lbb_4772:
    ldxdw r1, [r10-0x30]                    
    jgt r1, 7, lbb_4788                             if r1 > (7 as i32 as i64 as u64) { pc += 14 }
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    lsh64 r3, r1                                    r3 <<= r1   ///  r3 = r3.wrapping_shl(r1 as u32)
    and64 r3, 255                                   r3 &= 255   ///  r3 = r3.and(255)
    ldxdw r4, [r10-0x38]                    
    ldxb r2, [r4+0x0]                       
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    mov64 r3, r2                                    r3 = r2
    and64 r3, 255                                   r3 &= 255   ///  r3 = r3.and(255)
    ldxdw r1, [r10-0x28]                    
    jne r3, r2, lbb_4784                            if r3 != r2 { pc += 0 }
lbb_4784:
    jne r3, r2, lbb_4791                            if r3 != r2 { pc += 6 }
    stxb [r4+0x0], r2                       
lbb_4786:
    stxw [r1+0x0], r9                       
    exit                                    
lbb_4788:
    lddw r1, 0x100017690 --> b"\x00\x00\x00\x00\xe8f\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xe1\x02\x00…        r1 load str located at 4295063184
    call function_9597                      
lbb_4791:
    lddw r1, 0x1000176a8 --> b"\x00\x00\x00\x00\xe8f\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xe1\x02\x00…        r1 load str located at 4295063208
    call function_9564                      

function_4794:
    mov64 r8, r4                                    r8 = r4
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -76                                   r1 += -76   ///  r1 = r1.wrapping_add(-76 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_5580                      
    ldxb r1, [r10-0x4c]                     
    jne r1, 0, lbb_4840                             if r1 != (0 as i32 as i64 as u64) { pc += 37 }
    ldxb r1, [r10-0x2b]                     
    stxb [r10-0x8], r1                      
    ldxdw r1, [r10-0x33]                    
    stxdw [r10-0x10], r1                    
    ldxdw r1, [r10-0x3b]                    
    stxdw [r10-0x18], r1                    
    ldxdw r1, [r10-0x43]                    
    stxdw [r10-0x20], r1                    
    ldxdw r9, [r10-0x4b]                    
    stxdw [r10-0x28], r9                    
    ldxb r1, [r10-0x2a]                     
    stxdw [r10-0x58], r1                    
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r8                                    r2 = r8
    call function_5464                      
    mov64 r1, r0                                    r1 = r0
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, 3, lbb_4823                             if r1 == (3 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, r1                                    r0 = r1
lbb_4823:
    jne r1, 3, lbb_4833                             if r1 != (3 as i32 as i64 as u64) { pc += 9 }
    and64 r9, 1                                     r9 &= 1   ///  r9 = r9.and(1)
    jeq r9, 0, lbb_4833                             if r9 == (0 as i32 as i64 as u64) { pc += 7 }
    mov64 r2, r10                                   r2 = r10
    add64 r2, -39                                   r2 += -39   ///  r2 = r2.wrapping_add(-39 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_5464                      
    and64 r0, 255                                   r0 &= 255   ///  r0 = r0.and(255)
    jne r0, 3, lbb_4833                             if r0 != (3 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_4833:
    ldxdw r1, [r10-0x58]                    
    stxb [r6+0x8], r1                       
    and64 r0, 255                                   r0 &= 255   ///  r0 = r0.and(255)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    or64 r0, 26                                     r0 |= 26   ///  r0 = r0.or(26)
    stxdw [r6+0x0], r0                      
    ja lbb_4843                                     if true { pc += 3 }
lbb_4840:
    stb [r6+0x8], 0                         
    stb [r6+0x4], 1                         
    stw [r6+0x0], 26                        
lbb_4843:
    exit                                    

function_4844:
    mov64 r7, r5                                    r7 = r5
    stxdw [r10-0x148], r4                   
    mov64 r6, r3                                    r6 = r3
    mov64 r8, r2                                    r8 = r2
    mov64 r9, r1                                    r9 = r1
    ldxdw r1, [r7-0xff8]                    
    stxdw [r10-0x108], r1                   
    ldxdw r1, [r7-0x1000]                   
    stxdw [r10-0x110], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -152                                  r1 += -152   ///  r1 = r1.wrapping_add(-152 as i32 as i64 as u64)
    call function_7274                      
    ldxw r1, [r10-0x98]                     
    jne r1, 0, lbb_5066                             if r1 != (0 as i32 as i64 as u64) { pc += 208 }
    ldxdw r1, [r10-0x148]                   
    stxdw [r10-0x180], r6                   
    stxdw [r10-0x158], r9                   
    ldxdw r1, [r7-0xfe8]                    
    ldxdw r2, [r7-0xff0]                    
    stxdw [r10-0x178], r2                   
    ldxw r2, [r10-0x8c]                     
    stxdw [r10-0x168], r2                   
    ldxw r2, [r10-0x90]                     
    stxdw [r10-0x170], r2                   
    ldxdw r6, [r10-0x88]                    
    ldxdw r7, [r1+0x8]                      
    ldxdw r2, [r1+0x10]                     
    stxdw [r10-0x150], r2                   
    ldxdw r9, [r1+0x0]                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -152                                  r1 += -152   ///  r1 = r1.wrapping_add(-152 as i32 as i64 as u64)
    stxdw [r10-0x160], r8                   
    mov64 r2, r8                                    r2 = r8
    call function_7215                      
    jne r9, 0, lbb_4880                             if r9 != (0 as i32 as i64 as u64) { pc += 1 }
    stxdw [r10-0x150], r6                   
lbb_4880:
    mov64 r3, r7                                    r3 = r7
    jne r9, 0, lbb_4883                             if r9 != (0 as i32 as i64 as u64) { pc += 1 }
    ldxdw r3, [r10-0x170]                   
lbb_4883:
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    jne r9, 0, lbb_4886                             if r9 != (0 as i32 as i64 as u64) { pc += 1 }
    ldxdw r7, [r10-0x168]                   
lbb_4886:
    ldxw r1, [r10-0x8c]                     
    ldxw r2, [r10-0x90]                     
    ldxdw r4, [r10-0x98]                    
    ldxdw r6, [r10-0x148]                   
    jeq r4, 0, lbb_5364                             if r4 == (0 as i32 as i64 as u64) { pc += 473 }
    ldxdw r5, [r10-0x88]                    
    and64 r5, 255                                   r5 &= 255   ///  r5 = r5.and(255)
    ldxdw r9, [r10-0x158]                   
    jgt r5, 7, lbb_5368                             if r5 > (7 as i32 as i64 as u64) { pc += 473 }
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    lsh64 r0, r5                                    r0 <<= r5   ///  r0 = r0.wrapping_shl(r5 as u32)
    and64 r0, 255                                   r0 &= 255   ///  r0 = r0.and(255)
    ldxb r2, [r1+0x0]                       
    add64 r2, r0                                    r2 += r0   ///  r2 = r2.wrapping_add(r0)
    mov64 r5, r2                                    r5 = r2
    and64 r5, 255                                   r5 &= 255   ///  r5 = r5.and(255)
    jne r5, r2, lbb_4905                            if r5 != r2 { pc += 0 }
lbb_4905:
    jne r5, r2, lbb_5371                            if r5 != r2 { pc += 465 }
    ldxdw r8, [r4+0x0]                      
    stxb [r1+0x0], r2                       
    mov64 r2, r6                                    r2 = r6
    add64 r2, 128                                   r2 += 128   ///  r2 = r2.wrapping_add(128 as i32 as i64 as u64)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r2, r6, lbb_4913                            if r2 < r6 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_4913:
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    or64 r7, r3                                     r7 |= r3   ///  r7 = r7.or(r3)
    lddw r3, 0x3ff0000000000000                     r3 load str located at 4607182418800017408
    ldxdw r6, [r10-0x150]                   
    jeq r6, r3, lbb_5090                            if r6 == r3 { pc += 169 }
    lddw r3, 0x4000000000000000                     r3 load str located at 4611686018427387904
    jeq r6, r3, lbb_5069                            if r6 == r3 { pc += 145 }
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_5383                             if r1 != (0 as i32 as i64 as u64) { pc += 457 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -320                                  r1 += -320   ///  r1 = r1.wrapping_add(-320 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r7                                    r4 = r7
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x138]                   
    jne r2, 0, lbb_4936                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_4936:
    stxdw [r10-0x168], r8                   
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_5389                             if r1 != (0 as i32 as i64 as u64) { pc += 450 }
    ldxdw r1, [r10-0x140]                   
    call function_11237                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r0                                    r2 = r0
    call function_10944                     
    mov64 r7, r0                                    r7 = r0
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_11400                     
    mov64 r8, r0                                    r8 = r0
    mov64 r1, r7                                    r1 = r7
    call function_11300                     
    jslt r8, 0, lbb_4954                            if (r8 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r6, r0                                    r6 = r0
lbb_4954:
    mov64 r1, r7                                    r1 = r7
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_9997                      
    mov64 r8, -1                                    r8 = -1 as i32 as i64 as u64
    jsgt r0, 0, lbb_4961                            if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r8, r6                                    r8 = r6
lbb_4961:
    ldxdw r6, [r10-0x160]                   
    ldxdw r1, [r10-0x168]                   
    jeq r1, 0, lbb_4965                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5108                                     if true { pc += 143 }
lbb_4965:
    ldxdw r1, [r10-0x178]                   
    ldxdw r3, [r1+0x0]                      
    ldxdw r2, [r6+0x0]                      
    mov64 r1, r2                                    r1 = r2
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xe0], r1                    
    mov64 r4, r3                                    r4 = r3
    add64 r4, 8                                     r4 += 8   ///  r4 = r4.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xf0], r4                    
    sth [r10-0xd8], 257                     
    sth [r10-0xe8], 257                     
    ldxdw r5, [r10-0x148]                   
    stxdw [r10-0xc4], r5                    
    stxdw [r10-0xcc], r8                    
    stw [r10-0xd0], 0                       
    ldxdw r0, [r10-0x180]                   
    ldxdw r5, [r0+0x0]                      
    stxdw [r10-0xbc], r5                    
    ldxdw r5, [r0+0x8]                      
    stxdw [r10-0xb4], r5                    
    ldxdw r5, [r0+0x10]                     
    stxdw [r10-0xac], r5                    
    ldxdw r5, [r0+0x18]                     
    stxdw [r10-0xa4], r5                    
    ldxb r5, [r3+0x0]                       
    jne r5, 255, lbb_5145                           if r5 != (255 as i32 as i64 as u64) { pc += 154 }
    ldxb r6, [r3+0x1]                       
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r6, 0, lbb_4996                             if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_4996:
    ldxb r7, [r3+0x2]                       
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jne r7, 0, lbb_5000                             if r7 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_5000:
    ldxb r7, [r3+0x3]                       
    jne r7, 0, lbb_5003                             if r7 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_5003:
    ldxdw r7, [r3+0x50]                     
    mov64 r8, r3                                    r8 = r3
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r8                    
    mov64 r8, r3                                    r8 = r3
    add64 r8, 88                                    r8 += 88   ///  r8 = r8.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r8                    
    stxdw [r10-0x88], r7                    
    add64 r3, 72                                    r3 += 72   ///  r3 = r3.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r3                    
    stxdw [r10-0x98], r4                    
    stxb [r10-0x66], r0                     
    stxb [r10-0x67], r6                     
    stxb [r10-0x68], r5                     
    stdw [r10-0x70], 0                      
    ldxb r3, [r2+0x0]                       
    jne r3, 255, lbb_5145                           if r3 != (255 as i32 as i64 as u64) { pc += 125 }
    ldxb r3, [r2+0x1]                       
    ldxb r4, [r2+0x2]                       
    ldxb r5, [r2+0x3]                       
    ldxdw r0, [r2+0x50]                     
    mov64 r6, r2                                    r6 = r2
    add64 r6, 40                                    r6 += 40   ///  r6 = r6.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r6                    
    mov64 r6, r2                                    r6 = r2
    add64 r6, 88                                    r6 += 88   ///  r6 = r6.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r6                    
    stxdw [r10-0x50], r0                    
    add64 r2, 72                                    r2 += 72   ///  r2 = r2.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r2                    
    stxdw [r10-0x60], r1                    
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r5, 0, lbb_5037                             if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_5037:
    stxb [r10-0x2e], r1                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_5041                             if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_5041:
    stxb [r10-0x2f], r1                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_5045                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_5045:
    stxb [r10-0x30], r1                     
    stdw [r10-0x38], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -240                                  r1 += -240   ///  r1 = r1.wrapping_add(-240 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100016650 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295059024
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 52                      
    stdw [r10-0x18], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -152                                  r2 += -152   ///  r2 = r2.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -272                                  r4 += -272   ///  r4 = r4.wrapping_add(-272 as i32 as i64 as u64)
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    ja lbb_5359                                     if true { pc += 293 }
lbb_5066:
    ldxw r1, [r10-0x90]                     
    ldxw r2, [r10-0x94]                     
    ja lbb_5365                                     if true { pc += 296 }
lbb_5069:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_5377                             if r1 != (0 as i32 as i64 as u64) { pc += 306 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -288                                  r1 += -288   ///  r1 = r1.wrapping_add(-288 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r7                                    r4 = r7
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x118]                   
    jne r2, 0, lbb_5081                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_5081:
    mov64 r2, r8                                    r2 = r8
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    ldxdw r6, [r10-0x160]                   
    jne r1, 0, lbb_5374                             if r1 != (0 as i32 as i64 as u64) { pc += 289 }
    ldxdw r8, [r10-0x120]                   
    jslt r8, 0, lbb_5374                            if (r8 as i64) < (0 as i32 as i64) { pc += 287 }
    lsh64 r8, 1                                     r8 <<= 1   ///  r8 = r8.wrapping_shl(1)
    jeq r2, 0, lbb_4965                             if r2 == (0 as i32 as i64 as u64) { pc += -124 }
    ja lbb_5108                                     if true { pc += 18 }
lbb_5090:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_5380                             if r1 != (0 as i32 as i64 as u64) { pc += 288 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -304                                  r1 += -304   ///  r1 = r1.wrapping_add(-304 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r7                                    r4 = r7
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x128]                   
    jne r2, 0, lbb_5102                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_5102:
    mov64 r2, r8                                    r2 = r8
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    ldxdw r6, [r10-0x160]                   
    jne r1, 0, lbb_5386                             if r1 != (0 as i32 as i64 as u64) { pc += 280 }
    ldxdw r8, [r10-0x130]                   
    jeq r2, 0, lbb_4965                             if r2 == (0 as i32 as i64 as u64) { pc += -143 }
lbb_5108:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -152                                  r1 += -152   ///  r1 = r1.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    call function_7215                      
    ldxw r1, [r10-0x8c]                     
    ldxw r2, [r10-0x90]                     
    ldxdw r3, [r10-0x98]                    
    jeq r3, 0, lbb_5365                             if r3 == (0 as i32 as i64 as u64) { pc += 249 }
    ldxdw r4, [r10-0x88]                    
    and64 r4, 255                                   r4 &= 255   ///  r4 = r4.and(255)
    jgt r4, 7, lbb_5368                             if r4 > (7 as i32 as i64 as u64) { pc += 249 }
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    lsh64 r5, r4                                    r5 <<= r4   ///  r5 = r5.wrapping_shl(r4 as u32)
    and64 r5, 255                                   r5 &= 255   ///  r5 = r5.and(255)
    ldxb r2, [r1+0x0]                       
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    mov64 r4, r2                                    r4 = r2
    and64 r4, 255                                   r4 &= 255   ///  r4 = r4.and(255)
    jne r4, r2, lbb_5129                            if r4 != r2 { pc += 0 }
lbb_5129:
    jne r4, r2, lbb_5371                            if r4 != r2 { pc += 241 }
    ldxdw r3, [r3+0x0]                      
    stxb [r1+0x0], r2                       
    ldxdw r7, [r6+0x0]                      
    jgt r8, r3, lbb_5147                            if r8 > r3 { pc += 13 }
lbb_5134:
    mov64 r8, r7                                    r8 = r7
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x28], r8                    
    sth [r10-0x20], 257                     
    ldxdw r1, [r10-0x148]                   
    stxdw [r10-0xec], r1                    
    stb [r10-0xed], 0                       
    sth [r10-0xef], 0                       
    stb [r10-0xf0], 8                       
    ldxb r1, [r7+0x0]                       
    jeq r1, 255, lbb_5248                           if r1 == (255 as i32 as i64 as u64) { pc += 103 }
lbb_5145:
    mov64 r2, 11                                    r2 = 11 as i32 as i64 as u64
    ja lbb_5365                                     if true { pc += 218 }
lbb_5147:
    mov64 r1, r8                                    r1 = r8
    sub64 r1, r3                                    r1 -= r3   ///  r1 = r1.wrapping_sub(r3)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jgt r1, r8, lbb_5153                            if r1 > r8 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_5153:
    jne r2, 0, lbb_5155                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r1                                    r4 = r1
lbb_5155:
    ldxdw r1, [r10-0x178]                   
    ldxdw r2, [r1+0x0]                      
    mov64 r1, r7                                    r1 = r7
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x18], r1                    
    mov64 r3, r2                                    r3 = r2
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x28], r3                    
    sth [r10-0x10], 1                       
    sth [r10-0x20], 257                     
    stxdw [r10-0xec], r4                    
    stb [r10-0xed], 0                       
    sth [r10-0xef], 0                       
    stb [r10-0xf0], 2                       
    ldxb r4, [r2+0x0]                       
    jne r4, 255, lbb_5145                           if r4 != (255 as i32 as i64 as u64) { pc += -26 }
    ldxb r0, [r2+0x1]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_5176                             if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_5176:
    ldxb r6, [r2+0x2]                       
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r6, 0, lbb_5180                             if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_5180:
    ldxb r6, [r2+0x3]                       
    jne r6, 0, lbb_5183                             if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_5183:
    ldxdw r6, [r2+0x50]                     
    mov64 r8, r2                                    r8 = r2
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r8                    
    mov64 r8, r2                                    r8 = r2
    add64 r8, 88                                    r8 += 88   ///  r8 = r8.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r8                    
    stxdw [r10-0x88], r6                    
    add64 r2, 72                                    r2 += 72   ///  r2 = r2.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r2                    
    stxdw [r10-0x98], r3                    
    stxb [r10-0x66], r5                     
    stxb [r10-0x67], r0                     
    stxb [r10-0x68], r4                     
    stdw [r10-0x70], 0                      
    ldxb r2, [r7+0x0]                       
    jne r2, 255, lbb_5145                           if r2 != (255 as i32 as i64 as u64) { pc += -55 }
    ldxb r2, [r7+0x1]                       
    ldxb r3, [r7+0x2]                       
    ldxb r5, [r7+0x3]                       
    ldxdw r4, [r7+0x50]                     
    mov64 r0, r7                                    r0 = r7
    add64 r0, 40                                    r0 += 40   ///  r0 = r0.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r0                    
    mov64 r0, r7                                    r0 = r7
    add64 r0, 88                                    r0 += 88   ///  r0 = r0.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r0                    
    stxdw [r10-0x50], r4                    
    mov64 r4, r7                                    r4 = r7
    add64 r4, 72                                    r4 += 72   ///  r4 = r4.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r4                    
    stxdw [r10-0x60], r1                    
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r5, 0, lbb_5219                             if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_5219:
    stxb [r10-0x2e], r4                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_5223                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_5223:
    stxb [r10-0x2f], r4                     
    jne r2, 0, lbb_5226                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_5226:
    stxb [r10-0x30], r1                     
    stdw [r10-0x38], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -240                                  r1 += -240   ///  r1 = r1.wrapping_add(-240 as i32 as i64 as u64)
    stxdw [r10-0xb8], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    stxdw [r10-0xc8], r1                    
    lddw r1, 0x100016650 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295059024
    stxdw [r10-0xd0], r1                    
    stdw [r10-0xb0], 12                     
    stdw [r10-0xc0], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -152                                  r2 += -152   ///  r2 = r2.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    mov64 r4, 8                                     r4 = 8 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    syscall [invalid]                       
    ja lbb_5134                                     if true { pc += -114 }
lbb_5248:
    ldxb r2, [r7+0x1]                       
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_5252                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_5252:
    ldxb r3, [r7+0x2]                       
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_5256                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_5256:
    ldxb r4, [r7+0x3]                       
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_5260                             if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_5260:
    ldxdw r4, [r7+0x50]                     
    stxdw [r10-0x88], r4                    
    stxb [r10-0x66], r3                     
    stxb [r10-0x67], r2                     
    stxb [r10-0x68], r1                     
    mov64 r6, r7                                    r6 = r7
    add64 r6, 40                                    r6 += 40   ///  r6 = r6.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r6                    
    mov64 r1, r7                                    r1 = r7
    add64 r1, 88                                    r1 += 88   ///  r1 = r1.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x148], r1                   
    stxdw [r10-0x80], r1                    
    mov64 r9, r7                                    r9 = r7
    add64 r9, 72                                    r9 += 72   ///  r9 = r9.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r9                    
    stxdw [r10-0x98], r8                    
    stdw [r10-0x70], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -240                                  r1 += -240   ///  r1 = r1.wrapping_add(-240 as i32 as i64 as u64)
    stxdw [r10-0xb8], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    stxdw [r10-0xc8], r1                    
    lddw r1, 0x100016650 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295059024
    stxdw [r10-0xd0], r1                    
    stdw [r10-0xb0], 12                     
    stdw [r10-0xc0], 1                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -152                                  r2 += -152   ///  r2 = r2.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -272                                  r4 += -272   ///  r4 = r4.wrapping_add(-272 as i32 as i64 as u64)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    syscall [invalid]                       
    ldxdw r1, [r10-0x108]                   
    stxdw [r10-0xf8], r1                    
    ldxdw r1, [r10-0x110]                   
    stxdw [r10-0x100], r1                   
    stxdw [r10-0xf0], r8                    
    sth [r10-0xe8], 257                     
    stb [r10-0x25], 0                       
    sth [r10-0x27], 0                       
    stb [r10-0x28], 1                       
    ldxdw r2, [r10-0x180]                   
    ldxdw r1, [r2+0x0]                      
    stxdw [r10-0x24], r1                    
    ldxdw r1, [r2+0x8]                      
    stxdw [r10-0x1c], r1                    
    ldxdw r1, [r2+0x10]                     
    stxdw [r10-0x14], r1                    
    ldxdw r1, [r2+0x18]                     
    stxdw [r10-0xc], r1                     
    ldxb r1, [r7+0x0]                       
    jne r1, 255, lbb_5363                           if r1 != (255 as i32 as i64 as u64) { pc += 46 }
    ldxb r1, [r7+0x1]                       
    ldxb r2, [r7+0x2]                       
    ldxb r4, [r7+0x3]                       
    ldxdw r3, [r7+0x50]                     
    stxdw [r10-0x78], r6                    
    ldxdw r5, [r10-0x148]                   
    stxdw [r10-0x80], r5                    
    stxdw [r10-0x88], r3                    
    stxdw [r10-0x90], r9                    
    stxdw [r10-0x98], r8                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_5330                             if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_5330:
    stxb [r10-0x66], r3                     
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_5334                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_5334:
    stxb [r10-0x67], r3                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r9, [r10-0x158]                   
    jne r1, 0, lbb_5339                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_5339:
    stxb [r10-0x68], r2                     
    stdw [r10-0x70], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    stxdw [r10-0xb8], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -240                                  r1 += -240   ///  r1 = r1.wrapping_add(-240 as i32 as i64 as u64)
    stxdw [r10-0xc8], r1                    
    lddw r1, 0x100016650 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295059024
    stxdw [r10-0xd0], r1                    
    stdw [r10-0xb0], 36                     
    stdw [r10-0xc0], 1                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -152                                  r2 += -152   ///  r2 = r2.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -256                                  r4 += -256   ///  r4 = r4.wrapping_add(-256 as i32 as i64 as u64)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_5359:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    syscall [invalid]                       
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    ja lbb_5365                                     if true { pc += 2 }
lbb_5363:
    mov64 r2, 11                                    r2 = 11 as i32 as i64 as u64
lbb_5364:
    ldxdw r9, [r10-0x158]                   
lbb_5365:
    stxw [r9+0x4], r1                       
    stxw [r9+0x0], r2                       
    exit                                    
lbb_5368:
    lddw r1, 0x100017690 --> b"\x00\x00\x00\x00\xe8f\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xe1\x02\x00…        r1 load str located at 4295063184
    call function_9597                      
lbb_5371:
    lddw r1, 0x1000176a8 --> b"\x00\x00\x00\x00\xe8f\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xe1\x02\x00…        r1 load str located at 4295063208
    call function_9564                      
lbb_5374:
    lddw r1, 0x100017880 --> b"\x00\x00\x00\x00\xa3g\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xcd\x00\x00…        r1 load str located at 4295063680
    call function_9586                      
lbb_5377:
    lddw r1, 0x100017868 --> b"\x00\x00\x00\x00\xa3g\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xcd\x00\x00…        r1 load str located at 4295063656
    call function_9564                      
lbb_5380:
    lddw r1, 0x100017850 --> b"\x00\x00\x00\x00\xa3g\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xca\x00\x00…        r1 load str located at 4295063632
    call function_9564                      
lbb_5383:
    lddw r1, 0x100017898 --> b"\x00\x00\x00\x00\xa3g\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xd1\x00\x00…        r1 load str located at 4295063704
    call function_9564                      
lbb_5386:
    lddw r1, 0x100017850 --> b"\x00\x00\x00\x00\xa3g\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xca\x00\x00…        r1 load str located at 4295063632
    call function_9586                      
lbb_5389:
    lddw r1, 0x1000178b0 --> b"\x00\x00\x00\x00\xa3g\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xd1\x00\x00…        r1 load str located at 4295063728
    call function_9586                      

function_5392:
    ldxdw r3, [r3+0x0]                      
    ldxdw r2, [r2+0x0]                      
    ldxdw r4, [r2+0x48]                     
    ldxdw r0, [r3+0x48]                     
    mov64 r5, r0                                    r5 = r0
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jlt r5, r0, lbb_5401                            if r5 < r0 { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_5401:
    mov64 r5, 23                                    r5 = 23 as i32 as i64 as u64
    and64 r6, 1                                     r6 &= 1   ///  r6 = r6.and(1)
    jne r6, 0, lbb_5455                             if r6 != (0 as i32 as i64 as u64) { pc += 51 }
    mov64 r5, 11                                    r5 = 11 as i32 as i64 as u64
    ldxb r6, [r3+0x0]                       
    jlt r6, 240, lbb_5455                           if r6 < (240 as i32 as i64 as u64) { pc += 48 }
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    stxdw [r3+0x48], r4                     
    ldxb r3, [r2+0x0]                       
    jlt r3, 240, lbb_5455                           if r3 < (240 as i32 as i64 as u64) { pc += 44 }
    stdw [r2+0x28], 0                       
    stdw [r2+0x48], 0                       
    stdw [r2+0x30], 0                       
    stdw [r2+0x38], 0                       
    stdw [r2+0x40], 0                       
    and64 r3, 15                                    r3 &= 15   ///  r3 = r3.and(15)
    jne r3, 15, lbb_5455                            if r3 != (15 as i32 as i64 as u64) { pc += 37 }
    ldxdw r4, [r2+0x50]                     
    mov64 r0, r4                                    r0 = r4
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    mov64 r3, r0                                    r3 = r0
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    mov64 r5, 26                                    r5 = 26 as i32 as i64 as u64
    jeq r3, 0, lbb_5455                             if r3 == (0 as i32 as i64 as u64) { pc += 30 }
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    neg64 r0                                        r0 = -r0   ///  r0 = (r0 as i64).wrapping_neg() as u64
    mov64 r3, r0                                    r3 = r0
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    jne r3, r0, lbb_5458                            if r3 != r0 { pc += 27 }
    ldxw r5, [r2+0x4]                       
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    arsh64 r5, 32                                   r5 >>= 32 (signed)   ///  r5 = (r5 as i64).wrapping_shr(32)
    add64 r5, r3                                    r5 += r3   ///  r5 = r5.wrapping_add(r3)
    mov64 r0, r5                                    r0 = r5
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    jne r0, r5, lbb_5461                            if r0 != r5 { pc += 22 }
    mov64 r5, 19                                    r5 = 19 as i32 as i64 as u64
    jsgt r0, 10240, lbb_5455                        if (r0 as i64) > (10240 as i32 as i64) { pc += 14 }
    stxw [r2+0x4], r0                       
    stdw [r2+0x50], 0                       
    mov64 r5, 26                                    r5 = 26 as i32 as i64 as u64
    jslt r3, 1, lbb_5455                            if (r3 as i64) < (1 as i32 as i64) { pc += 10 }
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r2                                    r1 = r2
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    syscall [invalid]                       
    mov64 r1, r6                                    r1 = r6
    mov64 r5, 26                                    r5 = 26 as i32 as i64 as u64
lbb_5455:
    stxw [r1+0x0], r5                       
    stw [r1+0x4], 0                         
    exit                                    
lbb_5458:
    lddw r1, 0x1000177e0 --> b"\x00\x00\x00\x00\xe8f\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\x10\x02\x00…        r1 load str located at 4295063520
    call function_9575                      
lbb_5461:
    lddw r1, 0x1000177f8 --> b"\x00\x00\x00\x00\xe8f\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\x11\x02\x00…        r1 load str located at 4295063544
    call function_9564                      

function_5464:
    mov64 r6, r1                                    r6 = r1
    stxdw [r10-0x48], r2                    
    lddw r1, 0x1000167b6 --> b"blacklistprogram/src/instructions/blacklist_trader"        r1 load str located at 4295059382
    stxdw [r10-0x58], r1                    
    stdw [r10-0x40], 32                     
    stdw [r10-0x50], 9                      
    stb [r10-0x31], 255                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -48                                   r4 += -48   ///  r4 = r4.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -49                                   r5 += -49   ///  r5 = r5.wrapping_add(-49 as i32 as i64 as u64)
    mov64 r2, 2                                     r2 = 2 as i32 as i64 as u64
    lddw r3, 0x100016690 --> b"\x0a2\x93ma\xfe\xa6\xd8\xb6\x06\xe7\x06\xc2\xa1t\xb1kg\xa5\x82K\x0f\xd4\x…        r3 load str located at 4295059088
    syscall [invalid]                       
    jeq r0, 0, lbb_5495                             if r0 == (0 as i32 as i64 as u64) { pc += 12 }
    lddw r1, 0x100017810 --> b"\x00\x00\x00\x00Jg\x01\x001\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00{g…        r1 load str located at 4295063568
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    lddw r2, 0x100017820 --> b"\x00\x00\x00\x00{g\x01\x00\x0d\x00\x00\x00\x00\x00\x00\x00\x81\x00\x00\x0…        r2 load str located at 4295063584
    call function_7789                      
lbb_5495:
    ldxdw r1, [r10-0x18]                    
    stxdw [r10-0x60], r1                    
    ldxdw r1, [r10-0x20]                    
    stxdw [r10-0x68], r1                    
    ldxdw r1, [r10-0x28]                    
    stxdw [r10-0x70], r1                    
    ldxdw r1, [r10-0x30]                    
    stxdw [r10-0x78], r1                    
    ldxdw r7, [r6+0x0]                      
    mov64 r2, r7                                    r2 = r7
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -120                                  r1 += -120   ///  r1 = r1.wrapping_add(-120 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_5515                             if r0 == (0 as i32 as i64 as u64) { pc += 2 }
    mov64 r0, 3                                     r0 = 3 as i32 as i64 as u64
    ja lbb_5565                                     if true { pc += 50 }
lbb_5515:
    mov64 r0, 2                                     r0 = 2 as i32 as i64 as u64
    ldxdw r1, [r7+0x28]                     
    lddw r2, 0xd8a6fe616d93320a                     r2 load str located at -2835299220979502582
    jne r1, r2, lbb_5565                            if r1 != r2 { pc += 45 }
    ldxdw r1, [r7+0x30]                     
    lddw r2, 0xb174a1c206e706b6                     r2 load str located at -5659720976986339658
    jne r1, r2, lbb_5565                            if r1 != r2 { pc += 41 }
    ldxdw r1, [r7+0x38]                     
    lddw r2, 0xadd40f4b82a5676b                     r2 load str located at -5921090793096517781
    jne r1, r2, lbb_5565                            if r1 != r2 { pc += 37 }
    ldxdw r1, [r7+0x40]                     
    lddw r2, 0x7e6598cdd46af8a9                     r2 load str located at 9107853831226194089
    jeq r1, r2, lbb_5533                            if r1 == r2 { pc += 1 }
    ja lbb_5565                                     if true { pc += 32 }
lbb_5533:
    ldxdw r1, [r7+0x48]                     
    jeq r1, 0, lbb_5565                             if r1 == (0 as i32 as i64 as u64) { pc += 30 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    call function_7233                      
    ldxw r1, [r10-0x24]                     
    ldxw r2, [r10-0x28]                     
    ldxdw r3, [r10-0x30]                    
    jeq r3, 0, lbb_5563                             if r3 == (0 as i32 as i64 as u64) { pc += 20 }
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r2, 3                                     r2 = 3 as i32 as i64 as u64
    ldxb r4, [r10-0x18]                     
    jeq r1, 1, lbb_5549                             if r1 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5551                                     if true { pc += 2 }
lbb_5549:
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    ldxb r1, [r3+0x0]                       
lbb_5551:
    jgt r4, 7, lbb_5574                             if r4 > (7 as i32 as i64 as u64) { pc += 22 }
    ldxdw r3, [r10-0x20]                    
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    lsh64 r5, r4                                    r5 <<= r4   ///  r5 = r5.wrapping_shl(r4 as u32)
    and64 r5, 255                                   r5 &= 255   ///  r5 = r5.and(255)
    ldxb r4, [r3+0x0]                       
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r5, r4                                    r5 = r4
    and64 r5, 255                                   r5 &= 255   ///  r5 = r5.and(255)
    jne r5, r4, lbb_5561                            if r5 != r4 { pc += 0 }
lbb_5561:
    jne r5, r4, lbb_5577                            if r5 != r4 { pc += 15 }
    stxb [r3+0x0], r4                       
lbb_5563:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r2, 26, lbb_5566                            if r2 == (26 as i32 as i64 as u64) { pc += 1 }
lbb_5565:
    exit                                    
lbb_5566:
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r2, 2                                     r2 = 2 as i32 as i64 as u64
    jeq r1, 2, lbb_5570                             if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_5570:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_5565                             if r1 == (0 as i32 as i64 as u64) { pc += -7 }
    mov64 r0, r2                                    r0 = r2
    ja lbb_5565                                     if true { pc += -9 }
lbb_5574:
    lddw r1, 0x100017690 --> b"\x00\x00\x00\x00\xe8f\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xe1\x02\x00…        r1 load str located at 4295063184
    call function_9597                      
lbb_5577:
    lddw r1, 0x1000176a8 --> b"\x00\x00\x00\x00\xe8f\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xe1\x02\x00…        r1 load str located at 4295063208
    call function_9564                      

function_5580:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r7+0x0]                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x1000165d0 --> b"\x06\xa7\xd5\x17\x18{\xd1f5\xda\xd4\x04U\xfd\xc2\xc0\xc1$\xc6\x8f!Vu\xa5\…        r2 load str located at 4295058896
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    mov64 r2, 16                                    r2 = 16 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_5600                             if r0 != (0 as i32 as i64 as u64) { pc += 8 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_7233                      
    ldxw r1, [r10-0x14]                     
    ldxw r2, [r10-0x18]                     
    ldxdw r9, [r10-0x20]                    
    jne r9, 0, lbb_5604                             if r9 != (0 as i32 as i64 as u64) { pc += 4 }
lbb_5600:
    stxw [r6+0x8], r1                       
    stxw [r6+0x4], r2                       
    stb [r6+0x0], 1                         
    ja lbb_5808                                     if true { pc += 204 }
lbb_5604:
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    jgt r1, 1, lbb_5610                             if r1 > (1 as i32 as i64 as u64) { pc += 3 }
    lddw r1, 0x100017838 --> b"\x00\x00\x00\x00\x88g\x01\x00\x1b\x00\x00\x00\x00\x00\x00\x007\x00\x00\x0…        r1 load str located at 4295063608
    call function_9575                      
lbb_5610:
    ldxb r4, [r10-0x8]                      
    ldxdw r3, [r10-0x10]                    
    mov64 r2, r9                                    r2 = r9
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxh r1, [r2-0x2]                       
    ldxh r2, [r9+0x0]                       
    jlt r1, r2, lbb_5618                            if r1 < r2 { pc += 1 }
    ja lbb_5663                                     if true { pc += 45 }
lbb_5618:
    stxdw [r10-0x48], r4                    
    stxdw [r10-0x50], r3                    
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    mov64 r2, r9                                    r2 = r9
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxh r1, [r2+0x2]                       
    add64 r9, r1                                    r9 += r1   ///  r9 = r9.wrapping_add(r1)
    ldxh r1, [r9+0x0]                       
    stxdw [r10-0x58], r1                    
    mul64 r1, 33                                    r1 *= 33   ///  r1 = r1.wrapping_mul(33 as u64)
    mov64 r8, r9                                    r8 = r9
    add64 r8, r1                                    r8 += r1   ///  r8 = r8.wrapping_add(r1)
    lddw r7, 0x100016811 --> b"\x04y\xd5[\xf21\xc0n\xeet\xc5n\xceh\x15\x07\xfd\xb1\xb2\xde\xa3\xf4\x8eQ\…        r7 load str located at 4295059473
    stxdw [r10-0x40], r8                    
    add64 r8, 2                                     r8 += 2   ///  r8 = r8.wrapping_add(2 as i32 as i64 as u64)
    lddw r1, 0x100016811 --> b"\x04y\xd5[\xf21\xc0n\xeet\xc5n\xceh\x15\x07\xfd\xb1\xb2\xde\xa3\xf4\x8eQ\…        r1 load str located at 4295059473
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_5666                             if r0 == (0 as i32 as i64 as u64) { pc += 24 }
    add64 r7, 32                                    r7 += 32   ///  r7 = r7.wrapping_add(32 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_5666                             if r0 == (0 as i32 as i64 as u64) { pc += 16 }
    lddw r1, 0x100016811 --> b"\x04y\xd5[\xf21\xc0n\xeet\xc5n\xceh\x15\x07\xfd\xb1\xb2\xde\xa3\xf4\x8eQ\…        r1 load str located at 4295059473
    add64 r1, 64                                    r1 += 64   ///  r1 = r1.wrapping_add(64 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_5666                             if r0 == (0 as i32 as i64 as u64) { pc += 7 }
lbb_5659:
    stb [r6+0x22], 0                        
    stb [r6+0x1], 0                         
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_5793                                     if true { pc += 130 }
lbb_5663:
    stw [r6+0x4], 2                         
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ja lbb_5795                                     if true { pc += 129 }
lbb_5666:
    ldxdw r1, [r10-0x40]                    
    ldxh r2, [r1+0x22]                      
    jgt r2, 7, lbb_5673                             if r2 > (7 as i32 as i64 as u64) { pc += 4 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    lddw r3, 0x100017988 --> b"\x00\x00\x00\x00qh\x01\x00\x1c\x00\x00\x00\x00\x00\x00\x00\xa4\x00\x00\x0…        r3 load str located at 4295063944
    call function_8840                      
lbb_5673:
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0x1000165f0 --> b"\x04y\xd5[\xf21\xc0n\xeet\xc5n\xceh\x15\x07\xfd\xb1\xb2\xde\xa3\xf4\x8eQ\…        r2 load str located at 4295058928
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r7                                    r1 = r7
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jeq r1, 0, lbb_5707                             if r1 == (0 as i32 as i64 as u64) { pc += 24 }
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0x100016570 --> b"U\x91V\xf1\xa2\m\x13O*\xf7\xe6\x0a\x9a\x0d4~\xc7\x91Vcdb\xd5\xd1\xad&\xf1…        r2 load str located at 4295058800
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_5720                             if r0 == (0 as i32 as i64 as u64) { pc += 29 }
lbb_5691:
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0x100016670 --> b"\x06\xa9\x9cn\x12\xe7\x0e\xbb5\x18\\x14LK\x90n\x18\xff\xb0\x0aGt/*^\x04\x…        r2 load str located at 4295059056
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_5700                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5659                                     if true { pc += -41 }
lbb_5700:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0x40]                    
    ldxdw r2, [r2+0x24]                     
    lddw r3, 0x1cad320090a3b756                     r3 load str located at 2066362782040962902
    jeq r2, r3, lbb_5740                            if r2 == r3 { pc += 34 }
    ja lbb_5659                                     if true { pc += -48 }
lbb_5707:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x40]                    
    ldxdw r2, [r2+0x24]                     
    lddw r3, 0x14afc431ccfa64ba                     r3 load str located at 1490625719854326970
    jsgt r2, r3, lbb_5730                           if (r2 as i64) > (r3 as i64) { pc += 17 }
    lddw r3, 0x819cd641339b20c1                     r3 load str located at -9107168770922962751
    jeq r2, r3, lbb_5737                            if r2 == r3 { pc += 21 }
    lddw r3, 0xe9d8fe7c935398d1                     r3 load str located at -1596246256901711663
    jeq r2, r3, lbb_5740                            if r2 == r3 { pc += 21 }
    ja lbb_5659                                     if true { pc += -61 }
lbb_5720:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0x40]                    
    ldxdw r2, [r2+0x24]                     
    lddw r3, 0x9de0e18ef62cbf0e                     r3 load str located at -7070403410839945458
    jeq r2, r3, lbb_5740                            if r2 == r3 { pc += 14 }
    lddw r3, 0xaff11fb02126e0f0                     r3 load str located at -5768794806353993488
    jeq r2, r3, lbb_5740                            if r2 == r3 { pc += 11 }
    ja lbb_5691                                     if true { pc += -39 }
lbb_5730:
    lddw r3, 0x14afc431ccfa64bb                     r3 load str located at 1490625719854326971
    jeq r2, r3, lbb_5739                            if r2 == r3 { pc += 6 }
    lddw r3, 0x2aade37a97cb17e5                     r3 load str located at 3075364236236101605
    jeq r2, r3, lbb_5740                            if r2 == r3 { pc += 4 }
    ja lbb_5659                                     if true { pc += -78 }
lbb_5737:
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    ja lbb_5740                                     if true { pc += 1 }
lbb_5739:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_5740:
    ldxdw r2, [r10-0x58]                    
    jge r1, r2, lbb_5791                            if r1 >= r2 { pc += 49 }
    mul64 r1, 33                                    r1 *= 33   ///  r1 = r1.wrapping_mul(33 as u64)
    add64 r9, r1                                    r9 += r1   ///  r9 = r9.wrapping_add(r1)
    ldxb r5, [r9+0x2]                       
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jeq r5, 0, lbb_5758                             if r5 == (0 as i32 as i64 as u64) { pc += 10 }
    ldxw r3, [r9+0x9]                       
    ldxw r2, [r9+0x5]                       
    ldxh r1, [r9+0x3]                       
    ldxdw r4, [r9+0x1b]                     
    stxdw [r10-0x2a], r4                    
    ldxdw r4, [r9+0x15]                     
    stxdw [r10-0x30], r4                    
    ldxdw r4, [r9+0xd]                      
    stxdw [r10-0x38], r4                    
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
lbb_5758:
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r7, 0, lbb_5763                             if r7 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_5763:
    stxw [r6+0x8], r3                       
    stxw [r6+0x4], r2                       
    stxh [r6+0x2], r1                       
    stxb [r6+0x1], r4                       
    ldxdw r1, [r10-0x38]                    
    stxdw [r6+0xc], r1                      
    ldxdw r1, [r10-0x30]                    
    stxdw [r6+0x14], r1                     
    ldxdw r1, [r10-0x2a]                    
    stxdw [r6+0x1a], r1                     
    stxb [r6+0x22], r5                      
    stb [r6+0x0], 0                         
    ldxdw r1, [r10-0x48]                    
    jgt r1, 7, lbb_5809                             if r1 > (7 as i32 as i64 as u64) { pc += 32 }
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lsh64 r2, r1                                    r2 <<= r1   ///  r2 = r2.wrapping_shl(r1 as u32)
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    ldxdw r3, [r10-0x50]                    
    ldxb r1, [r3+0x0]                       
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, r1                                    r2 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jne r2, r1, lbb_5786                            if r2 != r1 { pc += 0 }
lbb_5786:
    jne r2, r1, lbb_5788                            if r2 != r1 { pc += 1 }
    ja lbb_5807                                     if true { pc += 19 }
lbb_5788:
    lddw r1, 0x1000176a8 --> b"\x00\x00\x00\x00\xe8f\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xe1\x02\x00…        r1 load str located at 4295063208
    call function_9564                      
lbb_5791:
    stw [r6+0x4], 1                         
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_5793:
    ldxdw r3, [r10-0x50]                    
    ldxdw r4, [r10-0x48]                    
lbb_5795:
    stxb [r6+0x0], r1                       
    mov64 r1, r4                                    r1 = r4
    jgt r1, 7, lbb_5809                             if r1 > (7 as i32 as i64 as u64) { pc += 11 }
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lsh64 r2, r1                                    r2 <<= r1   ///  r2 = r2.wrapping_shl(r1 as u32)
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    ldxb r1, [r3+0x0]                       
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, r1                                    r2 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jne r2, r1, lbb_5806                            if r2 != r1 { pc += 0 }
lbb_5806:
    jne r2, r1, lbb_5788                            if r2 != r1 { pc += -19 }
lbb_5807:
    stxb [r3+0x0], r1                       
lbb_5808:
    exit                                    
lbb_5809:
    lddw r1, 0x100017690 --> b"\x00\x00\x00\x00\xe8f\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xe1\x02\x00…        r1 load str located at 4295063184
    call function_9597                      

e:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r7, r1                                    r7 = r1
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_5833                             if r2 == (0 as i32 as i64 as u64) { pc += 16 }
    stxdw [r10-0x100], r7                   
    ldxdw r3, [r1+0x58]                     
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    mov64 r4, r1                                    r4 = r1
    add64 r4, 10344                                 r4 += 10344   ///  r4 = r4.wrapping_add(10344 as i32 as i64 as u64)
    add64 r1, 10351                                 r1 += 10351   ///  r1 = r1.wrapping_add(10351 as i32 as i64 as u64)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r1, r4, lbb_5826                            if r1 < r4 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_5826:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_6392                             if r3 != (0 as i32 as i64 as u64) { pc += 564 }
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    mov64 r7, r1                                    r7 = r1
    jeq r2, 1, lbb_5833                             if r2 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5868                                     if true { pc += 35 }
lbb_5833:
    ldxdw r8, [r7+0x0]                      
    mov64 r9, r7                                    r9 = r7
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    lddw r2, 0x100016690 --> b"\x0a2\x93ma\xfe\xa6\xd8\xb6\x06\xe7\x06\xc2\xa1t\xb1kg\xa5\x82K\x0f\xd4\x…        r2 load str located at 4295059088
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_9989                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_6220                             if r0 != (0 as i32 as i64 as u64) { pc += 375 }
    jeq r8, 0, lbb_5865                             if r8 == (0 as i32 as i64 as u64) { pc += 19 }
    ldxb r2, [r9+0x0]                       
    jgt r2, 10, lbb_5865                            if r2 > (10 as i32 as i64 as u64) { pc += 17 }
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    stxdw [r10-0xff8], r8                   
    add64 r7, 9                                     r7 += 9   ///  r7 = r7.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r10-0x1000], r7                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -264                                  r1 += -264   ///  r1 = r1.wrapping_add(-264 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -256                                  r3 += -256   ///  r3 = r3.wrapping_add(-256 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r4, r6                                    r4 = r6
    call function_3025                      
    ldxw r1, [r10-0x108]                    
    jsgt r1, 12, lbb_6123                           if (r1 as i64) > (12 as i32 as i64) { pc += 262 }
    jsgt r1, 5, lbb_6214                            if (r1 as i64) > (5 as i32 as i64) { pc += 352 }
    jsgt r1, 2, lbb_6238                            if (r1 as i64) > (2 as i32 as i64) { pc += 375 }
    jeq r1, 0, lbb_6278                             if r1 == (0 as i32 as i64 as u64) { pc += 414 }
    jeq r1, 1, lbb_6296                             if r1 == (1 as i32 as i64 as u64) { pc += 431 }
lbb_5865:
    lddw r0, 0x300000000                            r0 load str located at 12884901888
    ja lbb_6222                                     if true { pc += 354 }
lbb_5868:
    mov64 r6, r2                                    r6 = r2
    jlt r2, 32, lbb_5871                            if r2 < (32 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 32                                    r6 = 32 as i32 as i64 as u64
lbb_5871:
    jeq r2, 2, lbb_6016                             if r2 == (2 as i32 as i64 as u64) { pc += 144 }
    mov64 r3, r10                                   r3 = r10
    add64 r3, -256                                  r3 += -256   ///  r3 = r3.wrapping_add(-256 as i32 as i64 as u64)
    mov64 r4, r6                                    r4 = r6
    jlt r2, 6, lbb_5995                             if r2 < (6 as i32 as i64 as u64) { pc += 119 }
    mov64 r3, r10                                   r3 = r10
    add64 r3, -256                                  r3 += -256   ///  r3 = r3.wrapping_add(-256 as i32 as i64 as u64)
    mov64 r4, r6                                    r4 = r6
lbb_5879:
    ldxb r5, [r1+0x0]                       
    jeq r5, 255, lbb_5882                           if r5 == (255 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5954                                     if true { pc += 72 }
lbb_5882:
    stxdw [r3+0x8], r1                      
    ldxdw r5, [r1+0x50]                     
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    mov64 r0, r1                                    r0 = r1
    add64 r0, 10336                                 r0 += 10336   ///  r0 = r0.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r1, r0, lbb_5891                            if r1 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_5891:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_6386                             if r5 != (0 as i32 as i64 as u64) { pc += 493 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_5894:
    ldxb r5, [r1+0x0]                       
    jne r5, 255, lbb_5962                           if r5 != (255 as i32 as i64 as u64) { pc += 66 }
    stxdw [r3+0x10], r1                     
    ldxdw r5, [r1+0x50]                     
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    mov64 r0, r1                                    r0 = r1
    add64 r0, 10336                                 r0 += 10336   ///  r0 = r0.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r1, r0, lbb_5905                            if r1 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_5905:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_6386                             if r5 != (0 as i32 as i64 as u64) { pc += 479 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_5908:
    ldxb r5, [r1+0x0]                       
    jne r5, 255, lbb_5970                           if r5 != (255 as i32 as i64 as u64) { pc += 60 }
    stxdw [r3+0x18], r1                     
    ldxdw r5, [r1+0x50]                     
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    mov64 r0, r1                                    r0 = r1
    add64 r0, 10336                                 r0 += 10336   ///  r0 = r0.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r1, r0, lbb_5919                            if r1 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_5919:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_6386                             if r5 != (0 as i32 as i64 as u64) { pc += 465 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_5922:
    ldxb r5, [r1+0x0]                       
    jne r5, 255, lbb_5978                           if r5 != (255 as i32 as i64 as u64) { pc += 54 }
    stxdw [r3+0x20], r1                     
    ldxdw r5, [r1+0x50]                     
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    mov64 r0, r1                                    r0 = r1
    add64 r0, 10336                                 r0 += 10336   ///  r0 = r0.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r1, r0, lbb_5933                            if r1 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_5933:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_6386                             if r5 != (0 as i32 as i64 as u64) { pc += 451 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_5936:
    add64 r3, 40                                    r3 += 40   ///  r3 = r3.wrapping_add(40 as i32 as i64 as u64)
    ldxb r5, [r1+0x0]                       
    jne r5, 255, lbb_5986                           if r5 != (255 as i32 as i64 as u64) { pc += 47 }
    stxdw [r3+0x0], r1                      
    ldxdw r5, [r1+0x50]                     
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    mov64 r0, r1                                    r0 = r1
    add64 r0, 10336                                 r0 += 10336   ///  r0 = r0.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r1, r0, lbb_5948                            if r1 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_5948:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_6386                             if r5 != (0 as i32 as i64 as u64) { pc += 436 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    add64 r4, -5                                    r4 += -5   ///  r4 = r4.wrapping_add(-5 as i32 as i64 as u64)
    jgt r4, 5, lbb_5879                             if r4 > (5 as i32 as i64 as u64) { pc += -74 }
    ja lbb_5995                                     if true { pc += 41 }
lbb_5954:
    lsh64 r5, 3                                     r5 <<= 3   ///  r5 = r5.wrapping_shl(3)
    mov64 r0, r10                                   r0 = r10
    add64 r0, -256                                  r0 += -256   ///  r0 = r0.wrapping_add(-256 as i32 as i64 as u64)
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    ldxdw r5, [r0+0x0]                      
    stxdw [r3+0x8], r5                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_5894                                     if true { pc += -68 }
lbb_5962:
    lsh64 r5, 3                                     r5 <<= 3   ///  r5 = r5.wrapping_shl(3)
    mov64 r0, r10                                   r0 = r10
    add64 r0, -256                                  r0 += -256   ///  r0 = r0.wrapping_add(-256 as i32 as i64 as u64)
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    ldxdw r5, [r0+0x0]                      
    stxdw [r3+0x10], r5                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_5908                                     if true { pc += -62 }
lbb_5970:
    lsh64 r5, 3                                     r5 <<= 3   ///  r5 = r5.wrapping_shl(3)
    mov64 r0, r10                                   r0 = r10
    add64 r0, -256                                  r0 += -256   ///  r0 = r0.wrapping_add(-256 as i32 as i64 as u64)
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    ldxdw r5, [r0+0x0]                      
    stxdw [r3+0x18], r5                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_5922                                     if true { pc += -56 }
lbb_5978:
    lsh64 r5, 3                                     r5 <<= 3   ///  r5 = r5.wrapping_shl(3)
    mov64 r0, r10                                   r0 = r10
    add64 r0, -256                                  r0 += -256   ///  r0 = r0.wrapping_add(-256 as i32 as i64 as u64)
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    ldxdw r5, [r0+0x0]                      
    stxdw [r3+0x20], r5                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_5936                                     if true { pc += -50 }
lbb_5986:
    lsh64 r5, 3                                     r5 <<= 3   ///  r5 = r5.wrapping_shl(3)
    mov64 r0, r10                                   r0 = r10
    add64 r0, -256                                  r0 += -256   ///  r0 = r0.wrapping_add(-256 as i32 as i64 as u64)
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    ldxdw r5, [r0+0x0]                      
    stxdw [r3+0x0], r5                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    add64 r4, -5                                    r4 += -5   ///  r4 = r4.wrapping_add(-5 as i32 as i64 as u64)
    jgt r4, 5, lbb_5879                             if r4 > (5 as i32 as i64 as u64) { pc += -116 }
lbb_5995:
    jsgt r4, 2, lbb_6061                            if (r4 as i64) > (2 as i32 as i64) { pc += 65 }
    mov64 r7, r1                                    r7 = r1
    jeq r4, 1, lbb_6031                             if r4 == (1 as i32 as i64 as u64) { pc += 33 }
    ldxb r4, [r1+0x0]                       
    jeq r4, 255, lbb_6001                           if r4 == (255 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6334                                     if true { pc += 333 }
lbb_6001:
    stxdw [r3+0x8], r1                      
    ldxdw r3, [r1+0x50]                     
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    mov64 r4, r1                                    r4 = r1
    add64 r4, 10336                                 r4 += 10336   ///  r4 = r4.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r1, r4, lbb_6010                            if r1 < r4 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_6010:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_6013                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6029                                     if true { pc += 16 }
lbb_6013:
    lddw r1, 0x100017798 --> b"\x00\x00\x00\x00\x0dg\x01\x00\x15\x00\x00\x00\x00\x00\x00\x00|\x01\x00\x0…        r1 load str located at 4295063448
    call function_9564                      
lbb_6016:
    ldxb r3, [r1+0x0]                       
    jne r3, 255, lbb_6302                           if r3 != (255 as i32 as i64 as u64) { pc += 284 }
    stxdw [r10-0xf8], r1                    
    ldxdw r3, [r1+0x50]                     
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    mov64 r4, r1                                    r4 = r1
    add64 r4, 10336                                 r4 += 10336   ///  r4 = r4.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r1, r4, lbb_6027                            if r1 < r4 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_6027:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_6395                             if r3 != (0 as i32 as i64 as u64) { pc += 366 }
lbb_6029:
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_6030:
    mov64 r7, r1                                    r7 = r1
lbb_6031:
    mov64 r1, r2                                    r1 = r2
    sub64 r1, r6                                    r1 -= r6   ///  r1 = r1.wrapping_sub(r6)
    jeq r1, 0, lbb_5833                             if r1 == (0 as i32 as i64 as u64) { pc += -201 }
    mov64 r1, r6                                    r1 = r6
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r2, r7                                    r2 = r7
    ja lbb_6041                                     if true { pc += 3 }
lbb_6038:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    mov64 r2, r7                                    r2 = r7
    jne r3, 0, lbb_5833                             if r3 != (0 as i32 as i64 as u64) { pc += -208 }
lbb_6041:
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    ldxb r3, [r2+0x0]                       
    jne r3, 255, lbb_6056                           if r3 != (255 as i32 as i64 as u64) { pc += 12 }
    ldxdw r3, [r2+0x50]                     
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    mov64 r4, r2                                    r4 = r2
    add64 r4, 10336                                 r4 += 10336   ///  r4 = r4.wrapping_add(10336 as i32 as i64 as u64)
    add64 r2, 10343                                 r2 += 10343   ///  r2 = r2.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r2, r4, lbb_6052                            if r2 < r4 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_6052:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_6389                             if r3 != (0 as i32 as i64 as u64) { pc += 335 }
    and64 r2, -8                                    r2 &= -8   ///  r2 = r2.and(-8)
    mov64 r7, r2                                    r7 = r2
lbb_6056:
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_6038                             if r1 == (0 as i32 as i64 as u64) { pc += -21 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ja lbb_6038                                     if true { pc += -23 }
lbb_6061:
    jeq r4, 3, lbb_6130                             if r4 == (3 as i32 as i64 as u64) { pc += 68 }
    jeq r4, 4, lbb_6162                             if r4 == (4 as i32 as i64 as u64) { pc += 99 }
    ldxb r4, [r1+0x0]                       
    jeq r4, 255, lbb_6066                           if r4 == (255 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6318                                     if true { pc += 252 }
lbb_6066:
    stxdw [r3+0x8], r1                      
    ldxdw r4, [r1+0x50]                     
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    mov64 r5, r1                                    r5 = r1
    add64 r5, 10336                                 r5 += 10336   ///  r5 = r5.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r1, r5, lbb_6075                            if r1 < r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_6075:
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jne r4, 0, lbb_6120                             if r4 != (0 as i32 as i64 as u64) { pc += 43 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_6078:
    ldxb r4, [r1+0x0]                       
    jne r4, 255, lbb_6348                           if r4 != (255 as i32 as i64 as u64) { pc += 268 }
    stxdw [r3+0x10], r1                     
    ldxdw r4, [r1+0x50]                     
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    mov64 r5, r1                                    r5 = r1
    add64 r5, 10336                                 r5 += 10336   ///  r5 = r5.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r1, r5, lbb_6089                            if r1 < r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_6089:
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jne r4, 0, lbb_6120                             if r4 != (0 as i32 as i64 as u64) { pc += 29 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_6092:
    ldxb r4, [r1+0x0]                       
    jne r4, 255, lbb_6364                           if r4 != (255 as i32 as i64 as u64) { pc += 270 }
    stxdw [r3+0x18], r1                     
    ldxdw r4, [r1+0x50]                     
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    mov64 r5, r1                                    r5 = r1
    add64 r5, 10336                                 r5 += 10336   ///  r5 = r5.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r1, r5, lbb_6103                            if r1 < r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_6103:
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jne r4, 0, lbb_6120                             if r4 != (0 as i32 as i64 as u64) { pc += 15 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_6106:
    ldxb r4, [r1+0x0]                       
    jne r4, 255, lbb_6379                           if r4 != (255 as i32 as i64 as u64) { pc += 271 }
    stxdw [r3+0x20], r1                     
    ldxdw r3, [r1+0x50]                     
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    mov64 r4, r1                                    r4 = r1
    add64 r4, 10336                                 r4 += 10336   ///  r4 = r4.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r1, r4, lbb_6117                            if r1 < r4 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_6117:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_6120                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6029                                     if true { pc += -91 }
lbb_6120:
    lddw r1, 0x100017750 --> b"\x00\x00\x00\x00\x0dg\x01\x00\x15\x00\x00\x00\x00\x00\x00\x00s\x01\x00\x0…        r1 load str located at 4295063376
    call function_9564                      
lbb_6123:
    jsgt r1, 19, lbb_6208                           if (r1 as i64) > (19 as i32 as i64) { pc += 84 }
    jsgt r1, 15, lbb_6223                           if (r1 as i64) > (15 as i32 as i64) { pc += 98 }
    jeq r1, 13, lbb_6254                            if r1 == (13 as i32 as i64 as u64) { pc += 128 }
    jeq r1, 14, lbb_6260                            if r1 == (14 as i32 as i64 as u64) { pc += 133 }
    lddw r0, 0x1000000000                           r0 load str located at 68719476736
    ja lbb_6222                                     if true { pc += 92 }
lbb_6130:
    ldxb r4, [r1+0x0]                       
    jeq r4, 255, lbb_6133                           if r4 == (255 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6310                                     if true { pc += 177 }
lbb_6133:
    stxdw [r3+0x8], r1                      
    ldxdw r4, [r1+0x50]                     
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    mov64 r5, r1                                    r5 = r1
    add64 r5, 10336                                 r5 += 10336   ///  r5 = r5.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r1, r5, lbb_6142                            if r1 < r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_6142:
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jne r4, 0, lbb_6159                             if r4 != (0 as i32 as i64 as u64) { pc += 15 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_6145:
    ldxb r4, [r1+0x0]                       
    jne r4, 255, lbb_6341                           if r4 != (255 as i32 as i64 as u64) { pc += 194 }
    stxdw [r3+0x10], r1                     
    ldxdw r3, [r1+0x50]                     
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    mov64 r4, r1                                    r4 = r1
    add64 r4, 10336                                 r4 += 10336   ///  r4 = r4.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r1, r4, lbb_6156                            if r1 < r4 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_6156:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_6159                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6029                                     if true { pc += -130 }
lbb_6159:
    lddw r1, 0x100017780 --> b"\x00\x00\x00\x00\x0dg\x01\x00\x15\x00\x00\x00\x00\x00\x00\x00y\x01\x00\x0…        r1 load str located at 4295063424
    call function_9564                      
lbb_6162:
    ldxb r4, [r1+0x0]                       
    jeq r4, 255, lbb_6165                           if r4 == (255 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6326                                     if true { pc += 161 }
lbb_6165:
    stxdw [r3+0x8], r1                      
    ldxdw r4, [r1+0x50]                     
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    mov64 r5, r1                                    r5 = r1
    add64 r5, 10336                                 r5 += 10336   ///  r5 = r5.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r1, r5, lbb_6174                            if r1 < r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_6174:
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jne r4, 0, lbb_6205                             if r4 != (0 as i32 as i64 as u64) { pc += 29 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_6177:
    ldxb r4, [r1+0x0]                       
    jne r4, 255, lbb_6356                           if r4 != (255 as i32 as i64 as u64) { pc += 177 }
    stxdw [r3+0x10], r1                     
    ldxdw r4, [r1+0x50]                     
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    mov64 r5, r1                                    r5 = r1
    add64 r5, 10336                                 r5 += 10336   ///  r5 = r5.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r1, r5, lbb_6188                            if r1 < r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_6188:
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jne r4, 0, lbb_6205                             if r4 != (0 as i32 as i64 as u64) { pc += 15 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_6191:
    ldxb r4, [r1+0x0]                       
    jne r4, 255, lbb_6372                           if r4 != (255 as i32 as i64 as u64) { pc += 179 }
    stxdw [r3+0x18], r1                     
    ldxdw r3, [r1+0x50]                     
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    mov64 r4, r1                                    r4 = r1
    add64 r4, 10336                                 r4 += 10336   ///  r4 = r4.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r1, r4, lbb_6202                            if r1 < r4 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_6202:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_6205                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6029                                     if true { pc += -176 }
lbb_6205:
    lddw r1, 0x100017768 --> b"\x00\x00\x00\x00\x0dg\x01\x00\x15\x00\x00\x00\x00\x00\x00\x00v\x01\x00\x0…        r1 load str located at 4295063400
    call function_9564                      
lbb_6208:
    jsgt r1, 22, lbb_6228                           if (r1 as i64) > (22 as i32 as i64) { pc += 19 }
    jeq r1, 20, lbb_6257                            if r1 == (20 as i32 as i64 as u64) { pc += 47 }
    jeq r1, 21, lbb_6263                            if r1 == (21 as i32 as i64 as u64) { pc += 52 }
    lddw r0, 0x1700000000                           r0 load str located at 98784247808
    ja lbb_6222                                     if true { pc += 8 }
lbb_6214:
    jsgt r1, 8, lbb_6233                            if (r1 as i64) > (8 as i32 as i64) { pc += 18 }
    jeq r1, 6, lbb_6220                             if r1 == (6 as i32 as i64 as u64) { pc += 4 }
    jeq r1, 7, lbb_6287                             if r1 == (7 as i32 as i64 as u64) { pc += 70 }
    lddw r0, 0x900000000                            r0 load str located at 38654705664
    ja lbb_6222                                     if true { pc += 2 }
lbb_6220:
    lddw r0, 0x700000000                            r0 load str located at 30064771072
lbb_6222:
    exit                                    
lbb_6223:
    jsgt r1, 17, lbb_6243                           if (r1 as i64) > (17 as i32 as i64) { pc += 19 }
    jeq r1, 16, lbb_6266                            if r1 == (16 as i32 as i64 as u64) { pc += 41 }
    lddw r0, 0x1200000000                           r0 load str located at 77309411328
    ja lbb_6222                                     if true { pc += -6 }
lbb_6228:
    jsgt r1, 24, lbb_6247                           if (r1 as i64) > (24 as i32 as i64) { pc += 18 }
    jeq r1, 23, lbb_6269                            if r1 == (23 as i32 as i64 as u64) { pc += 39 }
    lddw r0, 0x1900000000                           r0 load str located at 107374182400
    ja lbb_6222                                     if true { pc += -11 }
lbb_6233:
    jsgt r1, 10, lbb_6250                           if (r1 as i64) > (10 as i32 as i64) { pc += 16 }
    jeq r1, 9, lbb_6290                             if r1 == (9 as i32 as i64 as u64) { pc += 55 }
    lddw r0, 0xb00000000                            r0 load str located at 47244640256
    ja lbb_6222                                     if true { pc += -16 }
lbb_6238:
    jeq r1, 3, lbb_6284                             if r1 == (3 as i32 as i64 as u64) { pc += 45 }
    jeq r1, 4, lbb_6299                             if r1 == (4 as i32 as i64 as u64) { pc += 59 }
    lddw r0, 0x600000000                            r0 load str located at 25769803776
    ja lbb_6222                                     if true { pc += -21 }
lbb_6243:
    jeq r1, 18, lbb_6272                            if r1 == (18 as i32 as i64 as u64) { pc += 28 }
    lddw r0, 0x1400000000                           r0 load str located at 85899345920
    ja lbb_6222                                     if true { pc += -25 }
lbb_6247:
    jeq r1, 25, lbb_6275                            if r1 == (25 as i32 as i64 as u64) { pc += 27 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_6222                                     if true { pc += -28 }
lbb_6250:
    jeq r1, 11, lbb_6293                            if r1 == (11 as i32 as i64 as u64) { pc += 42 }
    lddw r0, 0xd00000000                            r0 load str located at 55834574848
    ja lbb_6222                                     if true { pc += -32 }
lbb_6254:
    lddw r0, 0xe00000000                            r0 load str located at 60129542144
    ja lbb_6222                                     if true { pc += -35 }
lbb_6257:
    lddw r0, 0x1500000000                           r0 load str located at 90194313216
    ja lbb_6222                                     if true { pc += -38 }
lbb_6260:
    lddw r0, 0xf00000000                            r0 load str located at 64424509440
    ja lbb_6222                                     if true { pc += -41 }
lbb_6263:
    lddw r0, 0x1600000000                           r0 load str located at 94489280512
    ja lbb_6222                                     if true { pc += -44 }
lbb_6266:
    lddw r0, 0x1100000000                           r0 load str located at 73014444032
    ja lbb_6222                                     if true { pc += -47 }
lbb_6269:
    lddw r0, 0x1800000000                           r0 load str located at 103079215104
    ja lbb_6222                                     if true { pc += -50 }
lbb_6272:
    lddw r0, 0x1300000000                           r0 load str located at 81604378624
    ja lbb_6222                                     if true { pc += -53 }
lbb_6275:
    lddw r0, 0x1a00000000                           r0 load str located at 111669149696
    ja lbb_6222                                     if true { pc += -56 }
lbb_6278:
    ldxw r1, [r10-0x104]                    
    lddw r0, 0x100000000                            r0 load str located at 4294967296
    jeq r1, 0, lbb_6222                             if r1 == (0 as i32 as i64 as u64) { pc += -60 }
    mov64 r0, r1                                    r0 = r1
    ja lbb_6222                                     if true { pc += -62 }
lbb_6284:
    lddw r0, 0x400000000                            r0 load str located at 17179869184
    ja lbb_6222                                     if true { pc += -65 }
lbb_6287:
    lddw r0, 0x800000000                            r0 load str located at 34359738368
    ja lbb_6222                                     if true { pc += -68 }
lbb_6290:
    lddw r0, 0xa00000000                            r0 load str located at 42949672960
    ja lbb_6222                                     if true { pc += -71 }
lbb_6293:
    lddw r0, 0xc00000000                            r0 load str located at 51539607552
    ja lbb_6222                                     if true { pc += -74 }
lbb_6296:
    lddw r0, 0x200000000                            r0 load str located at 8589934592
    ja lbb_6222                                     if true { pc += -77 }
lbb_6299:
    lddw r0, 0x500000000                            r0 load str located at 21474836480
    ja lbb_6222                                     if true { pc += -80 }
lbb_6302:
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -256                                  r4 += -256   ///  r4 = r4.wrapping_add(-256 as i32 as i64 as u64)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxdw r3, [r4+0x0]                      
    stxdw [r10-0xf8], r3                    
lbb_6308:
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_6030                                     if true { pc += -280 }
lbb_6310:
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -256                                  r5 += -256   ///  r5 = r5.wrapping_add(-256 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxdw r4, [r5+0x0]                      
    stxdw [r3+0x8], r4                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_6145                                     if true { pc += -173 }
lbb_6318:
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -256                                  r5 += -256   ///  r5 = r5.wrapping_add(-256 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxdw r4, [r5+0x0]                      
    stxdw [r3+0x8], r4                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_6078                                     if true { pc += -248 }
lbb_6326:
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -256                                  r5 += -256   ///  r5 = r5.wrapping_add(-256 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxdw r4, [r5+0x0]                      
    stxdw [r3+0x8], r4                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_6177                                     if true { pc += -157 }
lbb_6334:
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -256                                  r5 += -256   ///  r5 = r5.wrapping_add(-256 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxdw r4, [r5+0x0]                      
    stxdw [r3+0x8], r4                      
    ja lbb_6308                                     if true { pc += -33 }
lbb_6341:
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -256                                  r5 += -256   ///  r5 = r5.wrapping_add(-256 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxdw r4, [r5+0x0]                      
    stxdw [r3+0x10], r4                     
    ja lbb_6308                                     if true { pc += -40 }
lbb_6348:
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -256                                  r5 += -256   ///  r5 = r5.wrapping_add(-256 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxdw r4, [r5+0x0]                      
    stxdw [r3+0x10], r4                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_6092                                     if true { pc += -264 }
lbb_6356:
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -256                                  r5 += -256   ///  r5 = r5.wrapping_add(-256 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxdw r4, [r5+0x0]                      
    stxdw [r3+0x10], r4                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_6191                                     if true { pc += -173 }
lbb_6364:
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -256                                  r5 += -256   ///  r5 = r5.wrapping_add(-256 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxdw r4, [r5+0x0]                      
    stxdw [r3+0x18], r4                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_6106                                     if true { pc += -266 }
lbb_6372:
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -256                                  r5 += -256   ///  r5 = r5.wrapping_add(-256 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxdw r4, [r5+0x0]                      
    stxdw [r3+0x18], r4                     
    ja lbb_6308                                     if true { pc += -71 }
lbb_6379:
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -256                                  r5 += -256   ///  r5 = r5.wrapping_add(-256 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxdw r4, [r5+0x0]                      
    stxdw [r3+0x20], r4                     
    ja lbb_6308                                     if true { pc += -78 }
lbb_6386:
    lddw r1, 0x1000177c8 --> b"\x00\x00\x00\x00\x0dg\x01\x00\x15\x00\x00\x00\x00\x00\x00\x00l\x01\x00\x0…        r1 load str located at 4295063496
    call function_9564                      
lbb_6389:
    lddw r1, 0x1000177b0 --> b"\x00\x00\x00\x00\x0dg\x01\x00\x15\x00\x00\x00\x00\x00\x00\x00\x9d\x01\x00…        r1 load str located at 4295063472
    call function_9564                      
lbb_6392:
    lddw r1, 0x100017720 --> b"\x00\x00\x00\x00\x0dg\x01\x00\x15\x00\x00\x00\x00\x00\x00\x00N\x01\x00\x0…        r1 load str located at 4295063328
    call function_9564                      
lbb_6395:
    lddw r1, 0x100017738 --> b"\x00\x00\x00\x00\x0dg\x01\x00\x15\x00\x00\x00\x00\x00\x00\x00h\x01\x00\x0…        r1 load str located at 4295063352
    call function_9564                      

function_6398:
    lddw r1, 0x1000176c0 --> b"\x00\x00\x00\x00\xfbf\x01\x00\x12\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295063232
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    lddw r2, 0x1000176d0 --> b"\x00\x00\x00\x00\x0dg\x01\x00\x15\x00\x00\x00\x00\x00\x00\x00\xaa\x02\x00…        r2 load str located at 4295063248
    call function_7789                      

function_6410:
    exit                                    

function_6411:
    lddw r1, 0x1000176c0 --> b"\x00\x00\x00\x00\xfbf\x01\x00\x12\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295063232
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    lddw r2, 0x1000176d0 --> b"\x00\x00\x00\x00\x0dg\x01\x00\x15\x00\x00\x00\x00\x00\x00\x00\xaa\x02\x00…        r2 load str located at 4295063248
    call function_7789                      

function_6423:
    call function_7319                      
    exit                                    

function_6425:
    mov64 r4, r2                                    r4 = r2
    ldxdw r1, [r1+0x0]                      
    ldxb r1, [r1+0x0]                       
    jsgt r1, 1, lbb_6437                            if (r1 as i64) > (1 as i32 as i64) { pc += 8 }
    lddw r2, 0x1000168e0 --> b"TargetAlignmentGreaterAndInputNotAligned"        r2 load str located at 4295059680
    mov64 r3, 40                                    r3 = 40 as i32 as i64 as u64
    jeq r1, 0, lbb_6445                             if r1 == (0 as i32 as i64 as u64) { pc += 12 }
    lddw r2, 0x100016908 --> b"OutputSliceWouldHaveSlop"        r2 load str located at 4295059720
    mov64 r3, 24                                    r3 = 24 as i32 as i64 as u64
    ja lbb_6445                                     if true { pc += 8 }
lbb_6437:
    jeq r1, 2, lbb_6442                             if r1 == (2 as i32 as i64 as u64) { pc += 4 }
    lddw r2, 0x10001692c --> b"AlignmentMismatch"        r2 load str located at 4295059756
    mov64 r3, 17                                    r3 = 17 as i32 as i64 as u64
    ja lbb_6445                                     if true { pc += 3 }
lbb_6442:
    lddw r2, 0x100016920 --> b"SizeMismatch"        r2 load str located at 4295059744
    mov64 r3, 12                                    r3 = 12 as i32 as i64 as u64
lbb_6445:
    mov64 r1, r4                                    r1 = r4
    call function_8446                      
    exit                                    

function_6448:
    stxdw [r10-0x48], r1                    
    lddw r1, 0x1000168c0 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00012345678…        r1 load str located at 4295059648
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x10000c9e8 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x…        r1 load str located at 4295018984
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    ldxdw r4, [r2+0x28]                     
    ldxdw r1, [r2+0x20]                     
    mov64 r3, r10                                   r3 = r10
    add64 r3, -64                                   r3 += -64   ///  r3 = r3.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r4                                    r2 = r4
    call function_7922                      
    exit                                    

function_6471:
    stxdw [r10-0x108], r1                   
    ldxdw r1, [r2+0x0]                      
    ldxdw r8, [r1+0x0]                      
    ldxdw r1, [r2+0x8]                      
    ldxdw r0, [r1+0x0]                      
    ldxdw r2, [r2+0x10]                     
    ldxdw r7, [r2+0x0]                      
    mov64 r2, r7                                    r2 = r7
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xe0], r2                    
    mov64 r6, r0                                    r6 = r0
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xf0], r6                    
    mov64 r1, r8                                    r1 = r8
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x100], r1                   
    sth [r10-0xd8], 256                     
    sth [r10-0xe8], 1                       
    sth [r10-0xf8], 1                       
    ldxb r5, [r8+0x0]                       
    jne r5, 255, lbb_6604                           if r5 != (255 as i32 as i64 as u64) { pc += 112 }
    stxdw [r10-0x118], r7                   
    stxdw [r10-0x120], r3                   
    stxdw [r10-0x110], r4                   
    ldxb r4, [r8+0x1]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_6500                             if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_6500:
    ldxb r9, [r8+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r9, 0, lbb_6504                             if r9 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_6504:
    ldxb r9, [r8+0x3]                       
    jne r9, 0, lbb_6507                             if r9 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_6507:
    ldxdw r9, [r8+0x50]                     
    mov64 r7, r8                                    r7 = r8
    add64 r7, 40                                    r7 += 40   ///  r7 = r7.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xb0], r7                    
    mov64 r7, r8                                    r7 = r8
    add64 r7, 88                                    r7 += 88   ///  r7 = r7.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xb8], r7                    
    stxdw [r10-0xc0], r9                    
    add64 r8, 72                                    r8 += 72   ///  r8 = r8.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0xc8], r8                    
    stxdw [r10-0xd0], r1                    
    stxb [r10-0x9e], r5                     
    stxb [r10-0x9f], r4                     
    stxb [r10-0xa0], r3                     
    stdw [r10-0xa8], 0                      
    ldxb r1, [r0+0x0]                       
    ldxdw r5, [r10-0x110]                   
    jne r1, 255, lbb_6604                           if r1 != (255 as i32 as i64 as u64) { pc += 79 }
    ldxb r1, [r0+0x1]                       
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_6530                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_6530:
    ldxb r1, [r0+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_6534                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_6534:
    ldxb r1, [r0+0x3]                       
    jne r1, 0, lbb_6537                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_6537:
    ldxdw r1, [r0+0x50]                     
    mov64 r8, r0                                    r8 = r0
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r8                    
    mov64 r8, r0                                    r8 = r0
    add64 r8, 88                                    r8 += 88   ///  r8 = r8.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r8                    
    stxdw [r10-0x88], r1                    
    add64 r0, 72                                    r0 += 72   ///  r0 = r0.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r0                    
    stxdw [r10-0x98], r6                    
    stxb [r10-0x66], r9                     
    stxb [r10-0x67], r4                     
    stxb [r10-0x68], r7                     
    stdw [r10-0x70], 0                      
    ldxdw r9, [r10-0x118]                   
    ldxb r1, [r9+0x0]                       
    and64 r1, 136                                   r1 &= 136   ///  r1 = r1.and(136)
    ldxdw r4, [r10-0x120]                   
    jne r1, 136, lbb_6604                           if r1 != (136 as i32 as i64 as u64) { pc += 47 }
    ldxb r1, [r9+0x1]                       
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_6562                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_6562:
    ldxb r1, [r9+0x2]                       
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_6566                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_6566:
    ldxdw r6, [r10-0x108]                   
    ldxb r1, [r9+0x3]                       
    jne r1, 0, lbb_6570                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_6570:
    ldxdw r1, [r9+0x50]                     
    mov64 r7, r9                                    r7 = r9
    add64 r7, 40                                    r7 += 40   ///  r7 = r7.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r7                    
    mov64 r7, r9                                    r7 = r9
    add64 r7, 88                                    r7 += 88   ///  r7 = r7.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r7                    
    stxdw [r10-0x50], r1                    
    add64 r9, 72                                    r9 += 72   ///  r9 = r9.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r9                    
    stxdw [r10-0x60], r2                    
    stxb [r10-0x2e], r8                     
    stxb [r10-0x2f], r3                     
    stxb [r10-0x30], r0                     
    stdw [r10-0x38], 0                      
    lddw r1, 0x10001693d --> b"\x09/home/runner/work/platform-tools/platform-tools/o"        r1 load str located at 4295059773
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -256                                  r1 += -256   ///  r1 = r1.wrapping_add(-256 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100016630 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r1 load str located at 4295058992
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 1                       
    stdw [r10-0x18], 3                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -208                                  r2 += -208   ///  r2 = r2.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r3, 3                                     r3 = 3 as i32 as i64 as u64
    syscall [invalid]                       
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    ja lbb_6606                                     if true { pc += 2 }
lbb_6604:
    mov64 r2, 11                                    r2 = 11 as i32 as i64 as u64
    ldxdw r6, [r10-0x108]                   
lbb_6606:
    stxw [r6+0x0], r2                       
    exit                                    

function_6608:
    ldxdw r9, [r2+0x8]                      
    ldxdw r6, [r9+0x0]                      
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r0, [r2+0x0]                      
    ldxdw r7, [r0+0x0]                      
    stxdw [r10-0xd0], r6                    
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xe0], r7                    
    sth [r10-0xc8], 0                       
    sth [r10-0xd8], 1                       
    stb [r10-0xb9], 18                      
    ldxdw r2, [r2+0x10]                     
    ldxb r6, [r2+0x0]                       
    stxb [r10-0xb8], r6                     
    ldxb r6, [r2+0x1]                       
    stxb [r10-0xb7], r6                     
    ldxb r6, [r2+0x2]                       
    stxb [r10-0xb6], r6                     
    ldxb r6, [r2+0x3]                       
    stxb [r10-0xb5], r6                     
    ldxb r6, [r2+0x4]                       
    stxb [r10-0xb4], r6                     
    ldxb r6, [r2+0x5]                       
    stxb [r10-0xb3], r6                     
    ldxb r6, [r2+0x6]                       
    stxb [r10-0xb2], r6                     
    ldxb r6, [r2+0x7]                       
    stxb [r10-0xb1], r6                     
    ldxb r6, [r2+0x8]                       
    stxb [r10-0xb0], r6                     
    ldxb r6, [r2+0x9]                       
    stxb [r10-0xaf], r6                     
    ldxb r6, [r2+0xa]                       
    stxb [r10-0xae], r6                     
    ldxb r6, [r2+0xb]                       
    stxb [r10-0xad], r6                     
    ldxb r6, [r2+0xc]                       
    stxb [r10-0xac], r6                     
    ldxb r6, [r2+0xd]                       
    stxb [r10-0xab], r6                     
    ldxb r6, [r2+0xe]                       
    stxb [r10-0xaa], r6                     
    ldxb r6, [r2+0xf]                       
    stxb [r10-0xa9], r6                     
    ldxb r6, [r2+0x10]                      
    stxb [r10-0xa8], r6                     
    ldxb r6, [r2+0x11]                      
    stxb [r10-0xa7], r6                     
    ldxb r6, [r2+0x12]                      
    stxb [r10-0xa6], r6                     
    ldxb r6, [r2+0x13]                      
    stxb [r10-0xa5], r6                     
    ldxb r6, [r2+0x14]                      
    stxb [r10-0xa4], r6                     
    ldxb r6, [r2+0x15]                      
    stxb [r10-0xa3], r6                     
    ldxb r6, [r2+0x16]                      
    stxb [r10-0xa2], r6                     
    ldxb r6, [r2+0x17]                      
    stxb [r10-0xa1], r6                     
    ldxb r6, [r2+0x18]                      
    stxb [r10-0xa0], r6                     
    ldxb r6, [r2+0x19]                      
    stxb [r10-0x9f], r6                     
    ldxb r6, [r2+0x1a]                      
    stxb [r10-0x9e], r6                     
    ldxb r6, [r2+0x1b]                      
    stxb [r10-0x9d], r6                     
    ldxb r6, [r2+0x1c]                      
    stxb [r10-0x9c], r6                     
    ldxb r6, [r2+0x1d]                      
    stxb [r10-0x9b], r6                     
    ldxb r6, [r2+0x1e]                      
    stxb [r10-0x9a], r6                     
    ldxb r2, [r2+0x1f]                      
    stxb [r10-0x99], r2                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r5, [r0+0x0]                      
    ldxdw r6, [r10-0xe0]                    
    ldxdw r7, [r6+0x0]                      
    ldxdw r8, [r5+0x8]                      
    jne r8, r7, lbb_6814                            if r8 != r7 { pc += 124 }
    ldxdw r7, [r6+0x8]                      
    ldxdw r8, [r5+0x10]                     
    jne r8, r7, lbb_6814                            if r8 != r7 { pc += 121 }
    ldxdw r7, [r6+0x10]                     
    ldxdw r8, [r5+0x18]                     
    jne r8, r7, lbb_6814                            if r8 != r7 { pc += 118 }
    ldxdw r6, [r6+0x18]                     
    ldxdw r7, [r5+0x20]                     
    jne r7, r6, lbb_6814                            if r7 != r6 { pc += 115 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxb r2, [r10-0xd8]                     
    jne r2, 0, lbb_6703                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 119                                   r6 = 119 as i32 as i64 as u64
lbb_6703:
    ldxb r2, [r5+0x0]                       
    or64 r6, r2                                     r6 |= r2   ///  r6 = r6.or(r2)
    and64 r6, 255                                   r6 &= 255   ///  r6 = r6.and(255)
    mov64 r2, 11                                    r2 = 11 as i32 as i64 as u64
    jne r6, 255, lbb_6814                           if r6 != (255 as i32 as i64 as u64) { pc += 106 }
    stxdw [r10-0xe8], r9                    
    mov64 r2, r5                                    r2 = r5
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    ldxb r6, [r5+0x1]                       
    ldxb r7, [r5+0x2]                       
    ldxb r8, [r5+0x3]                       
    ldxdw r9, [r5+0x50]                     
    mov64 r0, r5                                    r0 = r5
    add64 r0, 40                                    r0 += 40   ///  r0 = r0.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r0                    
    mov64 r0, r5                                    r0 = r5
    add64 r0, 88                                    r0 += 88   ///  r0 = r0.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r0                    
    stxdw [r10-0x88], r9                    
    add64 r5, 72                                    r5 += 72   ///  r5 = r5.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r5                    
    stxdw [r10-0x98], r2                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r8, 0, lbb_6729                             if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_6729:
    stxb [r10-0x66], r0                     
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r7, 0, lbb_6733                             if r7 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_6733:
    stxb [r10-0x67], r0                     
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r5, r4                                    r5 = r4
    mov64 r4, r3                                    r4 = r3
    ldxdw r3, [r10-0xe8]                    
    jne r6, 0, lbb_6740                             if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_6740:
    stxb [r10-0x68], r0                     
    stdw [r10-0x70], 0                      
    ldxdw r3, [r3+0x0]                      
    ldxdw r0, [r10-0xd0]                    
    ldxdw r7, [r0+0x0]                      
    ldxdw r6, [r3+0x8]                      
    jne r6, r7, lbb_6814                            if r6 != r7 { pc += 67 }
    ldxdw r7, [r0+0x8]                      
    ldxdw r6, [r3+0x10]                     
    jne r6, r7, lbb_6814                            if r6 != r7 { pc += 64 }
    ldxdw r7, [r0+0x10]                     
    ldxdw r6, [r3+0x18]                     
    jne r6, r7, lbb_6814                            if r6 != r7 { pc += 61 }
    ldxdw r6, [r0+0x18]                     
    ldxdw r0, [r3+0x20]                     
    jne r0, r6, lbb_6814                            if r0 != r6 { pc += 58 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxb r2, [r10-0xc8]                     
    jne r2, 0, lbb_6760                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 119                                   r0 = 119 as i32 as i64 as u64
lbb_6760:
    ldxb r2, [r3+0x0]                       
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    and64 r0, 255                                   r0 &= 255   ///  r0 = r0.and(255)
    mov64 r2, 11                                    r2 = 11 as i32 as i64 as u64
    jne r0, 255, lbb_6814                           if r0 != (255 as i32 as i64 as u64) { pc += 49 }
    ldxb r6, [r3+0x1]                       
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r6, 0, lbb_6770                             if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_6770:
    ldxb r6, [r3+0x2]                       
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jne r6, 0, lbb_6774                             if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_6774:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r3                                    r1 = r3
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ldxb r8, [r3+0x3]                       
    jne r8, 0, lbb_6780                             if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_6780:
    ldxdw r9, [r3+0x50]                     
    mov64 r8, r3                                    r8 = r3
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r8                    
    mov64 r8, r3                                    r8 = r3
    add64 r8, 88                                    r8 += 88   ///  r8 = r8.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r8                    
    stxdw [r10-0x50], r9                    
    add64 r3, 72                                    r3 += 72   ///  r3 = r3.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r3                    
    stxdw [r10-0x60], r1                    
    stxb [r10-0x2e], r0                     
    stxb [r10-0x2f], r7                     
    stxb [r10-0x30], r2                     
    stdw [r10-0x38], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -185                                  r1 += -185   ///  r1 = r1.wrapping_add(-185 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -224                                  r1 += -224   ///  r1 = r1.wrapping_add(-224 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100016630 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r1 load str located at 4295058992
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 33                      
    stdw [r10-0x18], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -152                                  r2 += -152   ///  r2 = r2.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    syscall [invalid]                       
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    mov64 r1, r6                                    r1 = r6
lbb_6814:
    stxw [r1+0x0], r2                       
    exit                                    

function_6816:
    stxdw [r10-0x120], r3                   
    ldxdw r3, [r2+0x0]                      
    ldxdw r9, [r3+0x0]                      
    ldxdw r3, [r2+0x8]                      
    ldxdw r6, [r3+0x0]                      
    ldxdw r3, [r2+0x10]                     
    ldxdw r3, [r3+0x0]                      
    stxdw [r10-0x118], r3                   
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xf0], r3                    
    mov64 r7, r6                                    r7 = r6
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x100], r7                   
    mov64 r0, r9                                    r0 = r9
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x110], r0                   
    sth [r10-0xe8], 256                     
    sth [r10-0xf8], 1                       
    sth [r10-0x108], 1                      
    stb [r10-0xd9], 3                       
    ldxdw r2, [r2+0x18]                     
    stxdw [r10-0xd8], r2                    
    ldxb r2, [r9+0x0]                       
    jne r2, 255, lbb_6957                           if r2 != (255 as i32 as i64 as u64) { pc += 117 }
    ldxb r2, [r9+0x1]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_6844                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_6844:
    stxdw [r10-0x130], r5                   
    ldxb r2, [r9+0x2]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_6849                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_6849:
    stxdw [r10-0x128], r1                   
    stxdw [r10-0x138], r4                   
    ldxb r2, [r9+0x3]                       
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_6855                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_6855:
    ldxdw r2, [r9+0x50]                     
    mov64 r8, r9                                    r8 = r9
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xb0], r8                    
    mov64 r8, r9                                    r8 = r9
    add64 r8, 88                                    r8 += 88   ///  r8 = r8.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xb8], r8                    
    stxdw [r10-0xc0], r2                    
    add64 r9, 72                                    r9 += 72   ///  r9 = r9.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0xc8], r9                    
    stxdw [r10-0xd0], r0                    
    stxb [r10-0x9e], r1                     
    stxb [r10-0x9f], r5                     
    ldxdw r1, [r10-0x130]                   
    stxb [r10-0xa0], r1                     
    stdw [r10-0xa8], 0                      
    ldxb r2, [r6+0x0]                       
    ldxdw r1, [r10-0x128]                   
    jne r2, 255, lbb_6957                           if r2 != (255 as i32 as i64 as u64) { pc += 83 }
    ldxb r1, [r6+0x1]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_6879                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_6879:
    ldxb r1, [r6+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r0, [r10-0x118]                   
    jne r1, 0, lbb_6884                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_6884:
    ldxb r1, [r6+0x3]                       
    jne r1, 0, lbb_6887                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_6887:
    ldxdw r1, [r6+0x50]                     
    mov64 r8, r6                                    r8 = r6
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r8                    
    mov64 r8, r6                                    r8 = r6
    add64 r8, 88                                    r8 += 88   ///  r8 = r8.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r8                    
    stxdw [r10-0x88], r1                    
    add64 r6, 72                                    r6 += 72   ///  r6 = r6.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r6                    
    stxdw [r10-0x98], r7                    
    stxb [r10-0x66], r5                     
    stxb [r10-0x67], r4                     
    stxb [r10-0x68], r2                     
    stdw [r10-0x70], 0                      
    ldxb r2, [r0+0x0]                       
    and64 r2, 136                                   r2 &= 136   ///  r2 = r2.and(136)
    ldxdw r5, [r10-0x138]                   
    ldxdw r1, [r10-0x128]                   
    jne r2, 136, lbb_6957                           if r2 != (136 as i32 as i64 as u64) { pc += 50 }
    ldxdw r0, [r10-0x118]                   
    ldxb r1, [r0+0x1]                       
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_6913                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_6913:
    ldxb r1, [r0+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_6917                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_6917:
    ldxdw r6, [r10-0x128]                   
    ldxb r1, [r0+0x3]                       
    jne r1, 0, lbb_6921                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_6921:
    ldxdw r1, [r0+0x50]                     
    mov64 r7, r0                                    r7 = r0
    add64 r7, 40                                    r7 += 40   ///  r7 = r7.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r7                    
    mov64 r7, r0                                    r7 = r0
    add64 r7, 88                                    r7 += 88   ///  r7 = r7.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r7                    
    stxdw [r10-0x50], r1                    
    add64 r0, 72                                    r0 += 72   ///  r0 = r0.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r0                    
    stxdw [r10-0x60], r3                    
    stxb [r10-0x2e], r8                     
    stxb [r10-0x2f], r4                     
    stxb [r10-0x30], r2                     
    stdw [r10-0x38], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -217                                  r1 += -217   ///  r1 = r1.wrapping_add(-217 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -272                                  r1 += -272   ///  r1 = r1.wrapping_add(-272 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100016630 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r1 load str located at 4295058992
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 9                       
    stdw [r10-0x18], 3                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -208                                  r2 += -208   ///  r2 = r2.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r3, 3                                     r3 = 3 as i32 as i64 as u64
    ldxdw r4, [r10-0x120]                   
    syscall [invalid]                       
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    mov64 r1, r6                                    r1 = r6
    ja lbb_6958                                     if true { pc += 1 }
lbb_6957:
    mov64 r2, 11                                    r2 = 11 as i32 as i64 as u64
lbb_6958:
    stxw [r1+0x0], r2                       
    exit                                    

function_6960:
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jsgt r1, 20, lbb_6972                           if (r1 as i64) > (20 as i32 as i64) { pc += 9 }
    jsgt r1, 9, lbb_6980                            if (r1 as i64) > (9 as i32 as i64) { pc += 16 }
    jsgt r1, 4, lbb_7008                            if (r1 as i64) > (4 as i32 as i64) { pc += 43 }
    jsgt r1, 1, lbb_7020                            if (r1 as i64) > (1 as i32 as i64) { pc += 54 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_7119                             if r1 == (0 as i32 as i64 as u64) { pc += 151 }
    jeq r1, 1, lbb_6970                             if r1 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7118                                     if true { pc += 148 }
lbb_6970:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 147 }
lbb_6972:
    jsgt r1, 30, lbb_6987                           if (r1 as i64) > (30 as i32 as i64) { pc += 14 }
    jsgt r1, 25, lbb_7014                           if (r1 as i64) > (25 as i32 as i64) { pc += 40 }
    jsgt r1, 22, lbb_7026                           if (r1 as i64) > (22 as i32 as i64) { pc += 51 }
    jeq r1, 21, lbb_7096                            if r1 == (21 as i32 as i64 as u64) { pc += 120 }
    jeq r1, 22, lbb_6978                            if r1 == (22 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7118                                     if true { pc += 140 }
lbb_6978:
    mov64 r0, 22                                    r0 = 22 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 139 }
lbb_6980:
    jsgt r1, 14, lbb_6994                           if (r1 as i64) > (14 as i32 as i64) { pc += 13 }
    jsgt r1, 11, lbb_7044                           if (r1 as i64) > (11 as i32 as i64) { pc += 62 }
    jeq r1, 10, lbb_7114                            if r1 == (10 as i32 as i64 as u64) { pc += 131 }
    jeq r1, 11, lbb_6985                            if r1 == (11 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7118                                     if true { pc += 133 }
lbb_6985:
    mov64 r0, 11                                    r0 = 11 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 132 }
lbb_6987:
    jsgt r1, 35, lbb_7001                           if (r1 as i64) > (35 as i32 as i64) { pc += 13 }
    jsgt r1, 32, lbb_7050                           if (r1 as i64) > (32 as i32 as i64) { pc += 61 }
    jeq r1, 31, lbb_7116                            if r1 == (31 as i32 as i64 as u64) { pc += 126 }
    jeq r1, 32, lbb_6992                            if r1 == (32 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7118                                     if true { pc += 126 }
lbb_6992:
    mov64 r0, 32                                    r0 = 32 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 125 }
lbb_6994:
    jsgt r1, 17, lbb_7056                           if (r1 as i64) > (17 as i32 as i64) { pc += 61 }
    jeq r1, 15, lbb_7088                            if r1 == (15 as i32 as i64 as u64) { pc += 92 }
    jeq r1, 16, lbb_7106                            if r1 == (16 as i32 as i64 as u64) { pc += 109 }
    jeq r1, 17, lbb_6999                            if r1 == (17 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7118                                     if true { pc += 119 }
lbb_6999:
    mov64 r0, 17                                    r0 = 17 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 118 }
lbb_7001:
    jsgt r1, 38, lbb_7062                           if (r1 as i64) > (38 as i32 as i64) { pc += 60 }
    jeq r1, 36, lbb_7090                            if r1 == (36 as i32 as i64 as u64) { pc += 87 }
    jeq r1, 37, lbb_7108                            if r1 == (37 as i32 as i64 as u64) { pc += 104 }
    jeq r1, 38, lbb_7006                            if r1 == (38 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7118                                     if true { pc += 112 }
lbb_7006:
    mov64 r0, 38                                    r0 = 38 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 111 }
lbb_7008:
    jsgt r1, 6, lbb_7032                            if (r1 as i64) > (6 as i32 as i64) { pc += 23 }
    jeq r1, 5, lbb_7098                             if r1 == (5 as i32 as i64 as u64) { pc += 88 }
    jeq r1, 6, lbb_7012                             if r1 == (6 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7118                                     if true { pc += 106 }
lbb_7012:
    mov64 r0, 6                                     r0 = 6 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 105 }
lbb_7014:
    jsgt r1, 27, lbb_7038                           if (r1 as i64) > (27 as i32 as i64) { pc += 23 }
    jeq r1, 26, lbb_7100                            if r1 == (26 as i32 as i64 as u64) { pc += 84 }
    jeq r1, 27, lbb_7018                            if r1 == (27 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7118                                     if true { pc += 100 }
lbb_7018:
    mov64 r0, 27                                    r0 = 27 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 99 }
lbb_7020:
    jeq r1, 2, lbb_7068                             if r1 == (2 as i32 as i64 as u64) { pc += 47 }
    jeq r1, 3, lbb_7080                             if r1 == (3 as i32 as i64 as u64) { pc += 58 }
    jeq r1, 4, lbb_7024                             if r1 == (4 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7118                                     if true { pc += 94 }
lbb_7024:
    mov64 r0, 4                                     r0 = 4 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 93 }
lbb_7026:
    jeq r1, 23, lbb_7070                            if r1 == (23 as i32 as i64 as u64) { pc += 43 }
    jeq r1, 24, lbb_7082                            if r1 == (24 as i32 as i64 as u64) { pc += 54 }
    jeq r1, 25, lbb_7030                            if r1 == (25 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7118                                     if true { pc += 88 }
lbb_7030:
    mov64 r0, 25                                    r0 = 25 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 87 }
lbb_7032:
    jeq r1, 7, lbb_7072                             if r1 == (7 as i32 as i64 as u64) { pc += 39 }
    jeq r1, 8, lbb_7084                             if r1 == (8 as i32 as i64 as u64) { pc += 50 }
    jeq r1, 9, lbb_7036                             if r1 == (9 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7118                                     if true { pc += 82 }
lbb_7036:
    mov64 r0, 9                                     r0 = 9 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 81 }
lbb_7038:
    jeq r1, 28, lbb_7074                            if r1 == (28 as i32 as i64 as u64) { pc += 35 }
    jeq r1, 29, lbb_7086                            if r1 == (29 as i32 as i64 as u64) { pc += 46 }
    jeq r1, 30, lbb_7042                            if r1 == (30 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7118                                     if true { pc += 76 }
lbb_7042:
    mov64 r0, 30                                    r0 = 30 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 75 }
lbb_7044:
    jeq r1, 12, lbb_7076                            if r1 == (12 as i32 as i64 as u64) { pc += 31 }
    jeq r1, 13, lbb_7102                            if r1 == (13 as i32 as i64 as u64) { pc += 56 }
    jeq r1, 14, lbb_7048                            if r1 == (14 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7118                                     if true { pc += 70 }
lbb_7048:
    mov64 r0, 14                                    r0 = 14 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 69 }
lbb_7050:
    jeq r1, 33, lbb_7078                            if r1 == (33 as i32 as i64 as u64) { pc += 27 }
    jeq r1, 34, lbb_7104                            if r1 == (34 as i32 as i64 as u64) { pc += 52 }
    jeq r1, 35, lbb_7054                            if r1 == (35 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7118                                     if true { pc += 64 }
lbb_7054:
    mov64 r0, 35                                    r0 = 35 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 63 }
lbb_7056:
    jeq r1, 18, lbb_7092                            if r1 == (18 as i32 as i64 as u64) { pc += 35 }
    jeq r1, 19, lbb_7110                            if r1 == (19 as i32 as i64 as u64) { pc += 52 }
    jeq r1, 20, lbb_7060                            if r1 == (20 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7118                                     if true { pc += 58 }
lbb_7060:
    mov64 r0, 20                                    r0 = 20 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 57 }
lbb_7062:
    jeq r1, 39, lbb_7094                            if r1 == (39 as i32 as i64 as u64) { pc += 31 }
    jeq r1, 40, lbb_7112                            if r1 == (40 as i32 as i64 as u64) { pc += 48 }
    jeq r1, 41, lbb_7066                            if r1 == (41 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7118                                     if true { pc += 52 }
lbb_7066:
    mov64 r0, 41                                    r0 = 41 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 51 }
lbb_7068:
    mov64 r0, 2                                     r0 = 2 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 49 }
lbb_7070:
    mov64 r0, 23                                    r0 = 23 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 47 }
lbb_7072:
    mov64 r0, 7                                     r0 = 7 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 45 }
lbb_7074:
    mov64 r0, 28                                    r0 = 28 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 43 }
lbb_7076:
    mov64 r0, 12                                    r0 = 12 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 41 }
lbb_7078:
    mov64 r0, 33                                    r0 = 33 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 39 }
lbb_7080:
    mov64 r0, 3                                     r0 = 3 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 37 }
lbb_7082:
    mov64 r0, 24                                    r0 = 24 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 35 }
lbb_7084:
    mov64 r0, 8                                     r0 = 8 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 33 }
lbb_7086:
    mov64 r0, 29                                    r0 = 29 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 31 }
lbb_7088:
    mov64 r0, 15                                    r0 = 15 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 29 }
lbb_7090:
    mov64 r0, 36                                    r0 = 36 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 27 }
lbb_7092:
    mov64 r0, 18                                    r0 = 18 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 25 }
lbb_7094:
    mov64 r0, 39                                    r0 = 39 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 23 }
lbb_7096:
    mov64 r0, 21                                    r0 = 21 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 21 }
lbb_7098:
    mov64 r0, 5                                     r0 = 5 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 19 }
lbb_7100:
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 17 }
lbb_7102:
    mov64 r0, 13                                    r0 = 13 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 15 }
lbb_7104:
    mov64 r0, 34                                    r0 = 34 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 13 }
lbb_7106:
    mov64 r0, 16                                    r0 = 16 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 11 }
lbb_7108:
    mov64 r0, 37                                    r0 = 37 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 9 }
lbb_7110:
    mov64 r0, 19                                    r0 = 19 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 7 }
lbb_7112:
    mov64 r0, 40                                    r0 = 40 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 5 }
lbb_7114:
    mov64 r0, 10                                    r0 = 10 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 3 }
lbb_7116:
    mov64 r0, 31                                    r0 = 31 as i32 as i64 as u64
    ja lbb_7119                                     if true { pc += 1 }
lbb_7118:
    mov64 r0, 42                                    r0 = 42 as i32 as i64 as u64
lbb_7119:
    exit                                    

function_7120:
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_6398                      
    mov64 r6, r0                                    r6 = r0
    jne r6, 0, lbb_7130                             if r6 != (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    lddw r3, 0x1000179a0 --> b"\x00\x00\x00\x00>i\x01\x00S\x00\x00\x00\x00\x00\x00\x00\x9f\x00\x00\x00\x…        r3 load str located at 4295063968
    call function_7501                      
lbb_7130:
    lddw r1, 0x706e6920666f2068                     r1 load str located at 8101528367564529768
    stxdw [r6+0x10], r1                     
    lddw r1, 0x74676e656c206465                     r1 load str located at 8387794212885652581
    stxdw [r6+0x8], r1                      
    lddw r1, 0x7463657078656e55                     r1 load str located at 8386658464824651349
    stxdw [r6+0x0], r1                      
    sth [r6+0x18], 29813                    
    mov64 r1, 24                                    r1 = 24 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6398                      
    jne r0, 0, lbb_7147                             if r0 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    call function_7505                      
lbb_7147:
    stxdw [r0+0x8], r6                      
    stdw [r0+0x10], 26                      
    stdw [r0+0x0], 26                       
    mov64 r1, 20                                    r1 = 20 as i32 as i64 as u64
    mov64 r2, r0                                    r2 = r0
    lddw r3, 0x1000179d8 --> b"\x00\x00\x00\x00\xc0\xe0\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r3 load str located at 4295064024
    call function_7291                      
    exit                                    

function_7156:
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_7161                             if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r1, [r1+0x8]                      
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6410                      
lbb_7161:
    exit                                    

function_7162:
    stdw [r1+0x0], 0                        
    exit                                    

function_7164:
    exit                                    

function_7165:
    lddw r2, 0x77694f7f95096e5                      r2 load str located at 537780998277994213
    stxdw [r1+0x8], r2                      
    lddw r2, 0xf26e267850bd6732                     r2 load str located at -977801770900297934
    stxdw [r1+0x0], r2                      
    exit                                    

function_7172:
    mov64 r6, r1                                    r6 = r1
    mov64 r7, r6                                    r7 = r6
    and64 r7, 3                                     r7 &= 3   ///  r7 = r7.and(3)
    jsgt r7, 1, lbb_7179                            if (r7 as i64) > (1 as i32 as i64) { pc += 3 }
    jeq r7, 0, lbb_7187                             if r7 == (0 as i32 as i64 as u64) { pc += 10 }
    ldxb r1, [r6+0xf]                       
    ja lbb_7188                                     if true { pc += 9 }
lbb_7179:
    mov64 r0, r6                                    r0 = r6
    jeq r7, 2, lbb_7214                             if r7 == (2 as i32 as i64 as u64) { pc += 33 }
    mov64 r1, r6                                    r1 = r6
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    call function_6960                      
    mov64 r1, r0                                    r1 = r0
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    ja lbb_7188                                     if true { pc += 1 }
lbb_7187:
    ldxb r1, [r6+0x10]                      
lbb_7188:
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r0, r6                                    r0 = r6
    jne r1, 37, lbb_7214                            if r1 != (37 as i32 as i64 as u64) { pc += 23 }
    call function_7120                      
    mov64 r1, r7                                    r1 = r7
    add64 r1, -2                                    r1 += -2   ///  r1 = r1.wrapping_add(-2 as i32 as i64 as u64)
    jlt r1, 2, lbb_7214                             if r1 < (2 as i32 as i64 as u64) { pc += 19 }
    jeq r7, 0, lbb_7214                             if r7 == (0 as i32 as i64 as u64) { pc += 18 }
    mov64 r8, r0                                    r8 = r0
    ldxdw r7, [r6-0x1]                      
    ldxdw r9, [r6+0x7]                      
    ldxdw r2, [r9+0x0]                      
    jeq r2, 0, lbb_7203                             if r2 == (0 as i32 as i64 as u64) { pc += 2 }
    mov64 r1, r7                                    r1 = r7
    callx r2                                
lbb_7203:
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    ldxdw r2, [r9+0x8]                      
    jeq r2, 0, lbb_7209                             if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r3, [r9+0x10]                     
    mov64 r1, r7                                    r1 = r7
    call function_6410                      
lbb_7209:
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6410                      
    mov64 r0, r8                                    r0 = r8
lbb_7214:
    exit                                    

function_7215:
    ldxdw r2, [r2+0x0]                      
    ldxb r3, [r2+0x0]                       
    lsh64 r3, 56                                    r3 <<= 56   ///  r3 = r3.wrapping_shl(56)
    arsh64 r3, 56                                   r3 >>= 56 (signed)   ///  r3 = (r3 as i64).wrapping_shr(56)
    jsgt r3, -1, lbb_7223                           if (r3 as i64) > (-1 as i32 as i64) { pc += 3 }
    mov64 r4, r3                                    r4 = r3
    and64 r4, 112                                   r4 &= 112   ///  r4 = r4.and(112)
    jne r4, 0, lbb_7226                             if r4 != (0 as i32 as i64 as u64) { pc += 3 }
lbb_7223:
    stw [r1+0x8], 11                        
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ja lbb_7231                                     if true { pc += 5 }
lbb_7226:
    add64 r3, -16                                   r3 += -16   ///  r3 = r3.wrapping_add(-16 as i32 as i64 as u64)
    stxb [r2+0x0], r3                       
    stxdw [r1+0x8], r2                      
    stb [r1+0x10], 4                        
    add64 r2, 72                                    r2 += 72   ///  r2 = r2.wrapping_add(72 as i32 as i64 as u64)
lbb_7231:
    stxdw [r1+0x0], r2                      
    exit                                    

function_7233:
    ldxdw r2, [r2+0x0]                      
    ldxb r3, [r2+0x0]                       
    mov64 r4, r3                                    r4 = r3
    and64 r4, 8                                     r4 &= 8   ///  r4 = r4.and(8)
    jeq r4, 0, lbb_7241                             if r4 == (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r4, r3                                    r4 = r3
    and64 r4, 7                                     r4 &= 7   ///  r4 = r4.and(7)
    jne r4, 0, lbb_7244                             if r4 != (0 as i32 as i64 as u64) { pc += 3 }
lbb_7241:
    stw [r1+0x8], 11                        
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ja lbb_7251                                     if true { pc += 7 }
lbb_7244:
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    stxb [r2+0x0], r3                       
    ldxdw r3, [r2+0x50]                     
    stxdw [r1+0x8], r3                      
    stxdw [r1+0x10], r2                     
    stb [r1+0x18], 0                        
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
lbb_7251:
    stxdw [r1+0x0], r2                      
    exit                                    

function_7253:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    syscall [invalid]                       
    jne r0, 0, lbb_7270                             if r0 != (0 as i32 as i64 as u64) { pc += 12 }
    ldxdw r1, [r10-0x8]                     
    stxdw [r6+0x28], r1                     
    ldxdw r1, [r10-0x10]                    
    stxdw [r6+0x20], r1                     
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x20]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x8], r1                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_7272                                     if true { pc += 2 }
lbb_7270:
    stw [r6+0x4], 16                        
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_7272:
    stxw [r6+0x0], r1                       
    exit                                    

function_7274:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    syscall [invalid]                       
    jne r0, 0, lbb_7287                             if r0 != (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r1, [r10-0x8]                     
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x10]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x8], r1                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_7289                                     if true { pc += 2 }
lbb_7287:
    stw [r6+0x4], 16                        
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_7289:
    stxw [r6+0x0], r1                       
    exit                                    

function_7291:
    mov64 r7, r3                                    r7 = r3
    mov64 r6, r2                                    r6 = r2
    mov64 r8, r1                                    r8 = r1
    mov64 r1, 24                                    r1 = 24 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6398                      
    jne r0, 0, lbb_7301                             if r0 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    call function_7505                      
lbb_7301:
    stxb [r0+0x10], r8                      
    stxdw [r0+0x8], r7                      
    stxdw [r0+0x0], r6                      
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    exit                                    

function_7306:
    call function_7313                      

function_7307:
    call function_7311                      

custom_panic:
    exit                                    

function_7309:
    syscall [invalid]                       
    exit                                    

function_7311:
    call custom_panic                       
    syscall [invalid]                       

function_7313:
    syscall [invalid]                       

function_7314:
    lddw r1, 0x100016991 --> b"Error: memory allocation failed, out of memory"        r1 load str located at 4295059857
    mov64 r2, 46                                    r2 = 46 as i32 as i64 as u64
    call function_7309                      
    call function_7306                      

function_7319:
    call function_7314                      
    mov64 r3, r2                                    r3 = r2
    lddw r2, 0x100017a30 --> b"\x00\x00\x00\x00\x08\xe6\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r2 load str located at 4295064112
    call function_7922                      
    exit                                    

function_7325:
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_7330                             if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r1, [r1+0x8]                      
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6410                      
lbb_7330:
    exit                                    

function_7331:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x1000169bf --> b"Error"               r2 load str located at 4295059903
    mov64 r3, 5                                     r3 = 5 as i32 as i64 as u64
    call function_8446                      
    exit                                    

function_7337:
    mov64 r2, r1                                    r2 = r1
    lddw r1, 0x100017a60 --> b"\x00\x00\x00\x00\xc4i\x01\x00\x11\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295064160
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_7789                      

function_7348:
    mov64 r6, r2                                    r6 = r2
    mov64 r2, r1                                    r2 = r1
    ldxdw r3, [r2+0x0]                      
    stdw [r10-0xff8], 1                     
    stdw [r10-0x1000], 1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_7385                      
    ldxdw r1, [r10-0x10]                    
    lddw r2, 0x8000000000000001                     r2 load str located at -9223372036854775807
    jeq r1, r2, lbb_7365                            if r1 == r2 { pc += 3 }
    ldxdw r2, [r10-0x8]                     
    mov64 r3, r6                                    r3 = r6
    call function_7501                      
lbb_7365:
    exit                                    

function_7366:
    mov64 r0, r3                                    r0 = r3
    mov64 r3, r2                                    r3 = r2
    mov64 r2, r1                                    r2 = r1
    stxdw [r10-0xff8], r5                   
    stxdw [r10-0x1000], r4                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r4, r0                                    r4 = r0
    call function_7385                      
    ldxdw r1, [r10-0x10]                    
    lddw r2, 0x8000000000000001                     r2 load str located at -9223372036854775807
    jeq r1, r2, lbb_7384                            if r1 == r2 { pc += 4 }
    ldxdw r2, [r10-0x8]                     
    lddw r3, 0x100017a70 --> b"\x00\x00\x00\x00\xd5i\x01\x00\x14\x00\x00\x00\x00\x00\x00\x00+\x02\x00\x0…        r3 load str located at 4295064176
    call function_7501                      
lbb_7384:
    exit                                    

function_7385:
    stxdw [r10-0x48], r2                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxdw r7, [r5-0xff8]                    
    jeq r7, 0, lbb_7395                             if r7 == (0 as i32 as i64 as u64) { pc += 6 }
    mov64 r0, r3                                    r0 = r3
    add64 r0, r4                                    r0 += r4   ///  r0 = r0.wrapping_add(r4)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r0, r3, lbb_7394                            if r0 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_7394:
    jne r2, 1, lbb_7398                             if r2 != (1 as i32 as i64 as u64) { pc += 3 }
lbb_7395:
    stxdw [r1+0x8], r0                      
    stxdw [r1+0x0], r6                      
    exit                                    
lbb_7398:
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxdw r2, [r10-0x48]                    
    ldxdw r2, [r2+0x0]                      
    stxdw [r10-0x58], r2                    
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    jgt r2, r4, lbb_7405                            if r2 > r4 { pc += 1 }
    mov64 r2, r4                                    r2 = r4
lbb_7405:
    mov64 r8, 8                                     r8 = 8 as i32 as i64 as u64
    jeq r7, 1, lbb_7409                             if r7 == (1 as i32 as i64 as u64) { pc += 2 }
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jlt r7, 1025, lbb_7462                          if r7 < (1025 as i32 as i64 as u64) { pc += 53 }
lbb_7409:
    stxdw [r10-0x50], r1                    
    ldxdw r9, [r5-0x1000]                   
    jgt r8, r2, lbb_7413                            if r8 > r2 { pc += 1 }
    mov64 r8, r2                                    r8 = r2
lbb_7413:
    mov64 r2, r9                                    r2 = r9
    add64 r2, r7                                    r2 += r7   ///  r2 = r2.wrapping_add(r7)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r8                                    r4 = r8
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r1, [r10-0x38]                    
    jne r1, 0, lbb_7429                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_7429:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    ldxdw r1, [r10-0x50]                    
    jne r2, 0, lbb_7395                             if r2 != (0 as i32 as i64 as u64) { pc += -37 }
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    sub64 r2, r9                                    r2 -= r9   ///  r2 = r2.wrapping_sub(r9)
    ldxdw r3, [r10-0x40]                    
    jgt r3, r2, lbb_7395                            if r3 > r2 { pc += -42 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0x58]                    
    jeq r2, 0, lbb_7446                             if r2 == (0 as i32 as i64 as u64) { pc += 6 }
    mul64 r2, r7                                    r2 *= r7   ///  r2 = r2.wrapping_mul(r7)
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x8]                      
    stxdw [r10-0x8], r2                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r9                                    r1 = r9
lbb_7446:
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -24                                   r4 += -24   ///  r4 = r4.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    call function_7468                      
    ldxdw r1, [r10-0x30]                    
    jne r1, 0, lbb_7464                             if r1 != (0 as i32 as i64 as u64) { pc += 9 }
    ldxdw r1, [r10-0x28]                    
    ldxdw r2, [r10-0x48]                    
    stxdw [r2+0x0], r8                      
    stxdw [r2+0x8], r1                      
    lddw r6, 0x8000000000000001                     r6 load str located at -9223372036854775807
    ja lbb_7466                                     if true { pc += 4 }
lbb_7462:
    mov64 r8, 4                                     r8 = 4 as i32 as i64 as u64
    ja lbb_7409                                     if true { pc += -55 }
lbb_7464:
    ldxdw r0, [r10-0x20]                    
    ldxdw r6, [r10-0x28]                    
lbb_7466:
    ldxdw r1, [r10-0x50]                    
    ja lbb_7395                                     if true { pc += -73 }

function_7468:
    mov64 r8, r3                                    r8 = r3
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r4+0x8]                      
    jeq r1, 0, lbb_7487                             if r1 == (0 as i32 as i64 as u64) { pc += 14 }
    ldxdw r2, [r4+0x10]                     
    jne r2, 0, lbb_7478                             if r2 != (0 as i32 as i64 as u64) { pc += 3 }
    jne r8, 0, lbb_7489                             if r8 != (0 as i32 as i64 as u64) { pc += 13 }
lbb_7476:
    mov64 r0, r7                                    r0 = r7
    ja lbb_7492                                     if true { pc += 14 }
lbb_7478:
    ldxdw r1, [r4+0x0]                      
    mov64 r3, r7                                    r3 = r7
    mov64 r4, r8                                    r4 = r8
    call function_6411                      
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_7492                             if r1 == (0 as i32 as i64 as u64) { pc += 7 }
    mov64 r0, r1                                    r0 = r1
    ja lbb_7492                                     if true { pc += 5 }
lbb_7487:
    jne r8, 0, lbb_7489                             if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7476                                     if true { pc += -13 }
lbb_7489:
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r7                                    r2 = r7
    call function_6398                      
lbb_7492:
    stxdw [r6+0x10], r8                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r0, 0, lbb_7496                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_7496:
    jeq r0, 0, lbb_7498                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, r0                                    r7 = r0
lbb_7498:
    stxdw [r6+0x8], r7                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_7501:
    jne r1, 0, lbb_7504                             if r1 != (0 as i32 as i64 as u64) { pc += 2 }
    mov64 r1, r3                                    r1 = r3
    call function_7337                      
lbb_7504:
    call function_7505                      

function_7505:
    mov64 r3, r1                                    r3 = r1
    mov64 r1, r2                                    r1 = r2
    mov64 r2, r3                                    r2 = r3
    call function_6423                      
    ldxdw r3, [r2+0x10]                     
    stxdw [r1+0x8], r3                      
    ldxdw r2, [r2+0x8]                      
    stxdw [r1+0x0], r2                      
    exit                                    

function_7514:
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r1+0x10]                     
    ldxdw r1, [r1+0x8]                      
    call function_8751                      
    exit                                    

function_7519:
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r1+0x10]                     
    ldxdw r1, [r1+0x8]                      
    call function_8452                      
    exit                                    

function_7524:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r2, [r7+0x8]                      
    jeq r2, 0, lbb_7552                             if r2 == (0 as i32 as i64 as u64) { pc += 24 }
    ldxdw r1, [r7+0x0]                      
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r3, r1                                    r3 = r1
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
lbb_7532:
    ldxdw r8, [r3+0x0]                      
    add64 r8, r4                                    r8 += r4   ///  r8 = r8.wrapping_add(r4)
    add64 r3, 16                                    r3 += 16   ///  r3 = r3.wrapping_add(16 as i32 as i64 as u64)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r4, r8                                    r4 = r8
    jne r2, 0, lbb_7532                             if r2 != (0 as i32 as i64 as u64) { pc += -6 }
    ldxdw r2, [r7+0x18]                     
    jeq r2, 0, lbb_7571                             if r2 == (0 as i32 as i64 as u64) { pc += 31 }
    ldxdw r3, [r1+0x8]                      
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jeq r3, 0, lbb_7545                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_7545:
    jlt r8, 16, lbb_7547                            if r8 < (16 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_7547:
    jslt r8, 0, lbb_7552                            if (r8 as i64) < (0 as i32 as i64) { pc += 4 }
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    lsh64 r8, 1                                     r8 <<= 1   ///  r8 = r8.wrapping_shl(1)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jeq r1, 0, lbb_7571                             if r1 == (0 as i32 as i64 as u64) { pc += 19 }
lbb_7552:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_7554:
    stxdw [r10-0x18], r0                    
    stxdw [r10-0x20], r1                    
    stdw [r10-0x10], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    lddw r2, 0x100017a30 --> b"\x00\x00\x00\x00\x08\xe6\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r2 load str located at 4295064112
    mov64 r3, r7                                    r3 = r7
    call function_7922                      
    jne r0, 0, lbb_7588                             if r0 != (0 as i32 as i64 as u64) { pc += 24 }
    ldxdw r1, [r10-0x10]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x8], r1                      
    ldxdw r1, [r10-0x20]                    
    stxdw [r6+0x0], r1                      
    exit                                    
lbb_7571:
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jslt r8, 0, lbb_7583                            if (r8 as i64) < (0 as i32 as i64) { pc += 10 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_7554                             if r8 == (0 as i32 as i64 as u64) { pc += -22 }
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    mov64 r1, r8                                    r1 = r8
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_6398                      
    jeq r0, 0, lbb_7583                             if r0 == (0 as i32 as i64 as u64) { pc += 2 }
    mov64 r1, r8                                    r1 = r8
    ja lbb_7554                                     if true { pc += -29 }
lbb_7583:
    mov64 r1, r9                                    r1 = r9
    mov64 r2, r8                                    r2 = r8
    lddw r3, 0x100017a88 --> b"\x00\x00\x00\x00\xe9i\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xea\x01\x00…        r3 load str located at 4295064200
    call function_7501                      
lbb_7588:
    mov64 r3, r10                                   r3 = r10
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    lddw r1, 0x1000169fc --> b"a formatting trait implementation returned an error when the underlying s…        r1 load str located at 4295059964
    mov64 r2, 86                                    r2 = 86 as i32 as i64 as u64
    lddw r4, 0x100017aa0 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\…        r4 load str located at 4295064224
    lddw r5, 0x100017ac0 --> b"\x00\x00\x00\x00\x90h\x01\x00\x10\x00\x00\x00\x00\x00\x00\x00\x88\x02\x00…        r5 load str located at 4295064256
    call function_7833                      
    mov64 r6, r3                                    r6 = r3
    mov64 r8, r2                                    r8 = r2
    mov64 r7, r1                                    r7 = r1
    ldxdw r9, [r7+0x10]                     
    ldxdw r1, [r7+0x0]                      
    sub64 r1, r9                                    r1 -= r9   ///  r1 = r1.wrapping_sub(r9)
    jge r1, r6, lbb_7612                            if r1 >= r6 { pc += 7 }
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r9                                    r2 = r9
    mov64 r3, r6                                    r3 = r6
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    call function_7366                      
    ldxdw r9, [r7+0x10]                     
lbb_7612:
    ldxdw r1, [r7+0x8]                      
    add64 r1, r9                                    r1 += r9   ///  r1 = r1.wrapping_add(r9)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r6                                    r3 = r6
    call function_9980                      
    add64 r9, r6                                    r9 += r6   ///  r9 = r9.wrapping_add(r6)
    stxdw [r7+0x10], r9                     
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    exit                                    

function_7621:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r7                                    r1 = r7
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jlt r1, 128, lbb_7651                           if r1 < (128 as i32 as i64 as u64) { pc += 24 }
    stw [r10-0x4], 0                        
    jlt r1, 2048, lbb_7664                          if r1 < (2048 as i32 as i64 as u64) { pc += 35 }
    mov64 r1, r7                                    r1 = r7
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jlt r1, 65536, lbb_7672                         if r1 < (65536 as i32 as i64 as u64) { pc += 39 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    rsh64 r2, 18                                    r2 >>= 18   ///  r2 = r2.wrapping_shr(18)
    or64 r2, 240                                    r2 |= 240   ///  r2 = r2.or(240)
    stxb [r10-0x4], r2                      
    mov64 r2, r7                                    r2 = r7
    rsh64 r2, 6                                     r2 >>= 6   ///  r2 = r2.wrapping_shr(6)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    or64 r2, 128                                    r2 |= 128   ///  r2 = r2.or(128)
    stxb [r10-0x2], r2                      
    mov64 r2, r7                                    r2 = r7
    rsh64 r2, 12                                    r2 >>= 12   ///  r2 = r2.wrapping_shr(12)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    or64 r2, 128                                    r2 |= 128   ///  r2 = r2.or(128)
    stxb [r10-0x3], r2                      
    mov64 r8, 4                                     r8 = 4 as i32 as i64 as u64
    ja lbb_7684                                     if true { pc += 33 }
lbb_7651:
    ldxdw r8, [r6+0x10]                     
    ldxdw r1, [r6+0x0]                      
    jne r8, r1, lbb_7658                            if r8 != r1 { pc += 4 }
    mov64 r1, r6                                    r1 = r6
    lddw r2, 0x100017ad8 --> b"\x00\x00\x00\x00\xe9i\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\x8d\x05\x00…        r2 load str located at 4295064280
    call function_7348                      
lbb_7658:
    ldxdw r1, [r6+0x8]                      
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    stxb [r1+0x0], r7                       
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r6+0x10], r8                     
    ja lbb_7706                                     if true { pc += 42 }
lbb_7664:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -3                                    r1 += -3   ///  r1 = r1.wrapping_add(-3 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    rsh64 r2, 6                                     r2 >>= 6   ///  r2 = r2.wrapping_shr(6)
    or64 r2, 192                                    r2 |= 192   ///  r2 = r2.or(192)
    stxb [r10-0x4], r2                      
    mov64 r8, 2                                     r8 = 2 as i32 as i64 as u64
    ja lbb_7684                                     if true { pc += 12 }
lbb_7672:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2                                    r1 += -2   ///  r1 = r1.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    rsh64 r2, 12                                    r2 >>= 12   ///  r2 = r2.wrapping_shr(12)
    or64 r2, 224                                    r2 |= 224   ///  r2 = r2.or(224)
    stxb [r10-0x4], r2                      
    mov64 r2, r7                                    r2 = r7
    rsh64 r2, 6                                     r2 >>= 6   ///  r2 = r2.wrapping_shr(6)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    or64 r2, 128                                    r2 |= 128   ///  r2 = r2.or(128)
    stxb [r10-0x3], r2                      
    mov64 r8, 3                                     r8 = 3 as i32 as i64 as u64
lbb_7684:
    and64 r7, 63                                    r7 &= 63   ///  r7 = r7.and(63)
    or64 r7, 128                                    r7 |= 128   ///  r7 = r7.or(128)
    stxb [r1+0x0], r7                       
    ldxdw r7, [r6+0x10]                     
    ldxdw r1, [r6+0x0]                      
    sub64 r1, r7                                    r1 -= r7   ///  r1 = r1.wrapping_sub(r7)
    jge r1, r8, lbb_7698                            if r1 >= r8 { pc += 7 }
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r8                                    r3 = r8
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    call function_7366                      
    ldxdw r7, [r6+0x10]                     
lbb_7698:
    ldxdw r1, [r6+0x8]                      
    add64 r1, r7                                    r1 += r7   ///  r1 = r1.wrapping_add(r7)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -4                                    r2 += -4   ///  r2 = r2.wrapping_add(-4 as i32 as i64 as u64)
    mov64 r3, r8                                    r3 = r8
    call function_9980                      
    add64 r7, r8                                    r7 += r8   ///  r7 = r7.wrapping_add(r8)
    stxdw [r6+0x10], r7                     
lbb_7706:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    exit                                    

function_7708:
    mov64 r6, r2                                    r6 = r2
    mov64 r7, r1                                    r7 = r1
    ldxw r1, [r6+0x34]                      
    mov64 r2, r1                                    r2 = r1
    and64 r2, 16                                    r2 &= 16   ///  r2 = r2.and(16)
    jne r2, 0, lbb_7729                             if r2 != (0 as i32 as i64 as u64) { pc += 15 }
    and64 r1, 32                                    r1 &= 32   ///  r1 = r1.and(32)
    jne r1, 0, lbb_7723                             if r1 != (0 as i32 as i64 as u64) { pc += 7 }
    ldxdw r1, [r7+0x0]                      
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r3, r6                                    r3 = r6
    call function_9733                      
    jne r0, 0, lbb_7762                             if r0 != (0 as i32 as i64 as u64) { pc += 40 }
    ja lbb_7734                                     if true { pc += 11 }
lbb_7723:
    ldxdw r2, [r7+0x0]                      
    mov64 r3, r6                                    r3 = r6
    call function_7892                      
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_7762                             if r0 != (0 as i32 as i64 as u64) { pc += 34 }
    ja lbb_7734                                     if true { pc += 5 }
lbb_7729:
    ldxdw r2, [r7+0x0]                      
    mov64 r3, r6                                    r3 = r6
    call function_7862                      
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_7762                             if r0 != (0 as i32 as i64 as u64) { pc += 28 }
lbb_7734:
    ldxdw r1, [r6+0x20]                     
    ldxdw r2, [r6+0x28]                     
    ldxdw r4, [r2+0x18]                     
    lddw r2, 0x100016ad4 --> b".."                  r2 load str located at 4295060180
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    callx r4                                
    jne r0, 0, lbb_7762                             if r0 != (0 as i32 as i64 as u64) { pc += 20 }
    ldxw r1, [r6+0x34]                      
    mov64 r2, r1                                    r2 = r1
    and64 r2, 16                                    r2 &= 16   ///  r2 = r2.and(16)
    jne r2, 0, lbb_7754                             if r2 != (0 as i32 as i64 as u64) { pc += 8 }
    and64 r1, 32                                    r1 &= 32   ///  r1 = r1.and(32)
    jeq r1, 0, lbb_7749                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7758                                     if true { pc += 9 }
lbb_7749:
    ldxdw r1, [r7+0x8]                      
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r3, r6                                    r3 = r6
    call function_9733                      
    ja lbb_7761                                     if true { pc += 7 }
lbb_7754:
    ldxdw r2, [r7+0x8]                      
    mov64 r3, r6                                    r3 = r6
    call function_7862                      
    ja lbb_7761                                     if true { pc += 3 }
lbb_7758:
    ldxdw r2, [r7+0x8]                      
    mov64 r3, r6                                    r3 = r6
    call function_7892                      
lbb_7761:
    mov64 r8, r0                                    r8 = r0
lbb_7762:
    mov64 r0, r8                                    r0 = r8
    exit                                    

function_7764:
    mov64 r4, r1                                    r4 = r1
    ldxb r1, [r4+0x0]                       
    jeq r1, 128, lbb_7777                           if r1 == (128 as i32 as i64 as u64) { pc += 10 }
    ldxb r3, [r4+0xb]                       
    ldxb r1, [r4+0xa]                       
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    ldxdw r1, [r2+0x20]                     
    ldxdw r2, [r2+0x28]                     
    ldxdw r5, [r2+0x18]                     
    mov64 r2, r4                                    r2 = r4
    callx r5                                
    ja lbb_7783                                     if true { pc += 6 }
lbb_7777:
    ldxw r3, [r4+0x4]                       
    ldxdw r1, [r2+0x20]                     
    ldxdw r2, [r2+0x28]                     
    ldxdw r4, [r2+0x20]                     
    mov64 r2, r3                                    r2 = r3
    callx r4                                
lbb_7783:
    exit                                    

function_7784:
    mov64 r3, r1                                    r3 = r1
    lddw r1, 0x100016ae2 --> b"called `Option::unwrap()` on a `None` value"        r1 load str located at 4295060194
    mov64 r2, 43                                    r2 = 43 as i32 as i64 as u64
    call function_7795                      

function_7789:
    stxdw [r10-0x10], r2                    
    stxdw [r10-0x18], r1                    
    sth [r10-0x8], 1                        
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    call function_7307                      

function_7795:
    mov64 r4, r10                                   r4 = r10
    add64 r4, -16                                   r4 += -16   ///  r4 = r4.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x40], r4                    
    stxdw [r10-0x8], r2                     
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 0                      
    stdw [r10-0x30], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_7789                      

function_7808:
    stxdw [r10-0x58], r2                    
    stxdw [r10-0x60], r1                    
    lddw r1, 0x100017b30 --> b"\x00\x00\x00\x00\x10f\x01\x00 \x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295064368
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x100013120 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295045408
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_7789                      

function_7833:
    stxdw [r10-0x68], r2                    
    stxdw [r10-0x70], r1                    
    stxdw [r10-0x58], r4                    
    stxdw [r10-0x60], r3                    
    lddw r1, 0x100017b50 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295064400
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    lddw r1, 0x1000133f0 --> b"y\x13\x00\x00\x00\x00\x00\x00y\x11\x08\x00\x00\x00\x00\x00y\x14\x18\x00\x…        r1 load str located at 4295046128
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x100013420 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x13\x08\x00\x00\x00\x00\x00y\x12\x00\x00\x…        r1 load str located at 4295046176
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r5                                    r2 = r5
    call function_7789                      

function_7862:
    stxdw [r10-0x88], r3                    
    mov64 r7, r2                                    r7 = r2
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64

function_7865:
    mov64 r9, r7                                    r9 = r7
    mov64 r6, r10                                   r6 = r10
    add64 r6, -128                                  r6 += -128   ///  r6 = r6.wrapping_add(-128 as i32 as i64 as u64)
    add64 r6, r8                                    r6 += r8   ///  r6 = r6.wrapping_add(r8)
    mov64 r1, r9                                    r1 = r9
    and64 r1, 15                                    r1 &= 15   ///  r1 = r1.and(15)
    call function_9608                      
    stxb [r6+0x7f], r0                      
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    rsh64 r7, 4                                     r7 >>= 4   ///  r7 = r7.wrapping_shr(4)
    jgt r9, 15, function_7865                       if r9 > (15 as i32 as i64 as u64) { pc += -11 }
    mov64 r1, r8                                    r1 = r8
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    add64 r1, 128                                   r1 += 128   ///  r1 = r1.wrapping_add(128 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    ldxdw r1, [r10-0x88]                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x100016b34 --> b"0x"                  r3 load str located at 4295060276
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    call function_8063                      
    exit                                    

function_7892:
    stxdw [r10-0x88], r3                    
    mov64 r7, r2                                    r7 = r2
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64

function_7895:
    mov64 r9, r7                                    r9 = r7
    mov64 r6, r10                                   r6 = r10
    add64 r6, -128                                  r6 += -128   ///  r6 = r6.wrapping_add(-128 as i32 as i64 as u64)
    add64 r6, r8                                    r6 += r8   ///  r6 = r6.wrapping_add(r8)
    mov64 r1, r9                                    r1 = r9
    and64 r1, 15                                    r1 &= 15   ///  r1 = r1.and(15)
    call function_9643                      
    stxb [r6+0x7f], r0                      
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    rsh64 r7, 4                                     r7 >>= 4   ///  r7 = r7.wrapping_shr(4)
    jgt r9, 15, function_7895                       if r9 > (15 as i32 as i64 as u64) { pc += -11 }
    mov64 r1, r8                                    r1 = r8
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    add64 r1, 128                                   r1 += 128   ///  r1 = r1.wrapping_add(128 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    ldxdw r1, [r10-0x88]                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x100016b34 --> b"0x"                  r3 load str located at 4295060276
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    call function_8063                      
    exit                                    

function_7922:
    stxdw [r10-0x18], r2                    
    stxdw [r10-0x20], r1                    
    stb [r10-0x8], 3                        
    stdw [r10-0x10], 32                     
    stdw [r10-0x30], 0                      
    stdw [r10-0x40], 0                      
    ldxdw r8, [r3+0x20]                     
    stxdw [r10-0x58], r3                    
    jne r8, 0, lbb_7960                             if r8 != (0 as i32 as i64 as u64) { pc += 29 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r1, [r3+0x18]                     
    jeq r1, 0, lbb_8044                             if r1 == (0 as i32 as i64 as u64) { pc += 110 }
    ldxdw r2, [r10-0x58]                    
    ldxdw r6, [r2+0x10]                     
    lsh64 r1, 4                                     r1 <<= 4   ///  r1 = r1.wrapping_shl(4)
    mov64 r8, r6                                    r8 = r6
    add64 r8, r1                                    r8 += r1   ///  r8 = r8.wrapping_add(r1)
    ldxdw r9, [r2+0x0]                      
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
lbb_7941:
    ldxdw r3, [r9+0x0]                      
    jeq r3, 0, lbb_7949                             if r3 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r1, [r10-0x18]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r2, [r9-0x8]                      
    ldxdw r1, [r10-0x20]                    
    callx r4                                
    jne r0, 0, lbb_8061                             if r0 != (0 as i32 as i64 as u64) { pc += 112 }
lbb_7949:
    ldxdw r3, [r6+0x8]                      
    ldxdw r1, [r6+0x0]                      
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    callx r3                                
    jne r0, 0, lbb_8061                             if r0 != (0 as i32 as i64 as u64) { pc += 106 }
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    add64 r9, 16                                    r9 += 16   ///  r9 = r9.wrapping_add(16 as i32 as i64 as u64)
    add64 r6, 16                                    r6 += 16   ///  r6 = r6.wrapping_add(16 as i32 as i64 as u64)
    jeq r6, r8, lbb_8044                            if r6 == r8 { pc += 85 }
    ja lbb_7941                                     if true { pc += -19 }
lbb_7960:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r1, [r3+0x28]                     
    stxdw [r10-0x50], r1                    
    jeq r1, 0, lbb_8044                             if r1 == (0 as i32 as i64 as u64) { pc += 80 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x50]                    
    mul64 r1, 56                                    r1 *= 56   ///  r1 = r1.wrapping_mul(56 as u64)
    stxdw [r10-0x50], r1                    
    ldxdw r1, [r10-0x58]                    
    ldxdw r2, [r1+0x10]                     
    stxdw [r10-0x48], r2                    
    ldxdw r9, [r1+0x0]                      
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
lbb_7973:
    ldxdw r3, [r9+0x0]                      
    jeq r3, 0, lbb_7981                             if r3 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r1, [r10-0x18]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r2, [r9-0x8]                      
    ldxdw r1, [r10-0x20]                    
    callx r4                                
    jne r0, 0, lbb_8061                             if r0 != (0 as i32 as i64 as u64) { pc += 80 }
lbb_7981:
    mov64 r2, r8                                    r2 = r8
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    ldxw r1, [r2+0x28]                      
    stxw [r10-0x10], r1                     
    ldxb r1, [r2+0x30]                      
    stxb [r10-0x8], r1                      
    ldxw r1, [r2+0x2c]                      
    stxw [r10-0xc], r1                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r5, [r2+0x10]                     
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jeq r5, 2, lbb_8006                             if r5 == (2 as i32 as i64 as u64) { pc += 13 }
    jeq r5, 1, lbb_7996                             if r5 == (1 as i32 as i64 as u64) { pc += 2 }
    add64 r2, 16                                    r2 += 16   ///  r2 = r2.wrapping_add(16 as i32 as i64 as u64)
    ja lbb_8004                                     if true { pc += 8 }
lbb_7996:
    mov64 r2, r8                                    r2 = r8
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    ldxdw r3, [r2+0x18]                     
    lsh64 r3, 4                                     r3 <<= 4   ///  r3 = r3.wrapping_shl(4)
    ldxdw r2, [r10-0x48]                    
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    ldxdw r5, [r2+0x0]                      
    jne r5, 0, lbb_8006                             if r5 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_8004:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r3, [r2+0x8]                      
lbb_8006:
    mov64 r2, r8                                    r2 = r8
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    stxdw [r10-0x38], r3                    
    stxdw [r10-0x40], r4                    
    ldxdw r4, [r2+0x0]                      
    jeq r4, 2, lbb_8024                             if r4 == (2 as i32 as i64 as u64) { pc += 12 }
    jeq r4, 1, lbb_8014                             if r4 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8022                                     if true { pc += 8 }
lbb_8014:
    mov64 r2, r8                                    r2 = r8
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    ldxdw r3, [r2+0x8]                      
    lsh64 r3, 4                                     r3 <<= 4   ///  r3 = r3.wrapping_shl(4)
    ldxdw r2, [r10-0x48]                    
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    ldxdw r4, [r2+0x0]                      
    jne r4, 0, lbb_8024                             if r4 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_8022:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r3, [r2+0x8]                      
lbb_8024:
    stxdw [r10-0x28], r3                    
    stxdw [r10-0x30], r1                    
    mov64 r1, r8                                    r1 = r8
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    ldxdw r1, [r1+0x20]                     
    lsh64 r1, 4                                     r1 <<= 4   ///  r1 = r1.wrapping_shl(4)
    ldxdw r2, [r10-0x48]                    
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxdw r3, [r2+0x8]                      
    ldxdw r1, [r2+0x0]                      
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    callx r3                                
    jne r0, 0, lbb_8061                             if r0 != (0 as i32 as i64 as u64) { pc += 23 }
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    add64 r9, 16                                    r9 += 16   ///  r9 = r9.wrapping_add(16 as i32 as i64 as u64)
    add64 r6, 56                                    r6 += 56   ///  r6 = r6.wrapping_add(56 as i32 as i64 as u64)
    ldxdw r1, [r10-0x50]                    
    jeq r1, r6, lbb_8044                            if r1 == r6 { pc += 1 }
    ja lbb_7973                                     if true { pc += -71 }
lbb_8044:
    ldxdw r1, [r10-0x58]                    
    ldxdw r1, [r1+0x8]                      
    jlt r7, r1, lbb_8048                            if r7 < r1 { pc += 1 }
    ja lbb_8059                                     if true { pc += 11 }
lbb_8048:
    lsh64 r7, 4                                     r7 <<= 4   ///  r7 = r7.wrapping_shl(4)
    ldxdw r1, [r10-0x58]                    
    ldxdw r1, [r1+0x0]                      
    add64 r1, r7                                    r1 += r7   ///  r1 = r1.wrapping_add(r7)
    ldxdw r3, [r1+0x8]                      
    ldxdw r2, [r1+0x0]                      
    ldxdw r1, [r10-0x18]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r10-0x20]                    
    callx r4                                
    jne r0, 0, lbb_8061                             if r0 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_8059:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_8062                                     if true { pc += 1 }
lbb_8061:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_8062:
    exit                                    

function_8063:
    mov64 r6, r1                                    r6 = r1
    ldxdw r0, [r5-0xff8]                    
    jne r2, 0, lbb_8069                             if r2 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r2, 45                                    r2 = 45 as i32 as i64 as u64
    ldxw r7, [r6+0x34]                      
    ja lbb_8092                                     if true { pc += 23 }
lbb_8069:
    mov64 r2, 1114112                               r2 = 1114112 as i32 as i64 as u64
    ldxw r7, [r6+0x34]                      
    mov64 r1, r7                                    r1 = r7
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    mov64 r9, r0                                    r9 = r0
    jne r1, 0, lbb_8091                             if r1 != (0 as i32 as i64 as u64) { pc += 16 }
lbb_8075:
    stxdw [r10-0x28], r0                    
    ldxdw r1, [r5-0x1000]                   
    stxdw [r10-0x30], r1                    
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, r7                                    r1 = r7
    and64 r1, 4                                     r1 &= 4   ///  r1 = r1.and(4)
    jeq r1, 0, lbb_8115                             if r1 == (0 as i32 as i64 as u64) { pc += 33 }
    stxdw [r10-0x40], r2                    
    stxdw [r10-0x48], r6                    
    stxdw [r10-0x38], r4                    
    jlt r4, 32, lbb_8095                            if r4 < (32 as i32 as i64 as u64) { pc += 9 }
    mov64 r8, r3                                    r8 = r3
    mov64 r1, r3                                    r1 = r3
    mov64 r2, r4                                    r2 = r4
    call function_8867                      
    ja lbb_8110                                     if true { pc += 19 }
lbb_8091:
    mov64 r2, 43                                    r2 = 43 as i32 as i64 as u64
lbb_8092:
    mov64 r9, r0                                    r9 = r0
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    ja lbb_8075                                     if true { pc += -20 }
lbb_8095:
    mov64 r8, r3                                    r8 = r3
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r4, 0, lbb_8110                             if r4 == (0 as i32 as i64 as u64) { pc += 12 }
    mov64 r1, r8                                    r1 = r8
    ldxdw r2, [r10-0x38]                    
lbb_8100:
    ldxb r4, [r1+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jsgt r4, -65, lbb_8106                          if (r4 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_8106:
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    jne r2, 0, lbb_8100                             if r2 != (0 as i32 as i64 as u64) { pc += -10 }
lbb_8110:
    add64 r0, r9                                    r0 += r9   ///  r0 = r0.wrapping_add(r9)
    mov64 r9, r0                                    r9 = r0
    ldxdw r6, [r10-0x48]                    
    ldxdw r4, [r10-0x38]                    
    ldxdw r2, [r10-0x40]                    
lbb_8115:
    ldxdw r1, [r6+0x0]                      
    jne r1, 0, lbb_8123                             if r1 != (0 as i32 as i64 as u64) { pc += 6 }
    mov64 r1, r6                                    r1 = r6
    mov64 r3, r8                                    r3 = r8
    call function_8239                      
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_8184                             if r0 != (0 as i32 as i64 as u64) { pc += 62 }
    ja lbb_8177                                     if true { pc += 54 }
lbb_8123:
    ldxdw r3, [r6+0x8]                      
    jgt r3, r9, lbb_8126                            if r3 > r9 { pc += 1 }
    ja lbb_8172                                     if true { pc += 46 }
lbb_8126:
    and64 r7, 8                                     r7 &= 8   ///  r7 = r7.and(8)
    jeq r7, 0, lbb_8129                             if r7 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8187                                     if true { pc += 58 }
lbb_8129:
    stxdw [r10-0x40], r2                    
    stxdw [r10-0x38], r4                    
    sub64 r3, r9                                    r3 -= r9   ///  r3 = r3.wrapping_sub(r9)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r2, r6                                    r2 = r6
    mov64 r9, r6                                    r9 = r6
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_8411                      
    ldxw r1, [r10-0x18]                     
    stxdw [r10-0x48], r1                    
    jeq r1, 1114112, lbb_8184                       if r1 == (1114112 as i32 as i64 as u64) { pc += 42 }
    ldxdw r6, [r10-0x20]                    
    mov64 r1, r9                                    r1 = r9
    ldxdw r2, [r10-0x40]                    
    mov64 r3, r8                                    r3 = r8
    ldxdw r4, [r10-0x38]                    
    call function_8239                      
    jne r0, 0, lbb_8184                             if r0 != (0 as i32 as i64 as u64) { pc += 35 }
    ldxdw r8, [r9+0x20]                     
    ldxdw r9, [r9+0x28]                     
    ldxdw r4, [r9+0x18]                     
    mov64 r1, r8                                    r1 = r8
    ldxdw r2, [r10-0x30]                    
    ldxdw r3, [r10-0x28]                    
    callx r4                                
    jne r0, 0, lbb_8184                             if r0 != (0 as i32 as i64 as u64) { pc += 27 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_8158:
    mov64 r1, r6                                    r1 = r6
    jeq r6, r7, lbb_8168                            if r6 == r7 { pc += 8 }
    ldxdw r3, [r9+0x20]                     
    mov64 r1, r8                                    r1 = r8
    ldxdw r2, [r10-0x48]                    
    callx r3                                
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    jeq r0, 0, lbb_8158                             if r0 == (0 as i32 as i64 as u64) { pc += -8 }
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
lbb_8168:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jlt r1, r6, lbb_8184                            if r1 < r6 { pc += 14 }
lbb_8170:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ja lbb_8184                                     if true { pc += 12 }
lbb_8172:
    mov64 r1, r6                                    r1 = r6
    mov64 r3, r8                                    r3 = r8
    call function_8239                      
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_8184                             if r0 != (0 as i32 as i64 as u64) { pc += 7 }
lbb_8177:
    ldxdw r1, [r6+0x20]                     
    ldxdw r2, [r6+0x28]                     
    ldxdw r4, [r2+0x18]                     
    ldxdw r2, [r10-0x30]                    
    ldxdw r3, [r10-0x28]                    
    callx r4                                
    mov64 r7, r0                                    r7 = r0
lbb_8184:
    and64 r7, 1                                     r7 &= 1   ///  r7 = r7.and(1)
    mov64 r0, r7                                    r0 = r7
    exit                                    
lbb_8187:
    stxdw [r10-0x40], r3                    
    ldxw r1, [r6+0x30]                      
    stxdw [r10-0x50], r1                    
    stw [r6+0x30], 48                       
    ldxb r1, [r6+0x38]                      
    stxdw [r10-0x58], r1                    
    stb [r6+0x38], 1                        
    mov64 r1, r6                                    r1 = r6
    mov64 r3, r8                                    r3 = r8
    call function_8239                      
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_8184                             if r0 != (0 as i32 as i64 as u64) { pc += -15 }
    ldxdw r3, [r10-0x40]                    
    sub64 r3, r9                                    r3 -= r9   ///  r3 = r3.wrapping_sub(r9)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_8411                      
    ldxw r8, [r10-0x8]                      
    jeq r8, 1114112, lbb_8184                       if r8 == (1114112 as i32 as i64 as u64) { pc += -24 }
    ldxdw r1, [r10-0x10]                    
    stxdw [r10-0x38], r1                    
    ldxdw r1, [r6+0x20]                     
    ldxdw r2, [r6+0x28]                     
    stxdw [r10-0x48], r2                    
    ldxdw r4, [r2+0x18]                     
    stxdw [r10-0x40], r1                    
    ldxdw r2, [r10-0x30]                    
    ldxdw r3, [r10-0x28]                    
    callx r4                                
    jne r0, 0, lbb_8184                             if r0 != (0 as i32 as i64 as u64) { pc += -35 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_8220:
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x38]                    
    jeq r2, r9, lbb_8234                            if r2 == r9 { pc += 11 }
    ldxdw r1, [r10-0x48]                    
    ldxdw r3, [r1+0x20]                     
    ldxdw r1, [r10-0x40]                    
    mov64 r2, r8                                    r2 = r8
    callx r3                                
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    jeq r0, 0, lbb_8220                             if r0 == (0 as i32 as i64 as u64) { pc += -10 }
    add64 r9, -1                                    r9 += -1   ///  r9 = r9.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x38]                    
    jlt r9, r2, lbb_8184                            if r9 < r2 { pc += -50 }
lbb_8234:
    ldxdw r2, [r10-0x58]                    
    stxb [r1+0x38], r2                      
    ldxdw r2, [r10-0x50]                    
    stxw [r1+0x30], r2                      
    ja lbb_8170                                     if true { pc += -69 }

function_8239:
    mov64 r6, r4                                    r6 = r4
    mov64 r7, r3                                    r7 = r3
    mov64 r8, r1                                    r8 = r1
    mov64 r1, r2                                    r1 = r2
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jeq r1, 1114112, lbb_8253                       if r1 == (1114112 as i32 as i64 as u64) { pc += 7 }
    ldxdw r1, [r8+0x20]                     
    ldxdw r3, [r8+0x28]                     
    ldxdw r3, [r3+0x20]                     
    callx r3                                
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_8255                             if r1 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_8253:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jne r7, 0, lbb_8256                             if r7 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_8255:
    exit                                    
lbb_8256:
    ldxdw r1, [r8+0x20]                     
    ldxdw r2, [r8+0x28]                     
    ldxdw r4, [r2+0x18]                     
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r6                                    r3 = r6
    callx r4                                
    ja lbb_8255                                     if true { pc += -8 }

function_8263:
    stxdw [r10-0x18], r2                    
    mov64 r8, r1                                    r8 = r1
    ldxdw r2, [r8+0x10]                     
    ldxdw r1, [r8+0x0]                      
    jne r1, 0, lbb_8271                             if r1 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r4, r2                                    r4 = r2
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jeq r4, 0, lbb_8320                             if r4 == (0 as i32 as i64 as u64) { pc += 49 }
lbb_8271:
    stxdw [r10-0x20], r3                    
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_8275                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8338                                     if true { pc += 63 }
lbb_8275:
    ldxdw r0, [r10-0x18]                    
    mov64 r3, r0                                    r3 = r0
    ldxdw r2, [r10-0x20]                    
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r4, [r8+0x18]                     
    jeq r4, 0, lbb_8307                             if r4 == (0 as i32 as i64 as u64) { pc += 25 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r0, [r10-0x18]                    
lbb_8284:
    mov64 r7, r0                                    r7 = r0
    mov64 r6, r2                                    r6 = r2
    jeq r7, r3, lbb_8338                            if r7 == r3 { pc += 51 }
    mov64 r0, r7                                    r0 = r7
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    ldxb r2, [r7+0x0]                       
    mov64 r9, r2                                    r9 = r2
    lsh64 r9, 56                                    r9 <<= 56   ///  r9 = r9.wrapping_shl(56)
    arsh64 r9, 56                                   r9 >>= 56 (signed)   ///  r9 = (r9 as i64).wrapping_shr(56)
    jsgt r9, -1, lbb_8302                           if (r9 as i64) > (-1 as i32 as i64) { pc += 8 }
    mov64 r0, r7                                    r0 = r7
    add64 r0, 2                                     r0 += 2   ///  r0 = r0.wrapping_add(2 as i32 as i64 as u64)
    jlt r2, 224, lbb_8302                           if r2 < (224 as i32 as i64 as u64) { pc += 5 }
    mov64 r0, r7                                    r0 = r7
    add64 r0, 3                                     r0 += 3   ///  r0 = r0.wrapping_add(3 as i32 as i64 as u64)
    jlt r2, 240, lbb_8302                           if r2 < (240 as i32 as i64 as u64) { pc += 2 }
    mov64 r0, r7                                    r0 = r7
    add64 r0, 4                                     r0 += 4   ///  r0 = r0.wrapping_add(4 as i32 as i64 as u64)
lbb_8302:
    add64 r5, 1                                     r5 += 1   ///  r5 = r5.wrapping_add(1 as i32 as i64 as u64)
    mov64 r2, r0                                    r2 = r0
    sub64 r2, r7                                    r2 -= r7   ///  r2 = r2.wrapping_sub(r7)
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    jlt r5, r4, lbb_8284                            if r5 < r4 { pc += -23 }
lbb_8307:
    jeq r0, r3, lbb_8338                            if r0 == r3 { pc += 30 }
    ldxb r3, [r0+0x0]                       
    mov64 r4, r3                                    r4 = r3
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    jsgt r4, -1, lbb_8314                           if (r4 as i64) > (-1 as i32 as i64) { pc += 1 }
    jlt r3, 224, lbb_8314                           if r3 < (224 as i32 as i64 as u64) { pc += 0 }
lbb_8314:
    ldxdw r5, [r10-0x20]                    
    jeq r2, 0, lbb_8332                             if r2 == (0 as i32 as i64 as u64) { pc += 16 }
    jlt r2, r5, lbb_8325                            if r2 < r5 { pc += 8 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jeq r2, r5, lbb_8332                            if r2 == r5 { pc += 13 }
    ja lbb_8333                                     if true { pc += 13 }
lbb_8320:
    ldxdw r1, [r8+0x20]                     
    ldxdw r2, [r8+0x28]                     
    ldxdw r4, [r2+0x18]                     
    ldxdw r2, [r10-0x18]                    
    ja lbb_8406                                     if true { pc += 81 }
lbb_8325:
    ldxdw r4, [r10-0x18]                    
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxb r4, [r4+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    jslt r4, -64, lbb_8333                          if (r4 as i64) < (-64 as i32 as i64) { pc += 1 }
lbb_8332:
    ldxdw r3, [r10-0x18]                    
lbb_8333:
    jeq r3, 0, lbb_8335                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, r2                                    r5 = r2
lbb_8335:
    stxdw [r10-0x20], r5                    
    jeq r3, 0, lbb_8338                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    stxdw [r10-0x18], r3                    
lbb_8338:
    jne r1, 0, lbb_8345                             if r1 != (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r1, [r8+0x20]                     
    ldxdw r2, [r8+0x28]                     
    ldxdw r4, [r2+0x18]                     
    ldxdw r2, [r10-0x18]                    
    ldxdw r3, [r10-0x20]                    
    ja lbb_8406                                     if true { pc += 61 }
lbb_8345:
    ldxdw r9, [r8+0x8]                      
    ldxdw r7, [r10-0x20]                    
    jlt r7, 32, lbb_8352                            if r7 < (32 as i32 as i64 as u64) { pc += 4 }
    ldxdw r1, [r10-0x18]                    
    mov64 r2, r7                                    r2 = r7
    call function_8867                      
    ja lbb_8366                                     if true { pc += 14 }
lbb_8352:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r7, 0, lbb_8366                             if r7 == (0 as i32 as i64 as u64) { pc += 12 }
    ldxdw r1, [r10-0x18]                    
    mov64 r2, r7                                    r2 = r7
lbb_8356:
    ldxb r4, [r1+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jsgt r4, -65, lbb_8362                          if (r4 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_8362:
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    jne r2, 0, lbb_8356                             if r2 != (0 as i32 as i64 as u64) { pc += -10 }
lbb_8366:
    jle r9, r0, lbb_8401                            if r9 <= r0 { pc += 34 }
    sub64 r9, r0                                    r9 -= r0   ///  r9 = r9.wrapping_sub(r0)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r9                                    r3 = r9
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_8411                      
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    ldxw r6, [r10-0x8]                      
    jeq r6, 1114112, lbb_8408                       if r6 == (1114112 as i32 as i64 as u64) { pc += 31 }
    mov64 r3, r7                                    r3 = r7
    ldxdw r7, [r10-0x10]                    
    ldxdw r1, [r8+0x20]                     
    ldxdw r8, [r8+0x28]                     
    ldxdw r4, [r8+0x18]                     
    stxdw [r10-0x28], r1                    
    ldxdw r2, [r10-0x18]                    
    callx r4                                
    jne r0, 0, lbb_8408                             if r0 != (0 as i32 as i64 as u64) { pc += 22 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_8387:
    mov64 r1, r7                                    r1 = r7
    jeq r7, r9, lbb_8397                            if r7 == r9 { pc += 8 }
    ldxdw r3, [r8+0x20]                     
    ldxdw r1, [r10-0x28]                    
    mov64 r2, r6                                    r2 = r6
    callx r3                                
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    jeq r0, 0, lbb_8387                             if r0 == (0 as i32 as i64 as u64) { pc += -8 }
    add64 r9, -1                                    r9 += -1   ///  r9 = r9.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
lbb_8397:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jlt r1, r7, lbb_8408                            if r1 < r7 { pc += 9 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ja lbb_8408                                     if true { pc += 7 }
lbb_8401:
    ldxdw r1, [r8+0x20]                     
    ldxdw r2, [r8+0x28]                     
    ldxdw r4, [r2+0x18]                     
    ldxdw r2, [r10-0x18]                    
    mov64 r3, r7                                    r3 = r7
lbb_8406:
    callx r4                                
    mov64 r9, r0                                    r9 = r0
lbb_8408:
    and64 r9, 1                                     r9 &= 1   ///  r9 = r9.and(1)
    mov64 r0, r9                                    r0 = r9
    exit                                    

function_8411:
    stxdw [r10-0x10], r1                    
    ldxb r6, [r2+0x38]                      
    jsgt r6, 1, lbb_8416                            if (r6 as i64) > (1 as i32 as i64) { pc += 2 }
    jeq r6, 0, lbb_8427                             if r6 == (0 as i32 as i64 as u64) { pc += 12 }
    ja lbb_8420                                     if true { pc += 4 }
lbb_8416:
    jeq r6, 2, lbb_8423                             if r6 == (2 as i32 as i64 as u64) { pc += 6 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    and64 r4, 255                                   r4 &= 255   ///  r4 = r4.and(255)
    jeq r4, 0, lbb_8427                             if r4 == (0 as i32 as i64 as u64) { pc += 7 }
lbb_8420:
    mov64 r6, r3                                    r6 = r3
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ja lbb_8427                                     if true { pc += 4 }
lbb_8423:
    mov64 r6, r3                                    r6 = r3
    rsh64 r6, 1                                     r6 >>= 1   ///  r6 = r6.wrapping_shr(1)
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
lbb_8427:
    stxdw [r10-0x8], r3                     
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    ldxw r8, [r2+0x30]                      
    ldxdw r7, [r2+0x28]                     
    ldxdw r9, [r2+0x20]                     
lbb_8432:
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    jeq r6, 0, lbb_8441                             if r6 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r3, [r7+0x20]                     
    mov64 r1, r9                                    r1 = r9
    mov64 r2, r8                                    r2 = r8
    callx r3                                
    mov64 r1, 1114112                               r1 = 1114112 as i32 as i64 as u64
    jeq r0, 0, lbb_8432                             if r0 == (0 as i32 as i64 as u64) { pc += -9 }
lbb_8441:
    ldxdw r2, [r10-0x10]                    
    stxw [r2+0x8], r1                       
    ldxdw r1, [r10-0x8]                     
    stxdw [r2+0x0], r1                      
    exit                                    

function_8446:
    ldxdw r4, [r1+0x20]                     
    ldxdw r1, [r1+0x28]                     
    ldxdw r5, [r1+0x18]                     
    mov64 r1, r4                                    r1 = r4
    callx r5                                
    exit                                    

function_8452:
    mov64 r9, r2                                    r9 = r2
    mov64 r8, r1                                    r8 = r1
    ldxdw r6, [r3+0x20]                     
    stxdw [r10-0x40], r3                    
    ldxdw r1, [r3+0x28]                     
    stxdw [r10-0x30], r1                    
    ldxdw r7, [r1+0x20]                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 34                                    r2 = 34 as i32 as i64 as u64
    callx r7                                
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_8732                             if r0 != (0 as i32 as i64 as u64) { pc += 268 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, r9                                    r2 = r9
    jeq r2, 0, lbb_8719                             if r2 == (0 as i32 as i64 as u64) { pc += 251 }
    stxdw [r10-0x58], r7                    
    stxdw [r10-0x50], r6                    
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r1, r2                                    r1 = r2
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    stxdw [r10-0x60], r1                    
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x20], r1                    
    stxdw [r10-0x48], r8                    
    mov64 r9, r8                                    r9 = r8
    mov64 r8, r2                                    r8 = r2
    stxdw [r10-0x38], r2                    
lbb_8480:
    mov64 r5, r8                                    r5 = r8
    mov64 r1, r9                                    r1 = r9
    mov64 r8, r1                                    r8 = r1
    add64 r8, r5                                    r8 += r5   ///  r8 = r8.wrapping_add(r5)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_8485:
    mov64 r9, r1                                    r9 = r1
    add64 r9, r6                                    r9 += r6   ///  r9 = r9.wrapping_add(r6)
    ldxb r3, [r9+0x0]                       
    mov64 r4, r3                                    r4 = r3
    add64 r4, -127                                  r4 += -127   ///  r4 = r4.wrapping_add(-127 as i32 as i64 as u64)
    and64 r4, 255                                   r4 &= 255   ///  r4 = r4.and(255)
    jlt r4, 161, lbb_8497                           if r4 < (161 as i32 as i64 as u64) { pc += 5 }
    jeq r3, 34, lbb_8497                            if r3 == (34 as i32 as i64 as u64) { pc += 4 }
    jeq r3, 92, lbb_8497                            if r3 == (92 as i32 as i64 as u64) { pc += 3 }
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    jeq r5, r6, lbb_8691                            if r5 == r6 { pc += 195 }
    ja lbb_8485                                     if true { pc += -12 }
lbb_8497:
    ldxb r3, [r9+0x0]                       
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    lsh64 r2, 56                                    r2 <<= 56   ///  r2 = r2.wrapping_shl(56)
    arsh64 r2, 56                                   r2 >>= 56 (signed)   ///  r2 = (r2 as i64).wrapping_shr(56)
    stxdw [r10-0x28], r0                    
    jsgt r2, -1, lbb_8540                           if (r2 as i64) > (-1 as i32 as i64) { pc += 36 }
    mov64 r5, r1                                    r5 = r1
    add64 r5, r6                                    r5 += r6   ///  r5 = r5.wrapping_add(r6)
    ldxb r2, [r9+0x0]                       
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    mov64 r4, r3                                    r4 = r3
    and64 r4, 31                                    r4 &= 31   ///  r4 = r4.and(31)
    mov64 r7, r4                                    r7 = r4
    lsh64 r7, 6                                     r7 <<= 6   ///  r7 = r7.wrapping_shl(6)
    or64 r7, r2                                     r7 |= r2   ///  r7 = r7.or(r2)
    mov64 r9, r5                                    r9 = r5
    add64 r9, 2                                     r9 += 2   ///  r9 = r9.wrapping_add(2 as i32 as i64 as u64)
    jgt r3, 223, lbb_8517                           if r3 > (223 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8541                                     if true { pc += 24 }
lbb_8517:
    lsh64 r2, 6                                     r2 <<= 6   ///  r2 = r2.wrapping_shl(6)
    ldxb r0, [r9+0x0]                       
    and64 r0, 63                                    r0 &= 63   ///  r0 = r0.and(63)
    or64 r2, r0                                     r2 |= r0   ///  r2 = r2.or(r0)
    add64 r5, 3                                     r5 += 3   ///  r5 = r5.wrapping_add(3 as i32 as i64 as u64)
    mov64 r0, r4                                    r0 = r4
    lsh64 r0, 12                                    r0 <<= 12   ///  r0 = r0.wrapping_shl(12)
    mov64 r7, r2                                    r7 = r2
    or64 r7, r0                                     r7 |= r0   ///  r7 = r7.or(r0)
    mov64 r9, r5                                    r9 = r5
    jlt r3, 240, lbb_8541                           if r3 < (240 as i32 as i64 as u64) { pc += 13 }
    lsh64 r2, 6                                     r2 <<= 6   ///  r2 = r2.wrapping_shl(6)
    ldxb r3, [r5+0x0]                       
    and64 r3, 63                                    r3 &= 63   ///  r3 = r3.and(63)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    lsh64 r4, 18                                    r4 <<= 18   ///  r4 = r4.wrapping_shl(18)
    and64 r4, 1835008                               r4 &= 1835008   ///  r4 = r4.and(1835008)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    add64 r1, 4                                     r1 += 4   ///  r1 = r1.wrapping_add(4 as i32 as i64 as u64)
    mov64 r9, r1                                    r9 = r1
    mov64 r7, r2                                    r7 = r2
    ja lbb_8541                                     if true { pc += 1 }
lbb_8540:
    mov64 r7, r3                                    r7 = r3
lbb_8541:
    jsgt r7, 12, lbb_8551                           if (r7 as i64) > (12 as i32 as i64) { pc += 9 }
    jeq r7, 0, lbb_8582                             if r7 == (0 as i32 as i64 as u64) { pc += 39 }
    jeq r7, 9, lbb_8597                             if r7 == (9 as i32 as i64 as u64) { pc += 53 }
    jeq r7, 10, lbb_8546                            if r7 == (10 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8562                                     if true { pc += 16 }
lbb_8546:
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 28252                   
    ja lbb_8601                                     if true { pc += 50 }
lbb_8551:
    jsgt r7, 38, lbb_8560                           if (r7 as i64) > (38 as i32 as i64) { pc += 8 }
    jeq r7, 13, lbb_8587                            if r7 == (13 as i32 as i64 as u64) { pc += 34 }
    jeq r7, 34, lbb_8555                            if r7 == (34 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8562                                     if true { pc += 7 }
lbb_8555:
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 8796                    
    ja lbb_8601                                     if true { pc += 41 }
lbb_8560:
    jeq r7, 39, lbb_8578                            if r7 == (39 as i32 as i64 as u64) { pc += 17 }
    jeq r7, 92, lbb_8592                            if r7 == (92 as i32 as i64 as u64) { pc += 30 }
lbb_8562:
    jgt r7, 767, lbb_8564                           if r7 > (767 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8578                                     if true { pc += 14 }
lbb_8564:
    mov64 r1, r7                                    r1 = r7
    call function_9880                      
    jeq r0, 0, lbb_8578                             if r0 == (0 as i32 as i64 as u64) { pc += 11 }
lbb_8567:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -12                                   r1 += -12   ///  r1 = r1.wrapping_add(-12 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_9449                      
    ldxh r1, [r10-0x4]                      
    stxh [r10-0x10], r1                     
    ldxdw r1, [r10-0xc]                     
    stxdw [r10-0x18], r1                    
    ldxb r1, [r10-0x1]                      
    ldxb r2, [r10-0x2]                      
    ja lbb_8601                                     if true { pc += 23 }
lbb_8578:
    mov64 r1, r7                                    r1 = r7
    call function_9362                      
    jne r0, 0, lbb_8657                             if r0 != (0 as i32 as i64 as u64) { pc += 76 }
    ja lbb_8567                                     if true { pc += -15 }
lbb_8582:
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 12380                   
    ja lbb_8601                                     if true { pc += 14 }
lbb_8587:
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 29276                   
    ja lbb_8601                                     if true { pc += 9 }
lbb_8592:
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 23644                   
    ja lbb_8601                                     if true { pc += 4 }
lbb_8597:
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 29788                   
lbb_8601:
    stxb [r10-0xd], r1                      
    stxb [r10-0xe], r2                      
    ldxb r3, [r10-0x18]                     
    jeq r3, 128, lbb_8657                           if r3 == (128 as i32 as i64 as u64) { pc += 52 }
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    jeq r1, 1, lbb_8657                             if r1 == (1 as i32 as i64 as u64) { pc += 49 }
    ldxdw r1, [r10-0x20]                    
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    ldxdw r2, [r10-0x38]                    
    ldxdw r5, [r10-0x28]                    
    jgt r5, r1, lbb_8684                            if r5 > r1 { pc += 71 }
    jeq r5, 0, lbb_8625                             if r5 == (0 as i32 as i64 as u64) { pc += 11 }
    jlt r5, r2, lbb_8616                            if r5 < r2 { pc += 1 }
    ja lbb_8623                                     if true { pc += 7 }
lbb_8616:
    ldxdw r3, [r10-0x48]                    
    add64 r3, r5                                    r3 += r5   ///  r3 = r3.wrapping_add(r5)
    ldxb r3, [r3+0x0]                       
    lsh64 r3, 56                                    r3 <<= 56   ///  r3 = r3.wrapping_shl(56)
    arsh64 r3, 56                                   r3 >>= 56 (signed)   ///  r3 = (r3 as i64).wrapping_shr(56)
    jsgt r3, -65, lbb_8625                          if (r3 as i64) > (-65 as i32 as i64) { pc += 3 }
    ja lbb_8684                                     if true { pc += 61 }
lbb_8623:
    jeq r5, r2, lbb_8625                            if r5 == r2 { pc += 1 }
    ja lbb_8684                                     if true { pc += 59 }
lbb_8625:
    jeq r1, 0, lbb_8631                             if r1 == (0 as i32 as i64 as u64) { pc += 5 }
    jlt r1, r2, lbb_8676                            if r1 < r2 { pc += 49 }
    ldxdw r3, [r10-0x60]                    
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    jeq r1, 0, lbb_8631                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8684                                     if true { pc += 53 }
lbb_8631:
    ldxdw r2, [r10-0x48]                    
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    ldxdw r3, [r10-0x20]                    
    sub64 r3, r5                                    r3 -= r5   ///  r3 = r3.wrapping_sub(r5)
    add64 r3, r6                                    r3 += r6   ///  r3 = r3.wrapping_add(r6)
    ldxdw r1, [r10-0x30]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r10-0x50]                    
    callx r4                                
    jne r0, 0, lbb_8749                             if r0 != (0 as i32 as i64 as u64) { pc += 108 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    ldxdw r2, [r10-0x40]                    
    call function_7764                      
    jne r0, 0, lbb_8749                             if r0 != (0 as i32 as i64 as u64) { pc += 103 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r7, 128, lbb_8653                           if r7 < (128 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    jlt r7, 2048, lbb_8653                          if r7 < (2048 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 3                                     r1 = 3 as i32 as i64 as u64
    jlt r7, 65536, lbb_8653                         if r7 < (65536 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 4                                     r1 = 4 as i32 as i64 as u64
lbb_8653:
    ldxdw r2, [r10-0x20]                    
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    stxdw [r10-0x28], r1                    
lbb_8657:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x38]                    
    jlt r7, 128, lbb_8661                           if r7 < (128 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8670                                     if true { pc += 9 }
lbb_8661:
    ldxdw r1, [r10-0x20]                    
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    mov64 r1, r4                                    r1 = r4
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    stxdw [r10-0x20], r1                    
    sub64 r8, r9                                    r8 -= r9   ///  r8 = r8.wrapping_sub(r9)
    ldxdw r0, [r10-0x28]                    
    jeq r8, 0, lbb_8747                             if r8 == (0 as i32 as i64 as u64) { pc += 78 }
    ja lbb_8480                                     if true { pc += -190 }
lbb_8670:
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    jlt r7, 2048, lbb_8661                          if r7 < (2048 as i32 as i64 as u64) { pc += -11 }
    mov64 r4, 3                                     r4 = 3 as i32 as i64 as u64
    jlt r7, 65536, lbb_8661                         if r7 < (65536 as i32 as i64 as u64) { pc += -13 }
    mov64 r4, 4                                     r4 = 4 as i32 as i64 as u64
    ja lbb_8661                                     if true { pc += -15 }
lbb_8676:
    ldxdw r1, [r10-0x48]                    
    ldxdw r3, [r10-0x20]                    
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    ldxb r1, [r1+0x0]                       
    lsh64 r1, 56                                    r1 <<= 56   ///  r1 = r1.wrapping_shl(56)
    arsh64 r1, 56                                   r1 >>= 56 (signed)   ///  r1 = (r1 as i64).wrapping_shr(56)
    jsgt r1, -65, lbb_8631                          if (r1 as i64) > (-65 as i32 as i64) { pc += -53 }
lbb_8684:
    ldxdw r4, [r10-0x20]                    
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    ldxdw r1, [r10-0x48]                    
    mov64 r3, r5                                    r3 = r5
    lddw r5, 0x100017b70 --> b"\x00\x00\x00\x00\xfek\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xa6\x09\x00…        r5 load str located at 4295064432
    call function_9030                      
lbb_8691:
    ldxdw r4, [r10-0x20]                    
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
lbb_8693:
    ldxdw r6, [r10-0x50]                    
    ldxdw r7, [r10-0x58]                    
    ldxdw r8, [r10-0x48]                    
    jgt r0, r4, lbb_8742                            if r0 > r4 { pc += 45 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jeq r0, 0, lbb_8713                             if r0 == (0 as i32 as i64 as u64) { pc += 13 }
    jlt r0, r2, lbb_8702                            if r0 < r2 { pc += 1 }
    ja lbb_8710                                     if true { pc += 8 }
lbb_8702:
    mov64 r1, r8                                    r1 = r8
    add64 r1, r0                                    r1 += r0   ///  r1 = r1.wrapping_add(r0)
    ldxb r5, [r1+0x0]                       
    lsh64 r5, 56                                    r5 <<= 56   ///  r5 = r5.wrapping_shl(56)
    arsh64 r5, 56                                   r5 >>= 56 (signed)   ///  r5 = (r5 as i64).wrapping_shr(56)
    mov64 r1, r0                                    r1 = r0
    jsgt r5, -65, lbb_8713                          if (r5 as i64) > (-65 as i32 as i64) { pc += 4 }
    ja lbb_8742                                     if true { pc += 32 }
lbb_8710:
    mov64 r1, r0                                    r1 = r0
    jeq r0, r2, lbb_8713                            if r0 == r2 { pc += 1 }
    ja lbb_8742                                     if true { pc += 29 }
lbb_8713:
    jeq r4, 0, lbb_8719                             if r4 == (0 as i32 as i64 as u64) { pc += 5 }
    jlt r4, r2, lbb_8734                            if r4 < r2 { pc += 19 }
    mov64 r0, r1                                    r0 = r1
    mov64 r3, r4                                    r3 = r4
    jeq r4, r2, lbb_8719                            if r4 == r2 { pc += 1 }
    ja lbb_8742                                     if true { pc += 23 }
lbb_8719:
    add64 r8, r1                                    r8 += r1   ///  r8 = r8.wrapping_add(r1)
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    ldxdw r1, [r10-0x30]                    
    ldxdw r4, [r1+0x18]                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r8                                    r2 = r8
    callx r4                                
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_8732                             if r0 != (0 as i32 as i64 as u64) { pc += 4 }
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 34                                    r2 = 34 as i32 as i64 as u64
    callx r7                                
    mov64 r1, r0                                    r1 = r0
lbb_8732:
    mov64 r0, r1                                    r0 = r1
    exit                                    
lbb_8734:
    mov64 r3, r8                                    r3 = r8
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    ldxb r5, [r3+0x0]                       
    lsh64 r5, 56                                    r5 <<= 56   ///  r5 = r5.wrapping_shl(56)
    arsh64 r5, 56                                   r5 >>= 56 (signed)   ///  r5 = (r5 as i64).wrapping_shr(56)
    mov64 r0, r1                                    r0 = r1
    mov64 r3, r4                                    r3 = r4
    jsgt r5, -65, lbb_8719                          if (r5 as i64) > (-65 as i32 as i64) { pc += -23 }
lbb_8742:
    mov64 r1, r8                                    r1 = r8
    mov64 r3, r0                                    r3 = r0
    lddw r5, 0x100017b88 --> b"\x00\x00\x00\x00\xfek\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xaf\x09\x00…        r5 load str located at 4295064456
    call function_9030                      
lbb_8747:
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    ja lbb_8693                                     if true { pc += -56 }
lbb_8749:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ja lbb_8732                                     if true { pc += -19 }

function_8751:
    mov64 r4, r2                                    r4 = r2
    mov64 r2, r1                                    r2 = r1
    mov64 r1, r3                                    r1 = r3
    mov64 r3, r4                                    r3 = r4
    call function_8263                      
    exit                                    

function_8757:
    mov64 r8, r2                                    r8 = r2
    mov64 r9, r1                                    r9 = r1
    ldxdw r6, [r8+0x20]                     
    ldxdw r1, [r8+0x28]                     
    ldxdw r3, [r1+0x20]                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 39                                    r2 = 39 as i32 as i64 as u64
    stxdw [r10-0x20], r3                    
    callx r3                                
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_8838                             if r0 != (0 as i32 as i64 as u64) { pc += 70 }
    ldxw r9, [r9+0x0]                       
    jsgt r9, 12, lbb_8778                           if (r9 as i64) > (12 as i32 as i64) { pc += 8 }
    jeq r9, 0, lbb_8809                             if r9 == (0 as i32 as i64 as u64) { pc += 38 }
    jeq r9, 9, lbb_8825                             if r9 == (9 as i32 as i64 as u64) { pc += 53 }
    jeq r9, 10, lbb_8774                            if r9 == (10 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8784                                     if true { pc += 10 }
lbb_8774:
    sth [r10-0xe], 512                      
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 28252                   
    ja lbb_8828                                     if true { pc += 50 }
lbb_8778:
    jsgt r9, 38, lbb_8782                           if (r9 as i64) > (38 as i32 as i64) { pc += 3 }
    jeq r9, 13, lbb_8813                            if r9 == (13 as i32 as i64 as u64) { pc += 33 }
    jeq r9, 34, lbb_8802                            if r9 == (34 as i32 as i64 as u64) { pc += 21 }
    ja lbb_8784                                     if true { pc += 2 }
lbb_8782:
    jeq r9, 39, lbb_8817                            if r9 == (39 as i32 as i64 as u64) { pc += 34 }
    jeq r9, 92, lbb_8821                            if r9 == (92 as i32 as i64 as u64) { pc += 37 }
lbb_8784:
    jgt r9, 767, lbb_8786                           if r9 > (767 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8802                                     if true { pc += 16 }
lbb_8786:
    mov64 r1, r9                                    r1 = r9
    call function_9880                      
    jeq r0, 0, lbb_8802                             if r0 == (0 as i32 as i64 as u64) { pc += 13 }
lbb_8789:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -12                                   r1 += -12   ///  r1 = r1.wrapping_add(-12 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    call function_9449                      
    ldxh r1, [r10-0x4]                      
    stxh [r10-0x10], r1                     
    ldxdw r1, [r10-0xc]                     
    stxdw [r10-0x18], r1                    
    ldxb r1, [r10-0x1]                      
    stxb [r10-0xd], r1                      
    ldxb r1, [r10-0x2]                      
    stxb [r10-0xe], r1                      
    ja lbb_8828                                     if true { pc += 26 }
lbb_8802:
    mov64 r1, r9                                    r1 = r9
    call function_9362                      
    jne r0, 0, lbb_8806                             if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8789                                     if true { pc += -17 }
lbb_8806:
    stxw [r10-0x14], r9                     
    stb [r10-0x18], 128                     
    ja lbb_8828                                     if true { pc += 19 }
lbb_8809:
    sth [r10-0xe], 512                      
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 12380                   
    ja lbb_8828                                     if true { pc += 15 }
lbb_8813:
    sth [r10-0xe], 512                      
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 29276                   
    ja lbb_8828                                     if true { pc += 11 }
lbb_8817:
    sth [r10-0xe], 512                      
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 10076                   
    ja lbb_8828                                     if true { pc += 7 }
lbb_8821:
    sth [r10-0xe], 512                      
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 23644                   
    ja lbb_8828                                     if true { pc += 3 }
lbb_8825:
    sth [r10-0xe], 512                      
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 29788                   
lbb_8828:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    call function_7764                      
    jne r0, 0, lbb_8838                             if r0 != (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 39                                    r2 = 39 as i32 as i64 as u64
    ldxdw r3, [r10-0x20]                    
    callx r3                                
    mov64 r7, r0                                    r7 = r0
lbb_8838:
    mov64 r0, r7                                    r0 = r7
    exit                                    

function_8840:
    call function_9830                      

function_8841:
    call function_9855                      

function_8842:
    stxdw [r10-0x58], r2                    
    stxdw [r10-0x60], r1                    
    lddw r1, 0x100017ba0 --> b"\x00\x00\x00\x00\x11l\x01\x00\x15\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295064480
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x100013120 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295045408
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 3                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_7789                      

function_8867:
    mov64 r7, r1                                    r7 = r1
    add64 r7, 7                                     r7 += 7   ///  r7 = r7.wrapping_add(7 as i32 as i64 as u64)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
    mov64 r3, r7                                    r3 = r7
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    jlt r2, r3, lbb_9016                            if r2 < r3 { pc += 143 }
    mov64 r5, r2                                    r5 = r2
    sub64 r5, r3                                    r5 -= r3   ///  r5 = r5.wrapping_sub(r3)
    jlt r5, 8, lbb_9016                             if r5 < (8 as i32 as i64 as u64) { pc += 140 }
    stxdw [r10-0x8], r3                     
    mov64 r2, r5                                    r2 = r5
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jeq r7, r1, lbb_8898                            if r7 == r1 { pc += 16 }
    mov64 r6, r1                                    r6 = r1
    sub64 r6, r7                                    r6 -= r7   ///  r6 = r6.wrapping_sub(r7)
    mov64 r7, r1                                    r7 = r1
lbb_8885:
    ldxb r4, [r7+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jsgt r4, -65, lbb_8892                          if (r4 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_8892:
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    jeq r6, 0, lbb_8895                             if r6 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_8895:
    add64 r3, r9                                    r3 += r9   ///  r3 = r3.wrapping_add(r9)
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    jne r8, 1, lbb_8885                             if r8 != (1 as i32 as i64 as u64) { pc += -13 }
lbb_8898:
    ldxdw r4, [r10-0x8]                     
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    jeq r2, 0, lbb_8916                             if r2 == (0 as i32 as i64 as u64) { pc += 15 }
    mov64 r0, r5                                    r0 = r5
    and64 r0, -8                                    r0 &= -8   ///  r0 = r0.and(-8)
    mov64 r4, r1                                    r4 = r1
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_8906:
    ldxb r7, [r4+0x0]                       
    lsh64 r7, 56                                    r7 <<= 56   ///  r7 = r7.wrapping_shl(56)
    arsh64 r7, 56                                   r7 >>= 56 (signed)   ///  r7 = (r7 as i64).wrapping_shr(56)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jsgt r7, -65, lbb_8912                          if (r7 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_8912:
    add64 r0, r6                                    r0 += r6   ///  r0 = r0.wrapping_add(r6)
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    jne r2, 0, lbb_8906                             if r2 != (0 as i32 as i64 as u64) { pc += -10 }
lbb_8916:
    rsh64 r5, 3                                     r5 >>= 3   ///  r5 = r5.wrapping_shr(3)
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    lddw r9, 0x101010101010101                      r9 load str located at 72340172838076673
lbb_8920:
    mov64 r3, r5                                    r3 = r5
    jeq r3, 0, lbb_9029                             if r3 == (0 as i32 as i64 as u64) { pc += 107 }
    stxdw [r10-0x8], r1                     
    mov64 r5, r3                                    r5 = r3
    jlt r3, 192, lbb_8926                           if r3 < (192 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 192                                   r5 = 192 as i32 as i64 as u64
lbb_8926:
    stxdw [r10-0x10], r5                    
    lsh64 r5, 3                                     r5 <<= 3   ///  r5 = r5.wrapping_shl(3)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jlt r3, 4, lbb_8955                             if r3 < (4 as i32 as i64 as u64) { pc += 25 }
    mov64 r2, r5                                    r2 = r5
    and64 r2, 2016                                  r2 &= 2016   ///  r2 = r2.and(2016)
    ldxdw r6, [r10-0x8]                     
    mov64 r1, r6                                    r1 = r6
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, r6                                    r2 = r6
    ja lbb_8939                                     if true { pc += 2 }
lbb_8937:
    add64 r2, 32                                    r2 += 32   ///  r2 = r2.wrapping_add(32 as i32 as i64 as u64)
    jeq r2, r1, lbb_8955                            if r2 == r1 { pc += 16 }
lbb_8939:
    mov64 r6, r4                                    r6 = r4
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_8941:
    mov64 r4, r2                                    r4 = r2
    add64 r4, r7                                    r4 += r7   ///  r4 = r4.wrapping_add(r7)
    ldxdw r4, [r4+0x0]                      
    mov64 r8, r4                                    r8 = r4
    rsh64 r8, 6                                     r8 >>= 6   ///  r8 = r8.wrapping_shr(6)
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    rsh64 r4, 7                                     r4 >>= 7   ///  r4 = r4.wrapping_shr(7)
    or64 r4, r8                                     r4 |= r8   ///  r4 = r4.or(r8)
    and64 r4, r9                                    r4 &= r9   ///  r4 = r4.and(r9)
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    mov64 r6, r4                                    r6 = r4
    jeq r7, 32, lbb_8937                            if r7 == (32 as i32 as i64 as u64) { pc += -17 }
    ja lbb_8941                                     if true { pc += -14 }
lbb_8955:
    ldxdw r1, [r10-0x8]                     
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    ldxdw r7, [r10-0x10]                    
    mov64 r2, r7                                    r2 = r7
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    mov64 r8, r3                                    r8 = r3
    sub64 r3, r7                                    r3 -= r7   ///  r3 = r3.wrapping_sub(r7)
    mov64 r6, r4                                    r6 = r4
    lddw r5, 0xff00ff00ff00ff                       r5 load str located at 71777214294589695
    and64 r6, r5                                    r6 &= r5   ///  r6 = r6.and(r5)
    rsh64 r4, 8                                     r4 >>= 8   ///  r4 = r4.wrapping_shr(8)
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    mov64 r5, r3                                    r5 = r3
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    lddw r6, 0x1000100010001                        r6 load str located at 281479271743489
    mul64 r4, r6                                    r4 *= r6   ///  r4 = r4.wrapping_mul(r6)
    rsh64 r4, 48                                    r4 >>= 48   ///  r4 = r4.wrapping_shr(48)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    mov64 r0, r4                                    r0 = r4
    jeq r2, 0, lbb_8920                             if r2 == (0 as i32 as i64 as u64) { pc += -57 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r10-0x8]                     
    jeq r2, 0, lbb_9003                             if r2 == (0 as i32 as i64 as u64) { pc += 23 }
    and64 r7, 252                                   r7 &= 252   ///  r7 = r7.and(252)
    lsh64 r7, 3                                     r7 <<= 3   ///  r7 = r7.wrapping_shl(3)
    jlt r8, 192, lbb_8984                           if r8 < (192 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, 192                                   r8 = 192 as i32 as i64 as u64
lbb_8984:
    add64 r6, r7                                    r6 += r7   ///  r6 = r6.wrapping_add(r7)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    and64 r8, 3                                     r8 &= 3   ///  r8 = r8.and(3)
    lsh64 r8, 3                                     r8 <<= 3   ///  r8 = r8.wrapping_shl(3)
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
lbb_8990:
    ldxdw r0, [r6+0x0]                      
    mov64 r5, r0                                    r5 = r0
    rsh64 r5, 6                                     r5 >>= 6   ///  r5 = r5.wrapping_shr(6)
    xor64 r0, -1                                    r0 ^= -1   ///  r0 = r0.xor(-1)
    rsh64 r0, 7                                     r0 >>= 7   ///  r0 = r0.wrapping_shr(7)
    or64 r0, r5                                     r0 |= r5   ///  r0 = r0.or(r5)
    and64 r0, r1                                    r0 &= r1   ///  r0 = r0.and(r1)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    add64 r8, -8                                    r8 += -8   ///  r8 = r8.wrapping_add(-8 as i32 as i64 as u64)
    mov64 r2, r0                                    r2 = r0
    jeq r8, 0, lbb_9003                             if r8 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8990                                     if true { pc += -13 }
lbb_9003:
    lddw r1, 0xff00ff00ff00ff                       r1 load str located at 71777214294589695
    mov64 r2, r0                                    r2 = r0
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    and64 r0, r1                                    r0 &= r1   ///  r0 = r0.and(r1)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    lddw r1, 0x1000100010001                        r1 load str located at 281479271743489
    mul64 r0, r1                                    r0 *= r1   ///  r0 = r0.wrapping_mul(r1)
    rsh64 r0, 48                                    r0 >>= 48   ///  r0 = r0.wrapping_shr(48)
    add64 r0, r4                                    r0 += r4   ///  r0 = r0.wrapping_add(r4)
    ja lbb_9029                                     if true { pc += 13 }
lbb_9016:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r2, 0, lbb_9029                             if r2 == (0 as i32 as i64 as u64) { pc += 11 }
lbb_9018:
    ldxb r4, [r1+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jsgt r4, -65, lbb_9024                          if (r4 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_9024:
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    jeq r2, 0, lbb_9029                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9018                                     if true { pc += -11 }
lbb_9029:
    exit                                    

function_9030:
    call function_9031                      

function_9031:
    stxdw [r10-0xc8], r4                    
    stxdw [r10-0xd0], r3                    
    mov64 r0, r2                                    r0 = r2
    jlt r2, 257, lbb_9048                           if r2 < (257 as i32 as i64 as u64) { pc += 13 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r6, r1                                    r6 = r1
    add64 r6, 256                                   r6 += 256   ///  r6 = r6.wrapping_add(256 as i32 as i64 as u64)
lbb_9038:
    jeq r0, -4, lbb_9047                            if r0 == (-4 as i32 as i64 as u64) { pc += 8 }
    mov64 r7, r6                                    r7 = r6
    add64 r7, r0                                    r7 += r0   ///  r7 = r7.wrapping_add(r0)
    add64 r0, -1                                    r0 += -1   ///  r0 = r0.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r7, [r7+0x0]                       
    lsh64 r7, 56                                    r7 <<= 56   ///  r7 = r7.wrapping_shl(56)
    arsh64 r7, 56                                   r7 >>= 56 (signed)   ///  r7 = (r7 as i64).wrapping_shr(56)
    jslt r7, -64, lbb_9038                          if (r7 as i64) < (-64 as i32 as i64) { pc += -8 }
    add64 r0, 4                                     r0 += 4   ///  r0 = r0.wrapping_add(4 as i32 as i64 as u64)
lbb_9047:
    add64 r0, 253                                   r0 += 253   ///  r0 = r0.wrapping_add(253 as i32 as i64 as u64)
lbb_9048:
    jeq r0, 0, lbb_9052                             if r0 == (0 as i32 as i64 as u64) { pc += 3 }
    jlt r0, r2, lbb_9175                            if r0 < r2 { pc += 125 }
    jeq r0, r2, lbb_9052                            if r0 == r2 { pc += 1 }
    ja lbb_9181                                     if true { pc += 129 }
lbb_9052:
    lddw r6, 0x100016c64 --> b"[...]begin <= end (`byte index  is not a char boun"        r6 load str located at 4295060580
    jlt r0, r2, lbb_9056                            if r0 < r2 { pc += 1 }
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
lbb_9056:
    mov64 r7, 5                                     r7 = 5 as i32 as i64 as u64
    jlt r0, r2, lbb_9059                            if r0 < r2 { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_9059:
    stxdw [r10-0xb8], r0                    
    stxdw [r10-0xc0], r1                    
    stxdw [r10-0xa8], r7                    
    stxdw [r10-0xb0], r6                    
    jgt r3, r2, lbb_9245                            if r3 > r2 { pc += 181 }
    jgt r4, r2, lbb_9245                            if r4 > r2 { pc += 180 }
    jle r3, r4, lbb_9096                            if r3 <= r4 { pc += 30 }
    lddw r1, 0x100017bd0 --> b"\x00\x00\x00\x00il\x01\x00\x0e\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295064528
    stxdw [r10-0x80], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    stxdw [r10-0x70], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100013420 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x13\x08\x00\x00\x00\x00\x00y\x12\x00\x00\x…        r1 load str located at 4295046176
    stxdw [r10-0x18], r1                    
    stxdw [r10-0x28], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -192                                  r1 += -192   ///  r1 = r1.wrapping_add(-192 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -200                                  r1 += -200   ///  r1 = r1.wrapping_add(-200 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    lddw r1, 0x100013120 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295045408
    stxdw [r10-0x38], r1                    
    stxdw [r10-0x48], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    stxdw [r10-0x50], r1                    
    stdw [r10-0x60], 0                      
    stdw [r10-0x78], 4                      
    stdw [r10-0x68], 4                      
    ja lbb_9241                                     if true { pc += 145 }
lbb_9096:
    jeq r3, 0, lbb_9105                             if r3 == (0 as i32 as i64 as u64) { pc += 8 }
    jge r3, r2, lbb_9105                            if r3 >= r2 { pc += 7 }
    mov64 r0, r1                                    r0 = r1
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    ldxb r0, [r0+0x0]                       
    lsh64 r0, 56                                    r0 <<= 56   ///  r0 = r0.wrapping_shl(56)
    arsh64 r0, 56                                   r0 >>= 56 (signed)   ///  r0 = (r0 as i64).wrapping_shr(56)
    jsgt r0, -65, lbb_9105                          if (r0 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r4, r3                                    r4 = r3
lbb_9105:
    stxdw [r10-0xa0], r4                    
    mov64 r3, r2                                    r3 = r2
    jge r4, r2, lbb_9135                            if r4 >= r2 { pc += 27 }
    mov64 r3, r4                                    r3 = r4
    add64 r3, -3                                    r3 += -3   ///  r3 = r3.wrapping_add(-3 as i32 as i64 as u64)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jgt r3, r4, lbb_9114                            if r3 > r4 { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_9114:
    jne r6, 0, lbb_9116                             if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, r3                                    r0 = r3
lbb_9116:
    mov64 r3, r4                                    r3 = r4
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    jge r3, r0, lbb_9124                            if r3 >= r0 { pc += 5 }
    mov64 r1, r0                                    r1 = r0
    mov64 r2, r3                                    r2 = r3
    lddw r3, 0x100017c90 --> b"\x00\x00\x00\x00Ql\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\xf4\x00\x00\x0…        r3 load str located at 4295064720
    call function_8841                      
lbb_9124:
    mov64 r6, r1                                    r6 = r1
    add64 r6, r4                                    r6 += r4   ///  r6 = r6.wrapping_add(r4)
    sub64 r3, r0                                    r3 -= r0   ///  r3 = r3.wrapping_sub(r0)
lbb_9127:
    jeq r3, 0, lbb_9134                             if r3 == (0 as i32 as i64 as u64) { pc += 6 }
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r4, [r6+0x0]                       
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    jslt r4, -64, lbb_9127                          if (r4 as i64) < (-64 as i32 as i64) { pc += -7 }
lbb_9134:
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
lbb_9135:
    jeq r3, 0, lbb_9139                             if r3 == (0 as i32 as i64 as u64) { pc += 3 }
    jlt r3, r2, lbb_9184                            if r3 < r2 { pc += 47 }
    jeq r3, r2, lbb_9139                            if r3 == r2 { pc += 1 }
    ja lbb_9190                                     if true { pc += 51 }
lbb_9139:
    jeq r3, r2, lbb_9173                            if r3 == r2 { pc += 33 }
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    ldxb r0, [r1+0x0]                       
    mov64 r2, r0                                    r2 = r0
    lsh64 r2, 56                                    r2 <<= 56   ///  r2 = r2.wrapping_shl(56)
    arsh64 r2, 56                                   r2 >>= 56 (signed)   ///  r2 = (r2 as i64).wrapping_shr(56)
    jsgt r2, -1, lbb_9192                           if (r2 as i64) > (-1 as i32 as i64) { pc += 46 }
    ldxb r2, [r1+0x1]                       
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    mov64 r4, r0                                    r4 = r0
    and64 r4, 31                                    r4 &= 31   ///  r4 = r4.and(31)
    mov64 r6, r4                                    r6 = r4
    lsh64 r6, 6                                     r6 <<= 6   ///  r6 = r6.wrapping_shl(6)
    or64 r6, r2                                     r6 |= r2   ///  r6 = r6.or(r2)
    jgt r0, 223, lbb_9155                           if r0 > (223 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9193                                     if true { pc += 38 }
lbb_9155:
    lsh64 r2, 6                                     r2 <<= 6   ///  r2 = r2.wrapping_shl(6)
    ldxb r6, [r1+0x2]                       
    and64 r6, 63                                    r6 &= 63   ///  r6 = r6.and(63)
    or64 r2, r6                                     r2 |= r6   ///  r2 = r2.or(r6)
    mov64 r7, r4                                    r7 = r4
    lsh64 r7, 12                                    r7 <<= 12   ///  r7 = r7.wrapping_shl(12)
    mov64 r6, r2                                    r6 = r2
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    jlt r0, 240, lbb_9193                           if r0 < (240 as i32 as i64 as u64) { pc += 29 }
    lsh64 r2, 6                                     r2 <<= 6   ///  r2 = r2.wrapping_shl(6)
    ldxb r1, [r1+0x3]                       
    and64 r1, 63                                    r1 &= 63   ///  r1 = r1.and(63)
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    lsh64 r4, 18                                    r4 <<= 18   ///  r4 = r4.wrapping_shl(18)
    and64 r4, 1835008                               r4 &= 1835008   ///  r4 = r4.and(1835008)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    mov64 r6, r2                                    r6 = r2
    ja lbb_9193                                     if true { pc += 20 }
lbb_9173:
    mov64 r1, r5                                    r1 = r5
    call function_7784                      
lbb_9175:
    mov64 r6, r1                                    r6 = r1
    add64 r6, r0                                    r6 += r0   ///  r6 = r6.wrapping_add(r0)
    ldxb r6, [r6+0x0]                       
    lsh64 r6, 56                                    r6 <<= 56   ///  r6 = r6.wrapping_shl(56)
    arsh64 r6, 56                                   r6 >>= 56 (signed)   ///  r6 = (r6 as i64).wrapping_shr(56)
    jsgt r6, -65, lbb_9052                          if (r6 as i64) > (-65 as i32 as i64) { pc += -129 }
lbb_9181:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r0                                    r4 = r0
    call function_9030                      
lbb_9184:
    mov64 r4, r1                                    r4 = r1
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxb r4, [r4+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    jsgt r4, -65, lbb_9139                          if (r4 as i64) > (-65 as i32 as i64) { pc += -51 }
lbb_9190:
    mov64 r4, r2                                    r4 = r2
    call function_9030                      
lbb_9192:
    mov64 r6, r0                                    r6 = r0
lbb_9193:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxw [r10-0x94], r6                     
    jlt r6, 128, lbb_9201                           if r6 < (128 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    jlt r6, 2048, lbb_9201                          if r6 < (2048 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 3                                     r1 = 3 as i32 as i64 as u64
    jlt r6, 65536, lbb_9201                         if r6 < (65536 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 4                                     r1 = 4 as i32 as i64 as u64
lbb_9201:
    stxdw [r10-0x90], r3                    
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    stxdw [r10-0x88], r1                    
    lddw r1, 0x100017c10 --> b"\x00\x00\x00\x00xl\x01\x00\x0b\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295064592
    stxdw [r10-0x80], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    stxdw [r10-0x70], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x100013420 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x13\x08\x00\x00\x00\x00\x00y\x12\x00\x00\x…        r1 load str located at 4295046176
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -192                                  r1 += -192   ///  r1 = r1.wrapping_add(-192 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x10000f200 --> b"\xbf&\x00\x00\x00\x00\x00\x00\xbf\x17\x00\x00\x00\x00\x00\x00aa4\x00\x00\…        r1 load str located at 4295029248
    stxdw [r10-0x28], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x1000112c8 --> b"\xbf(\x00\x00\x00\x00\x00\x00\xbf\x19\x00\x00\x00\x00\x00\x00y\x86 \x00\x…        r1 load str located at 4295037640
    stxdw [r10-0x38], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -148                                  r1 += -148   ///  r1 = r1.wrapping_add(-148 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    lddw r1, 0x100013120 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295045408
    stxdw [r10-0x48], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -160                                  r1 += -160   ///  r1 = r1.wrapping_add(-160 as i32 as i64 as u64)
    stxdw [r10-0x50], r1                    
    stdw [r10-0x60], 0                      
    stdw [r10-0x78], 5                      
    stdw [r10-0x68], 5                      
lbb_9241:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r2, r5                                    r2 = r5
    call function_7789                      
lbb_9245:
    jgt r3, r2, lbb_9247                            if r3 > r2 { pc += 1 }
    mov64 r3, r4                                    r3 = r4
lbb_9247:
    stxdw [r10-0x90], r3                    
    lddw r1, 0x100017c60 --> b"\x00\x00\x00\x00xl\x01\x00\x0b\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295064672
    stxdw [r10-0x80], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    stxdw [r10-0x70], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100013420 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x13\x08\x00\x00\x00\x00\x00y\x12\x00\x00\x…        r1 load str located at 4295046176
    stxdw [r10-0x28], r1                    
    stxdw [r10-0x38], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -192                                  r1 += -192   ///  r1 = r1.wrapping_add(-192 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    lddw r1, 0x100013120 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295045408
    stxdw [r10-0x48], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    stxdw [r10-0x50], r1                    
    stdw [r10-0x60], 0                      
    stdw [r10-0x78], 3                      
    stdw [r10-0x68], 3                      
    ja lbb_9241                                     if true { pc += -33 }

function_9274:
    mov64 r9, r1                                    r9 = r1
    ldxdw r1, [r5-0xff0]                    
    stxdw [r10-0x10], r1                    
    ldxdw r6, [r5-0xff8]                    
    jeq r3, 0, lbb_9320                             if r3 == (0 as i32 as i64 as u64) { pc += 41 }
    lsh64 r3, 1                                     r3 <<= 1   ///  r3 = r3.wrapping_shl(1)
    mov64 r1, r2                                    r1 = r2
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    stxdw [r10-0x8], r1                     
    ldxdw r8, [r5-0x1000]                   
    mov64 r3, r9                                    r3 = r9
    and64 r3, 65280                                 r3 &= 65280   ///  r3 = r3.and(65280)
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    stxdw [r10-0x18], r4                    
    stxdw [r10-0x20], r8                    
lbb_9290:
    ldxb r7, [r2+0x1]                       
    mov64 r5, r0                                    r5 = r0
    add64 r5, r7                                    r5 += r7   ///  r5 = r5.wrapping_add(r7)
    ldxb r1, [r2+0x0]                       
    add64 r2, 2                                     r2 += 2   ///  r2 = r2.wrapping_add(2 as i32 as i64 as u64)
    jeq r1, r3, lbb_9297                            if r1 == r3 { pc += 1 }
    ja lbb_9309                                     if true { pc += 12 }
lbb_9297:
    jlt r5, r0, lbb_9349                            if r5 < r0 { pc += 51 }
    jgt r5, r8, lbb_9354                            if r5 > r8 { pc += 55 }
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
lbb_9300:
    jeq r7, 0, lbb_9314                             if r7 == (0 as i32 as i64 as u64) { pc += 13 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    ldxb r8, [r4+0x0]                       
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    jeq r8, r1, lbb_9347                            if r8 == r1 { pc += 39 }
    ja lbb_9300                                     if true { pc += -9 }
lbb_9309:
    jgt r1, r3, lbb_9320                            if r1 > r3 { pc += 10 }
    mov64 r0, r5                                    r0 = r5
    ldxdw r1, [r10-0x8]                     
    jeq r2, r1, lbb_9320                            if r2 == r1 { pc += 7 }
    ja lbb_9290                                     if true { pc += -24 }
lbb_9314:
    mov64 r0, r5                                    r0 = r5
    ldxdw r4, [r10-0x18]                    
    ldxdw r8, [r10-0x20]                    
    ldxdw r1, [r10-0x8]                     
    jeq r2, r1, lbb_9320                            if r2 == r1 { pc += 1 }
    ja lbb_9290                                     if true { pc += -30 }
lbb_9320:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ldxdw r1, [r10-0x10]                    
    jeq r1, 0, lbb_9347                             if r1 == (0 as i32 as i64 as u64) { pc += 24 }
    mov64 r2, r6                                    r2 = r6
    ldxdw r1, [r10-0x10]                    
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    and64 r9, 65535                                 r9 &= 65535   ///  r9 = r9.and(65535)
lbb_9327:
    mov64 r4, r6                                    r4 = r6
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    ldxb r3, [r6+0x0]                       
    lsh64 r3, 56                                    r3 <<= 56   ///  r3 = r3.wrapping_shl(56)
    arsh64 r3, 56                                   r3 >>= 56 (signed)   ///  r3 = (r3 as i64).wrapping_shr(56)
    jslt r3, 0, lbb_9335                            if (r3 as i64) < (0 as i32 as i64) { pc += 2 }
    mov64 r6, r4                                    r6 = r4
    ja lbb_9341                                     if true { pc += 6 }
lbb_9335:
    jeq r4, r2, lbb_9359                            if r4 == r2 { pc += 23 }
    and64 r3, 127                                   r3 &= 127   ///  r3 = r3.and(127)
    lsh64 r3, 8                                     r3 <<= 8   ///  r3 = r3.wrapping_shl(8)
    ldxb r4, [r6+0x1]                       
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    add64 r6, 2                                     r6 += 2   ///  r6 = r6.wrapping_add(2 as i32 as i64 as u64)
lbb_9341:
    sub64 r9, r3                                    r9 -= r3   ///  r9 = r9.wrapping_sub(r3)
    lsh64 r9, 32                                    r9 <<= 32   ///  r9 = r9.wrapping_shl(32)
    arsh64 r9, 32                                   r9 >>= 32 (signed)   ///  r9 = (r9 as i64).wrapping_shr(32)
    jslt r9, 0, lbb_9347                            if (r9 as i64) < (0 as i32 as i64) { pc += 2 }
    xor64 r0, 1                                     r0 ^= 1   ///  r0 = r0.xor(1)
    jne r6, r2, lbb_9327                            if r6 != r2 { pc += -20 }
lbb_9347:
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    exit                                    
lbb_9349:
    mov64 r1, r0                                    r1 = r0
    mov64 r2, r5                                    r2 = r5
    lddw r3, 0x100017cc0 --> b"\x00\x00\x00\x00\xc5l\x01\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x0a\x00\x00…        r3 load str located at 4295064768
    call function_8841                      
lbb_9354:
    mov64 r1, r5                                    r1 = r5
    mov64 r2, r8                                    r2 = r8
    lddw r3, 0x100017cc0 --> b"\x00\x00\x00\x00\xc5l\x01\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x0a\x00\x00…        r3 load str located at 4295064768
    call function_8840                      
lbb_9359:
    lddw r1, 0x100017ca8 --> b"\x00\x00\x00\x00\xc5l\x01\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x1a\x00\x00…        r1 load str located at 4295064744
    call function_7784                      

function_9362:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 32, lbb_9399                            if r2 < (32 as i32 as i64 as u64) { pc += 32 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r2, 127, lbb_9399                           if r2 < (127 as i32 as i64 as u64) { pc += 30 }
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 65536, lbb_9387                         if r2 < (65536 as i32 as i64 as u64) { pc += 14 }
    jlt r2, 131072, lbb_9375                        if r2 < (131072 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9401                                     if true { pc += 26 }
lbb_9375:
    lddw r2, 0x100016e0a --> b"^"{\x05\x03\x04-\x03f\x03\x01/.\x80\x82\x1d\x031\x0f\x1c\x04$\x09\x1e\x05…        r2 load str located at 4295061002
    stxdw [r10-0xff8], r2                   
    stdw [r10-0xff0], 486                   
    stdw [r10-0x1000], 208                  
    mov64 r5, r10                                   r5 = r10
    lddw r2, 0x100016ce2 --> b"\x00\x06\x01\x01\x03\x01\x04\x02\x05\x07\x07\x02\x08\x08\x09\x02\x0a\x05\…        r2 load str located at 4295060706
    mov64 r3, 44                                    r3 = 44 as i32 as i64 as u64
    lddw r4, 0x100016d3a --> b"\x0c';>NO\x8f\x9e\x9e\x9f{\x8b\x93\x96\xa2\xb2\xba\x86\xb1\x06\x07\x096=>…        r4 load str located at 4295060794
    ja lbb_9398                                     if true { pc += 11 }
lbb_9387:
    lddw r2, 0x100017162 --> b"\x00 _"\x82\xdf\x04\x82D\x08\x1b\x04\x06\x11\x81\xac\x0e\x80\xab\x05\x1f\…        r2 load str located at 4295061858
    stxdw [r10-0xff8], r2                   
    stdw [r10-0xff0], 297                   
    stdw [r10-0x1000], 290                  
    mov64 r5, r10                                   r5 = r10
    lddw r2, 0x100016ff0 --> b"\x00\x01\x03\x05\x05\x06\x06\x02\x07\x06\x08\x07\x09\x11\x0a\x1c\x0b\x19\…        r2 load str located at 4295061488
    mov64 r3, 40                                    r3 = 40 as i32 as i64 as u64
    lddw r4, 0x100017040 --> b"\xadxy\x8b\x8d\xa20WX\x8b\x8c\x90\x1c\xdd\x0e\x0fKL\xfb\xfc./?\]_\xe2\x84…        r4 load str located at 4295061568
lbb_9398:
    call function_9274                      
lbb_9399:
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    exit                                    
lbb_9401:
    mov64 r2, r1                                    r2 = r1
    and64 r2, 2097150                               r2 &= 2097150   ///  r2 = r2.and(2097150)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r2, 178206, lbb_9399                        if r2 == (178206 as i32 as i64 as u64) { pc += -6 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 2097120                               r2 &= 2097120   ///  r2 = r2.and(2097120)
    jeq r2, 173792, lbb_9399                        if r2 == (173792 as i32 as i64 as u64) { pc += -9 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -177978                               r2 += -177978   ///  r2 = r2.wrapping_add(-177978 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 6, lbb_9399                             if r2 < (6 as i32 as i64 as u64) { pc += -14 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -183970                               r2 += -183970   ///  r2 = r2.wrapping_add(-183970 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 14, lbb_9399                            if r2 < (14 as i32 as i64 as u64) { pc += -19 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -191457                               r2 += -191457   ///  r2 = r2.wrapping_add(-191457 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 15, lbb_9399                            if r2 < (15 as i32 as i64 as u64) { pc += -24 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -192094                               r2 += -192094   ///  r2 = r2.wrapping_add(-192094 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 2466, lbb_9399                          if r2 < (2466 as i32 as i64 as u64) { pc += -29 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -195102                               r2 += -195102   ///  r2 = r2.wrapping_add(-195102 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 1506, lbb_9399                          if r2 < (1506 as i32 as i64 as u64) { pc += -34 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -201547                               r2 += -201547   ///  r2 = r2.wrapping_add(-201547 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 5, lbb_9399                             if r2 < (5 as i32 as i64 as u64) { pc += -39 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -205744                               r2 += -205744   ///  r2 = r2.wrapping_add(-205744 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 712016, lbb_9399                        if r2 < (712016 as i32 as i64 as u64) { pc += -44 }
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r1, 918000, lbb_9399                        if r1 < (918000 as i32 as i64 as u64) { pc += -48 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_9399                                     if true { pc += -50 }

function_9449:
    lddw r3, 0xfffffffc                             r3 load str located at 4294967292
    mov64 r4, r2                                    r4 = r2
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    mov64 r3, r2                                    r3 = r2
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    lddw r4, 0xfffffff8                             r4 load str located at 4294967288
    mov64 r5, r3                                    r5 = r3
    and64 r5, r4                                    r5 &= r4   ///  r5 = r5.and(r4)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    or64 r3, r5                                     r3 |= r5   ///  r3 = r3.or(r5)
    lddw r4, 0xffffffe0                             r4 load str located at 4294967264
    mov64 r5, r3                                    r5 = r3
    and64 r5, r4                                    r5 &= r4   ///  r5 = r5.and(r4)
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    or64 r3, r5                                     r3 |= r5   ///  r3 = r3.or(r5)
    lddw r5, 0xfff00000                             r5 load str located at 4293918720
    mov64 r4, r2                                    r4 = r2
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    rsh64 r4, 20                                    r4 >>= 20   ///  r4 = r4.wrapping_shr(20)
    mov64 r6, r2                                    r6 = r2
    rsh64 r6, 16                                    r6 >>= 16   ///  r6 = r6.wrapping_shr(16)
    and64 r6, 15                                    r6 &= 15   ///  r6 = r6.and(15)
    mov64 r7, r2                                    r7 = r2
    rsh64 r7, 4                                     r7 >>= 4   ///  r7 = r7.wrapping_shr(4)
    and64 r7, 15                                    r7 &= 15   ///  r7 = r7.and(15)
    lddw r0, 0x1000168d0 --> b"0123456789abcdefTargetAlignmentGreaterAndInputNotA"        r0 load str located at 4295059664
    lddw r5, 0x1000168d0 --> b"0123456789abcdefTargetAlignmentGreaterAndInputNotA"        r5 load str located at 4295059664
    add64 r5, r7                                    r5 += r7   ///  r5 = r5.wrapping_add(r7)
    mov64 r8, r2                                    r8 = r2
    rsh64 r8, 8                                     r8 >>= 8   ///  r8 = r8.wrapping_shr(8)
    and64 r8, 15                                    r8 &= 15   ///  r8 = r8.and(15)
    mov64 r9, r2                                    r9 = r2
    rsh64 r9, 12                                    r9 >>= 12   ///  r9 = r9.wrapping_shr(12)
    and64 r9, 15                                    r9 &= 15   ///  r9 = r9.and(15)
    lddw r7, 0x1000168d0 --> b"0123456789abcdefTargetAlignmentGreaterAndInputNotA"        r7 load str located at 4295059664
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    lddw r4, 0x1000168d0 --> b"0123456789abcdefTargetAlignmentGreaterAndInputNotA"        r4 load str located at 4295059664
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    lddw r6, 0x1000168d0 --> b"0123456789abcdefTargetAlignmentGreaterAndInputNotA"        r6 load str located at 4295059664
    add64 r6, r9                                    r6 += r9   ///  r6 = r6.wrapping_add(r9)
    lddw r9, 0x1000168d0 --> b"0123456789abcdefTargetAlignmentGreaterAndInputNotA"        r9 load str located at 4295059664
    add64 r9, r8                                    r9 += r8   ///  r9 = r9.wrapping_add(r8)
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    ldxb r2, [r0+0x0]                       
    stxb [r10-0x2], r2                      
    ldxb r2, [r5+0x0]                       
    stxb [r10-0x3], r2                      
    ldxb r2, [r9+0x0]                       
    stxb [r10-0x4], r2                      
    ldxb r2, [r6+0x0]                       
    stxb [r10-0x5], r2                      
    ldxb r2, [r4+0x0]                       
    stxb [r10-0x6], r2                      
    ldxb r2, [r7+0x0]                       
    stxb [r10-0x7], r2                      
    lddw r2, 0xfffffe00                             r2 load str located at 4294966784
    mov64 r4, r3                                    r4 = r3
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    rsh64 r4, 8                                     r4 >>= 8   ///  r4 = r4.wrapping_shr(8)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    lddw r2, 0xfffe0000                             r2 load str located at 4294836224
    mov64 r4, r3                                    r4 = r3
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    rsh64 r4, 16                                    r4 >>= 16   ///  r4 = r4.wrapping_shr(16)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    mov64 r4, r3                                    r4 = r3
    and64 r4, -2                                    r4 &= -2   ///  r4 = r4.and(-2)
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    and64 r3, 1431655765                            r3 &= 1431655765   ///  r3 = r3.and(1431655765)
    sub64 r4, r3                                    r4 -= r3   ///  r4 = r4.wrapping_sub(r3)
    mov64 r2, r4                                    r2 = r4
    and64 r2, 858993459                             r2 &= 858993459   ///  r2 = r2.and(858993459)
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    and64 r4, 858993459                             r4 &= 858993459   ///  r4 = r4.and(858993459)
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    stb [r10-0x8], 0                        
    sth [r10-0xa], 0                        
    stb [r10-0x1], 125                      
    and64 r2, 252645135                             r2 &= 252645135   ///  r2 = r2.and(252645135)
    mul64 r2, 16843009                              r2 *= 16843009   ///  r2 = r2.wrapping_mul(16843009 as u64)
    rsh64 r2, 26                                    r2 >>= 26   ///  r2 = r2.wrapping_shr(26)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -10                                   r3 += -10   ///  r3 = r3.wrapping_add(-10 as i32 as i64 as u64)
    mov64 r4, r3                                    r4 = r3
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    stb [r4+0x0], 123                       
    add64 r2, -2                                    r2 += -2   ///  r2 = r2.wrapping_add(-2 as i32 as i64 as u64)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    sth [r3+0x0], 30044                     
    ldxh r3, [r10-0x2]                      
    stxh [r1+0x8], r3                       
    ldxdw r3, [r10-0xa]                     
    stxdw [r1+0x0], r3                      
    stxb [r1+0xa], r2                       
    stb [r1+0xb], 10                        
    exit                                    

function_9564:
    mov64 r2, r1                                    r2 = r1
    lddw r1, 0x100017af0 --> b"\x00\x00\x00\x00Rj\x01\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295064304
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_7789                      

function_9575:
    mov64 r2, r1                                    r2 = r1
    lddw r1, 0x100017b00 --> b"\x00\x00\x00\x00nj\x01\x00!\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x…        r1 load str located at 4295064320
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_7789                      

function_9586:
    mov64 r2, r1                                    r2 = r1
    lddw r1, 0x100017b10 --> b"\x00\x00\x00\x00\x8fj\x01\x00!\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295064336
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_7789                      

function_9597:
    mov64 r2, r1                                    r2 = r1
    lddw r1, 0x100017b20 --> b"\x00\x00\x00\x00\xb0j\x01\x00#\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295064352
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_7789                      

function_9608:
    mov64 r2, r1                                    r2 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jlt r2, 10, lbb_9615                            if r2 < (10 as i32 as i64 as u64) { pc += 4 }
    jlt r2, 16, lbb_9613                            if r2 < (16 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9618                                     if true { pc += 5 }
lbb_9613:
    add64 r1, 87                                    r1 += 87   ///  r1 = r1.wrapping_add(87 as i32 as i64 as u64)
    ja lbb_9616                                     if true { pc += 1 }
lbb_9615:
    or64 r1, 48                                     r1 |= 48   ///  r1 = r1.or(48)
lbb_9616:
    mov64 r0, r1                                    r0 = r1
    exit                                    
lbb_9618:
    stxb [r10-0x51], r1                     
    lddw r1, 0x100017d08 --> b"\x00\x00\x00\x00\x8br\x01\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295064840
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -81                                   r1 += -81   ///  r1 = r1.wrapping_add(-81 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x100012f90 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295045008
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    lddw r1, 0x1000172a7 --> b"\x0f out of range for slice of length slice index sta"        r1 load str located at 4295062183
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    lddw r2, 0x100017d28 --> b"\x00\x00\x00\x00!k\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\x8c\x00\x00\x0…        r2 load str located at 4295064872
    call function_7789                      

function_9643:
    mov64 r2, r1                                    r2 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jlt r2, 10, lbb_9650                            if r2 < (10 as i32 as i64 as u64) { pc += 4 }
    jlt r2, 16, lbb_9648                            if r2 < (16 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9653                                     if true { pc += 5 }
lbb_9648:
    add64 r1, 55                                    r1 += 55   ///  r1 = r1.wrapping_add(55 as i32 as i64 as u64)
    ja lbb_9651                                     if true { pc += 1 }
lbb_9650:
    or64 r1, 48                                     r1 |= 48   ///  r1 = r1.or(48)
lbb_9651:
    mov64 r0, r1                                    r0 = r1
    exit                                    
lbb_9653:
    stxb [r10-0x51], r1                     
    lddw r1, 0x100017d08 --> b"\x00\x00\x00\x00\x8br\x01\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295064840
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -81                                   r1 += -81   ///  r1 = r1.wrapping_add(-81 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x100012f90 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295045008
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    lddw r1, 0x1000172a7 --> b"\x0f out of range for slice of length slice index sta"        r1 load str located at 4295062183
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    lddw r2, 0x100017d40 --> b"\x00\x00\x00\x00!k\x01\x00\x13\x00\x00\x00\x00\x00\x00\x00\x8d\x00\x00\x0…        r2 load str located at 4295064896
    call function_7789                      
    mov64 r3, r2                                    r3 = r2
    ldxb r1, [r1+0x0]                       
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_9683                      
    exit                                    

function_9683:
    mov64 r5, r1                                    r5 = r1
    and64 r5, 255                                   r5 &= 255   ///  r5 = r5.and(255)
    jlt r5, 100, lbb_9705                           if r5 < (100 as i32 as i64 as u64) { pc += 19 }
    div64 r5, 100                                   r5 /= 100   ///  r5 = r5 / (100 as u64)
    mov64 r4, r5                                    r4 = r5
    mul64 r4, 100                                   r4 *= 100   ///  r4 = r4.wrapping_mul(100 as u64)
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    and64 r1, 254                                   r1 &= 254   ///  r1 = r1.and(254)
    lddw r4, 0x100016b36 --> b"00010203040506070809101112131415161718192021222324"        r4 load str located at 4295060278
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    ldxh r1, [r4+0x0]                       
    stxh [r10-0x2], r1                      
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r1, r5                                    r1 = r5
lbb_9699:
    mov64 r5, r10                                   r5 = r10
    add64 r5, -3                                    r5 += -3   ///  r5 = r5.wrapping_add(-3 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    or64 r1, 48                                     r1 |= 48   ///  r1 = r1.or(48)
    stxb [r5+0x0], r1                       
    ja lbb_9715                                     if true { pc += 10 }
lbb_9705:
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    jlt r5, 10, lbb_9699                            if r5 < (10 as i32 as i64 as u64) { pc += -8 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    and64 r1, 254                                   r1 &= 254   ///  r1 = r1.and(254)
    lddw r4, 0x100016b36 --> b"00010203040506070809101112131415161718192021222324"        r4 load str located at 4295060278
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    ldxh r1, [r4+0x0]                       
    stxh [r10-0x2], r1                      
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
lbb_9715:
    mov64 r1, r4                                    r1 = r4
    xor64 r1, 3                                     r1 ^= 3   ///  r1 = r1.xor(3)
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -3                                    r1 += -3   ///  r1 = r1.wrapping_add(-3 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r3                                    r1 = r3
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_8063                      
    exit                                    

function_9728:
    mov64 r3, r2                                    r3 = r2
    ldxdw r1, [r1+0x0]                      
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_9733                      
    exit                                    

function_9733:
    mov64 r4, 20                                    r4 = 20 as i32 as i64 as u64
    jlt r1, 10000, lbb_9767                         if r1 < (10000 as i32 as i64 as u64) { pc += 32 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_9736:
    mov64 r5, r1                                    r5 = r1
    div64 r1, 10000                                 r1 /= 10000   ///  r1 = r1 / (10000 as u64)
    mov64 r6, r1                                    r6 = r1
    mul64 r6, 10000                                 r6 *= 10000   ///  r6 = r6.wrapping_mul(10000 as u64)
    mov64 r0, r5                                    r0 = r5
    sub64 r0, r6                                    r0 -= r6   ///  r0 = r0.wrapping_sub(r6)
    mov64 r6, r0                                    r6 = r0
    and64 r6, 65535                                 r6 &= 65535   ///  r6 = r6.and(65535)
    div64 r6, 100                                   r6 /= 100   ///  r6 = r6 / (100 as u64)
    mov64 r7, r6                                    r7 = r6
    mul64 r7, 100                                   r7 *= 100   ///  r7 = r7.wrapping_mul(100 as u64)
    sub64 r0, r7                                    r0 -= r7   ///  r0 = r0.wrapping_sub(r7)
    mov64 r7, r10                                   r7 = r10
    add64 r7, -20                                   r7 += -20   ///  r7 = r7.wrapping_add(-20 as i32 as i64 as u64)
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    lsh64 r6, 1                                     r6 <<= 1   ///  r6 = r6.wrapping_shl(1)
    lddw r8, 0x100016b36 --> b"00010203040506070809101112131415161718192021222324"        r8 load str located at 4295060278
    add64 r8, r6                                    r8 += r6   ///  r8 = r8.wrapping_add(r6)
    ldxh r6, [r8+0x0]                       
    stxh [r7+0x10], r6                      
    lsh64 r0, 1                                     r0 <<= 1   ///  r0 = r0.wrapping_shl(1)
    and64 r0, 65534                                 r0 &= 65534   ///  r0 = r0.and(65534)
    lddw r6, 0x100016b36 --> b"00010203040506070809101112131415161718192021222324"        r6 load str located at 4295060278
    add64 r6, r0                                    r6 += r0   ///  r6 = r6.wrapping_add(r0)
    ldxh r0, [r6+0x0]                       
    stxh [r7+0x12], r0                      
    add64 r4, -4                                    r4 += -4   ///  r4 = r4.wrapping_add(-4 as i32 as i64 as u64)
    jgt r5, 99999999, lbb_9736                      if r5 > (99999999 as i32 as i64 as u64) { pc += -30 }
    add64 r4, 20                                    r4 += 20   ///  r4 = r4.wrapping_add(20 as i32 as i64 as u64)
lbb_9767:
    jgt r1, 99, lbb_9769                            if r1 > (99 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9787                                     if true { pc += 18 }
lbb_9769:
    mov64 r5, r1                                    r5 = r1
    and64 r5, 65535                                 r5 &= 65535   ///  r5 = r5.and(65535)
    div64 r5, 100                                   r5 /= 100   ///  r5 = r5 / (100 as u64)
    mov64 r0, r5                                    r0 = r5
    mul64 r0, 100                                   r0 *= 100   ///  r0 = r0.wrapping_mul(100 as u64)
    sub64 r1, r0                                    r1 -= r0   ///  r1 = r1.wrapping_sub(r0)
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    and64 r1, 65534                                 r1 &= 65534   ///  r1 = r1.and(65534)
    lddw r0, 0x100016b36 --> b"00010203040506070809101112131415161718192021222324"        r0 load str located at 4295060278
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -20                                   r1 += -20   ///  r1 = r1.wrapping_add(-20 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    ldxh r0, [r0+0x0]                       
    stxh [r1+0x0], r0                       
    mov64 r1, r5                                    r1 = r5
lbb_9787:
    jlt r1, 10, lbb_9799                            if r1 < (10 as i32 as i64 as u64) { pc += 11 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    lddw r5, 0x100016b36 --> b"00010203040506070809101112131415161718192021222324"        r5 load str located at 4295060278
    add64 r5, r1                                    r5 += r1   ///  r5 = r5.wrapping_add(r1)
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -20                                   r1 += -20   ///  r1 = r1.wrapping_add(-20 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    ldxh r5, [r5+0x0]                       
    stxh [r1+0x0], r5                       
    ja lbb_9805                                     if true { pc += 6 }
lbb_9799:
    add64 r4, -1                                    r4 += -1   ///  r4 = r4.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -20                                   r5 += -20   ///  r5 = r5.wrapping_add(-20 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    or64 r1, 48                                     r1 |= 48   ///  r1 = r1.or(48)
    stxb [r5+0x0], r1                       
lbb_9805:
    mov64 r1, 20                                    r1 = 20 as i32 as i64 as u64
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -20                                   r1 += -20   ///  r1 = r1.wrapping_add(-20 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r3                                    r1 = r3
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_8063                      
    exit                                    

function_9818:
    ldxdw r3, [r1+0x0]                      
    ldxdw r1, [r1+0x8]                      
    ldxdw r4, [r1+0x18]                     
    mov64 r1, r3                                    r1 = r3
    callx r4                                
    exit                                    

function_9824:
    mov64 r4, r2                                    r4 = r2
    ldxdw r3, [r1+0x8]                      
    ldxdw r2, [r1+0x0]                      
    mov64 r1, r4                                    r1 = r4
    call function_8263                      
    exit                                    

function_9830:
    stxdw [r10-0x58], r2                    
    stxdw [r10-0x60], r1                    
    lddw r1, 0x100017d58 --> b"\x00\x00\x00\x00\xb0h\x01\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295064920
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x100013120 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295045408
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_7789                      

function_9855:
    stxdw [r10-0x58], r2                    
    stxdw [r10-0x60], r1                    
    lddw r1, 0x100017d78 --> b"\x00\x00\x00\x00\xcar\x01\x00\x16\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295064952
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x100013120 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295045408
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_7789                      

function_9880:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r4, 34                                    r4 = 34 as i32 as i64 as u64
    mov64 r3, r1                                    r3 = r1
    lsh64 r3, 11                                    r3 <<= 11   ///  r3 = r3.wrapping_shl(11)

function_9884:
    mov64 r5, r4                                    r5 = r4
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    mov64 r0, r5                                    r0 = r5
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    mov64 r6, r0                                    r6 = r0
    lsh64 r6, 2                                     r6 <<= 2   ///  r6 = r6.wrapping_shl(2)
    lddw r7, 0x1000172f0 --> b"\x00\x03\x00\x00\x83\x04 \x00\x91\x05`\x00]\x13\xa0\x00\x12\x17 \x1f\x0c …        r7 load str located at 4295062256
    add64 r7, r6                                    r7 += r6   ///  r7 = r7.wrapping_add(r6)
    ldxw r6, [r7+0x0]                       
    lsh64 r6, 11                                    r6 <<= 11   ///  r6 = r6.wrapping_shl(11)
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    mov64 r7, r3                                    r7 = r3
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    jgt r6, r7, lbb_9902                            if r6 > r7 { pc += 1 }
    mov64 r2, r0                                    r2 = r0
lbb_9902:
    sub64 r4, r5                                    r4 -= r5   ///  r4 = r4.wrapping_sub(r5)
    jgt r4, 1, function_9884                        if r4 > (1 as i32 as i64 as u64) { pc += -20 }
    mov64 r4, r2                                    r4 = r2
    lsh64 r4, 2                                     r4 <<= 2   ///  r4 = r4.wrapping_shl(2)
    lddw r5, 0x1000172f0 --> b"\x00\x03\x00\x00\x83\x04 \x00\x91\x05`\x00]\x13\xa0\x00\x12\x17 \x1f\x0c …        r5 load str located at 4295062256
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxw r4, [r5+0x0]                       
    lsh64 r4, 11                                    r4 <<= 11   ///  r4 = r4.wrapping_shl(11)
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r4, r3, lbb_9919                            if r4 == r3 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_9919:
    jlt r4, r3, lbb_9921                            if r4 < r3 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_9921:
    add64 r2, r0                                    r2 += r0   ///  r2 = r2.wrapping_add(r0)
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    jgt r2, 33, lbb_9975                            if r2 > (33 as i32 as i64 as u64) { pc += 51 }
    mov64 r4, r2                                    r4 = r2
    lsh64 r4, 2                                     r4 <<= 2   ///  r4 = r4.wrapping_shl(2)
    lddw r6, 0x1000172f0 --> b"\x00\x03\x00\x00\x83\x04 \x00\x91\x05`\x00]\x13\xa0\x00\x12\x17 \x1f\x0c …        r6 load str located at 4295062256
    lddw r7, 0x1000172f0 --> b"\x00\x03\x00\x00\x83\x04 \x00\x91\x05`\x00]\x13\xa0\x00\x12\x17 \x1f\x0c …        r7 load str located at 4295062256
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    mov64 r3, 751                                   r3 = 751 as i32 as i64 as u64
    ldxw r0, [r7+0x0]                       
    rsh64 r0, 21                                    r0 >>= 21   ///  r0 = r0.wrapping_shr(21)
    jeq r2, 33, lbb_9939                            if r2 == (33 as i32 as i64 as u64) { pc += 4 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxw r3, [r7+0x4]                       
    rsh64 r3, 21                                    r3 >>= 21   ///  r3 = r3.wrapping_shr(21)
    jeq r2, 0, lbb_9942                             if r2 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_9939:
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    ldxw r5, [r4-0x4]                       
    and64 r5, 2097151                               r5 &= 2097151   ///  r5 = r5.and(2097151)
lbb_9942:
    mov64 r2, r0                                    r2 = r0
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    jeq r3, 0, lbb_9968                             if r3 == (0 as i32 as i64 as u64) { pc += 22 }
    sub64 r1, r5                                    r1 -= r5   ///  r1 = r1.wrapping_sub(r5)
    lddw r5, 0x100017378 --> b"\x00p\x00\x07\x00-\x01\x01\x01\x02\x01\x02\x01\x01H\x0b0\x15\x10\x01e\x07…        r5 load str located at 4295062392
    add64 r5, r0                                    r5 += r0   ///  r5 = r5.wrapping_add(r0)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_9954:
    mov64 r2, r0                                    r2 = r0
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    jgt r2, 750, lbb_9970                           if r2 > (750 as i32 as i64 as u64) { pc += 13 }
    mov64 r2, r5                                    r2 = r5
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    ldxb r2, [r2+0x0]                       
    add64 r6, r2                                    r6 += r2   ///  r6 = r6.wrapping_add(r2)
    mov64 r2, r6                                    r2 = r6
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jgt r2, r1, lbb_9967                            if r2 > r1 { pc += 2 }
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    jlt r4, r3, lbb_9954                            if r4 < r3 { pc += -13 }
lbb_9967:
    add64 r0, r4                                    r0 += r4   ///  r0 = r0.wrapping_add(r4)
lbb_9968:
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    exit                                    
lbb_9970:
    mov64 r1, r2                                    r1 = r2
    mov64 r2, 751                                   r2 = 751 as i32 as i64 as u64
    lddw r3, 0x100017cf0 --> b"\x00\x00\x00\x00\x90e\x01\x00 \x00\x00\x00\x00\x00\x00\x00Z\x00\x00\x00\x…        r3 load str located at 4295064816
    call function_7808                      
lbb_9975:
    mov64 r1, r2                                    r1 = r2
    mov64 r2, 34                                    r2 = 34 as i32 as i64 as u64
    lddw r3, 0x100017cd8 --> b"\x00\x00\x00\x00\x90e\x01\x00 \x00\x00\x00\x00\x00\x00\x00N\x00\x00\x00(\…        r3 load str located at 4295064792
    call function_7808                      

function_9980:
    mov64 r6, r1                                    r6 = r1
    syscall [invalid]                       
    mov64 r0, r6                                    r0 = r6
    exit                                    

function_9984:
    mov64 r6, r1                                    r6 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    syscall [invalid]                       
    mov64 r0, r6                                    r0 = r6
    exit                                    

function_9989:
    stw [r10-0x4], 0                        
    mov64 r4, r10                                   r4 = r10
    add64 r4, -4                                    r4 += -4   ///  r4 = r4.wrapping_add(-4 as i32 as i64 as u64)
    syscall [invalid]                       
    ldxw r0, [r10-0x4]                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    exit                                    

function_9997:
    call function_10027                     
    exit                                    

function_9999:
    mov64 r0, 3                                     r0 = 3 as i32 as i64 as u64
    lddw r5, 0x7fffffffffffffff                     r5 load str located at 9223372036854775807
    mov64 r3, r1                                    r3 = r1
    and64 r3, r5                                    r3 &= r5   ///  r3 = r3.and(r5)
    lddw r6, 0x7ff0000000000000                     r6 load str located at 9218868437227405312
    jgt r3, r6, lbb_10026                           if r3 > r6 { pc += 19 }
    mov64 r4, r2                                    r4 = r2
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    jgt r4, r6, lbb_10026                           if r4 > r6 { pc += 16 }
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r4, 0, lbb_10026                            if r4 == (0 as i32 as i64 as u64) { pc += 13 }
    mov64 r3, r2                                    r3 = r2
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    jsgt r3, -1, lbb_10021                          if (r3 as i64) > (-1 as i32 as i64) { pc += 5 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jsgt r1, r2, lbb_10026                          if (r1 as i64) > (r2 as i64) { pc += 8 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_10026                           if r1 == r2 { pc += 6 }
    ja lbb_10025                                    if true { pc += 4 }
lbb_10021:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jslt r1, r2, lbb_10026                          if (r1 as i64) < (r2 as i64) { pc += 3 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_10026                           if r1 == r2 { pc += 1 }
lbb_10025:
    mov64 r0, 2                                     r0 = 2 as i32 as i64 as u64
lbb_10026:
    exit                                    

function_10027:
    call function_9999                      
    mov64 r1, r0                                    r1 = r0
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    jsgt r1, 1, lbb_10036                           if (r1 as i64) > (1 as i32 as i64) { pc += 3 }
    jeq r1, 0, lbb_10039                            if r1 == (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_10039                                    if true { pc += 3 }
lbb_10036:
    jeq r1, 2, lbb_10038                            if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_10039                                    if true { pc += 1 }
lbb_10038:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_10039:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    exit                                    

function_10042:
    stxdw [r10-0xc0], r4                    
    mov64 r0, r3                                    r0 = r3
    mov64 r9, r2                                    r9 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r5                                    r1 = r5
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
    stxdw [r10-0xb8], r5                    
    mov64 r3, r5                                    r3 = r5
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 8                                     r1 >>= 8   ///  r1 = r1.wrapping_shr(8)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 16                                    r1 >>= 16   ///  r1 = r1.wrapping_shr(16)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r0                                    r1 = r0
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
    mov64 r4, r0                                    r4 = r0
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 8                                     r1 >>= 8   ///  r1 = r1.wrapping_shr(8)
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    lddw r1, 0x5555555555555555                     r1 load str located at 6148914691236517205
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    mov64 r2, r3                                    r2 = r3
    rsh64 r2, 1                                     r2 >>= 1   ///  r2 = r2.wrapping_shr(1)
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    sub64 r3, r2                                    r3 -= r2   ///  r3 = r3.wrapping_sub(r2)
    mov64 r2, r4                                    r2 = r4
    rsh64 r2, 16                                    r2 >>= 16   ///  r2 = r2.wrapping_shr(16)
    or64 r4, r2                                     r4 |= r2   ///  r4 = r4.or(r2)
    mov64 r2, r4                                    r2 = r4
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    or64 r4, r2                                     r4 |= r2   ///  r4 = r4.or(r2)
    lddw r2, 0x3333333333333333                     r2 load str located at 3689348814741910323
    mov64 r7, r3                                    r7 = r3
    and64 r7, r2                                    r7 &= r2   ///  r7 = r7.and(r2)
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    and64 r3, r2                                    r3 &= r2   ///  r3 = r3.and(r2)
    add64 r7, r3                                    r7 += r3   ///  r7 = r7.wrapping_add(r3)
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    sub64 r4, r3                                    r4 -= r3   ///  r4 = r4.wrapping_sub(r3)
    mov64 r3, r7                                    r3 = r7
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r7, r3                                    r7 += r3   ///  r7 = r7.wrapping_add(r3)
    mov64 r8, r4                                    r8 = r4
    and64 r8, r2                                    r8 &= r2   ///  r8 = r8.and(r2)
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    add64 r8, r4                                    r8 += r4   ///  r8 = r8.wrapping_add(r4)
    mov64 r3, r8                                    r3 = r8
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r8, r3                                    r8 += r3   ///  r8 = r8.wrapping_add(r3)
    lddw r3, 0xf0f0f0f0f0f0f0f                      r3 load str located at 1085102592571150095
    and64 r8, r3                                    r8 &= r3   ///  r8 = r8.and(r3)
    and64 r7, r3                                    r7 &= r3   ///  r7 = r7.and(r3)
    lddw r4, 0x101010101010101                      r4 load str located at 72340172838076673
    mul64 r7, r4                                    r7 *= r4   ///  r7 = r7.wrapping_mul(r4)
    mul64 r8, r4                                    r8 *= r4   ///  r8 = r8.wrapping_mul(r4)
    rsh64 r8, 56                                    r8 >>= 56   ///  r8 = r8.wrapping_shr(56)
    jne r0, 0, lbb_10163                            if r0 != (0 as i32 as i64 as u64) { pc += 38 }
    mov64 r8, r0                                    r8 = r0
    mov64 r0, r9                                    r0 = r9
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    mov64 r5, r9                                    r5 = r9
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r1                                    r0 &= r1   ///  r0 = r0.and(r1)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    mov64 r0, r8                                    r0 = r8
    mov64 r8, r5                                    r8 = r5
    and64 r8, r2                                    r8 &= r2   ///  r8 = r8.and(r2)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, r2                                    r5 &= r2   ///  r5 = r5.and(r2)
    add64 r8, r5                                    r8 += r5   ///  r8 = r8.wrapping_add(r5)
    mov64 r5, r8                                    r5 = r8
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    add64 r8, r5                                    r8 += r5   ///  r8 = r8.wrapping_add(r5)
    and64 r8, r3                                    r8 &= r3   ///  r8 = r8.and(r3)
    mul64 r8, r4                                    r8 *= r4   ///  r8 = r8.wrapping_mul(r4)
    rsh64 r8, 56                                    r8 >>= 56   ///  r8 = r8.wrapping_shr(56)
    add64 r8, 64                                    r8 += 64   ///  r8 = r8.wrapping_add(64 as i32 as i64 as u64)
lbb_10163:
    rsh64 r7, 56                                    r7 >>= 56   ///  r7 = r7.wrapping_shr(56)
    ldxdw r5, [r10-0xb8]                    
    jne r5, 0, lbb_10202                            if r5 != (0 as i32 as i64 as u64) { pc += 36 }
    ldxdw r7, [r10-0xc0]                    
    mov64 r5, r7                                    r5 = r7
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    or64 r7, r5                                     r7 |= r5   ///  r7 = r7.or(r5)
    mov64 r5, r7                                    r5 = r7
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    or64 r7, r5                                     r7 |= r5   ///  r7 = r7.or(r5)
    mov64 r5, r7                                    r5 = r7
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    or64 r7, r5                                     r7 |= r5   ///  r7 = r7.or(r5)
    mov64 r5, r7                                    r5 = r7
    rsh64 r5, 8                                     r5 >>= 8   ///  r5 = r5.wrapping_shr(8)
    or64 r7, r5                                     r7 |= r5   ///  r7 = r7.or(r5)
    mov64 r5, r7                                    r5 = r7
    rsh64 r5, 16                                    r5 >>= 16   ///  r5 = r5.wrapping_shr(16)
    or64 r7, r5                                     r7 |= r5   ///  r7 = r7.or(r5)
    mov64 r5, r7                                    r5 = r7
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    or64 r7, r5                                     r7 |= r5   ///  r7 = r7.or(r5)
    xor64 r7, -1                                    r7 ^= -1   ///  r7 = r7.xor(-1)
    mov64 r5, r7                                    r5 = r7
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    and64 r5, r1                                    r5 &= r1   ///  r5 = r5.and(r1)
    sub64 r7, r5                                    r7 -= r5   ///  r7 = r7.wrapping_sub(r5)
    mov64 r1, r7                                    r1 = r7
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    and64 r7, r2                                    r7 &= r2   ///  r7 = r7.and(r2)
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    add64 r7, r1                                    r7 += r1   ///  r7 = r7.wrapping_add(r1)
    mov64 r1, r7                                    r1 = r7
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    add64 r7, r1                                    r7 += r1   ///  r7 = r7.wrapping_add(r1)
    and64 r7, r3                                    r7 &= r3   ///  r7 = r7.and(r3)
    mul64 r7, r4                                    r7 *= r4   ///  r7 = r7.wrapping_mul(r4)
    rsh64 r7, 56                                    r7 >>= 56   ///  r7 = r7.wrapping_shr(56)
    add64 r7, 64                                    r7 += 64   ///  r7 = r7.wrapping_add(64 as i32 as i64 as u64)
lbb_10202:
    jle r7, r8, lbb_10218                           if r7 <= r8 { pc += 15 }
    mov64 r1, r8                                    r1 = r8
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r5, r9                                    r5 = r9
    jgt r1, 63, lbb_10209                           if r1 > (63 as i32 as i64 as u64) { pc += 1 }
    ja lbb_10245                                    if true { pc += 36 }
lbb_10209:
    mov64 r8, r6                                    r8 = r6
    mov64 r6, r5                                    r6 = r5
    ldxdw r2, [r10-0xc0]                    
    div64 r6, r2                                    r6 /= r2   ///  r6 = r6 / r2
    mov64 r1, r6                                    r1 = r6
    mul64 r1, r2                                    r1 *= r2   ///  r1 = r1.wrapping_mul(r2)
    sub64 r5, r1                                    r5 -= r1   ///  r5 = r5.wrapping_sub(r1)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_10283                                    if true { pc += 65 }
lbb_10218:
    mov64 r8, r6                                    r8 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0xc0]                    
    mov64 r5, r9                                    r5 = r9
    jlt r5, r3, lbb_10226                           if r5 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_10226:
    ldxdw r4, [r10-0xb8]                    
    jlt r0, r4, lbb_10229                           if r0 < r4 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_10229:
    ldxdw r4, [r10-0xb8]                    
    jeq r0, r4, lbb_10232                           if r0 == r4 { pc += 1 }
    mov64 r2, r1                                    r2 = r1
lbb_10232:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jne r2, 0, lbb_10284                            if r2 != (0 as i32 as i64 as u64) { pc += 49 }
    ldxdw r1, [r10-0xb8]                    
    sub64 r0, r1                                    r0 -= r1   ///  r0 = r0.wrapping_sub(r1)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r5, r3, lbb_10242                           if r5 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_10242:
    sub64 r0, r2                                    r0 -= r2   ///  r0 = r0.wrapping_sub(r2)
    sub64 r5, r3                                    r5 -= r3   ///  r5 = r5.wrapping_sub(r3)
    ja lbb_10284                                    if true { pc += 39 }
lbb_10245:
    mov64 r1, r7                                    r1 = r7
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jgt r1, 95, lbb_10250                           if r1 > (95 as i32 as i64 as u64) { pc += 1 }
    ja lbb_10289                                    if true { pc += 39 }
lbb_10250:
    mov64 r8, r6                                    r8 = r6
    ldxdw r7, [r10-0xc0]                    
    mov64 r2, r7                                    r2 = r7
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mov64 r4, r0                                    r4 = r0
    div64 r4, r2                                    r4 /= r2   ///  r4 = r4 / r2
    mov64 r1, r4                                    r1 = r4
    mul64 r1, r7                                    r1 *= r7   ///  r1 = r1.wrapping_mul(r7)
    sub64 r0, r1                                    r0 -= r1   ///  r0 = r0.wrapping_sub(r1)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    div64 r0, r2                                    r0 /= r2   ///  r0 = r0 / r2
    mov64 r1, r0                                    r1 = r0
    mul64 r1, r7                                    r1 *= r7   ///  r1 = r1.wrapping_mul(r7)
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    mov64 r1, r0                                    r1 = r0
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    or64 r1, r4                                     r1 |= r4   ///  r1 = r1.or(r4)
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    or64 r3, r5                                     r3 |= r5   ///  r3 = r3.or(r5)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    mov64 r4, r3                                    r4 = r3
    div64 r4, r2                                    r4 /= r2   ///  r4 = r4 / r2
    or64 r0, r4                                     r0 |= r4   ///  r0 = r0.or(r4)
    mul64 r4, r2                                    r4 *= r2   ///  r4 = r4.wrapping_mul(r2)
    sub64 r3, r4                                    r3 -= r4   ///  r3 = r3.wrapping_sub(r4)
    mov64 r6, r0                                    r6 = r0
    mov64 r5, r3                                    r5 = r3
lbb_10283:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_10284:
    stxdw [r8+0x10], r5                     
    stxdw [r8+0x0], r6                      
    stxdw [r8+0x18], r0                     
    stxdw [r8+0x8], r1                      
    exit                                    
lbb_10289:
    stxdw [r10-0xd0], r5                    
    stxdw [r10-0xc8], r0                    
    stxdw [r10-0x100], r6                   
    mov64 r1, r7                                    r1 = r7
    sub64 r1, r8                                    r1 -= r8   ///  r1 = r1.wrapping_sub(r8)
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jlt r1, 32, lbb_10298                           if r1 < (32 as i32 as i64 as u64) { pc += 1 }
    ja lbb_10365                                    if true { pc += 67 }
lbb_10298:
    mov64 r6, 64                                    r6 = 64 as i32 as i64 as u64
    sub64 r6, r8                                    r6 -= r8   ///  r6 = r6.wrapping_sub(r8)
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    arsh64 r6, 32                                   r6 >>= 32 (signed)   ///  r6 = (r6 as i64).wrapping_shr(32)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    ldxdw r7, [r10-0xc0]                    
    mov64 r2, r7                                    r2 = r7
    ldxdw r8, [r10-0xb8]                    
    mov64 r3, r8                                    r3 = r8
    mov64 r4, r6                                    r4 = r6
    call function_11324                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    ldxdw r2, [r10-0xd0]                    
    ldxdw r3, [r10-0xc8]                    
    mov64 r4, r6                                    r4 = r6
    call function_11324                     
    ldxdw r1, [r10-0x80]                    
    ldxdw r6, [r10-0x90]                    
    div64 r6, r1                                    r6 /= r1   ///  r6 = r6 / r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -160                                  r1 += -160   ///  r1 = r1.wrapping_add(-160 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    ldxdw r1, [r10-0xb0]                    
    ldxdw r2, [r10-0x98]                    
    mov64 r3, r2                                    r3 = r2
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r3, r2, lbb_10340                           if r3 < r2 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_10340:
    ldxdw r4, [r10-0xa8]                    
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    ldxdw r2, [r10-0xa0]                    
    ldxdw r8, [r10-0x100]                   
    ldxdw r0, [r10-0xc8]                    
    ldxdw r5, [r10-0xd0]                    
    jne r4, 0, lbb_10579                            if r4 != (0 as i32 as i64 as u64) { pc += 232 }
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r5, r2, lbb_10351                           if r5 < r2 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_10351:
    jlt r0, r3, lbb_10353                           if r0 < r3 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_10353:
    jeq r0, r3, lbb_10355                           if r0 == r3 { pc += 1 }
    mov64 r1, r4                                    r1 = r4
lbb_10355:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_10579                            if r1 != (0 as i32 as i64 as u64) { pc += 222 }
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r5, r2, lbb_10362                           if r5 < r2 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_10362:
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
    sub64 r5, r2                                    r5 -= r2   ///  r5 = r5.wrapping_sub(r2)
    ja lbb_10284                                    if true { pc += -81 }
lbb_10365:
    mov64 r1, 96                                    r1 = 96 as i32 as i64 as u64
    sub64 r1, r7                                    r1 -= r7   ///  r1 = r1.wrapping_sub(r7)
    stxdw [r10-0xf0], r1                    
    mov64 r4, r1                                    r4 = r1
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    stxdw [r10-0xe8], r4                    
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    ldxdw r2, [r10-0xc0]                    
    ldxdw r3, [r10-0xb8]                    
    call function_11324                     
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x10]                    
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0xf8], r1                    
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r0, [r10-0xc8]                    
    ldxdw r5, [r10-0xd0]                    
lbb_10386:
    stxdw [r10-0xe0], r4                    
    stxdw [r10-0xd8], r3                    
    mov64 r9, 64                                    r9 = 64 as i32 as i64 as u64
    sub64 r9, r8                                    r9 -= r8   ///  r9 = r9.wrapping_sub(r8)
    mov64 r8, r9                                    r8 = r9
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    mov64 r6, r8                                    r6 = r8
    arsh64 r6, 32                                   r6 >>= 32 (signed)   ///  r6 = (r6 as i64).wrapping_shr(32)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0xd0], r5                    
    mov64 r2, r5                                    r2 = r5
    stxdw [r10-0xc8], r0                    
    mov64 r3, r0                                    r3 = r0
    mov64 r4, r6                                    r4 = r6
    call function_11324                     
    ldxdw r2, [r10-0x20]                    
    rsh64 r8, 32                                    r8 >>= 32   ///  r8 = r8.wrapping_shr(32)
    ldxdw r1, [r10-0xe8]                    
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jge r8, r1, lbb_10420                           if r8 >= r1 { pc += 13 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    ldxdw r7, [r10-0xc0]                    
    mov64 r8, r2                                    r8 = r2
    mov64 r2, r7                                    r2 = r7
    ldxdw r3, [r10-0xb8]                    
    mov64 r4, r6                                    r4 = r6
    call function_11324                     
    mov64 r2, r8                                    r2 = r8
    ldxdw r1, [r10-0x60]                    
    jeq r1, 0, lbb_10600                            if r1 == (0 as i32 as i64 as u64) { pc += 182 }
    div64 r2, r1                                    r2 /= r1   ///  r2 = r2 / r1
    ja lbb_10600                                    if true { pc += 180 }
lbb_10420:
    ldxdw r1, [r10-0xf8]                    
    div64 r2, r1                                    r2 /= r1   ///  r2 = r2 / r1
    ldxdw r1, [r10-0xf0]                    
    sub64 r9, r1                                    r9 -= r1   ///  r9 = r9.wrapping_sub(r1)
    and64 r9, 127                                   r9 &= 127   ///  r9 = r9.and(127)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r6, r2                                    r6 = r2
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r9                                    r4 = r9
    call function_11382                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0xc0]                    
    ldxdw r5, [r10-0xb8]                    
    call function_11391                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    ldxdw r2, [r10-0x40]                    
    ldxdw r3, [r10-0x38]                    
    mov64 r4, r9                                    r4 = r9
    call function_11382                     
    ldxdw r3, [r10-0x30]                    
    mov64 r6, r3                                    r6 = r3
    ldxdw r1, [r10-0xe0]                    
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r6, r3, lbb_10452                           if r6 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_10452:
    ldxdw r4, [r10-0x50]                    
    ldxdw r0, [r10-0xc8]                    
    ldxdw r5, [r10-0xd0]                    
    jlt r5, r4, lbb_10457                           if r5 < r4 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_10457:
    ldxdw r3, [r10-0x48]                    
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
    sub64 r0, r1                                    r0 -= r1   ///  r0 = r0.wrapping_sub(r1)
    mov64 r1, r0                                    r1 = r0
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
    mov64 r3, r0                                    r3 = r0
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    ldxdw r1, [r10-0x28]                    
    sub64 r5, r4                                    r5 -= r4   ///  r5 = r5.wrapping_sub(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 8                                     r4 >>= 8   ///  r4 = r4.wrapping_shr(8)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 16                                    r4 >>= 16   ///  r4 = r4.wrapping_shr(16)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    lddw r8, 0x5555555555555555                     r8 load str located at 6148914691236517205
    and64 r4, r8                                    r4 &= r8   ///  r4 = r4.and(r8)
    sub64 r3, r4                                    r3 -= r4   ///  r3 = r3.wrapping_sub(r4)
    mov64 r8, r3                                    r8 = r3
    lddw r4, 0x3333333333333333                     r4 load str located at 3689348814741910323
    and64 r8, r4                                    r8 &= r4   ///  r8 = r8.and(r4)
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    and64 r3, r4                                    r3 &= r4   ///  r3 = r3.and(r4)
    add64 r8, r3                                    r8 += r3   ///  r8 = r8.wrapping_add(r3)
    mov64 r3, r8                                    r3 = r8
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r8, r3                                    r8 += r3   ///  r8 = r8.wrapping_add(r3)
    lddw r3, 0xf0f0f0f0f0f0f0f                      r3 load str located at 1085102592571150095
    and64 r8, r3                                    r8 &= r3   ///  r8 = r8.and(r3)
    lddw r3, 0x101010101010101                      r3 load str located at 72340172838076673
    mul64 r8, r3                                    r8 *= r3   ///  r8 = r8.wrapping_mul(r3)
    rsh64 r8, 56                                    r8 >>= 56   ///  r8 = r8.wrapping_shr(56)
    jne r0, 0, lbb_10550                            if r0 != (0 as i32 as i64 as u64) { pc += 44 }
    mov64 r4, r5                                    r4 = r5
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    mov64 r3, r5                                    r3 = r5
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 8                                     r4 >>= 8   ///  r4 = r4.wrapping_shr(8)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 16                                    r4 >>= 16   ///  r4 = r4.wrapping_shr(16)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    lddw r8, 0x5555555555555555                     r8 load str located at 6148914691236517205
    and64 r4, r8                                    r4 &= r8   ///  r4 = r4.and(r8)
    sub64 r3, r4                                    r3 -= r4   ///  r3 = r3.wrapping_sub(r4)
    mov64 r8, r3                                    r8 = r3
    lddw r4, 0x3333333333333333                     r4 load str located at 3689348814741910323
    and64 r8, r4                                    r8 &= r4   ///  r8 = r8.and(r4)
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    and64 r3, r4                                    r3 &= r4   ///  r3 = r3.and(r4)
    add64 r8, r3                                    r8 += r3   ///  r8 = r8.wrapping_add(r3)
    mov64 r3, r8                                    r3 = r8
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r8, r3                                    r8 += r3   ///  r8 = r8.wrapping_add(r3)
    lddw r3, 0xf0f0f0f0f0f0f0f                      r3 load str located at 1085102592571150095
    and64 r8, r3                                    r8 &= r3   ///  r8 = r8.and(r3)
    lddw r3, 0x101010101010101                      r3 load str located at 72340172838076673
    mul64 r8, r3                                    r8 *= r3   ///  r8 = r8.wrapping_mul(r3)
    rsh64 r8, 56                                    r8 >>= 56   ///  r8 = r8.wrapping_shr(56)
    add64 r8, 64                                    r8 += 64   ///  r8 = r8.wrapping_add(64 as i32 as i64 as u64)
lbb_10550:
    ldxdw r3, [r10-0xd8]                    
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, r7                                    r2 = r7
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jle r2, r8, lbb_10638                           if r2 <= r8 { pc += 81 }
    mov64 r2, r8                                    r2 = r8
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mov64 r4, r6                                    r4 = r6
    mov64 r3, r1                                    r3 = r1
    jgt r2, 63, lbb_10564                           if r2 > (63 as i32 as i64 as u64) { pc += 1 }
    ja lbb_10386                                    if true { pc += -178 }
lbb_10564:
    ldxdw r4, [r10-0xc0]                    
    jeq r4, 0, lbb_10568                            if r4 == (0 as i32 as i64 as u64) { pc += 2 }
    mov64 r3, r5                                    r3 = r5
    div64 r3, r4                                    r3 /= r4   ///  r3 = r3 / r4
lbb_10568:
    mov64 r2, r6                                    r2 = r6
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    ldxdw r8, [r10-0x100]                   
    jlt r2, r6, lbb_10575                           if r2 < r6 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_10575:
    mod64 r5, r4                                    r5 %= r4   ///  r5 = r5 % r4
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    mov64 r6, r2                                    r6 = r2
    ja lbb_10284                                    if true { pc += -295 }
lbb_10579:
    ldxdw r1, [r10-0xb8]                    
    add64 r1, r0                                    r1 += r0   ///  r1 = r1.wrapping_add(r0)
    stxdw [r10-0xb8], r1                    
    mov64 r4, r7                                    r4 = r7
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r4, r7, lbb_10589                           if r4 < r7 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_10589:
    ldxdw r7, [r10-0xb8]                    
    add64 r7, r0                                    r7 += r0   ///  r7 = r7.wrapping_add(r0)
    sub64 r7, r3                                    r7 -= r3   ///  r7 = r7.wrapping_sub(r3)
    mov64 r0, r7                                    r0 = r7
    jlt r4, r2, lbb_10595                           if r4 < r2 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_10595:
    sub64 r0, r5                                    r0 -= r5   ///  r0 = r0.wrapping_sub(r5)
    sub64 r4, r2                                    r4 -= r2   ///  r4 = r4.wrapping_sub(r2)
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r5, r4                                    r5 = r4
    ja lbb_10284                                    if true { pc += -316 }
lbb_10600:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    stxdw [r10-0xe8], r2                    
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r7                                    r4 = r7
    ldxdw r5, [r10-0xb8]                    
    call function_11391                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r1, [r10-0x70]                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    ldxdw r8, [r10-0x100]                   
    ldxdw r5, [r10-0xd0]                    
    ldxdw r9, [r10-0xe0]                    
    jlt r5, r1, lbb_10615                           if r5 < r1 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_10615:
    ldxdw r2, [r10-0x68]                    
    ldxdw r0, [r10-0xc8]                    
    jlt r0, r2, lbb_10619                           if r0 < r2 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_10619:
    jeq r0, r2, lbb_10621                           if r0 == r2 { pc += 1 }
    mov64 r3, r4                                    r3 = r4
lbb_10621:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_10668                            if r3 != (0 as i32 as i64 as u64) { pc += 45 }
    mov64 r6, r9                                    r6 = r9
    ldxdw r3, [r10-0xe8]                    
    add64 r6, r3                                    r6 += r3   ///  r6 = r6.wrapping_add(r3)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r6, r9, lbb_10630                           if r6 < r9 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_10630:
    jlt r5, r1, lbb_10632                           if r5 < r1 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_10632:
    sub64 r0, r2                                    r0 -= r2   ///  r0 = r0.wrapping_sub(r2)
    sub64 r0, r4                                    r0 -= r4   ///  r0 = r0.wrapping_sub(r4)
    sub64 r5, r1                                    r5 -= r1   ///  r5 = r5.wrapping_sub(r1)
    ldxdw r1, [r10-0xd8]                    
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    ja lbb_10284                                    if true { pc += -354 }
lbb_10638:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r8, [r10-0x100]                   
    ldxdw r4, [r10-0xc0]                    
    mov64 r7, r5                                    r7 = r5
    jlt r5, r4, lbb_10645                           if r5 < r4 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_10645:
    ldxdw r5, [r10-0xb8]                    
    jlt r0, r5, lbb_10648                           if r0 < r5 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_10648:
    ldxdw r5, [r10-0xb8]                    
    jeq r0, r5, lbb_10651                           if r0 == r5 { pc += 1 }
    mov64 r2, r3                                    r2 = r3
lbb_10651:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    mov64 r5, r7                                    r5 = r7
    jne r2, 0, lbb_10284                            if r2 != (0 as i32 as i64 as u64) { pc += -370 }
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r5, r4, lbb_10658                           if r5 < r4 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_10658:
    ldxdw r5, [r10-0xb8]                    
    sub64 r0, r5                                    r0 -= r5   ///  r0 = r0.wrapping_sub(r5)
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    jeq r6, 0, lbb_10663                            if r6 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_10663:
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r5, r7                                    r5 = r7
    sub64 r5, r4                                    r5 -= r4   ///  r5 = r5.wrapping_sub(r4)
    ja lbb_10284                                    if true { pc += -384 }
lbb_10668:
    ldxdw r3, [r10-0xb8]                    
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    mov64 r3, r5                                    r3 = r5
    add64 r3, r7                                    r3 += r7   ///  r3 = r3.wrapping_add(r7)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r7, r5                                    r7 = r5
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r3, r7, lbb_10677                           if r3 < r7 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_10677:
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    ldxdw r7, [r10-0xe8]                    
    add64 r7, r9                                    r7 += r9   ///  r7 = r7.wrapping_add(r9)
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r6, r7                                    r6 = r7
    jlt r7, r9, lbb_10685                           if r7 < r9 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_10685:
    sub64 r0, r2                                    r0 -= r2   ///  r0 = r0.wrapping_sub(r2)
    jlt r3, r1, lbb_10688                           if r3 < r1 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_10688:
    sub64 r0, r4                                    r0 -= r4   ///  r0 = r0.wrapping_sub(r4)
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    ldxdw r1, [r10-0xd8]                    
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    mov64 r5, r3                                    r5 = r3
    ja lbb_10284                                    if true { pc += -410 }

function_10694:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    call function_10042                     
    ldxdw r1, [r10-0x20]                    
    ldxdw r2, [r10-0x18]                    
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_10703:
    mov64 r3, r2                                    r3 = r2
    mov64 r0, r1                                    r0 = r1
    mov64 r6, r3                                    r6 = r3
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    and64 r6, r1                                    r6 &= r1   ///  r6 = r6.and(r1)
    lddw r1, 0xfffffffffffff                        r1 load str located at 4503599627370495
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    mov64 r4, r0                                    r4 = r0
    and64 r4, r1                                    r4 &= r1   ///  r4 = r4.and(r1)
    mov64 r7, r3                                    r7 = r3
    rsh64 r7, 52                                    r7 >>= 52   ///  r7 = r7.wrapping_shr(52)
    and64 r7, 2047                                  r7 &= 2047   ///  r7 = r7.and(2047)
    mov64 r8, r0                                    r8 = r0
    rsh64 r8, 52                                    r8 >>= 52   ///  r8 = r8.wrapping_shr(52)
    and64 r8, 2047                                  r8 &= 2047   ///  r8 = r8.and(2047)
    mov64 r1, r8                                    r1 = r8
    add64 r1, -2047                                 r1 += -2047   ///  r1 = r1.wrapping_add(-2047 as i32 as i64 as u64)
    jlt r1, -2046, lbb_10756                        if r1 < (-2046 as i32 as i64 as u64) { pc += 32 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r1, r7                                    r1 = r7
    add64 r1, -2047                                 r1 += -2047   ///  r1 = r1.wrapping_add(-2047 as i32 as i64 as u64)
    jlt r1, -2046, lbb_10756                        if r1 < (-2046 as i32 as i64 as u64) { pc += 28 }
lbb_10728:
    stxdw [r10-0x18], r6                    
    lsh64 r2, 11                                    r2 <<= 11   ///  r2 = r2.wrapping_shl(11)
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    lddw r6, 0x10000000000000                       r6 load str located at 4503599627370496
    or64 r4, r6                                     r4 |= r6   ///  r4 = r4.or(r6)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_11391                     
    add64 r7, r8                                    r7 += r8   ///  r7 = r7.wrapping_add(r8)
    add64 r7, r9                                    r7 += r9   ///  r7 = r7.wrapping_add(r9)
    ldxdw r1, [r10-0x8]                     
    mov64 r3, r1                                    r3 = r1
    and64 r3, r6                                    r3 &= r6   ///  r3 = r3.and(r6)
    ldxdw r2, [r10-0x10]                    
    jeq r3, 0, lbb_10749                            if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_10772                                    if true { pc += 23 }
lbb_10749:
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 63                                    r3 >>= 63   ///  r3 = r3.wrapping_shr(63)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    add64 r7, -1023                                 r7 += -1023   ///  r7 = r7.wrapping_add(-1023 as i32 as i64 as u64)
    ja lbb_10773                                    if true { pc += 17 }
lbb_10756:
    lddw r9, 0x7fffffffffffffff                     r9 load str located at 9223372036854775807
    mov64 r5, r0                                    r5 = r0
    and64 r5, r9                                    r5 &= r9   ///  r5 = r5.and(r9)
    lddw r1, 0x7ff0000000000000                     r1 load str located at 9218868437227405312
    jgt r5, r1, lbb_10783                           if r5 > r1 { pc += 20 }
    mov64 r0, r3                                    r0 = r3
    and64 r0, r9                                    r0 &= r9   ///  r0 = r0.and(r9)
    jgt r0, r1, lbb_10767                           if r0 > r1 { pc += 1 }
    ja lbb_10787                                    if true { pc += 20 }
lbb_10767:
    lddw r1, 0x8000000000000                        r1 load str located at 2251799813685248
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r0, r3                                    r0 = r3
    ja lbb_10826                                    if true { pc += 54 }
lbb_10772:
    add64 r7, -1022                                 r7 += -1022   ///  r7 = r7.wrapping_add(-1022 as i32 as i64 as u64)
lbb_10773:
    ldxdw r6, [r10-0x18]                    
    jsgt r7, 2046, lbb_10793                        if (r7 as i64) > (2046 as i32 as i64) { pc += 18 }
    jslt r7, 1, lbb_10798                           if (r7 as i64) < (1 as i32 as i64) { pc += 22 }
    lddw r3, 0xfffffffffffff                        r3 load str located at 4503599627370495
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    lsh64 r7, 52                                    r7 <<= 52   ///  r7 = r7.wrapping_shl(52)
    or64 r7, r1                                     r7 |= r1   ///  r7 = r7.or(r1)
    mov64 r1, r7                                    r1 = r7
    ja lbb_10815                                    if true { pc += 32 }
lbb_10783:
    lddw r1, 0x8000000000000                        r1 load str located at 2251799813685248
    or64 r0, r1                                     r0 |= r1   ///  r0 = r0.or(r1)
    ja lbb_10826                                    if true { pc += 39 }
lbb_10787:
    jeq r5, r1, lbb_10789                           if r5 == r1 { pc += 1 }
    ja lbb_10827                                    if true { pc += 38 }
lbb_10789:
    mov64 r1, r0                                    r1 = r0
    lddw r0, 0x7ff8000000000000                     r0 load str located at 9221120237041090560
    jeq r1, 0, lbb_10826                            if r1 == (0 as i32 as i64 as u64) { pc += 33 }
lbb_10793:
    lddw r1, 0x7ff0000000000000                     r1 load str located at 9218868437227405312
lbb_10795:
    or64 r6, r1                                     r6 |= r1   ///  r6 = r6.or(r1)
lbb_10796:
    mov64 r0, r6                                    r0 = r6
    ja lbb_10826                                    if true { pc += 28 }
lbb_10798:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    sub64 r3, r7                                    r3 -= r7   ///  r3 = r3.wrapping_sub(r7)
    jgt r3, 63, lbb_10796                           if r3 > (63 as i32 as i64 as u64) { pc += -6 }
    add64 r7, 63                                    r7 += 63   ///  r7 = r7.wrapping_add(63 as i32 as i64 as u64)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r5, r2                                    r5 = r2
    lsh64 r5, r7                                    r5 <<= r7   ///  r5 = r5.wrapping_shl(r7 as u32)
    jne r5, 0, lbb_10809                            if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_10809:
    rsh64 r2, r3                                    r2 >>= r3   ///  r2 = r2.wrapping_shr(r3 as u32)
    mov64 r5, r1                                    r5 = r1
    lsh64 r5, r7                                    r5 <<= r7   ///  r5 = r5.wrapping_shl(r7 as u32)
    or64 r2, r5                                     r2 |= r5   ///  r2 = r2.or(r5)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    rsh64 r1, r3                                    r1 >>= r3   ///  r1 = r1.wrapping_shr(r3 as u32)
lbb_10815:
    mov64 r0, r1                                    r0 = r1
    or64 r0, r6                                     r0 |= r6   ///  r0 = r0.or(r6)
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    jgt r2, r3, lbb_10821                           if r2 > r3 { pc += 1 }
    ja lbb_10823                                    if true { pc += 2 }
lbb_10821:
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    ja lbb_10826                                    if true { pc += 3 }
lbb_10823:
    jne r2, r3, lbb_10826                           if r2 != r3 { pc += 2 }
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
lbb_10826:
    exit                                    
lbb_10827:
    jeq r0, r1, lbb_10829                           if r0 == r1 { pc += 1 }
    ja lbb_10833                                    if true { pc += 4 }
lbb_10829:
    lddw r0, 0x7ff8000000000000                     r0 load str located at 9221120237041090560
    jeq r5, 0, lbb_10826                            if r5 == (0 as i32 as i64 as u64) { pc += -6 }
    ja lbb_10795                                    if true { pc += -38 }
lbb_10833:
    jeq r5, 0, lbb_10796                            if r5 == (0 as i32 as i64 as u64) { pc += -38 }
    jeq r0, 0, lbb_10796                            if r0 == (0 as i32 as i64 as u64) { pc += -39 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    lddw r1, 0x10000000000000                       r1 load str located at 4503599627370496
    jlt r5, r1, lbb_10840                           if r5 < r1 { pc += 1 }
    ja lbb_10890                                    if true { pc += 50 }
lbb_10840:
    mov64 r5, 64                                    r5 = 64 as i32 as i64 as u64
    jeq r4, 0, lbb_10885                            if r4 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
    mov64 r3, r4                                    r3 = r4
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 8                                     r1 >>= 8   ///  r1 = r1.wrapping_shr(8)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 16                                    r1 >>= 16   ///  r1 = r1.wrapping_shr(16)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    lddw r1, 0x5555555555555555                     r1 load str located at 6148914691236517205
    mov64 r5, r3                                    r5 = r3
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    and64 r5, r1                                    r5 &= r1   ///  r5 = r5.and(r1)
    sub64 r3, r5                                    r3 -= r5   ///  r3 = r3.wrapping_sub(r5)
    lddw r1, 0x3333333333333333                     r1 load str located at 3689348814741910323
    mov64 r5, r3                                    r5 = r3
    and64 r5, r1                                    r5 &= r1   ///  r5 = r5.and(r1)
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    add64 r5, r3                                    r5 += r3   ///  r5 = r5.wrapping_add(r3)
    mov64 r1, r5                                    r1 = r5
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    add64 r5, r1                                    r5 += r1   ///  r5 = r5.wrapping_add(r1)
    lddw r1, 0xf0f0f0f0f0f0f0f                      r1 load str located at 1085102592571150095
    and64 r5, r1                                    r5 &= r1   ///  r5 = r5.and(r1)
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
    mul64 r5, r1                                    r5 *= r1   ///  r5 = r5.wrapping_mul(r1)
    rsh64 r5, 56                                    r5 >>= 56   ///  r5 = r5.wrapping_shr(56)
lbb_10885:
    mov64 r9, 12                                    r9 = 12 as i32 as i64 as u64
    sub64 r9, r5                                    r9 -= r5   ///  r9 = r9.wrapping_sub(r5)
    add64 r5, 53                                    r5 += 53   ///  r5 = r5.wrapping_add(53 as i32 as i64 as u64)
    and64 r5, 63                                    r5 &= 63   ///  r5 = r5.and(63)
    lsh64 r4, r5                                    r4 <<= r5   ///  r4 = r4.wrapping_shl(r5 as u32)
lbb_10890:
    lddw r1, 0xfffffffffffff                        r1 load str located at 4503599627370495
    jgt r0, r1, lbb_10728                           if r0 > r1 { pc += -165 }
    mov64 r3, 64                                    r3 = 64 as i32 as i64 as u64
    jeq r2, 0, lbb_10938                            if r2 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    mov64 r1, r2                                    r1 = r2
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 16                                    r3 >>= 16   ///  r3 = r3.wrapping_shr(16)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    xor64 r1, -1                                    r1 ^= -1   ///  r1 = r1.xor(-1)
    lddw r3, 0x5555555555555555                     r3 load str located at 6148914691236517205
    mov64 r5, r1                                    r5 = r1
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    and64 r5, r3                                    r5 &= r3   ///  r5 = r5.and(r3)
    sub64 r1, r5                                    r1 -= r5   ///  r1 = r1.wrapping_sub(r5)
    lddw r5, 0x3333333333333333                     r5 load str located at 3689348814741910323
    mov64 r3, r1                                    r3 = r1
    and64 r3, r5                                    r3 &= r5   ///  r3 = r3.and(r5)
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    and64 r1, r5                                    r1 &= r5   ///  r1 = r1.and(r5)
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    lddw r1, 0xf0f0f0f0f0f0f0f                      r1 load str located at 1085102592571150095
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
    mul64 r3, r1                                    r3 *= r1   ///  r3 = r3.wrapping_mul(r1)
    rsh64 r3, 56                                    r3 >>= 56   ///  r3 = r3.wrapping_shr(56)
lbb_10938:
    sub64 r9, r3                                    r9 -= r3   ///  r9 = r9.wrapping_sub(r3)
    add64 r3, 53                                    r3 += 53   ///  r3 = r3.wrapping_add(53 as i32 as i64 as u64)
    and64 r3, 63                                    r3 &= 63   ///  r3 = r3.and(63)
    lsh64 r2, r3                                    r2 <<= r3   ///  r2 = r2.wrapping_shl(r3 as u32)
    add64 r9, 12                                    r9 += 12   ///  r9 = r9.wrapping_add(12 as i32 as i64 as u64)
    ja lbb_10728                                    if true { pc += -216 }

function_10944:
    call function_10703                     
    exit                                    

function_10946:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_10955                     
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x8]                     
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_10955:
    mov64 r8, r5                                    r8 = r5
    mov64 r7, r3                                    r7 = r3
    mov64 r3, r2                                    r3 = r2
    mov64 r6, r1                                    r6 = r1
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    jslt r7, 0, lbb_10962                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r2, r3                                    r2 = r3
lbb_10962:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_10966                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_10966:
    mov64 r3, r7                                    r3 = r7
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r0, r4                                    r0 = r4
    neg64 r0                                        r0 = -r0   ///  r0 = (r0 as i64).wrapping_neg() as u64
    jslt r8, 0, lbb_10972                           if (r8 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r0, r4                                    r0 = r4
lbb_10972:
    neg64 r3                                        r3 = -r3   ///  r3 = (r3 as i64).wrapping_neg() as u64
    jslt r7, 0, lbb_10975                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r3, r7                                    r3 = r7
lbb_10975:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_10978                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_10978:
    mov64 r5, r8                                    r5 = r8
    add64 r5, r1                                    r5 += r1   ///  r5 = r5.wrapping_add(r1)
    neg64 r5                                        r5 = -r5   ///  r5 = (r5 as i64).wrapping_neg() as u64
    jslt r8, 0, lbb_10983                           if (r8 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r5, r8                                    r5 = r8
lbb_10983:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r4, r0                                    r4 = r0
    call function_10042                     
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jslt r8, 0, lbb_10990                           if (r8 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_10990:
    jslt r7, 0, lbb_10992                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_10992:
    ldxdw r2, [r10-0x18]                    
    ldxdw r1, [r10-0x20]                    
    jeq r9, r3, lbb_11001                           if r9 == r3 { pc += 6 }
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_10998                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_10998:
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
lbb_11001:
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_11004:
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r5-0xff8]                    
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r5-0x1000]                   
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    call function_11223                     
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x8]                     
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_11018:
    stxdw [r10-0x10], r3                    
    stxdw [r10-0x8], r1                     
    mov64 r1, r2                                    r1 = r2
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r7, r4                                    r7 = r4
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r6, r2                                    r6 = r2
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    mov64 r3, r7                                    r3 = r7
    mul64 r3, r1                                    r3 *= r1   ///  r3 = r3.wrapping_mul(r1)
    mul64 r7, r6                                    r7 *= r6   ///  r7 = r7.wrapping_mul(r6)
    mov64 r0, r4                                    r0 = r4
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r9, r0                                    r9 = r0
    mul64 r9, r1                                    r9 *= r1   ///  r9 = r9.wrapping_mul(r1)
    mov64 r1, r9                                    r1 = r9
    add64 r1, r7                                    r1 += r7   ///  r1 = r1.wrapping_add(r7)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jlt r1, r9, lbb_11040                           if r1 < r9 { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_11040:
    mov64 r9, r1                                    r9 = r1
    lsh64 r9, 32                                    r9 <<= 32   ///  r9 = r9.wrapping_shl(32)
    mov64 r7, r3                                    r7 = r3
    add64 r7, r9                                    r7 += r9   ///  r7 = r7.wrapping_add(r9)
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jlt r7, r3, lbb_11047                           if r7 < r3 { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_11047:
    ldxdw r3, [r10-0x8]                     
    stxdw [r3+0x0], r7                      
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    or64 r8, r1                                     r8 |= r1   ///  r8 = r8.or(r1)
    ldxdw r1, [r10-0x10]                    
    mul64 r4, r1                                    r4 *= r1   ///  r4 = r4.wrapping_mul(r1)
    mul64 r5, r2                                    r5 *= r2   ///  r5 = r5.wrapping_mul(r2)
    mul64 r0, r6                                    r0 *= r6   ///  r0 = r0.wrapping_mul(r6)
    add64 r0, r8                                    r0 += r8   ///  r0 = r0.wrapping_add(r8)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    add64 r0, r9                                    r0 += r9   ///  r0 = r0.wrapping_add(r9)
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    stxdw [r3+0x8], r0                      
    exit                                    

function_11062:
    mov64 r8, r5                                    r8 = r5
    mov64 r6, r4                                    r6 = r4
    mov64 r7, r3                                    r7 = r3
    mov64 r9, r2                                    r9 = r2
    stxdw [r10-0x58], r1                    
    jeq r7, 0, lbb_11102                            if r7 == (0 as i32 as i64 as u64) { pc += 34 }
    jeq r8, 0, lbb_11070                            if r8 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11118                                    if true { pc += 48 }
lbb_11070:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    mov64 r5, r8                                    r5 = r8
    call function_11391                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    mov64 r5, r8                                    r5 = r8
    call function_11391                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0x28]                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_11089                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11089:
    ldxdw r4, [r10-0x30]                    
    ldxdw r5, [r10-0x18]                    
    mov64 r3, r5                                    r3 = r5
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    jlt r3, r5, lbb_11095                           if r3 < r5 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_11095:
    ldxdw r4, [r10-0x20]                    
    ldxdw r5, [r10-0x58]                    
    stxdw [r5+0x0], r4                      
    stxdw [r5+0x8], r3                      
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    stxb [r5+0x10], r1                      
    ja lbb_11156                                    if true { pc += 54 }
lbb_11102:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r9                                    r4 = r9
    mov64 r5, r7                                    r5 = r7
    call function_11391                     
    ldxdw r6, [r10-0x38]                    
    ldxdw r2, [r10-0x40]                    
    jeq r8, 0, lbb_11113                            if r8 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11132                                    if true { pc += 19 }
lbb_11113:
    ldxdw r1, [r10-0x58]                    
    stxdw [r1+0x0], r2                      
    stxdw [r1+0x8], r6                      
    stb [r1+0x10], 0                        
    ja lbb_11156                                    if true { pc += 38 }
lbb_11118:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r8                                    r3 = r8
    mov64 r4, r9                                    r4 = r9
    mov64 r5, r7                                    r5 = r7
    call function_11391                     
    ldxdw r1, [r10-0x8]                     
    ldxdw r2, [r10-0x58]                    
    stxdw [r2+0x8], r1                      
    ldxdw r1, [r10-0x10]                    
    stxdw [r2+0x0], r1                      
    stb [r2+0x10], 1                        
    ja lbb_11156                                    if true { pc += 24 }
lbb_11132:
    stxdw [r10-0x60], r2                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r9                                    r4 = r9
    mov64 r5, r7                                    r5 = r7
    call function_11391                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0x48]                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_11145                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11145:
    ldxdw r4, [r10-0x50]                    
    mov64 r3, r6                                    r3 = r6
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    jlt r3, r6, lbb_11150                           if r3 < r6 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_11150:
    ldxdw r4, [r10-0x58]                    
    ldxdw r5, [r10-0x60]                    
    stxdw [r4+0x0], r5                      
    stxdw [r4+0x8], r3                      
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    stxb [r4+0x10], r1                      
lbb_11156:
    exit                                    

function_11157:
    mov64 r7, r5                                    r7 = r5
    mov64 r8, r3                                    r8 = r3
    mov64 r3, r2                                    r3 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r3                                    r1 = r3
    or64 r1, r8                                     r1 |= r8   ///  r1 = r1.or(r8)
    jeq r1, 0, lbb_11167                            if r1 == (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, r4                                    r1 = r4
    or64 r1, r7                                     r1 |= r7   ///  r1 = r1.or(r7)
    jne r1, 0, lbb_11171                            if r1 != (0 as i32 as i64 as u64) { pc += 4 }
lbb_11167:
    stdw [r6+0x8], 0                        
    stdw [r6+0x0], 0                        
    stb [r6+0x10], 0                        
    ja lbb_11222                                    if true { pc += 51 }
lbb_11171:
    mov64 r2, r3                                    r2 = r3
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    jslt r8, 0, lbb_11175                           if (r8 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r2, r3                                    r2 = r3
lbb_11175:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_11179                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_11179:
    mov64 r3, r8                                    r3 = r8
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r0, r4                                    r0 = r4
    neg64 r0                                        r0 = -r0   ///  r0 = (r0 as i64).wrapping_neg() as u64
    jslt r7, 0, lbb_11185                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r0, r4                                    r0 = r4
lbb_11185:
    neg64 r3                                        r3 = -r3   ///  r3 = (r3 as i64).wrapping_neg() as u64
    jslt r8, 0, lbb_11188                           if (r8 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r3, r8                                    r3 = r8
lbb_11188:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_11191                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_11191:
    mov64 r5, r7                                    r5 = r7
    add64 r5, r1                                    r5 += r1   ///  r5 = r5.wrapping_add(r1)
    neg64 r5                                        r5 = -r5   ///  r5 = (r5 as i64).wrapping_neg() as u64
    jslt r7, 0, lbb_11196                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r5, r7                                    r5 = r7
lbb_11196:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r4, r0                                    r4 = r0
    call function_11062                     
    xor64 r7, r8                                    r7 ^= r8   ///  r7 = r7.xor(r8)
    ldxdw r1, [r10-0x18]                    
    jne r1, 0, lbb_11204                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_11204:
    mov64 r2, r1                                    r2 = r1
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    jslt r7, 0, lbb_11208                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r2, r1                                    r2 = r1
lbb_11208:
    ldxb r1, [r10-0x8]                      
    ldxdw r3, [r10-0x10]                    
    stxdw [r6+0x0], r2                      
    mov64 r2, r3                                    r2 = r3
    add64 r2, r9                                    r2 += r9   ///  r2 = r2.wrapping_add(r9)
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    jslt r7, 0, lbb_11216                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r2, r3                                    r2 = r3
lbb_11216:
    stxdw [r6+0x8], r2                      
    xor64 r2, r7                                    r2 ^= r7   ///  r2 = r2.xor(r7)
    jslt r2, 0, lbb_11221                           if (r2 as i64) < (0 as i32 as i64) { pc += 2 }
    stxb [r6+0x10], r1                      
    ja lbb_11222                                    if true { pc += 1 }
lbb_11221:
    stb [r6+0x10], 1                        
lbb_11222:
    exit                                    

function_11223:
    mov64 r6, r5                                    r6 = r5
    mov64 r7, r1                                    r7 = r1
    ldxdw r5, [r6-0x1000]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    call function_11157                     
    ldxdw r1, [r10-0x18]                    
    ldxdw r2, [r10-0x10]                    
    ldxb r3, [r10-0x8]                      
    ldxdw r4, [r6-0xff8]                    
    stxw [r4+0x0], r3                       
    stxdw [r7+0x8], r2                      
    stxdw [r7+0x0], r1                      
    exit                                    

function_11237:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_11299                            if r1 == (0 as i32 as i64 as u64) { pc += 60 }
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    mov64 r2, r1                                    r2 = r1
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 16                                    r3 >>= 16   ///  r3 = r3.wrapping_shr(16)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    lddw r3, 0x5555555555555555                     r3 load str located at 6148914691236517205
    mov64 r4, r2                                    r4 = r2
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    sub64 r2, r4                                    r2 -= r4   ///  r2 = r2.wrapping_sub(r4)
    lddw r4, 0x3333333333333333                     r4 load str located at 3689348814741910323
    mov64 r3, r2                                    r3 = r2
    and64 r3, r4                                    r3 &= r4   ///  r3 = r3.and(r4)
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    and64 r2, r4                                    r2 &= r4   ///  r2 = r2.and(r4)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    mov64 r2, r3                                    r2 = r3
    rsh64 r2, 4                                     r2 >>= 4   ///  r2 = r2.wrapping_shr(4)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    lddw r2, 0xf0f0f0f0f0f0f0f                      r2 load str located at 1085102592571150095
    and64 r3, r2                                    r3 &= r2   ///  r3 = r3.and(r2)
    lddw r2, 0x101010101010101                      r2 load str located at 72340172838076673
    mul64 r3, r2                                    r3 *= r2   ///  r3 = r3.wrapping_mul(r2)
    rsh64 r3, 56                                    r3 >>= 56   ///  r3 = r3.wrapping_shr(56)
    lsh64 r1, r3                                    r1 <<= r3   ///  r1 = r1.wrapping_shl(r3 as u32)
    lsh64 r3, 52                                    r3 <<= 52   ///  r3 = r3.wrapping_shl(52)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 11                                    r2 >>= 11   ///  r2 = r2.wrapping_shr(11)
    mov64 r0, r2                                    r0 = r2
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    lsh64 r1, 53                                    r1 <<= 53   ///  r1 = r1.wrapping_shl(53)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 63                                    r3 >>= 63   ///  r3 = r3.wrapping_shr(63)
    and64 r3, r2                                    r3 &= r2   ///  r3 = r3.and(r2)
    sub64 r1, r3                                    r1 -= r3   ///  r1 = r1.wrapping_sub(r3)
    rsh64 r1, 63                                    r1 >>= 63   ///  r1 = r1.wrapping_shr(63)
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
    lddw r1, 0x43d0000000000000                     r1 load str located at 4886405595696988160
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
lbb_11299:
    exit                                    

function_11300:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    lddw r2, 0x3ff0000000000000                     r2 load str located at 4607182418800017408
    jlt r1, r2, lbb_11323                           if r1 < r2 { pc += 19 }
    lddw r2, 0x43f0000000000000                     r2 load str located at 4895412794951729152
    jlt r1, r2, lbb_11313                           if r1 < r2 { pc += 6 }
    lddw r2, 0x7ff0000000000001                     r2 load str located at 9218868437227405313
    jlt r1, r2, lbb_11311                           if r1 < r2 { pc += 1 }
    ja lbb_11323                                    if true { pc += 12 }
lbb_11311:
    mov64 r0, -1                                    r0 = -1 as i32 as i64 as u64
    ja lbb_11323                                    if true { pc += 10 }
lbb_11313:
    mov64 r0, r1                                    r0 = r1
    lsh64 r0, 11                                    r0 <<= 11   ///  r0 = r0.wrapping_shl(11)
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    rsh64 r1, 52                                    r1 >>= 52   ///  r1 = r1.wrapping_shr(52)
    mov64 r2, 62                                    r2 = 62 as i32 as i64 as u64
    sub64 r2, r1                                    r2 -= r1   ///  r2 = r2.wrapping_sub(r1)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    rsh64 r0, r2                                    r0 >>= r2   ///  r0 = r0.wrapping_shr(r2 as u32)
lbb_11323:
    exit                                    

function_11324:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_11357                     
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x8]                     
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_11333:
    mov64 r5, r4                                    r5 = r4
    and64 r5, 64                                    r5 &= 64   ///  r5 = r5.and(64)
    jne r5, 0, lbb_11350                            if r5 != (0 as i32 as i64 as u64) { pc += 14 }
    mov64 r5, r4                                    r5 = r4
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    jeq r5, 0, lbb_11354                            if r5 == (0 as i32 as i64 as u64) { pc += 14 }
    mov64 r5, r4                                    r5 = r4
    and64 r5, 63                                    r5 &= 63   ///  r5 = r5.and(63)
    lsh64 r3, r5                                    r3 <<= r5   ///  r3 = r3.wrapping_shl(r5 as u32)
    neg64 r4                                        r4 = -r4   ///  r4 = (r4 as i64).wrapping_neg() as u64
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    mov64 r0, r2                                    r0 = r2
    rsh64 r0, r4                                    r0 >>= r4   ///  r0 = r0.wrapping_shr(r4 as u32)
    or64 r3, r0                                     r3 |= r0   ///  r3 = r3.or(r0)
    lsh64 r2, r5                                    r2 <<= r5   ///  r2 = r2.wrapping_shl(r5 as u32)
    ja lbb_11354                                    if true { pc += 4 }
lbb_11350:
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    mov64 r3, r2                                    r3 = r2
    lsh64 r3, r4                                    r3 <<= r4   ///  r3 = r3.wrapping_shl(r4 as u32)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11354:
    stxdw [r1+0x0], r2                      
    stxdw [r1+0x8], r3                      
    exit                                    

function_11357:
    mov64 r5, r4                                    r5 = r4
    and64 r5, 64                                    r5 &= 64   ///  r5 = r5.and(64)
    jne r5, 0, lbb_11375                            if r5 != (0 as i32 as i64 as u64) { pc += 15 }
    mov64 r5, r4                                    r5 = r4
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    jeq r5, 0, lbb_11379                            if r5 == (0 as i32 as i64 as u64) { pc += 15 }
    mov64 r5, r4                                    r5 = r4
    and64 r5, 63                                    r5 &= 63   ///  r5 = r5.and(63)
    rsh64 r2, r5                                    r2 >>= r5   ///  r2 = r2.wrapping_shr(r5 as u32)
    neg64 r4                                        r4 = -r4   ///  r4 = (r4 as i64).wrapping_neg() as u64
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    mov64 r0, r3                                    r0 = r3
    lsh64 r0, r4                                    r0 <<= r4   ///  r0 = r0.wrapping_shl(r4 as u32)
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    rsh64 r3, r5                                    r3 >>= r5   ///  r3 = r3.wrapping_shr(r5 as u32)
    mov64 r2, r0                                    r2 = r0
    ja lbb_11379                                    if true { pc += 4 }
lbb_11375:
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    rsh64 r3, r4                                    r3 >>= r4   ///  r3 = r3.wrapping_shr(r4 as u32)
    mov64 r2, r3                                    r2 = r3
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_11379:
    stxdw [r1+0x0], r2                      
    stxdw [r1+0x8], r3                      
    exit                                    

function_11382:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_11333                     
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x8]                     
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_11391:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_11018                     
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x8]                     
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_11400:
    call function_10027                     
    exit                                    
