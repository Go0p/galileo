function_0:
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r2+0x0]                      
    ldxdw r3, [r1+0x50]                     
    jne r3, 82, lbb_16                              if r3 != (82 as i32 as i64 as u64) { pc += 12 }
    mov64 r7, r2                                    r7 = r2
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x10002cf08 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295151368
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_19                               if r0 == (0 as i32 as i64 as u64) { pc += 6 }
    stdw [r6+0x0], 0                        
    stw [r6+0x8], 22                        
    ja lbb_18                                       if true { pc += 2 }
lbb_16:
    stdw [r6+0x0], 0                        
    stw [r6+0x8], 3                         
lbb_18:
    exit                                    
lbb_19:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_18010                     
    ldxdw r1, [r10-0x20]                    
    jeq r1, 0, lbb_31                               if r1 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r2, [r10-0x10]                    
    ldxb r3, [r10-0x8]                      
    stxb [r6+0x10], r3                      
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    ja lbb_18                                       if true { pc += -13 }
lbb_31:
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x8], r1                      
    stdw [r6+0x0], 0                        
    ja lbb_18                                       if true { pc += -17 }

function_35:
    lddw r2, 0x300000000                            r2 load str located at 12884901888
    ldxdw r3, [r2+0x0]                      
    mov64 r2, r3                                    r2 = r3
    add64 r2, -24                                   r2 += -24   ///  r2 = r2.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jgt r2, r3, lbb_44                              if r2 > r3 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_44:
    jne r5, 0, lbb_46                               if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r2                                    r4 = r2
lbb_46:
    lddw r2, 0x300007fe8                            r2 load str located at 12884934632
    jeq r3, 0, lbb_51                               if r3 == (0 as i32 as i64 as u64) { pc += 2 }
    and64 r4, -8                                    r4 &= -8   ///  r4 = r4.and(-8)
    mov64 r2, r4                                    r2 = r4
lbb_51:
    lddw r3, 0x300000007                            r3 load str located at 12884901895
    jgt r2, r3, lbb_57                              if r2 > r3 { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    call function_18352                     
lbb_57:
    lddw r3, 0x300000000                            r3 load str located at 12884901888
    stxdw [r3+0x0], r2                      
    ldxdw r3, [r1+0x10]                     
    stxdw [r2+0x10], r3                     
    ldxdw r3, [r1+0x8]                      
    stxdw [r2+0x8], r3                      
    ldxdw r1, [r1+0x0]                      
    stxdw [r2+0x0], r1                      
    mov64 r1, 21                                    r1 = 21 as i32 as i64 as u64
    lddw r3, 0x10002ea68 --> b"\x00\x00\x00\x00\xf0\x06\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r3 load str located at 4295158376
    call function_18139                     
    exit                                    

function_71:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jslt r2, 0, lbb_125                             if (r2 as i64) < (0 as i32 as i64) { pc += 52 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_98                               if r2 == (0 as i32 as i64 as u64) { pc += 22 }
    lddw r3, 0x300000000                            r3 load str located at 12884901888
    ldxdw r3, [r3+0x0]                      
    lddw r5, 0x300008000                            r5 load str located at 12884934656
    jeq r3, 0, lbb_83                               if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, r3                                    r5 = r3
lbb_83:
    mov64 r4, r5                                    r4 = r5
    sub64 r4, r2                                    r4 -= r2   ///  r4 = r4.wrapping_sub(r2)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jgt r4, r5, lbb_90                              if r4 > r5 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_90:
    jne r0, 0, lbb_92                               if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r4                                    r6 = r4
lbb_92:
    lddw r4, 0x300000008                            r4 load str located at 12884901896
    jlt r6, r4, lbb_125                             if r6 < r4 { pc += 30 }
    lddw r3, 0x300000000                            r3 load str located at 12884901888
    stxdw [r3+0x0], r6                      
lbb_98:
    mov64 r4, r1                                    r4 = r1
    mov64 r1, r6                                    r1 = r6
    mov64 r3, r2                                    r3 = r2
    mov64 r2, r4                                    r2 = r4
    mov64 r8, r3                                    r8 = r3
    call function_21513                     
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    mov64 r2, r1                                    r2 = r1
    add64 r2, -24                                   r2 += -24   ///  r2 = r2.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jgt r2, r1, lbb_112                             if r2 > r1 { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_112:
    jne r7, 0, lbb_114                              if r7 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r2                                    r3 = r2
lbb_114:
    lddw r2, 0x300007fe8                            r2 load str located at 12884934632
    jeq r1, 0, lbb_119                              if r1 == (0 as i32 as i64 as u64) { pc += 2 }
    and64 r3, -8                                    r3 &= -8   ///  r3 = r3.and(-8)
    mov64 r2, r3                                    r2 = r3
lbb_119:
    lddw r1, 0x300000007                            r1 load str located at 12884901895
    jgt r2, r1, lbb_129                             if r2 > r1 { pc += 7 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    call function_18352                     
lbb_125:
    mov64 r1, r3                                    r1 = r3
    lddw r3, 0x10002ea20 --> b"\x00\x00\x00\x00\x08\xd1\x02\x00T\x00\x00\x00\x00\x00\x00\x00\x9f\x00\x00…        r3 load str located at 4295158304
    call function_18348                     
lbb_129:
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    stxdw [r1+0x0], r2                      
    stxdw [r2+0x8], r6                      
    stxdw [r2+0x10], r8                     
    stxdw [r2+0x0], r8                      
    mov64 r1, 21                                    r1 = 21 as i32 as i64 as u64
    lddw r3, 0x10002ea68 --> b"\x00\x00\x00\x00\xf0\x06\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r3 load str located at 4295158376
    call function_18139                     
    exit                                    

function_140:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002d068 --> b"()"                  r2 load str located at 4295151720
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_19571                     
    exit                                    

function_146:
    ldxdw r1, [r1+0x0]                      
    call function_18636                     
    exit                                    

function_149:
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r1+0x8]                      
    ldxdw r1, [r1+0x0]                      
    call function_20083                     
    exit                                    

function_154:
    ldxw r3, [r2+0x34]                      
    mov64 r4, r3                                    r4 = r3
    and64 r4, 16                                    r4 &= 16   ///  r4 = r4.and(16)
    jne r4, 0, lbb_163                              if r4 != (0 as i32 as i64 as u64) { pc += 5 }
    and64 r3, 32                                    r3 &= 32   ///  r3 = r3.and(32)
    jeq r3, 0, lbb_161                              if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_165                                      if true { pc += 4 }
lbb_161:
    call function_21087                     
    ja lbb_166                                      if true { pc += 3 }
lbb_163:
    call function_21071                     
    ja lbb_166                                      if true { pc += 1 }
lbb_165:
    call function_21075                     
lbb_166:
    exit                                    

function_167:
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jgt r2, 7, lbb_180                              if r2 > (7 as i32 as i64 as u64) { pc += 11 }
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    lsh64 r3, r2                                    r3 <<= r2   ///  r3 = r3.wrapping_shl(r2 as u32)
    and64 r3, 255                                   r3 &= 255   ///  r3 = r3.and(255)
    ldxb r2, [r1+0x0]                       
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    mov64 r3, r2                                    r3 = r2
    and64 r3, 255                                   r3 &= 255   ///  r3 = r3.and(255)
    jne r3, r2, lbb_177                             if r3 != r2 { pc += 0 }
lbb_177:
    jne r3, r2, lbb_183                             if r3 != r2 { pc += 5 }
    stxb [r1+0x0], r2                       
    exit                                    
lbb_180:
    lddw r1, 0x10002eb30 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158576
    call function_20990                     
lbb_183:
    lddw r1, 0x10002eb48 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158600
    call function_20946                     
    exit                                    

function_187:
    stdw [r1+0x0], 0                        
    exit                                    

function_189:
    exit                                    

function_190:
    lddw r2, 0xcd7cc2605a5a778b                     r2 load str located at -3639820679733676149
    stxdw [r1+0x8], r2                      
    lddw r2, 0x38559c1419c640e8                     r2 load str located at 4059322249290072296
    stxdw [r1+0x0], r2                      
    exit                                    

function_197:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r8, [r7+0x0]                      
    ldxdw r1, [r7+0x8]                      
    jlt r1, 8, lbb_207                              if r1 < (8 as i32 as i64 as u64) { pc += 5 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r0, [r8+0x0]                      
    add64 r1, -8                                    r1 += -8   ///  r1 = r1.wrapping_add(-8 as i32 as i64 as u64)
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_213                                      if true { pc += 6 }
lbb_207:
    add64 r8, r1                                    r8 += r1   ///  r8 = r8.wrapping_add(r1)
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_213:
    stxdw [r7+0x8], r1                      
    stxdw [r7+0x0], r8                      
    stxdw [r6+0x8], r0                      
    stxdw [r6+0x0], r2                      
    exit                                    

function_218:
    ldxdw r5, [r4+0x8]                      
    jeq r5, 0, lbb_264                              if r5 == (0 as i32 as i64 as u64) { pc += 44 }
    ldxdw r5, [r4+0x10]                     
    jne r5, 0, lbb_226                              if r5 != (0 as i32 as i64 as u64) { pc += 4 }
    jne r3, 0, lbb_304                              if r3 != (0 as i32 as i64 as u64) { pc += 81 }
lbb_223:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r0, r2                                    r0 = r2
    ja lbb_293                                      if true { pc += 67 }
lbb_226:
    lddw r0, 0x300000000                            r0 load str located at 12884901888
    ldxdw r0, [r0+0x0]                      
    lddw r6, 0x300008000                            r6 load str located at 12884934656
    jeq r0, 0, lbb_233                              if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r0                                    r6 = r0
lbb_233:
    mov64 r7, r6                                    r7 = r6
    sub64 r7, r3                                    r7 -= r3   ///  r7 = r7.wrapping_sub(r3)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jgt r7, r6, lbb_239                             if r7 > r6 { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_239:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r8, 0, lbb_242                              if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r7                                    r6 = r7
lbb_242:
    mov64 r7, r2                                    r7 = r2
    neg64 r7                                        r7 = -r7   ///  r7 = (r7 as i64).wrapping_neg() as u64
    and64 r6, r7                                    r6 &= r7   ///  r6 = r6.and(r7)
    lddw r8, 0x300000008                            r8 load str located at 12884901896
    jlt r6, r8, lbb_293                             if r6 < r8 { pc += 45 }
    ldxdw r4, [r4+0x0]                      
    lddw r0, 0x300000000                            r0 load str located at 12884901888
    stxdw [r0+0x0], r6                      
    mov64 r7, r1                                    r7 = r1
    mov64 r1, r6                                    r1 = r6
    mov64 r8, r2                                    r8 = r2
    mov64 r2, r4                                    r2 = r4
    mov64 r9, r3                                    r9 = r3
    mov64 r3, r5                                    r3 = r5
    call function_21513                     
    mov64 r3, r9                                    r3 = r9
    mov64 r2, r8                                    r2 = r8
    mov64 r1, r7                                    r1 = r7
    mov64 r0, r6                                    r0 = r6
    ja lbb_292                                      if true { pc += 28 }
lbb_264:
    jne r3, 0, lbb_266                              if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_223                                      if true { pc += -43 }
lbb_266:
    lddw r4, 0x300000000                            r4 load str located at 12884901888
    ldxdw r5, [r4+0x0]                      
    lddw r4, 0x300008000                            r4 load str located at 12884934656
    jeq r5, 0, lbb_273                              if r5 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r5                                    r4 = r5
lbb_273:
    mov64 r5, r4                                    r5 = r4
    sub64 r5, r3                                    r5 -= r3   ///  r5 = r5.wrapping_sub(r3)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jgt r5, r4, lbb_279                             if r5 > r4 { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_279:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jne r6, 0, lbb_282                              if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r5                                    r4 = r5
lbb_282:
    mov64 r5, r2                                    r5 = r2
    neg64 r5                                        r5 = -r5   ///  r5 = (r5 as i64).wrapping_neg() as u64
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    lddw r5, 0x300000008                            r5 load str located at 12884901896
    jlt r4, r5, lbb_293                             if r4 < r5 { pc += 5 }
lbb_288:
    lddw r5, 0x300000000                            r5 load str located at 12884901888
    stxdw [r5+0x0], r4                      
    mov64 r0, r4                                    r0 = r4
lbb_292:
    mov64 r7, r3                                    r7 = r3
lbb_293:
    jeq r0, 0, lbb_295                              if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r7                                    r3 = r7
lbb_295:
    stxdw [r1+0x10], r3                     
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jeq r0, 0, lbb_299                              if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_299:
    jeq r0, 0, lbb_301                              if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, r0                                    r2 = r0
lbb_301:
    stxdw [r1+0x8], r2                      
    stxdw [r1+0x0], r3                      
    exit                                    
lbb_304:
    lddw r4, 0x300000000                            r4 load str located at 12884901888
    ldxdw r5, [r4+0x0]                      
    lddw r4, 0x300008000                            r4 load str located at 12884934656
    jeq r5, 0, lbb_311                              if r5 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r5                                    r4 = r5
lbb_311:
    mov64 r5, r4                                    r5 = r4
    sub64 r5, r3                                    r5 -= r3   ///  r5 = r5.wrapping_sub(r3)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jgt r5, r4, lbb_317                             if r5 > r4 { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_317:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jne r6, 0, lbb_320                              if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r5                                    r4 = r5
lbb_320:
    mov64 r5, r2                                    r5 = r2
    neg64 r5                                        r5 = -r5   ///  r5 = (r5 as i64).wrapping_neg() as u64
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    lddw r5, 0x300000008                            r5 load str located at 12884901896
    jlt r4, r5, lbb_293                             if r4 < r5 { pc += -33 }
    ja lbb_288                                      if true { pc += -39 }

function_327:
    mov64 r6, r1                                    r6 = r1
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r9, [r6+0x0]                      
    jeq r9, -1, lbb_374                             if r9 == (-1 as i32 as i64 as u64) { pc += 43 }
    mov64 r1, r9                                    r1 = r9
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    mov64 r7, r9                                    r7 = r9
    lsh64 r7, 1                                     r7 <<= 1   ///  r7 = r7.wrapping_shl(1)
    jgt r7, r1, lbb_337                             if r7 > r1 { pc += 1 }
    mov64 r7, r1                                    r7 = r1
lbb_337:
    jgt r7, 4, lbb_339                              if r7 > (4 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 4                                     r7 = 4 as i32 as i64 as u64
lbb_339:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, 24                                    r4 = 24 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x38]                    
    jne r2, 0, lbb_350                              if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_350:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_374                              if r1 != (0 as i32 as i64 as u64) { pc += 22 }
    ldxdw r3, [r10-0x40]                    
    lddw r1, 0x7ffffffffffffff8                     r1 load str located at 9223372036854775800
    jgt r3, r1, lbb_374                             if r3 > r1 { pc += 18 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jeq r9, 0, lbb_363                              if r9 == (0 as i32 as i64 as u64) { pc += 5 }
    ldxdw r1, [r6+0x8]                      
    mul64 r9, 24                                    r9 *= 24   ///  r9 = r9.wrapping_mul(24 as u64)
    stxdw [r10-0x8], r9                     
    stxdw [r10-0x18], r1                    
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
lbb_363:
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -24                                   r4 += -24   ///  r4 = r4.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_218                       
    ldxdw r1, [r10-0x30]                    
    jne r1, 1, lbb_378                              if r1 != (1 as i32 as i64 as u64) { pc += 6 }
    ldxdw r2, [r10-0x20]                    
    ldxdw r8, [r10-0x28]                    
lbb_374:
    mov64 r1, r8                                    r1 = r8
    lddw r3, 0x10002eaf8 --> b"\x00\x00\x00\x00(\xd0\x02\x00\x0d\x00\x00\x00\x00\x00\x00\x00\xa1\x01\x00…        r3 load str located at 4295158520
    call function_18348                     
lbb_378:
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x0], r7                      
    stxdw [r6+0x8], r1                      
    exit                                    

function_382:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r5, [r1+0x0]                      
    jeq r5, -1, lbb_419                             if r5 == (-1 as i32 as i64 as u64) { pc += 34 }
    mov64 r2, r5                                    r2 = r5
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    mov64 r3, r5                                    r3 = r5
    lsh64 r3, 1                                     r3 <<= 1   ///  r3 = r3.wrapping_shl(1)
    jgt r3, r2, lbb_391                             if r3 > r2 { pc += 1 }
    mov64 r3, r2                                    r3 = r2
lbb_391:
    mov64 r6, r3                                    r6 = r3
    jgt r3, 4, lbb_394                              if r3 > (4 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 4                                     r6 = 4 as i32 as i64 as u64
lbb_394:
    lddw r0, 0x7ffffffffffffff                      r0 load str located at 576460752303423487
    jgt r3, r0, lbb_419                             if r3 > r0 { pc += 22 }
    mov64 r3, r6                                    r3 = r6
    lsh64 r3, 5                                     r3 <<= 5   ///  r3 = r3.wrapping_shl(5)
    jslt r3, 0, lbb_419                             if (r3 as i64) < (0 as i32 as i64) { pc += 19 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jeq r5, 0, lbb_407                              if r5 == (0 as i32 as i64 as u64) { pc += 5 }
    ldxdw r2, [r1+0x8]                      
    lsh64 r5, 5                                     r5 <<= 5   ///  r5 = r5.wrapping_shl(5)
    stxdw [r10-0x8], r5                     
    stxdw [r10-0x18], r2                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_407:
    mov64 r7, r1                                    r7 = r1
    stxdw [r10-0x10], r2                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -24                                   r4 += -24   ///  r4 = r4.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_218                       
    ldxdw r1, [r10-0x30]                    
    jne r1, 1, lbb_423                              if r1 != (1 as i32 as i64 as u64) { pc += 6 }
    ldxdw r2, [r10-0x20]                    
    ldxdw r4, [r10-0x28]                    
lbb_419:
    mov64 r1, r4                                    r1 = r4
    lddw r3, 0x10002eaf8 --> b"\x00\x00\x00\x00(\xd0\x02\x00\x0d\x00\x00\x00\x00\x00\x00\x00\xa1\x01\x00…        r3 load str located at 4295158520
    call function_18348                     
lbb_423:
    ldxdw r1, [r10-0x28]                    
    stxdw [r7+0x0], r6                      
    stxdw [r7+0x8], r1                      
    exit                                    

function_427:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r5, [r1+0x0]                      
    jeq r5, -1, lbb_466                             if r5 == (-1 as i32 as i64 as u64) { pc += 36 }
    mov64 r2, r5                                    r2 = r5
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    mov64 r3, r5                                    r3 = r5
    lsh64 r3, 1                                     r3 <<= 1   ///  r3 = r3.wrapping_shl(1)
    jgt r3, r2, lbb_436                             if r3 > r2 { pc += 1 }
    mov64 r3, r2                                    r3 = r2
lbb_436:
    mov64 r6, r3                                    r6 = r3
    jgt r3, 4, lbb_439                              if r3 > (4 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 4                                     r6 = 4 as i32 as i64 as u64
lbb_439:
    lddw r0, 0x7ffffffffffffff                      r0 load str located at 576460752303423487
    jgt r3, r0, lbb_466                             if r3 > r0 { pc += 24 }
    mov64 r3, r6                                    r3 = r6
    lsh64 r3, 5                                     r3 <<= 5   ///  r3 = r3.wrapping_shl(5)
    lddw r0, 0x7ffffffffffffff8                     r0 load str located at 9223372036854775800
    jgt r3, r0, lbb_466                             if r3 > r0 { pc += 19 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jeq r5, 0, lbb_454                              if r5 == (0 as i32 as i64 as u64) { pc += 5 }
    ldxdw r2, [r1+0x8]                      
    lsh64 r5, 5                                     r5 <<= 5   ///  r5 = r5.wrapping_shl(5)
    stxdw [r10-0x8], r5                     
    stxdw [r10-0x18], r2                    
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
lbb_454:
    mov64 r7, r1                                    r7 = r1
    stxdw [r10-0x10], r2                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -24                                   r4 += -24   ///  r4 = r4.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_218                       
    ldxdw r1, [r10-0x30]                    
    jne r1, 1, lbb_470                              if r1 != (1 as i32 as i64 as u64) { pc += 6 }
    ldxdw r2, [r10-0x20]                    
    ldxdw r4, [r10-0x28]                    
lbb_466:
    mov64 r1, r4                                    r1 = r4
    lddw r3, 0x10002eaf8 --> b"\x00\x00\x00\x00(\xd0\x02\x00\x0d\x00\x00\x00\x00\x00\x00\x00\xa1\x01\x00…        r3 load str located at 4295158520
    call function_18348                     
lbb_470:
    ldxdw r1, [r10-0x28]                    
    stxdw [r7+0x0], r6                      
    stxdw [r7+0x8], r1                      
    exit                                    

function_474:
    mov64 r6, r1                                    r6 = r1
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r9, [r6+0x0]                      
    jeq r9, -1, lbb_521                             if r9 == (-1 as i32 as i64 as u64) { pc += 43 }
    mov64 r1, r9                                    r1 = r9
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    mov64 r7, r9                                    r7 = r9
    lsh64 r7, 1                                     r7 <<= 1   ///  r7 = r7.wrapping_shl(1)
    jgt r7, r1, lbb_484                             if r7 > r1 { pc += 1 }
    mov64 r7, r1                                    r7 = r1
lbb_484:
    jgt r7, 4, lbb_486                              if r7 > (4 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 4                                     r7 = 4 as i32 as i64 as u64
lbb_486:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, 80                                    r4 = 80 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x38]                    
    jne r2, 0, lbb_497                              if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_497:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_521                              if r1 != (0 as i32 as i64 as u64) { pc += 22 }
    ldxdw r3, [r10-0x40]                    
    lddw r1, 0x7ffffffffffffff8                     r1 load str located at 9223372036854775800
    jgt r3, r1, lbb_521                             if r3 > r1 { pc += 18 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jeq r9, 0, lbb_510                              if r9 == (0 as i32 as i64 as u64) { pc += 5 }
    ldxdw r1, [r6+0x8]                      
    mul64 r9, 80                                    r9 *= 80   ///  r9 = r9.wrapping_mul(80 as u64)
    stxdw [r10-0x8], r9                     
    stxdw [r10-0x18], r1                    
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
lbb_510:
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -24                                   r4 += -24   ///  r4 = r4.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_218                       
    ldxdw r1, [r10-0x30]                    
    jne r1, 1, lbb_525                              if r1 != (1 as i32 as i64 as u64) { pc += 6 }
    ldxdw r2, [r10-0x20]                    
    ldxdw r8, [r10-0x28]                    
lbb_521:
    mov64 r1, r8                                    r1 = r8
    lddw r3, 0x10002eaf8 --> b"\x00\x00\x00\x00(\xd0\x02\x00\x0d\x00\x00\x00\x00\x00\x00\x00\xa1\x01\x00…        r3 load str located at 4295158520
    call function_18348                     
lbb_525:
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x0], r7                      
    stxdw [r6+0x8], r1                      
    exit                                    

function_529:
    stxdw [r10-0x8], r1                     
    mov64 r4, r10                                   r4 = r10
    add64 r4, -8                                    r4 += -8   ///  r4 = r4.wrapping_add(-8 as i32 as i64 as u64)
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002d18b --> b"TryFromIntError"        r2 load str located at 4295152011
    mov64 r3, 15                                    r3 = 15 as i32 as i64 as u64
    lddw r5, 0x10002eac0 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r5 load str located at 4295158464
    call function_19760                     
    exit                                    

function_540:
    ldxdw r6, [r2+0x0]                      
    ldxdw r9, [r2+0x8]                      
    jlt r9, 4, lbb_579                              if r9 < (4 as i32 as i64 as u64) { pc += 36 }
    add64 r9, -4                                    r9 += -4   ///  r9 = r9.wrapping_add(-4 as i32 as i64 as u64)
    ldxw r5, [r6+0x0]                       
    stxdw [r2+0x8], r9                      
    add64 r6, 4                                     r6 += 4   ///  r6 = r6.wrapping_add(4 as i32 as i64 as u64)
    stxdw [r2+0x0], r6                      
    jeq r5, 0, lbb_825                              if r5 == (0 as i32 as i64 as u64) { pc += 276 }
    stxdw [r10-0xb8], r1                    
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r4, 0x300008000                            r4 load str located at 12884934656
    jeq r1, 0, lbb_557                              if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r1                                    r4 = r1
lbb_557:
    stxdw [r10-0xb0], r2                    
    mov64 r3, r5                                    r3 = r5
    stxdw [r10-0x90], r5                    
    jlt r5, 170, lbb_562                            if r5 < (170 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 170                                   r3 = 170 as i32 as i64 as u64
lbb_562:
    mov64 r2, r3                                    r2 = r3
    mul64 r2, 24                                    r2 *= 24   ///  r2 = r2.wrapping_mul(24 as u64)
    mov64 r5, r4                                    r5 = r4
    sub64 r5, r2                                    r5 -= r2   ///  r5 = r5.wrapping_sub(r2)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jgt r5, r4, lbb_570                             if r5 > r4 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_570:
    jne r0, 0, lbb_572                              if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r5                                    r1 = r5
lbb_572:
    lddw r4, 0x300000007                            r4 load str located at 12884901895
    jgt r1, r4, lbb_592                             if r1 > r4 { pc += 17 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    lddw r3, 0x10002eae0 --> b"\x00\x00\x00\x00(\xd0\x02\x00\x0d\x00\x00\x00\x00\x00\x00\x00\x9f\x01\x00…        r3 load str located at 4295158496
    call function_18348                     
lbb_579:
    add64 r6, r9                                    r6 += r9   ///  r6 = r6.wrapping_add(r9)
    mov64 r7, r1                                    r7 = r1
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    mov64 r8, r2                                    r8 = r2
    call function_17790                     
    stxdw [r8+0x0], r6                      
    stdw [r8+0x8], 0                        
    stxdw [r7+0x8], r0                      
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    stxdw [r7+0x0], r1                      
    ja lbb_890                                      if true { pc += 298 }
lbb_592:
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    lddw r2, 0x300000000                            r2 load str located at 12884901888
    stxdw [r2+0x0], r1                      
    stxdw [r10-0x78], r3                    
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    stxdw [r10-0x70], r1                    
    stdw [r10-0x68], 0                      
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r5, [r10-0xb0]                    
    ldxdw r2, [r10-0x90]                    
    ja lbb_626                                      if true { pc += 22 }
lbb_604:
    mov64 r2, r1                                    r2 = r1
    add64 r2, r8                                    r2 += r8   ///  r2 = r2.wrapping_add(r8)
    stxdw [r2+0x10], r7                     
    ldxdw r3, [r10-0xa0]                    
    stxdw [r2+0x8], r3                      
    ldxdw r3, [r10-0xa8]                    
    stxb [r2+0x2], r3                       
    ldxdw r3, [r10-0x98]                    
    stxb [r2+0x1], r3                       
    ldxdw r3, [r10-0x80]                    
    stxb [r2+0x0], r3                       
    add64 r8, 24                                    r8 += 24   ///  r8 = r8.wrapping_add(24 as i32 as i64 as u64)
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x68], r0                    
    mov64 r2, r0                                    r2 = r0
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mov64 r4, r8                                    r4 = r8
    ldxdw r3, [r10-0x90]                    
    ldxdw r9, [r10-0x88]                    
    jlt r2, r3, lbb_626                             if r2 < r3 { pc += 1 }
    ja lbb_840                                      if true { pc += 214 }
lbb_626:
    jeq r9, 0, lbb_829                              if r9 == (0 as i32 as i64 as u64) { pc += 202 }
    mov64 r2, r9                                    r2 = r9
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r3, [r6+0x0]                       
    stxdw [r5+0x8], r2                      
    mov64 r8, r6                                    r8 = r6
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r5+0x0], r8                      
    stxb [r10-0x5a], r3                     
    jsgt r3, 85, lbb_641                            if (r3 as i64) > (85 as i32 as i64) { pc += 5 }
    jsgt r3, 60, lbb_669                            if (r3 as i64) > (60 as i32 as i64) { pc += 32 }
    jeq r3, 39, lbb_674                             if r3 == (39 as i32 as i64 as u64) { pc += 36 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jeq r3, 51, lbb_781                             if r3 == (51 as i32 as i64 as u64) { pc += 141 }
    ja lbb_716                                      if true { pc += 75 }
lbb_641:
    jsgt r3, 88, lbb_714                            if (r3 as i64) > (88 as i32 as i64) { pc += 72 }
    jeq r3, 86, lbb_688                             if r3 == (86 as i32 as i64 as u64) { pc += 45 }
    jeq r3, 87, lbb_645                             if r3 == (87 as i32 as i64 as u64) { pc += 1 }
    ja lbb_716                                      if true { pc += 71 }
lbb_645:
    jlt r9, 9, lbb_854                              if r9 < (9 as i32 as i64 as u64) { pc += 208 }
    mov64 r2, r9                                    r2 = r9
    add64 r2, -9                                    r2 += -9   ///  r2 = r2.wrapping_add(-9 as i32 as i64 as u64)
    ldxdw r3, [r6+0x1]                      
    stxdw [r5+0x8], r2                      
    mov64 r7, r6                                    r7 = r6
    add64 r7, 9                                     r7 += 9   ///  r7 = r7.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r7                      
    jeq r2, 0, lbb_891                              if r2 == (0 as i32 as i64 as u64) { pc += 237 }
    stxdw [r10-0xa0], r3                    
    add64 r9, -10                                   r9 += -10   ///  r9 = r9.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r2, [r6+0x9]                       
    stxdw [r5+0x8], r9                      
    add64 r6, 10                                    r6 += 10   ///  r6 = r6.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r6                      
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    stxb [r10-0x59], r2                     
    jeq r2, 0, lbb_666                              if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    jeq r2, 1, lbb_665                              if r2 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_767                                      if true { pc += 102 }
lbb_665:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_666:
    stxdw [r10-0x98], r3                    
    mov64 r7, 6                                     r7 = 6 as i32 as i64 as u64
    ja lbb_711                                      if true { pc += 42 }
lbb_669:
    jeq r3, 61, lbb_757                             if r3 == (61 as i32 as i64 as u64) { pc += 87 }
    jeq r3, 67, lbb_672                             if r3 == (67 as i32 as i64 as u64) { pc += 1 }
    ja lbb_716                                      if true { pc += 44 }
lbb_672:
    mov64 r7, 4                                     r7 = 4 as i32 as i64 as u64
    ja lbb_781                                      if true { pc += 107 }
lbb_674:
    jeq r2, 0, lbb_829                              if r2 == (0 as i32 as i64 as u64) { pc += 154 }
    add64 r9, -2                                    r9 += -2   ///  r9 = r9.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r2, [r6+0x1]                       
    stxdw [r5+0x8], r9                      
    add64 r6, 2                                     r6 += 2   ///  r6 = r6.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r5+0x0], r6                      
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    stxb [r10-0x59], r2                     
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jeq r2, 0, lbb_686                              if r2 == (0 as i32 as i64 as u64) { pc += 2 }
    jne r2, 1, lbb_862                              if r2 != (1 as i32 as i64 as u64) { pc += 177 }
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_686:
    stxdw [r10-0x98], r3                    
    ja lbb_779                                      if true { pc += 91 }
lbb_688:
    jeq r2, 0, lbb_848                              if r2 == (0 as i32 as i64 as u64) { pc += 159 }
    mov64 r2, r9                                    r2 = r9
    add64 r2, -2                                    r2 += -2   ///  r2 = r2.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r3, [r6+0x1]                       
    stxdw [r5+0x8], r2                      
    mov64 r7, r6                                    r7 = r6
    add64 r7, 2                                     r7 += 2   ///  r7 = r7.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r5+0x0], r7                      
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    stxb [r10-0x59], r3                     
    jeq r3, 0, lbb_702                              if r3 == (0 as i32 as i64 as u64) { pc += 3 }
    jeq r3, 1, lbb_701                              if r3 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_767                                      if true { pc += 66 }
lbb_701:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
lbb_702:
    jeq r2, 0, lbb_897                              if r2 == (0 as i32 as i64 as u64) { pc += 194 }
    stxdw [r10-0xa8], r8                    
    ldxb r2, [r6+0x2]                       
    stxdw [r10-0x98], r2                    
    add64 r6, 3                                     r6 += 3   ///  r6 = r6.wrapping_add(3 as i32 as i64 as u64)
    add64 r9, -3                                    r9 += -3   ///  r9 = r9.wrapping_add(-3 as i32 as i64 as u64)
    stxdw [r5+0x8], r9                      
    stxdw [r5+0x0], r6                      
    mov64 r7, 5                                     r7 = 5 as i32 as i64 as u64
lbb_711:
    mov64 r8, r6                                    r8 = r6
    mov64 r2, r9                                    r2 = r9
    ja lbb_781                                      if true { pc += 67 }
lbb_714:
    jeq r3, 89, lbb_728                             if r3 == (89 as i32 as i64 as u64) { pc += 13 }
    jeq r3, 95, lbb_742                             if r3 == (95 as i32 as i64 as u64) { pc += 26 }
lbb_716:
    lddw r1, 0x10002f388 --> b"\x00\x00\x00\x00\xba\xd8\x02\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295160712
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100029418 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295136280
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -90                                   r1 += -90   ///  r1 = r1.wrapping_add(-90 as i32 as i64 as u64)
    ja lbb_873                                      if true { pc += 145 }
lbb_728:
    jeq r2, 0, lbb_829                              if r2 == (0 as i32 as i64 as u64) { pc += 100 }
    add64 r9, -2                                    r9 += -2   ///  r9 = r9.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r2, [r6+0x1]                       
    stxdw [r5+0x8], r9                      
    add64 r6, 2                                     r6 += 2   ///  r6 = r6.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r5+0x0], r6                      
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    stxb [r10-0x59], r2                     
    jeq r2, 0, lbb_739                              if r2 == (0 as i32 as i64 as u64) { pc += 2 }
    jne r2, 1, lbb_862                              if r2 != (1 as i32 as i64 as u64) { pc += 124 }
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_739:
    stxdw [r10-0x98], r3                    
    mov64 r7, 7                                     r7 = 7 as i32 as i64 as u64
    ja lbb_779                                      if true { pc += 37 }
lbb_742:
    jeq r2, 0, lbb_848                              if r2 == (0 as i32 as i64 as u64) { pc += 105 }
    add64 r9, -2                                    r9 += -2   ///  r9 = r9.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r2, [r6+0x1]                       
    stxdw [r5+0x8], r9                      
    add64 r6, 2                                     r6 += 2   ///  r6 = r6.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r5+0x0], r6                      
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    stxb [r10-0x59], r2                     
    jeq r2, 0, lbb_754                              if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    jeq r2, 1, lbb_753                              if r2 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_767                                      if true { pc += 14 }
lbb_753:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_754:
    stxdw [r10-0x98], r3                    
    mov64 r7, 3                                     r7 = 3 as i32 as i64 as u64
    ja lbb_779                                      if true { pc += 22 }
lbb_757:
    jeq r2, 0, lbb_848                              if r2 == (0 as i32 as i64 as u64) { pc += 90 }
    add64 r9, -2                                    r9 += -2   ///  r9 = r9.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r2, [r6+0x1]                       
    stxdw [r5+0x8], r9                      
    add64 r6, 2                                     r6 += 2   ///  r6 = r6.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r5+0x0], r6                      
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    stxb [r10-0x59], r2                     
    jeq r2, 0, lbb_777                              if r2 == (0 as i32 as i64 as u64) { pc += 11 }
    jeq r2, 1, lbb_776                              if r2 == (1 as i32 as i64 as u64) { pc += 9 }
lbb_767:
    lddw r1, 0x10002ea38 --> b"\x00\x00\x00\x00\\xd1\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295158328
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100029418 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295136280
    ja lbb_870                                      if true { pc += 94 }
lbb_776:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_777:
    stxdw [r10-0x98], r3                    
    mov64 r7, 2                                     r7 = 2 as i32 as i64 as u64
lbb_779:
    mov64 r8, r6                                    r8 = r6
    mov64 r2, r9                                    r2 = r9
lbb_781:
    jeq r2, 0, lbb_833                              if r2 == (0 as i32 as i64 as u64) { pc += 51 }
    mov64 r9, r0                                    r9 = r0
    mov64 r0, r4                                    r0 = r4
    mov64 r4, r2                                    r4 = r2
    add64 r4, -1                                    r4 += -1   ///  r4 = r4.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r3, [r8+0x0]                       
    stxdw [r5+0x8], r4                      
    mov64 r6, r8                                    r6 = r8
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r5+0x0], r6                      
    jeq r4, 0, lbb_855                              if r4 == (0 as i32 as i64 as u64) { pc += 63 }
    stxdw [r10-0x80], r7                    
    mov64 r4, r2                                    r4 = r2
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r7, [r8+0x1]                       
    stxdw [r5+0x8], r4                      
    mov64 r6, r8                                    r6 = r8
    add64 r6, 2                                     r6 += 2   ///  r6 = r6.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r5+0x0], r6                      
    jeq r4, 0, lbb_855                              if r4 == (0 as i32 as i64 as u64) { pc += 54 }
    ldxb r4, [r8+0x2]                       
    mov64 r6, r8                                    r6 = r8
    add64 r6, 3                                     r6 += 3   ///  r6 = r6.wrapping_add(3 as i32 as i64 as u64)
    add64 r2, -3                                    r2 += -3   ///  r2 = r2.wrapping_add(-3 as i32 as i64 as u64)
    stxdw [r10-0x88], r2                    
    stxdw [r5+0x8], r2                      
    stxdw [r5+0x0], r6                      
    lsh64 r7, 8                                     r7 <<= 8   ///  r7 = r7.wrapping_shl(8)
    or64 r7, r3                                     r7 |= r3   ///  r7 = r7.or(r3)
    lsh64 r4, 16                                    r4 <<= 16   ///  r4 = r4.wrapping_shl(16)
    or64 r7, r4                                     r7 |= r4   ///  r7 = r7.or(r4)
    ldxdw r2, [r10-0x78]                    
    mov64 r8, r0                                    r8 = r0
    mov64 r0, r9                                    r0 = r9
    jeq r0, r2, lbb_817                             if r0 == r2 { pc += 1 }
    ja lbb_604                                      if true { pc += -213 }
lbb_817:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -120                                  r1 += -120   ///  r1 = r1.wrapping_add(-120 as i32 as i64 as u64)
    call function_327                       
    mov64 r0, r9                                    r0 = r9
    ldxdw r1, [r10-0x90]                    
    ldxdw r5, [r10-0xb0]                    
    ldxdw r1, [r10-0x70]                    
    ja lbb_604                                      if true { pc += -221 }
lbb_825:
    stdw [r1+0x10], 0                       
    stdw [r1+0x8], 8                        
    stdw [r1+0x0], 0                        
    ja lbb_890                                      if true { pc += 61 }
lbb_829:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    ja lbb_885                                      if true { pc += 52 }
lbb_833:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    mov64 r6, r5                                    r6 = r5
    call function_17790                     
    stxdw [r6+0x0], r8                      
    stdw [r6+0x8], 0                        
    ja lbb_885                                      if true { pc += 45 }
lbb_840:
    ldxdw r1, [r10-0x68]                    
    ldxdw r2, [r10-0xb8]                    
    stxdw [r2+0x10], r1                     
    ldxdw r1, [r10-0x70]                    
    stxdw [r2+0x8], r1                      
    ldxdw r1, [r10-0x78]                    
    stxdw [r2+0x0], r1                      
    ja lbb_890                                      if true { pc += 42 }
lbb_848:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    mov64 r6, r5                                    r6 = r5
    call function_17790                     
    stxdw [r6+0x0], r8                      
    ja lbb_885                                      if true { pc += 31 }
lbb_854:
    add64 r6, r9                                    r6 += r9   ///  r6 = r6.wrapping_add(r9)
lbb_855:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    mov64 r7, r5                                    r7 = r5
    call function_17790                     
    stxdw [r7+0x0], r6                      
    stdw [r7+0x8], 0                        
    ja lbb_885                                      if true { pc += 23 }
lbb_862:
    lddw r1, 0x10002f578 --> b"\x00\x00\x00\x00\x91\xd9\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295161208
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x1000005f0 --> b"a#4\x00\x00\x00\x00\x00\xbf4\x00\x00\x00\x00\x00\x00W\x04\x00\x00\x10\x00…        r1 load str located at 4294968816
lbb_870:
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
lbb_873:
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_18371                     
    mov64 r1, r7                                    r1 = r7
    call function_35                        
lbb_885:
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    ldxdw r2, [r10-0xb8]                    
    stxdw [r2+0x0], r1                      
    stxdw [r2+0x8], r0                      
lbb_890:
    exit                                    
lbb_891:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    mov64 r6, r5                                    r6 = r5
    call function_17790                     
    stxdw [r6+0x0], r7                      
    ja lbb_885                                      if true { pc += -12 }
lbb_897:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    mov64 r6, r5                                    r6 = r5
    call function_17790                     
    stxdw [r6+0x0], r7                      
    stdw [r6+0x8], 0                        
    ja lbb_885                                      if true { pc += -19 }

function_904:
    mov64 r6, r1                                    r6 = r1
    ldxdw r8, [r2+0x0]                      
    ldxdw r9, [r2+0x8]                      
    jlt r9, 4, lbb_945                              if r9 < (4 as i32 as i64 as u64) { pc += 37 }
    add64 r9, -4                                    r9 += -4   ///  r9 = r9.wrapping_add(-4 as i32 as i64 as u64)
    ldxw r1, [r8+0x0]                       
    stxdw [r2+0x8], r9                      
    add64 r8, 4                                     r8 += 4   ///  r8 = r8.wrapping_add(4 as i32 as i64 as u64)
    stxdw [r2+0x0], r8                      
    stxdw [r10-0x88], r1                    
    jeq r1, 0, lbb_1108                             if r1 == (0 as i32 as i64 as u64) { pc += 193 }
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r4, 0x300008000                            r4 load str located at 12884934656
    jeq r1, 0, lbb_922                              if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r1                                    r4 = r1
lbb_922:
    stxdw [r10-0xb0], r2                    
    ldxdw r1, [r10-0x88]                    
    mov64 r3, r1                                    r3 = r1
    jlt r1, 128, lbb_927                            if r1 < (128 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 128                                   r3 = 128 as i32 as i64 as u64
lbb_927:
    mov64 r2, r3                                    r2 = r3
    lsh64 r2, 5                                     r2 <<= 5   ///  r2 = r2.wrapping_shl(5)
    mov64 r5, r4                                    r5 = r4
    sub64 r5, r2                                    r5 -= r2   ///  r5 = r5.wrapping_sub(r2)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jgt r5, r4, lbb_935                             if r5 > r4 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_935:
    jne r0, 0, lbb_937                              if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r5                                    r1 = r5
lbb_937:
    stxdw [r10-0xb8], r6                    
    lddw r4, 0x300000007                            r4 load str located at 12884901895
    jgt r1, r4, lbb_957                             if r1 > r4 { pc += 16 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    lddw r3, 0x10002eae0 --> b"\x00\x00\x00\x00(\xd0\x02\x00\x0d\x00\x00\x00\x00\x00\x00\x00\x9f\x01\x00…        r3 load str located at 4295158496
    call function_18348                     
lbb_945:
    add64 r8, r9                                    r8 += r9   ///  r8 = r8.wrapping_add(r9)
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    mov64 r7, r2                                    r7 = r2
    call function_17790                     
    stxdw [r7+0x0], r8                      
    stdw [r7+0x8], 0                        
    stxdw [r6+0x8], r0                      
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    stxdw [r6+0x0], r1                      
    ja lbb_1137                                     if true { pc += 180 }
lbb_957:
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    lddw r2, 0x300000000                            r2 load str located at 12884901888
    stxdw [r2+0x0], r1                      
    stxdw [r10-0x78], r3                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    stxdw [r10-0x70], r1                    
    stdw [r10-0x68], 0                      
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r5, [r10-0xb0]                    
    mov64 r0, r9                                    r0 = r9
    ja lbb_990                                      if true { pc += 21 }
lbb_969:
    and64 r3, 255                                   r3 &= 255   ///  r3 = r3.and(255)
    ldxdw r2, [r10-0x98]                    
    lsh64 r2, 8                                     r2 <<= 8   ///  r2 = r2.wrapping_shl(8)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r1                                    r3 = r1
    add64 r3, r6                                    r3 += r6   ///  r3 = r3.wrapping_add(r6)
    ldxdw r4, [r10-0x90]                    
    stxb [r3+0x18], r4                      
    stxdw [r3+0x10], r2                     
    stxdw [r3+0x8], r9                      
    ldxdw r2, [r10-0x80]                    
    stxdw [r3+0x0], r2                      
    add64 r6, 32                                    r6 += 32   ///  r6 = r6.wrapping_add(32 as i32 as i64 as u64)
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x68], r7                    
    mov64 r2, r7                                    r2 = r7
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    ldxdw r3, [r10-0x88]                    
    jlt r2, r3, lbb_990                             if r2 < r3 { pc += 1 }
    ja lbb_1130                                     if true { pc += 140 }
lbb_990:
    jeq r0, 0, lbb_1121                             if r0 == (0 as i32 as i64 as u64) { pc += 130 }
    mov64 r2, r0                                    r2 = r0
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r4, [r8+0x0]                       
    stxdw [r5+0x8], r2                      
    mov64 r3, r8                                    r3 = r8
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r5+0x0], r3                      
    stxb [r10-0x59], r4                     
    jsgt r4, 22, lbb_1018                           if (r4 as i64) > (22 as i32 as i64) { pc += 18 }
    jeq r4, 14, lbb_1052                            if r4 == (14 as i32 as i64 as u64) { pc += 51 }
    jeq r4, 19, lbb_1003                            if r4 == (19 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1138                                     if true { pc += 135 }
lbb_1003:
    jlt r0, 9, lbb_1112                             if r0 < (9 as i32 as i64 as u64) { pc += 108 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 9                                     r9 += 9   ///  r9 = r9.wrapping_add(9 as i32 as i64 as u64)
    jeq r2, 8, lbb_1114                             if r2 == (8 as i32 as i64 as u64) { pc += 107 }
    ldxdw r9, [r8+0x1]                      
    add64 r0, -10                                   r0 += -10   ///  r0 = r0.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r3, [r8+0x9]                       
    stxdw [r5+0x8], r0                      
    add64 r8, 10                                    r8 += 10   ///  r8 = r8.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r8                      
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    stxdw [r10-0x80], r2                    
    ldxdw r2, [r10-0x78]                    
    jeq r7, r2, lbb_1090                            if r7 == r2 { pc += 73 }
    ja lbb_969                                      if true { pc += -49 }
lbb_1018:
    jeq r4, 23, lbb_1067                            if r4 == (23 as i32 as i64 as u64) { pc += 48 }
    jeq r4, 26, lbb_1037                            if r4 == (26 as i32 as i64 as u64) { pc += 17 }
    jeq r4, 29, lbb_1022                            if r4 == (29 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1138                                     if true { pc += 116 }
lbb_1022:
    jlt r0, 9, lbb_1112                             if r0 < (9 as i32 as i64 as u64) { pc += 89 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 9                                     r9 += 9   ///  r9 = r9.wrapping_add(9 as i32 as i64 as u64)
    jeq r2, 8, lbb_1114                             if r2 == (8 as i32 as i64 as u64) { pc += 88 }
    ldxdw r9, [r8+0x1]                      
    add64 r0, -10                                   r0 += -10   ///  r0 = r0.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r3, [r8+0x9]                       
    stxdw [r5+0x8], r0                      
    add64 r8, 10                                    r8 += 10   ///  r8 = r8.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r8                      
    mov64 r2, 4                                     r2 = 4 as i32 as i64 as u64
    stxdw [r10-0x80], r2                    
    ldxdw r2, [r10-0x78]                    
    jeq r7, r2, lbb_1090                            if r7 == r2 { pc += 54 }
    ja lbb_969                                      if true { pc += -68 }
lbb_1037:
    jlt r0, 9, lbb_1112                             if r0 < (9 as i32 as i64 as u64) { pc += 74 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 9                                     r9 += 9   ///  r9 = r9.wrapping_add(9 as i32 as i64 as u64)
    jeq r2, 8, lbb_1114                             if r2 == (8 as i32 as i64 as u64) { pc += 73 }
    ldxdw r9, [r8+0x1]                      
    add64 r0, -10                                   r0 += -10   ///  r0 = r0.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r3, [r8+0x9]                       
    stxdw [r5+0x8], r0                      
    add64 r8, 10                                    r8 += 10   ///  r8 = r8.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r8                      
    mov64 r2, 3                                     r2 = 3 as i32 as i64 as u64
    stxdw [r10-0x80], r2                    
    ldxdw r2, [r10-0x78]                    
    jeq r7, r2, lbb_1090                            if r7 == r2 { pc += 39 }
    ja lbb_969                                      if true { pc += -83 }
lbb_1052:
    jlt r0, 9, lbb_1112                             if r0 < (9 as i32 as i64 as u64) { pc += 59 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 9                                     r9 += 9   ///  r9 = r9.wrapping_add(9 as i32 as i64 as u64)
    jeq r2, 8, lbb_1114                             if r2 == (8 as i32 as i64 as u64) { pc += 58 }
    ldxdw r9, [r8+0x1]                      
    add64 r0, -10                                   r0 += -10   ///  r0 = r0.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r3, [r8+0x9]                       
    stxdw [r5+0x8], r0                      
    add64 r8, 10                                    r8 += 10   ///  r8 = r8.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r8                      
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    stxdw [r10-0x80], r2                    
    ldxdw r2, [r10-0x78]                    
    jeq r7, r2, lbb_1090                            if r7 == r2 { pc += 24 }
    ja lbb_969                                      if true { pc += -98 }
lbb_1067:
    jlt r0, 9, lbb_1112                             if r0 < (9 as i32 as i64 as u64) { pc += 44 }
    mov64 r4, r2                                    r4 = r2
    and64 r4, -8                                    r4 &= -8   ///  r4 = r4.and(-8)
    jeq r4, 8, lbb_1112                             if r4 == (8 as i32 as i64 as u64) { pc += 41 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 17                                    r9 += 17   ///  r9 = r9.wrapping_add(17 as i32 as i64 as u64)
    jeq r2, 16, lbb_1114                            if r2 == (16 as i32 as i64 as u64) { pc += 40 }
    ldxdw r9, [r3+0x0]                      
    ldxdw r3, [r8+0x9]                      
    ldxb r2, [r8+0x11]                      
    stxdw [r10-0x90], r2                    
    add64 r8, 18                                    r8 += 18   ///  r8 = r8.wrapping_add(18 as i32 as i64 as u64)
    add64 r0, -18                                   r0 += -18   ///  r0 = r0.wrapping_add(-18 as i32 as i64 as u64)
    stxdw [r5+0x8], r0                      
    stxdw [r5+0x0], r8                      
    mov64 r2, 2                                     r2 = 2 as i32 as i64 as u64
    stxdw [r10-0x80], r2                    
    mov64 r2, r3                                    r2 = r3
    rsh64 r2, 8                                     r2 >>= 8   ///  r2 = r2.wrapping_shr(8)
    stxdw [r10-0x98], r2                    
    ldxdw r2, [r10-0x78]                    
    jeq r7, r2, lbb_1090                            if r7 == r2 { pc += 1 }
    ja lbb_969                                      if true { pc += -121 }
lbb_1090:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -120                                  r1 += -120   ///  r1 = r1.wrapping_add(-120 as i32 as i64 as u64)
    stxdw [r10-0xa0], r9                    
    stxdw [r10-0xa8], r8                    
    mov64 r8, r6                                    r8 = r6
    mov64 r6, r0                                    r6 = r0
    mov64 r9, r7                                    r9 = r7
    mov64 r7, r3                                    r7 = r3
    call function_427                       
    mov64 r3, r7                                    r3 = r7
    mov64 r7, r9                                    r7 = r9
    mov64 r0, r6                                    r0 = r6
    mov64 r6, r8                                    r6 = r8
    ldxdw r8, [r10-0xa8]                    
    ldxdw r9, [r10-0xa0]                    
    ldxdw r5, [r10-0xb0]                    
    ldxdw r1, [r10-0x70]                    
    ja lbb_969                                      if true { pc += -139 }
lbb_1108:
    stdw [r6+0x10], 0                       
    stdw [r6+0x8], 8                        
    stdw [r6+0x0], 0                        
    ja lbb_1137                                     if true { pc += 25 }
lbb_1112:
    add64 r8, r0                                    r8 += r0   ///  r8 = r8.wrapping_add(r0)
    mov64 r9, r8                                    r9 = r8
lbb_1114:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    mov64 r6, r5                                    r6 = r5
    call function_17790                     
    stxdw [r6+0x0], r9                      
    stdw [r6+0x8], 0                        
    ja lbb_1124                                     if true { pc += 3 }
lbb_1121:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
lbb_1124:
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    ldxdw r2, [r10-0xb8]                    
    stxdw [r2+0x0], r1                      
    stxdw [r2+0x8], r0                      
    ja lbb_1137                                     if true { pc += 7 }
lbb_1130:
    ldxdw r1, [r10-0x68]                    
    ldxdw r2, [r10-0xb8]                    
    stxdw [r2+0x10], r1                     
    ldxdw r1, [r10-0x70]                    
    stxdw [r2+0x8], r1                      
    ldxdw r1, [r10-0x78]                    
    stxdw [r2+0x0], r1                      
lbb_1137:
    exit                                    
lbb_1138:
    lddw r1, 0x10002f378 --> b"\x00\x00\x00\x00\xae\xd8\x02\x00\x0c\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295160696
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100029418 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295136280
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_18371                     
    mov64 r1, r7                                    r1 = r7
    call function_35                        
    ja lbb_1124                                     if true { pc += -38 }

function_1162:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r8, [r7+0x0]                      
    ldxdw r1, [r7+0x8]                      
    jeq r1, 0, lbb_1187                             if r1 == (0 as i32 as i64 as u64) { pc += 20 }
    mov64 r3, r1                                    r3 = r1
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r2, [r8+0x0]                       
    stxdw [r7+0x8], r3                      
    mov64 r3, r8                                    r3 = r8
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r7+0x0], r3                      
    stxb [r10-0x59], r2                     
    jeq r2, 0, lbb_1194                             if r2 == (0 as i32 as i64 as u64) { pc += 18 }
    jeq r2, 1, lbb_1178                             if r2 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1196                                     if true { pc += 18 }
lbb_1178:
    jlt r1, 33, lbb_1180                            if r1 < (33 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1222                                     if true { pc += 42 }
lbb_1180:
    add64 r8, r1                                    r8 += r1   ///  r8 = r8.wrapping_add(r1)
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    stxdw [r7+0x0], r8                      
    stdw [r7+0x8], 0                        
    ja lbb_1191                                     if true { pc += 4 }
lbb_1187:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    stxdw [r7+0x0], r8                      
lbb_1191:
    stxdw [r6+0x8], r0                      
    stb [r6+0x0], 1                         
    ja lbb_1247                                     if true { pc += 53 }
lbb_1194:
    stb [r6+0x1], 0                         
    ja lbb_1245                                     if true { pc += 49 }
lbb_1196:
    lddw r1, 0x10002eb10 --> b"\x00\x00\x00\x00\x9a\xd1\x02\x00\x1f\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295158544
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100029418 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295136280
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 2                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_18371                     
    mov64 r1, r7                                    r1 = r7
    call function_35                        
    stxdw [r6+0x8], r0                      
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ja lbb_1246                                     if true { pc += 24 }
lbb_1222:
    ldxdw r2, [r8+0x7]                      
    ldxb r4, [r8+0xf]                       
    stxb [r10-0x38], r4                     
    ldxh r4, [r3+0x4]                       
    stxh [r6+0x6], r4                       
    ldxw r3, [r3+0x0]                       
    stxw [r6+0x2], r3                       
    stxdw [r10-0x40], r2                    
    add64 r1, -33                                   r1 += -33   ///  r1 = r1.wrapping_add(-33 as i32 as i64 as u64)
    stxdw [r7+0x8], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, 33                                    r1 += 33   ///  r1 = r1.wrapping_add(33 as i32 as i64 as u64)
    stxdw [r7+0x0], r1                      
    ldxdw r1, [r10-0x3f]                    
    ldxb r3, [r8+0x20]                      
    stxb [r6+0x21], r3                      
    ldxdw r3, [r8+0x18]                     
    stxdw [r6+0x19], r3                     
    ldxdw r3, [r8+0x10]                     
    stxdw [r6+0x11], r3                     
    stxdw [r6+0x9], r1                      
    stxb [r6+0x8], r2                       
    stb [r6+0x1], 1                         
lbb_1245:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_1246:
    stxb [r6+0x0], r1                       
lbb_1247:
    exit                                    

function_1248:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r8, [r7+0x0]                      
    ldxdw r9, [r7+0x8]                      
    jeq r9, 0, lbb_1303                             if r9 == (0 as i32 as i64 as u64) { pc += 50 }
    mov64 r2, r9                                    r2 = r9
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r1, [r8+0x0]                       
    stxdw [r7+0x8], r2                      
    mov64 r2, r8                                    r2 = r8
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r7+0x0], r2                      
    stxb [r10-0x59], r1                     
    jeq r1, 0, lbb_1309                             if r1 == (0 as i32 as i64 as u64) { pc += 47 }
    jeq r1, 1, lbb_1264                             if r1 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1313                                     if true { pc += 49 }
lbb_1264:
    stxdw [r10-0x68], r6                    
    jlt r9, 5, lbb_1404                             if r9 < (5 as i32 as i64 as u64) { pc += 138 }
    ldxw r2, [r8+0x1]                       
    add64 r8, 5                                     r8 += 5   ///  r8 = r8.wrapping_add(5 as i32 as i64 as u64)
    add64 r9, -5                                    r9 += -5   ///  r9 = r9.wrapping_add(-5 as i32 as i64 as u64)
    stxdw [r7+0x8], r9                      
    stxdw [r7+0x0], r8                      
    mov64 r0, 8                                     r0 = 8 as i32 as i64 as u64
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jeq r2, 0, lbb_1416                             if r2 == (0 as i32 as i64 as u64) { pc += 141 }
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r4, 0x300008000                            r4 load str located at 12884934656
    jeq r1, 0, lbb_1282                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r1                                    r4 = r1
lbb_1282:
    mov64 r3, r2                                    r3 = r2
    stxdw [r10-0x78], r2                    
    jlt r2, 170, lbb_1286                           if r2 < (170 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 170                                   r3 = 170 as i32 as i64 as u64
lbb_1286:
    mov64 r2, r3                                    r2 = r3
    mul64 r2, 24                                    r2 *= 24   ///  r2 = r2.wrapping_mul(24 as u64)
    mov64 r5, r4                                    r5 = r4
    sub64 r5, r2                                    r5 -= r2   ///  r5 = r5.wrapping_sub(r2)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jgt r5, r4, lbb_1294                            if r5 > r4 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_1294:
    jne r0, 0, lbb_1296                             if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r5                                    r1 = r5
lbb_1296:
    lddw r4, 0x300000007                            r4 load str located at 12884901895
    jgt r1, r4, lbb_1341                            if r1 > r4 { pc += 42 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    lddw r3, 0x10002eae0 --> b"\x00\x00\x00\x00(\xd0\x02\x00\x0d\x00\x00\x00\x00\x00\x00\x00\x9f\x01\x00…        r3 load str located at 4295158496
    call function_18348                     
lbb_1303:
    stxdw [r10-0x68], r6                    
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    stxdw [r7+0x0], r8                      
    ja lbb_1410                                     if true { pc += 101 }
lbb_1309:
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    stxdw [r6+0x0], r1                      
    ja lbb_1415                                     if true { pc += 102 }
lbb_1313:
    lddw r1, 0x10002eb10 --> b"\x00\x00\x00\x00\x9a\xd1\x02\x00\x1f\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295158544
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100029418 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295136280
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 2                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_18371                     
    mov64 r1, r7                                    r1 = r7
    call function_35                        
    stxdw [r6+0x8], r0                      
    lddw r1, 0x8000000000000001                     r1 load str located at -9223372036854775807
    stxdw [r6+0x0], r1                      
    ja lbb_1415                                     if true { pc += 74 }
lbb_1341:
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    lddw r2, 0x300000000                            r2 load str located at 12884901888
    stxdw [r2+0x0], r1                      
    stxdw [r10-0x40], r3                    
    mov64 r4, 16                                    r4 = 16 as i32 as i64 as u64
    stxdw [r10-0x38], r1                    
    stdw [r10-0x30], 0                      
    ldxdw r2, [r10-0x78]                    
    ja lbb_1372                                     if true { pc += 21 }
lbb_1351:
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    stxw [r2+0x0], r3                       
    ldxdw r3, [r10-0x70]                    
    stxw [r2-0x4], r3                       
    stxw [r2-0x8], r0                       
    stxdw [r2-0x10], r5                     
    stxdw [r10-0x30], r6                    
    add64 r4, 24                                    r4 += 24   ///  r4 = r4.wrapping_add(24 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    ldxdw r3, [r10-0x78]                    
    jgt r3, r2, lbb_1372                            if r3 > r2 { pc += 6 }
    ldxdw r0, [r10-0x38]                    
    ldxdw r1, [r10-0x40]                    
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    jeq r1, r2, lbb_1410                            if r1 == r2 { pc += 39 }
    ja lbb_1416                                     if true { pc += 44 }
lbb_1372:
    jlt r9, 8, lbb_1404                             if r9 < (8 as i32 as i64 as u64) { pc += 31 }
    mov64 r2, r9                                    r2 = r9
    and64 r2, -4                                    r2 &= -4   ///  r2 = r2.and(-4)
    jeq r2, 8, lbb_1404                             if r2 == (8 as i32 as i64 as u64) { pc += 28 }
    jeq r2, 12, lbb_1404                            if r2 == (12 as i32 as i64 as u64) { pc += 27 }
    jeq r2, 16, lbb_1404                            if r2 == (16 as i32 as i64 as u64) { pc += 26 }
    ldxdw r5, [r8+0x0]                      
    ldxw r0, [r8+0x8]                       
    ldxw r2, [r8+0xc]                       
    stxdw [r10-0x70], r2                    
    add64 r9, -20                                   r9 += -20   ///  r9 = r9.wrapping_add(-20 as i32 as i64 as u64)
    ldxw r3, [r8+0x10]                      
    stxdw [r7+0x8], r9                      
    add64 r8, 20                                    r8 += 20   ///  r8 = r8.wrapping_add(20 as i32 as i64 as u64)
    stxdw [r7+0x0], r8                      
    ldxdw r2, [r10-0x40]                    
    jeq r6, r2, lbb_1390                            if r6 == r2 { pc += 1 }
    ja lbb_1351                                     if true { pc += -39 }
lbb_1390:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    stxdw [r10-0x80], r4                    
    stxdw [r10-0x88], r5                    
    stxdw [r10-0x90], r0                    
    stxdw [r10-0x98], r3                    
    call function_327                       
    ldxdw r3, [r10-0x98]                    
    ldxdw r0, [r10-0x90]                    
    ldxdw r5, [r10-0x88]                    
    ldxdw r4, [r10-0x80]                    
    ldxdw r1, [r10-0x78]                    
    ldxdw r1, [r10-0x38]                    
    ja lbb_1351                                     if true { pc += -53 }
lbb_1404:
    add64 r8, r9                                    r8 += r9   ///  r8 = r8.wrapping_add(r9)
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    stxdw [r7+0x0], r8                      
    stdw [r7+0x8], 0                        
lbb_1410:
    lddw r1, 0x8000000000000001                     r1 load str located at -9223372036854775807
    ldxdw r2, [r10-0x68]                    
    stxdw [r2+0x0], r1                      
    stxdw [r2+0x8], r0                      
lbb_1415:
    exit                                    
lbb_1416:
    ldxdw r2, [r10-0x68]                    
    stxdw [r2+0x10], r6                     
    stxdw [r2+0x8], r0                      
    stxdw [r2+0x0], r1                      
    ja lbb_1415                                     if true { pc += -6 }

function_1421:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r8, [r7+0x0]                      
    ldxdw r1, [r7+0x8]                      
    jeq r1, 0, lbb_1448                             if r1 == (0 as i32 as i64 as u64) { pc += 22 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r3, [r8+0x0]                       
    stxdw [r7+0x8], r2                      
    mov64 r9, r8                                    r9 = r8
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r7+0x0], r9                      
    stxb [r10-0x79], r3                     
    jeq r3, 0, lbb_1455                             if r3 == (0 as i32 as i64 as u64) { pc += 20 }
    jeq r3, 1, lbb_1437                             if r3 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1458                                     if true { pc += 21 }
lbb_1437:
    jeq r2, 0, lbb_1485                             if r2 == (0 as i32 as i64 as u64) { pc += 47 }
    add64 r1, -2                                    r1 += -2   ///  r1 = r1.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r2, [r8+0x1]                       
    stxdw [r7+0x8], r1                      
    add64 r8, 2                                     r8 += 2   ///  r8 = r8.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r7+0x0], r8                      
    stxb [r10-0x59], r2                     
    jlt r2, 2, lbb_1446                             if r2 < (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1490                                     if true { pc += 44 }
lbb_1446:
    stxb [r6+0x1], r2                       
    ja lbb_1456                                     if true { pc += 8 }
lbb_1448:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    stxdw [r7+0x0], r8                      
lbb_1452:
    stxdw [r6+0x8], r0                      
    stb [r6+0x0], 1                         
    ja lbb_1484                                     if true { pc += 29 }
lbb_1455:
    stb [r6+0x1], 2                         
lbb_1456:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_1483                                     if true { pc += 25 }
lbb_1458:
    lddw r1, 0x10002eb10 --> b"\x00\x00\x00\x00\x9a\xd1\x02\x00\x1f\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295158544
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100029418 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295136280
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -121                                  r1 += -121   ///  r1 = r1.wrapping_add(-121 as i32 as i64 as u64)
    stxdw [r10-0x58], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 2                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -120                                  r7 += -120   ///  r7 = r7.wrapping_add(-120 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_18371                     
    mov64 r1, r7                                    r1 = r7
    call function_35                        
    stxdw [r6+0x8], r0                      
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_1483:
    stxb [r6+0x0], r1                       
lbb_1484:
    exit                                    
lbb_1485:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    stxdw [r7+0x0], r9                      
    ja lbb_1452                                     if true { pc += -38 }
lbb_1490:
    lddw r1, 0x10002ea38 --> b"\x00\x00\x00\x00\\xd1\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295158328
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100029418 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295136280
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_18371                     
    mov64 r1, r7                                    r1 = r7
    call function_35                        
    ja lbb_1452                                     if true { pc += -62 }

function_1514:
    mov64 r6, r2                                    r6 = r2
    mov64 r8, r1                                    r8 = r1
    ldxdw r3, [r6+0x0]                      
    ldxdw r5, [r6+0x8]                      
    jeq r5, 0, lbb_1545                             if r5 == (0 as i32 as i64 as u64) { pc += 26 }
    mov64 r2, r5                                    r2 = r5
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r1, [r3+0x0]                       
    stxdw [r6+0x8], r2                      
    mov64 r2, r3                                    r2 = r3
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r6+0x0], r2                      
    stxb [r10-0x79], r1                     
    jeq r1, 0, lbb_1551                             if r1 == (0 as i32 as i64 as u64) { pc += 23 }
    jeq r1, 1, lbb_1530                             if r1 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1555                                     if true { pc += 25 }
lbb_1530:
    jlt r5, 5, lbb_1583                             if r5 < (5 as i32 as i64 as u64) { pc += 52 }
    ldxw r2, [r3+0x1]                       
    mov64 r1, r3                                    r1 = r3
    add64 r1, 5                                     r1 += 5   ///  r1 = r1.wrapping_add(5 as i32 as i64 as u64)
    stxdw [r6+0x0], r1                      
    add64 r5, -5                                    r5 += -5   ///  r5 = r5.wrapping_add(-5 as i32 as i64 as u64)
    stxdw [r6+0x8], r5                      
    jne r2, 0, lbb_1591                             if r2 != (0 as i32 as i64 as u64) { pc += 53 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_1541:
    stxdw [r8+0x10], r6                     
    stxdw [r8+0x8], r0                      
    stxdw [r8+0x0], r1                      
    ja lbb_1716                                     if true { pc += 171 }
lbb_1545:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    mov64 r7, r3                                    r7 = r3
    call function_17790                     
    stxdw [r6+0x0], r7                      
    ja lbb_1712                                     if true { pc += 161 }
lbb_1551:
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    stxdw [r8+0x0], r1                      
    ja lbb_1716                                     if true { pc += 161 }
lbb_1555:
    lddw r1, 0x10002eb10 --> b"\x00\x00\x00\x00\x9a\xd1\x02\x00\x1f\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295158544
    stxdw [r10-0x60], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x50], r1                    
    lddw r1, 0x100029418 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295136280
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -121                                  r1 += -121   ///  r1 = r1.wrapping_add(-121 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x40], 0                      
    stdw [r10-0x58], 2                      
    stdw [r10-0x48], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -120                                  r7 += -120   ///  r7 = r7.wrapping_add(-120 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -96                                   r2 += -96   ///  r2 = r2.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_18371                     
    mov64 r1, r7                                    r1 = r7
    call function_35                        
    stxdw [r8+0x8], r0                      
    lddw r1, 0x8000000000000001                     r1 load str located at -9223372036854775807
    stxdw [r8+0x0], r1                      
    ja lbb_1716                                     if true { pc += 133 }
lbb_1583:
    add64 r3, r5                                    r3 += r5   ///  r3 = r3.wrapping_add(r5)
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    mov64 r7, r3                                    r7 = r3
    call function_17790                     
    stxdw [r6+0x0], r7                      
    stdw [r6+0x8], 0                        
    ja lbb_1712                                     if true { pc += 121 }
lbb_1591:
    stxdw [r10-0x98], r3                    
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r4, 0x300008000                            r4 load str located at 12884934656
    jeq r1, 0, lbb_1599                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r1                                    r4 = r1
lbb_1599:
    mov64 r7, r5                                    r7 = r5
    mov64 r3, r2                                    r3 = r2
    stxdw [r10-0xa0], r2                    
    jlt r2, 128, lbb_1604                           if r2 < (128 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 128                                   r3 = 128 as i32 as i64 as u64
lbb_1604:
    mov64 r2, r3                                    r2 = r3
    lsh64 r2, 5                                     r2 <<= 5   ///  r2 = r2.wrapping_shl(5)
    mov64 r5, r4                                    r5 = r4
    sub64 r5, r2                                    r5 -= r2   ///  r5 = r5.wrapping_sub(r2)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jgt r5, r4, lbb_1612                            if r5 > r4 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_1612:
    jne r0, 0, lbb_1614                             if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r5                                    r1 = r5
lbb_1614:
    stxdw [r10-0x90], r6                    
    stxdw [r10-0xa8], r8                    
    lddw r4, 0x300000007                            r4 load str located at 12884901895
    jgt r1, r4, lbb_1623                            if r1 > r4 { pc += 4 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    lddw r3, 0x10002eae0 --> b"\x00\x00\x00\x00(\xd0\x02\x00\x0d\x00\x00\x00\x00\x00\x00\x00\x9f\x01\x00…        r3 load str located at 4295158496
    call function_18348                     
lbb_1623:
    lddw r2, 0x300000000                            r2 load str located at 12884901888
    stxdw [r2+0x0], r1                      
    stxdw [r10-0x60], r3                    
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    stxdw [r10-0x58], r1                    
    stdw [r10-0x50], 0                      
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0x98]                    
    mov64 r5, r7                                    r5 = r7
    ldxdw r0, [r10-0xa0]                    
    ja lbb_1664                                     if true { pc += 29 }
lbb_1635:
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, r8                                    r2 += r8   ///  r2 = r2.wrapping_add(r8)
    ldxh r3, [r10-0x4]                      
    stxh [r2+0x4], r3                       
    ldxw r3, [r10-0x8]                      
    stxw [r2+0x0], r3                       
    ldxdw r3, [r10-0x88]                    
    stxdw [r2+0x7], r3                      
    stxb [r2+0x6], r7                       
    ldxdw r3, [r10-0x20]                    
    stxdw [r2+0xf], r3                      
    ldxdw r3, [r10-0x18]                    
    stxdw [r2+0x17], r3                     
    ldxb r3, [r10-0x10]                     
    stxb [r2+0x1f], r3                      
    stxdw [r10-0x50], r6                    
    add64 r8, 32                                    r8 += 32   ///  r8 = r8.wrapping_add(32 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jgt r0, r2, lbb_1664                            if r0 > r2 { pc += 7 }
    ldxdw r0, [r10-0x58]                    
    ldxdw r1, [r10-0x60]                    
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    ldxdw r8, [r10-0xa8]                    
    jeq r1, r2, lbb_1712                            if r1 == r2 { pc += 49 }
    ja lbb_1541                                     if true { pc += -123 }
lbb_1664:
    mov64 r9, r4                                    r9 = r4
    add64 r9, r8                                    r9 += r8   ///  r9 = r9.wrapping_add(r8)
    jlt r5, 32, lbb_1703                            if r5 < (32 as i32 as i64 as u64) { pc += 36 }
    mov64 r2, r9                                    r2 = r9
    add64 r2, 5                                     r2 += 5   ///  r2 = r2.wrapping_add(5 as i32 as i64 as u64)
    ldxdw r7, [r9+0xb]                      
    ldxb r3, [r9+0x13]                      
    stxb [r10-0x28], r3                     
    ldxw r3, [r2+0x0]                       
    stxw [r10-0x8], r3                      
    ldxh r2, [r2+0x4]                       
    stxh [r10-0x4], r2                      
    stxdw [r10-0x30], r7                    
    mov64 r2, r9                                    r2 = r9
    add64 r2, 37                                    r2 += 37   ///  r2 = r2.wrapping_add(37 as i32 as i64 as u64)
    ldxdw r3, [r10-0x90]                    
    stxdw [r3+0x0], r2                      
    add64 r5, -32                                   r5 += -32   ///  r5 = r5.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r3+0x8], r5                      
    ldxdw r2, [r10-0x2f]                    
    stxdw [r10-0x88], r2                    
    ldxdw r2, [r9+0x14]                     
    stxdw [r10-0x20], r2                    
    ldxdw r2, [r9+0x1c]                     
    stxdw [r10-0x18], r2                    
    ldxb r2, [r9+0x24]                      
    stxb [r10-0x10], r2                     
    ldxdw r2, [r10-0x60]                    
    jeq r6, r2, lbb_1694                            if r6 == r2 { pc += 1 }
    ja lbb_1635                                     if true { pc += -59 }
lbb_1694:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r9, r5                                    r9 = r5
    call function_382                       
    ldxdw r0, [r10-0xa0]                    
    mov64 r5, r9                                    r5 = r9
    ldxdw r4, [r10-0x98]                    
    ldxdw r1, [r10-0x58]                    
    ja lbb_1635                                     if true { pc += -68 }
lbb_1703:
    add64 r9, r5                                    r9 += r5   ///  r9 = r9.wrapping_add(r5)
    add64 r9, 5                                     r9 += 5   ///  r9 = r9.wrapping_add(5 as i32 as i64 as u64)
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    ldxdw r1, [r10-0x90]                    
    stxdw [r1+0x0], r9                      
    stdw [r1+0x8], 0                        
    ldxdw r8, [r10-0xa8]                    
lbb_1712:
    lddw r1, 0x8000000000000001                     r1 load str located at -9223372036854775807
    stxdw [r8+0x0], r1                      
    stxdw [r8+0x8], r0                      
lbb_1716:
    exit                                    

function_1717:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r8, [r7+0x0]                      
    ldxdw r1, [r7+0x8]                      
    jeq r1, 0, lbb_1742                             if r1 == (0 as i32 as i64 as u64) { pc += 20 }
    mov64 r3, r1                                    r3 = r1
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r2, [r8+0x0]                       
    stxdw [r7+0x8], r3                      
    mov64 r3, r8                                    r3 = r8
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r7+0x0], r3                      
    stxb [r10-0x59], r2                     
    jeq r2, 0, lbb_1749                             if r2 == (0 as i32 as i64 as u64) { pc += 18 }
    jeq r2, 1, lbb_1733                             if r2 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1751                                     if true { pc += 18 }
lbb_1733:
    jlt r1, 5, lbb_1735                             if r1 < (5 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1777                                     if true { pc += 42 }
lbb_1735:
    add64 r8, r1                                    r8 += r1   ///  r8 = r8.wrapping_add(r1)
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    stxdw [r7+0x0], r8                      
    stdw [r7+0x8], 0                        
    ja lbb_1746                                     if true { pc += 4 }
lbb_1742:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    stxdw [r7+0x0], r8                      
lbb_1746:
    stxdw [r6+0x8], r0                      
    stw [r6+0x0], 1                         
    ja lbb_1786                                     if true { pc += 37 }
lbb_1749:
    stw [r6+0x4], 0                         
    ja lbb_1784                                     if true { pc += 33 }
lbb_1751:
    lddw r1, 0x10002eb10 --> b"\x00\x00\x00\x00\x9a\xd1\x02\x00\x1f\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295158544
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100029418 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295136280
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 2                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_18371                     
    mov64 r1, r7                                    r1 = r7
    call function_35                        
    stxdw [r6+0x8], r0                      
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ja lbb_1785                                     if true { pc += 8 }
lbb_1777:
    add64 r1, -5                                    r1 += -5   ///  r1 = r1.wrapping_add(-5 as i32 as i64 as u64)
    ldxw r2, [r8+0x1]                       
    stxdw [r7+0x8], r1                      
    add64 r8, 5                                     r8 += 5   ///  r8 = r8.wrapping_add(5 as i32 as i64 as u64)
    stxdw [r7+0x0], r8                      
    stxw [r6+0x8], r2                       
    stw [r6+0x4], 1                         
lbb_1784:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_1785:
    stxw [r6+0x0], r1                       
lbb_1786:
    exit                                    

function_1787:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r8, [r7+0x0]                      
    ldxdw r1, [r7+0x8]                      
    jeq r1, 0, lbb_1907                             if r1 == (0 as i32 as i64 as u64) { pc += 115 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r3, [r8+0x0]                       
    stxdw [r7+0x8], r2                      
    mov64 r9, r8                                    r9 = r8
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r7+0x0], r9                      
    stxb [r10-0x59], r3                     
    jeq r3, 0, lbb_1912                             if r3 == (0 as i32 as i64 as u64) { pc += 111 }
    jeq r3, 1, lbb_1803                             if r3 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1914                                     if true { pc += 111 }
lbb_1803:
    jlt r1, 9, lbb_1900                             if r1 < (9 as i32 as i64 as u64) { pc += 96 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 9                                     r9 += 9   ///  r9 = r9.wrapping_add(9 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -9                                    r2 += -9   ///  r2 = r2.wrapping_add(-9 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 91 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 17                                    r9 += 17   ///  r9 = r9.wrapping_add(17 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -17                                   r2 += -17   ///  r2 = r2.wrapping_add(-17 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 86 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 25                                    r9 += 25   ///  r9 = r9.wrapping_add(25 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -25                                   r2 += -25   ///  r2 = r2.wrapping_add(-25 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 81 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 33                                    r9 += 33   ///  r9 = r9.wrapping_add(33 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -33                                   r2 += -33   ///  r2 = r2.wrapping_add(-33 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 76 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 41                                    r9 += 41   ///  r9 = r9.wrapping_add(41 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -41                                   r2 += -41   ///  r2 = r2.wrapping_add(-41 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 71 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 49                                    r9 += 49   ///  r9 = r9.wrapping_add(49 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -49                                   r2 += -49   ///  r2 = r2.wrapping_add(-49 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 66 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 57                                    r9 += 57   ///  r9 = r9.wrapping_add(57 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -57                                   r2 += -57   ///  r2 = r2.wrapping_add(-57 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 61 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 65                                    r9 += 65   ///  r9 = r9.wrapping_add(65 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -65                                   r2 += -65   ///  r2 = r2.wrapping_add(-65 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 56 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 73                                    r9 += 73   ///  r9 = r9.wrapping_add(73 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -73                                   r2 += -73   ///  r2 = r2.wrapping_add(-73 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 51 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 81                                    r9 += 81   ///  r9 = r9.wrapping_add(81 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -81                                   r2 += -81   ///  r2 = r2.wrapping_add(-81 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 46 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 89                                    r9 += 89   ///  r9 = r9.wrapping_add(89 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -89                                   r2 += -89   ///  r2 = r2.wrapping_add(-89 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 41 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 97                                    r9 += 97   ///  r9 = r9.wrapping_add(97 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -97                                   r2 += -97   ///  r2 = r2.wrapping_add(-97 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 36 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 105                                   r9 += 105   ///  r9 = r9.wrapping_add(105 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -105                                  r2 += -105   ///  r2 = r2.wrapping_add(-105 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 31 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 113                                   r9 += 113   ///  r9 = r9.wrapping_add(113 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -113                                  r2 += -113   ///  r2 = r2.wrapping_add(-113 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 26 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 121                                   r9 += 121   ///  r9 = r9.wrapping_add(121 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -121                                  r2 += -121   ///  r2 = r2.wrapping_add(-121 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 21 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 129                                   r9 += 129   ///  r9 = r9.wrapping_add(129 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -129                                  r2 += -129   ///  r2 = r2.wrapping_add(-129 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 16 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 137                                   r9 += 137   ///  r9 = r9.wrapping_add(137 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -137                                  r2 += -137   ///  r2 = r2.wrapping_add(-137 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 11 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 145                                   r9 += 145   ///  r9 = r9.wrapping_add(145 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -145                                  r2 += -145   ///  r2 = r2.wrapping_add(-145 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 6 }
    mov64 r9, r8                                    r9 = r8
    add64 r9, 153                                   r9 += 153   ///  r9 = r9.wrapping_add(153 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -153                                  r2 += -153   ///  r2 = r2.wrapping_add(-153 as i32 as i64 as u64)
    jlt r2, 8, lbb_1900                             if r2 < (8 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1940                                     if true { pc += 40 }
lbb_1900:
    add64 r9, r2                                    r9 += r2   ///  r9 = r9.wrapping_add(r2)
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    stxdw [r7+0x0], r9                      
    stdw [r7+0x8], 0                        
    ja lbb_1937                                     if true { pc += 30 }
lbb_1907:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    stxdw [r7+0x0], r8                      
    ja lbb_1937                                     if true { pc += 25 }
lbb_1912:
    stdw [r6+0x0], 0                        
    ja lbb_1939                                     if true { pc += 25 }
lbb_1914:
    lddw r1, 0x10002eb10 --> b"\x00\x00\x00\x00\x9a\xd1\x02\x00\x1f\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295158544
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100029418 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295136280
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 2                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_18371                     
    mov64 r1, r7                                    r1 = r7
    call function_35                        
lbb_1937:
    stxdw [r6+0x8], r0                      
    stdw [r6+0x0], 2                        
lbb_1939:
    exit                                    
lbb_1940:
    ldxdw r2, [r8+0x1]                      
    stxdw [r10-0x68], r2                    
    ldxdw r2, [r8+0x9]                      
    stxdw [r10-0x70], r2                    
    ldxdw r2, [r8+0x11]                     
    stxdw [r10-0x78], r2                    
    ldxdw r2, [r8+0x19]                     
    stxdw [r10-0x80], r2                    
    ldxdw r2, [r8+0x21]                     
    stxdw [r10-0x88], r2                    
    ldxdw r2, [r8+0x29]                     
    stxdw [r10-0x90], r2                    
    ldxdw r2, [r8+0x31]                     
    stxdw [r10-0x98], r2                    
    ldxdw r2, [r8+0x39]                     
    stxdw [r10-0xa0], r2                    
    ldxdw r2, [r8+0x41]                     
    stxdw [r10-0xa8], r2                    
    ldxdw r2, [r8+0x49]                     
    stxdw [r10-0xb0], r2                    
    ldxdw r2, [r8+0x51]                     
    stxdw [r10-0xb8], r2                    
    ldxdw r2, [r8+0x59]                     
    stxdw [r10-0xc0], r2                    
    ldxdw r2, [r8+0x61]                     
    stxdw [r10-0xc8], r2                    
    ldxdw r2, [r8+0x69]                     
    stxdw [r10-0xd0], r2                    
    ldxdw r9, [r8+0x71]                     
    ldxdw r0, [r8+0x79]                     
    ldxdw r5, [r8+0x81]                     
    ldxdw r4, [r8+0x89]                     
    ldxdw r3, [r8+0x91]                     
    add64 r1, -161                                  r1 += -161   ///  r1 = r1.wrapping_add(-161 as i32 as i64 as u64)
    ldxdw r2, [r8+0x99]                     
    stxdw [r7+0x8], r1                      
    add64 r8, 161                                   r8 += 161   ///  r8 = r8.wrapping_add(161 as i32 as i64 as u64)
    stxdw [r7+0x0], r8                      
    stxdw [r6+0xa0], r2                     
    stxdw [r6+0x98], r3                     
    stxdw [r6+0x90], r4                     
    stxdw [r6+0x88], r5                     
    stxdw [r6+0x80], r0                     
    stxdw [r6+0x78], r9                     
    ldxdw r1, [r10-0xd0]                    
    stxdw [r6+0x70], r1                     
    ldxdw r1, [r10-0xc8]                    
    stxdw [r6+0x68], r1                     
    ldxdw r1, [r10-0xc0]                    
    stxdw [r6+0x60], r1                     
    ldxdw r1, [r10-0xb8]                    
    stxdw [r6+0x58], r1                     
    ldxdw r1, [r10-0xb0]                    
    stxdw [r6+0x50], r1                     
    ldxdw r1, [r10-0xa8]                    
    stxdw [r6+0x48], r1                     
    ldxdw r1, [r10-0xa0]                    
    stxdw [r6+0x40], r1                     
    ldxdw r1, [r10-0x98]                    
    stxdw [r6+0x38], r1                     
    ldxdw r1, [r10-0x90]                    
    stxdw [r6+0x30], r1                     
    ldxdw r1, [r10-0x88]                    
    stxdw [r6+0x28], r1                     
    ldxdw r1, [r10-0x80]                    
    stxdw [r6+0x20], r1                     
    ldxdw r1, [r10-0x78]                    
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x70]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x68]                    
    stxdw [r6+0x8], r1                      
    stdw [r6+0x0], 1                        
    ja lbb_1939                                     if true { pc += -75 }

function_2014:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r8, [r7+0x0]                      
    ldxdw r1, [r7+0x8]                      
    jeq r1, 0, lbb_2039                             if r1 == (0 as i32 as i64 as u64) { pc += 20 }
    mov64 r3, r1                                    r3 = r1
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r2, [r8+0x0]                       
    stxdw [r7+0x8], r3                      
    mov64 r3, r8                                    r3 = r8
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r7+0x0], r3                      
    stxb [r10-0x59], r2                     
    jeq r2, 0, lbb_2044                             if r2 == (0 as i32 as i64 as u64) { pc += 16 }
    jeq r2, 1, lbb_2030                             if r2 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2046                                     if true { pc += 16 }
lbb_2030:
    jlt r1, 9, lbb_2032                             if r1 < (9 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2072                                     if true { pc += 40 }
lbb_2032:
    add64 r8, r1                                    r8 += r1   ///  r8 = r8.wrapping_add(r1)
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    stxdw [r7+0x0], r8                      
    stdw [r7+0x8], 0                        
    ja lbb_2069                                     if true { pc += 30 }
lbb_2039:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    stxdw [r7+0x0], r8                      
    ja lbb_2069                                     if true { pc += 25 }
lbb_2044:
    stdw [r6+0x0], 0                        
    ja lbb_2071                                     if true { pc += 25 }
lbb_2046:
    lddw r1, 0x10002eb10 --> b"\x00\x00\x00\x00\x9a\xd1\x02\x00\x1f\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295158544
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100029418 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295136280
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 2                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_18371                     
    mov64 r1, r7                                    r1 = r7
    call function_35                        
lbb_2069:
    stxdw [r6+0x8], r0                      
    stdw [r6+0x0], 2                        
lbb_2071:
    exit                                    
lbb_2072:
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    ldxdw r2, [r8+0x1]                      
    stxdw [r7+0x8], r1                      
    add64 r8, 9                                     r8 += 9   ///  r8 = r8.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r7+0x0], r8                      
    stxdw [r6+0x8], r2                      
    stdw [r6+0x0], 1                        
    ja lbb_2071                                     if true { pc += -9 }

function_2080:
    jne r2, 1072, lbb_2087                          if r2 != (1072 as i32 as i64 as u64) { pc += 6 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    jeq r2, 0, lbb_2085                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2092                                     if true { pc += 7 }
lbb_2085:
    mov64 r0, r1                                    r0 = r1
    exit                                    
lbb_2087:
    lddw r1, 0x10002d1f5 --> b"from_bytes_mut"        r1 load str located at 4295152117
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_2131                      
lbb_2092:
    lddw r1, 0x10002d1f5 --> b"from_bytes_mut"        r1 load str located at 4295152117
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_2131                      

function_2097:
    jne r2, 7456, lbb_2104                          if r2 != (7456 as i32 as i64 as u64) { pc += 6 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    jeq r2, 0, lbb_2102                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2109                                     if true { pc += 7 }
lbb_2102:
    mov64 r0, r1                                    r0 = r1
    exit                                    
lbb_2104:
    lddw r1, 0x10002d1f5 --> b"from_bytes_mut"        r1 load str located at 4295152117
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_2131                      
lbb_2109:
    lddw r1, 0x10002d1f5 --> b"from_bytes_mut"        r1 load str located at 4295152117
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_2131                      

function_2114:
    jne r2, 296, lbb_2121                           if r2 != (296 as i32 as i64 as u64) { pc += 6 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    jeq r2, 0, lbb_2119                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2126                                     if true { pc += 7 }
lbb_2119:
    mov64 r0, r1                                    r0 = r1
    exit                                    
lbb_2121:
    lddw r1, 0x10002d1f5 --> b"from_bytes_mut"        r1 load str located at 4295152117
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_2131                      
lbb_2126:
    lddw r1, 0x10002d1f5 --> b"from_bytes_mut"        r1 load str located at 4295152117
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_2131                      

function_2131:
    stxdw [r10-0x60], r2                    
    stxdw [r10-0x68], r1                    
    stxb [r10-0x51], r3                     
    lddw r1, 0x10002eb60 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295158624
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    lddw r1, 0x100023740 --> b"{\x1a\xb8\xff\x00\x00\x00\x00\x18\x01\x00\x00\xc8\xd2\x02\x00\x00\x00\x00…        r1 load str located at 4295112512
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -81                                   r1 += -81   ///  r1 = r1.wrapping_add(-81 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x1000005c8 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x12\x08\x00\x00\x00\x00\x00y\x11\x00\x00\x…        r1 load str located at 4294968776
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    lddw r2, 0x10002eb80 --> b"\x00\x00\x00\x00\x04\xd2\x02\x00\x0f\x00\x00\x00\x00\x00\x00\x00!\x00\x00…        r2 load str located at 4295158656
    call function_18698                     

function_2160:
    mov64 r3, r2                                    r3 = r2
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r5, [r3+0x8]                      
    ldxdw r0, [r3+0x0]                      
    mov64 r2, r5                                    r2 = r5
    sub64 r2, r0                                    r2 -= r0   ///  r2 = r2.wrapping_sub(r0)
    lddw r6, 0x7ffffffffffffff8                     r6 load str located at 9223372036854775800
    jgt r2, r6, lbb_2191                            if r2 > r6 { pc += 22 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r6, 8                                     r6 = 8 as i32 as i64 as u64
    jeq r5, r0, lbb_2214                            if r5 == r0 { pc += 42 }
    lddw r4, 0x300000000                            r4 load str located at 12884901888
    ldxdw r4, [r4+0x0]                      
    lddw r7, 0x300008000                            r7 load str located at 12884934656
    jeq r4, 0, lbb_2179                             if r4 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, r4                                    r7 = r4
lbb_2179:
    mov64 r4, r7                                    r4 = r7
    sub64 r4, r2                                    r4 -= r2   ///  r4 = r4.wrapping_sub(r2)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jgt r4, r7, lbb_2185                            if r4 > r7 { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_2185:
    jne r8, 0, lbb_2187                             if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r4                                    r6 = r4
lbb_2187:
    mov64 r4, 8                                     r4 = 8 as i32 as i64 as u64
    lddw r7, 0x300000007                            r7 load str located at 12884901895
    jgt r6, r7, lbb_2195                            if r6 > r7 { pc += 4 }
lbb_2191:
    mov64 r1, r4                                    r1 = r4
    lddw r3, 0x10002f560 --> b"\x00\x00\x00\x00j\xd0\x02\x00b\x00\x00\x00\x00\x00\x00\x00\xb3\x07\x00\x0…        r3 load str located at 4295161184
    call function_18348                     
lbb_2195:
    rsh64 r2, 3                                     r2 >>= 3   ///  r2 = r2.wrapping_shr(3)
    and64 r6, -8                                    r6 &= -8   ///  r6 = r6.and(-8)
    lddw r4, 0x300000000                            r4 load str located at 12884901888
    stxdw [r4+0x0], r6                      
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_2202:
    mov64 r8, r6                                    r8 = r6
    add64 r8, r7                                    r8 += r7   ///  r8 = r8.wrapping_add(r7)
    mov64 r9, r0                                    r9 = r0
    add64 r9, r7                                    r9 += r7   ///  r9 = r9.wrapping_add(r7)
    stxdw [r8+0x0], r9                      
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    mov64 r8, r0                                    r8 = r0
    add64 r8, r7                                    r8 += r7   ///  r8 = r8.wrapping_add(r7)
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    jne r8, r5, lbb_2202                            if r8 != r5 { pc += -10 }
    stxdw [r3+0x0], r8                      
    mov64 r7, r2                                    r7 = r2
lbb_2214:
    stxdw [r1+0x10], r4                     
    stxdw [r1+0x8], r6                      
    stxdw [r1+0x0], r7                      
    exit                                    

function_2218:
    mov64 r7, r3                                    r7 = r3
    mov64 r3, r2                                    r3 = r2
    mov64 r6, r1                                    r6 = r1
    stxdw [r10-0x48], r7                    
    lddw r1, 0x10002d2e8 --> b"VaultInitSettingsprograms/amm/src/dispatcher.rsIns"        r1 load str located at 4295152360
    stxdw [r10-0x58], r1                    
    stdw [r10-0x40], 32                     
    stdw [r10-0x50], 5                      
    stb [r10-0x31], 255                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -48                                   r4 += -48   ///  r4 = r4.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -49                                   r5 += -49   ///  r5 = r5.wrapping_add(-49 as i32 as i64 as u64)
    mov64 r2, 2                                     r2 = 2 as i32 as i64 as u64
    syscall [invalid]                       
    jeq r0, 0, lbb_2249                             if r0 == (0 as i32 as i64 as u64) { pc += 12 }
    lddw r1, 0x10002ec28 --> b"\x00\x00\x00\x00(\xd2\x02\x001\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295158824
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    lddw r2, 0x10002ec38 --> b"\x00\x00\x00\x00Y\xd2\x02\x00\x0d\x00\x00\x00\x00\x00\x00\x00m\x00\x00\x0…        r2 load str located at 4295158840
    call function_18698                     
lbb_2249:
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x20]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x8], r1                      
    ldxdw r1, [r10-0x30]                    
    stxdw [r6+0x0], r1                      
    ldxb r1, [r10-0x31]                     
    stxb [r6+0x20], r1                      
    ldxdw r2, [r7+0x0]                      
    stxdw [r6+0x21], r2                     
    ldxdw r2, [r7+0x8]                      
    stxdw [r6+0x29], r2                     
    ldxdw r2, [r7+0x10]                     
    stxdw [r6+0x31], r2                     
    ldxdw r2, [r7+0x18]                     
    stxdw [r6+0x39], r2                     
    stxb [r6+0x41], r1                      
    exit                                    

function_2269:
    stxdw [r10-0x28], r4                    
    mov64 r8, r3                                    r8 = r3
    mov64 r9, r2                                    r9 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r7, [r9+0x0]                      
    mov64 r1, r7                                    r1 = r7
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r2, r5                                    r2 = r5
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r1, 22                                    r1 = 22 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2316                             if r0 != (0 as i32 as i64 as u64) { pc += 33 }
    lddw r1, 0x800000000                            r1 load str located at 34359738368
    ldxdw r2, [r7+0x50]                     
    jne r2, 1072, lbb_2316                          if r2 != (1072 as i32 as i64 as u64) { pc += 29 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    call function_18010                     
    ldxdw r1, [r10-0x18]                    
    ldxdw r9, [r10-0x20]                    
    jeq r9, 0, lbb_2316                             if r9 == (0 as i32 as i64 as u64) { pc += 22 }
    jne r1, 1072, lbb_2359                          if r1 != (1072 as i32 as i64 as u64) { pc += 64 }
    mov64 r1, r9                                    r1 = r9
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jeq r1, 0, lbb_2299                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2364                                     if true { pc += 65 }
lbb_2299:
    ldxb r3, [r10-0x8]                      
    ldxdw r7, [r10-0x10]                    
    ldxdw r1, [r9+0x0]                      
    jeq r1, 1, lbb_2319                             if r1 == (1 as i32 as i64 as u64) { pc += 16 }
    jgt r3, 7, lbb_2369                             if r3 > (7 as i32 as i64 as u64) { pc += 65 }
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lsh64 r2, r3                                    r2 <<= r3   ///  r2 = r2.wrapping_shl(r3 as u32)
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    ldxb r1, [r7+0x0]                       
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, r1                                    r2 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jne r2, r1, lbb_2312                            if r2 != r1 { pc += 0 }
lbb_2312:
    jne r2, r1, lbb_2372                            if r2 != r1 { pc += 59 }
    stxb [r7+0x0], r1                       
    lddw r1, 0x900000000                            r1 load str located at 38654705664
lbb_2316:
    stxdw [r6+0x8], r1                      
    stdw [r6+0x0], 0                        
lbb_2318:
    exit                                    
lbb_2319:
    stxdw [r10-0x30], r3                    
    mov64 r2, r9                                    r2 = r9
    add64 r2, 40                                    r2 += 40   ///  r2 = r2.wrapping_add(40 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_2345                             if r0 == (0 as i32 as i64 as u64) { pc += 16 }
lbb_2329:
    stxw [r6+0xc], r1                       
    stw [r6+0x8], 0                         
    stdw [r6+0x0], 0                        
    ldxdw r1, [r10-0x30]                    
    jgt r1, 7, lbb_2369                             if r1 > (7 as i32 as i64 as u64) { pc += 35 }
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lsh64 r2, r1                                    r2 <<= r1   ///  r2 = r2.wrapping_shl(r1 as u32)
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    ldxb r1, [r7+0x0]                       
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, r1                                    r2 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jne r2, r1, lbb_2342                            if r2 != r1 { pc += 0 }
lbb_2342:
    jne r2, r1, lbb_2372                            if r2 != r1 { pc += 29 }
    stxb [r7+0x0], r1                       
    ja lbb_2318                                     if true { pc += -27 }
lbb_2345:
    mov64 r2, r9                                    r2 = r9
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r1, [r10-0x28]                    
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2329                             if r0 != (0 as i32 as i64 as u64) { pc += -25 }
    ldxdw r1, [r10-0x30]                    
    stxdw [r6+0x10], r1                     
    stxdw [r6+0x8], r7                      
    stxdw [r6+0x0], r9                      
    ja lbb_2318                                     if true { pc += -41 }
lbb_2359:
    lddw r1, 0x10002d1eb --> b"from_bytes"          r1 load str located at 4295152107
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_2131                      
lbb_2364:
    lddw r1, 0x10002d1eb --> b"from_bytes"          r1 load str located at 4295152107
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_2131                      
lbb_2369:
    lddw r1, 0x10002eb30 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158576
    call function_20990                     
lbb_2372:
    lddw r1, 0x10002eb48 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158600
    call function_20946                     

function_2375:
    mov64 r7, r4                                    r7 = r4
    mov64 r8, r3                                    r8 = r3
    mov64 r6, r1                                    r6 = r1
    ldxdw r9, [r2+0x0]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r2, r5                                    r2 = r5
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r1, 22                                    r1 = 22 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2419                             if r0 != (0 as i32 as i64 as u64) { pc += 31 }
    lddw r1, 0x800000000                            r1 load str located at 34359738368
    ldxdw r2, [r9+0x50]                     
    jne r2, 1072, lbb_2419                          if r2 != (1072 as i32 as i64 as u64) { pc += 27 }
    lddw r1, 0x200000000                            r1 load str located at 8589934592
    ldxb r2, [r9+0x2]                       
    jeq r2, 0, lbb_2419                             if r2 == (0 as i32 as i64 as u64) { pc += 23 }
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    ldxb r3, [r9+0x0]                       
    mov64 r2, r3                                    r2 = r3
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    jne r2, 15, lbb_2419                            if r2 != (15 as i32 as i64 as u64) { pc += 18 }
    mov64 r1, r3                                    r1 = r3
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r9+0x0], r1                       
    mov64 r2, r9                                    r2 = r9
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r2                                    r1 = r2
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jeq r1, 0, lbb_2414                             if r1 == (0 as i32 as i64 as u64) { pc += 5 }
    lddw r1, 0x10002d1f5 --> b"from_bytes_mut"        r1 load str located at 4295152117
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_2131                      
lbb_2414:
    ldxdw r1, [r2+0x0]                      
    jeq r1, 1, lbb_2422                             if r1 == (1 as i32 as i64 as u64) { pc += 6 }
    stxb [r9+0x0], r3                       
    lddw r1, 0x900000000                            r1 load str located at 38654705664
lbb_2419:
    stxdw [r6+0x8], r1                      
    stdw [r6+0x0], 0                        
lbb_2421:
    exit                                    
lbb_2422:
    stxdw [r10-0x10], r2                    
    stxdw [r10-0x8], r3                     
    mov64 r2, r9                                    r2 = r9
    add64 r2, 128                                   r2 += 128   ///  r2 = r2.wrapping_add(128 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_2439                             if r0 == (0 as i32 as i64 as u64) { pc += 6 }
lbb_2433:
    stxw [r6+0xc], r1                       
    stw [r6+0x8], 0                         
    stdw [r6+0x0], 0                        
    ldxdw r1, [r10-0x8]                     
    stxb [r9+0x0], r1                       
    ja lbb_2421                                     if true { pc += -18 }
lbb_2439:
    mov64 r2, r9                                    r2 = r9
    add64 r2, 912                                   r2 += 912   ///  r2 = r2.wrapping_add(912 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2433                             if r0 != (0 as i32 as i64 as u64) { pc += -15 }
    stxdw [r6+0x8], r9                      
    ldxdw r1, [r10-0x10]                    
    stxdw [r6+0x0], r1                      
    stdw [r6+0x10], 8                       
    ja lbb_2421                                     if true { pc += -32 }

function_2453:
    mov64 r8, r3                                    r8 = r3
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    lddw r1, 0x10002d2ed --> b"InitSettingsprograms/amm/src/dispatcher.rsInstruct"        r1 load str located at 4295152365
    stxdw [r10-0x48], r1                    
    stdw [r10-0x40], 12                     
    stb [r10-0x31], 255                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -48                                   r4 += -48   ///  r4 = r4.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -49                                   r5 += -49   ///  r5 = r5.wrapping_add(-49 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    syscall [invalid]                       
    jeq r0, 0, lbb_2482                             if r0 == (0 as i32 as i64 as u64) { pc += 12 }
    lddw r1, 0x10002ec28 --> b"\x00\x00\x00\x00(\xd2\x02\x001\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295158824
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    lddw r2, 0x10002ec38 --> b"\x00\x00\x00\x00Y\xd2\x02\x00\x0d\x00\x00\x00\x00\x00\x00\x00m\x00\x00\x0…        r2 load str located at 4295158840
    call function_18698                     
lbb_2482:
    ldxdw r1, [r10-0x18]                    
    stxdw [r10-0x50], r1                    
    ldxdw r1, [r10-0x20]                    
    stxdw [r10-0x58], r1                    
    ldxdw r1, [r10-0x28]                    
    stxdw [r10-0x60], r1                    
    ldxdw r1, [r10-0x30]                    
    stxdw [r10-0x68], r1                    
    ldxdw r9, [r7+0x0]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -104                                  r2 += -104   ///  r2 = r2.wrapping_add(-104 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2532                             if r0 != (0 as i32 as i64 as u64) { pc += 32 }
    mov64 r1, r9                                    r1 = r9
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2535                             if r0 != (0 as i32 as i64 as u64) { pc += 27 }
    ldxdw r1, [r9+0x50]                     
    jeq r1, 296, lbb_2511                           if r1 == (296 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2538                                     if true { pc += 27 }
lbb_2511:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_18010                     
    ldxdw r2, [r10-0x30]                    
    jeq r2, 0, lbb_2542                             if r2 == (0 as i32 as i64 as u64) { pc += 25 }
    ldxdw r1, [r10-0x28]                    
    jne r1, 296, lbb_2562                           if r1 != (296 as i32 as i64 as u64) { pc += 43 }
    mov64 r1, r2                                    r1 = r2
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jeq r1, 0, lbb_2523                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2567                                     if true { pc += 44 }
lbb_2523:
    ldxb r3, [r10-0x18]                     
    ldxdw r1, [r10-0x20]                    
    ldxdw r4, [r2+0x0]                      
    jeq r4, 3, lbb_2528                             if r4 == (3 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2545                                     if true { pc += 17 }
lbb_2528:
    stxb [r6+0x10], r3                      
    stxdw [r6+0x8], r1                      
    stxdw [r6+0x0], r2                      
    ja lbb_2537                                     if true { pc += 5 }
lbb_2532:
    stdw [r6+0x8], 0                        
lbb_2533:
    stdw [r6+0x0], 0                        
    ja lbb_2537                                     if true { pc += 2 }
lbb_2535:
    stdw [r6+0x0], 0                        
    stw [r6+0x8], 22                        
lbb_2537:
    exit                                    
lbb_2538:
    lddw r1, 0x800000000                            r1 load str located at 34359738368
    stxdw [r6+0x8], r1                      
    ja lbb_2533                                     if true { pc += -9 }
lbb_2542:
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x8], r1                      
    ja lbb_2533                                     if true { pc += -12 }
lbb_2545:
    lddw r2, 0x900000000                            r2 load str located at 38654705664
    stxdw [r6+0x8], r2                      
    stdw [r6+0x0], 0                        
    mov64 r2, r3                                    r2 = r3
    jgt r2, 7, lbb_2572                             if r2 > (7 as i32 as i64 as u64) { pc += 21 }
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    lsh64 r3, r2                                    r3 <<= r2   ///  r3 = r3.wrapping_shl(r2 as u32)
    and64 r3, 255                                   r3 &= 255   ///  r3 = r3.and(255)
    ldxb r2, [r1+0x0]                       
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    mov64 r3, r2                                    r3 = r2
    and64 r3, 255                                   r3 &= 255   ///  r3 = r3.and(255)
    jne r3, r2, lbb_2559                            if r3 != r2 { pc += 0 }
lbb_2559:
    jne r3, r2, lbb_2575                            if r3 != r2 { pc += 15 }
    stxb [r1+0x0], r2                       
    ja lbb_2537                                     if true { pc += -25 }
lbb_2562:
    lddw r1, 0x10002d1eb --> b"from_bytes"          r1 load str located at 4295152107
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_2131                      
lbb_2567:
    lddw r1, 0x10002d1eb --> b"from_bytes"          r1 load str located at 4295152107
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_2131                      
lbb_2572:
    lddw r1, 0x10002eb30 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158576
    call function_20990                     
lbb_2575:
    lddw r1, 0x10002eb48 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158600
    call function_20946                     

function_2578:
    mov64 r7, r3                                    r7 = r3
    mov64 r8, r2                                    r8 = r2
    mov64 r6, r1                                    r6 = r1
    lddw r1, 0x10002d2ed --> b"InitSettingsprograms/amm/src/dispatcher.rsInstruct"        r1 load str located at 4295152365
    stxdw [r10-0x48], r1                    
    stdw [r10-0x40], 12                     
    stb [r10-0x31], 255                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -48                                   r4 += -48   ///  r4 = r4.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -49                                   r5 += -49   ///  r5 = r5.wrapping_add(-49 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    syscall [invalid]                       
    jeq r0, 0, lbb_2607                             if r0 == (0 as i32 as i64 as u64) { pc += 12 }
    lddw r1, 0x10002ec28 --> b"\x00\x00\x00\x00(\xd2\x02\x001\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295158824
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    lddw r2, 0x10002ec38 --> b"\x00\x00\x00\x00Y\xd2\x02\x00\x0d\x00\x00\x00\x00\x00\x00\x00m\x00\x00\x0…        r2 load str located at 4295158840
    call function_18698                     
lbb_2607:
    ldxdw r1, [r10-0x18]                    
    stxdw [r10-0x50], r1                    
    ldxdw r1, [r10-0x20]                    
    stxdw [r10-0x58], r1                    
    ldxdw r1, [r10-0x28]                    
    stxdw [r10-0x60], r1                    
    ldxdw r1, [r10-0x30]                    
    stxdw [r10-0x68], r1                    
    ldxdw r8, [r8+0x0]                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -104                                  r2 += -104   ///  r2 = r2.wrapping_add(-104 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2642                             if r0 != (0 as i32 as i64 as u64) { pc += 17 }
    mov64 r1, r8                                    r1 = r8
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2645                             if r0 != (0 as i32 as i64 as u64) { pc += 12 }
    ldxdw r1, [r8+0x50]                     
    jeq r1, 296, lbb_2636                           if r1 == (296 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2648                                     if true { pc += 12 }
lbb_2636:
    ldxb r1, [r8+0x2]                       
    jne r1, 0, lbb_2652                             if r1 != (0 as i32 as i64 as u64) { pc += 14 }
    lddw r1, 0x200000000                            r1 load str located at 8589934592
    stxdw [r6+0x8], r1                      
    ja lbb_2643                                     if true { pc += 1 }
lbb_2642:
    stdw [r6+0x8], 0                        
lbb_2643:
    stdw [r6+0x0], 0                        
    ja lbb_2647                                     if true { pc += 2 }
lbb_2645:
    stdw [r6+0x0], 0                        
    stw [r6+0x8], 22                        
lbb_2647:
    exit                                    
lbb_2648:
    lddw r1, 0x800000000                            r1 load str located at 34359738368
    stxdw [r6+0x8], r1                      
    ja lbb_2643                                     if true { pc += -9 }
lbb_2652:
    ldxb r1, [r8+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    jne r2, 15, lbb_2669                            if r2 != (15 as i32 as i64 as u64) { pc += 13 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 247                                   r2 &= 247   ///  r2 = r2.and(247)
    stxb [r8+0x0], r2                       
    mov64 r2, r8                                    r2 = r8
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    and64 r3, 7                                     r3 &= 7   ///  r3 = r3.and(7)
    jeq r3, 0, lbb_2672                             if r3 == (0 as i32 as i64 as u64) { pc += 8 }
    lddw r1, 0x10002d1f5 --> b"from_bytes_mut"        r1 load str located at 4295152117
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_2131                      
lbb_2669:
    stdw [r6+0x0], 0                        
    stdw [r6+0x8], 11                       
    ja lbb_2647                                     if true { pc += -25 }
lbb_2672:
    ldxdw r3, [r2+0x0]                      
    jeq r3, 3, lbb_2675                             if r3 == (3 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2679                                     if true { pc += 4 }
lbb_2675:
    stxdw [r6+0x8], r8                      
    stxdw [r6+0x0], r2                      
    stb [r6+0x10], 8                        
    ja lbb_2647                                     if true { pc += -32 }
lbb_2679:
    lddw r2, 0x900000000                            r2 load str located at 38654705664
    stxdw [r6+0x8], r2                      
    stdw [r6+0x0], 0                        
    stxb [r8+0x0], r1                       
    ja lbb_2647                                     if true { pc += -38 }

function_2685:
    mov64 r7, r5                                    r7 = r5
    stxdw [r10-0x10], r4                    
    mov64 r8, r3                                    r8 = r3
    mov64 r9, r2                                    r9 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r9                                    r1 = r9
    lddw r2, 0x10002cf88 --> b"\x08A\xc4\xc6\xee7o\x17\xcaK\x03\x1a\x94\xf8\xb8*j\xb2\xae\x91\xae\x19\xe…        r2 load str located at 4295151496
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r3, 6                                     r3 = 6 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2702                             if r0 != (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r1, [r7-0xff8]                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_2705                             if r1 != (0 as i32 as i64 as u64) { pc += 3 }
lbb_2702:
    stxw [r6+0x4], r5                       
    stxw [r6+0x0], r3                       
    exit                                    
lbb_2705:
    ldxdw r2, [r7-0x1000]                   
    ldxb r4, [r2+0x0]                       
    jsgt r4, 7, lbb_2718                            if (r4 as i64) > (7 as i32 as i64) { pc += 10 }
    jsgt r4, 3, lbb_2726                            if (r4 as i64) > (3 as i32 as i64) { pc += 17 }
    jeq r4, 1, lbb_2764                             if r4 == (1 as i32 as i64 as u64) { pc += 54 }
    lddw r0, 0x100017d18 --> b"\xbfW\x00\x00\x00\x00\x00\x00\xbfI\x00\x00\x00\x00\x00\x00{: \xfe\x00\x00…        r0 load str located at 4295064856
    jeq r4, 2, lbb_2772                             if r4 == (2 as i32 as i64 as u64) { pc += 59 }
    jeq r4, 3, lbb_2715                             if r4 == (3 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2702                                     if true { pc += -13 }
lbb_2715:
    lddw r0, 0x100018e40 --> b"\xbfW\x00\x00\x00\x00\x00\x00\xbfH\x00\x00\x00\x00\x00\x00\xbf9\x00\x00\x…        r0 load str located at 4295069248
    ja lbb_2772                                     if true { pc += 54 }
lbb_2718:
    jsgt r4, 10, lbb_2733                           if (r4 as i64) > (10 as i32 as i64) { pc += 14 }
    jeq r4, 8, lbb_2767                             if r4 == (8 as i32 as i64 as u64) { pc += 47 }
    jeq r4, 9, lbb_2770                             if r4 == (9 as i32 as i64 as u64) { pc += 49 }
    jeq r4, 10, lbb_2723                            if r4 == (10 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2702                                     if true { pc += -21 }
lbb_2723:
    lddw r0, 0x10001b2d8 --> b"\xbf\x18\x00\x00\x00\x00\x00\x00yQ\x08\xf0\x00\x00\x00\x00\x15\x01\xda\x0…        r0 load str located at 4295078616
    ja lbb_2772                                     if true { pc += 46 }
lbb_2726:
    jsgt r4, 5, lbb_2740                            if (r4 as i64) > (5 as i32 as i64) { pc += 13 }
    jeq r4, 4, lbb_2752                             if r4 == (4 as i32 as i64 as u64) { pc += 24 }
    jeq r4, 5, lbb_2730                             if r4 == (5 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2702                                     if true { pc += -28 }
lbb_2730:
    lddw r0, 0x10001a558 --> b"\xbfI\x00\x00\x00\x00\x00\x00\xbf(\x00\x00\x00\x00\x00\x00\xbf\x16\x00\x0…        r0 load str located at 4295075160
    ja lbb_2772                                     if true { pc += 39 }
lbb_2733:
    jsgt r4, 12, lbb_2746                           if (r4 as i64) > (12 as i32 as i64) { pc += 12 }
    jeq r4, 11, lbb_2755                            if r4 == (11 as i32 as i64 as u64) { pc += 20 }
    jeq r4, 12, lbb_2737                            if r4 == (12 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2702                                     if true { pc += -35 }
lbb_2737:
    lddw r0, 0x10001c028 --> b"\xbf\x16\x00\x00\x00\x00\x00\x00yQ\x08\xf0\x00\x00\x00\x00\x15\x01\x14\x0…        r0 load str located at 4295082024
    ja lbb_2772                                     if true { pc += 32 }
lbb_2740:
    jeq r4, 6, lbb_2758                             if r4 == (6 as i32 as i64 as u64) { pc += 17 }
    jeq r4, 7, lbb_2743                             if r4 == (7 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2702                                     if true { pc += -41 }
lbb_2743:
    lddw r0, 0x10001a940 --> b"\xbfW\x00\x00\x00\x00\x00\x00\xbfH\x00\x00\x00\x00\x00\x00\xbf9\x00\x00\x…        r0 load str located at 4295076160
    ja lbb_2772                                     if true { pc += 26 }
lbb_2746:
    jeq r4, 13, lbb_2761                            if r4 == (13 as i32 as i64 as u64) { pc += 14 }
    jeq r4, 14, lbb_2749                            if r4 == (14 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2702                                     if true { pc += -47 }
lbb_2749:
    lddw r0, 0x100005ab8 --> b"\xbf@\x00\x00\x00\x00\x00\x00\xb7\x06\x00\x00\x01\x00\x00\x00\xb7\x07\x00…        r0 load str located at 4294990520
    ja lbb_2772                                     if true { pc += 20 }
lbb_2752:
    lddw r0, 0x100019040 --> b"\xbfH\x00\x00\x00\x00\x00\x00\xbf9\x00\x00\x00\x00\x00\x00\xbf\x16\x00\x0…        r0 load str located at 4295069760
    ja lbb_2772                                     if true { pc += 17 }
lbb_2755:
    lddw r0, 0x10001b9e8 --> b"\xbf\x17\x00\x00\x00\x00\x00\x00yV\x08\xf0\x00\x00\x00\x00\x15\x06N\x00\x…        r0 load str located at 4295080424
    ja lbb_2772                                     if true { pc += 14 }
lbb_2758:
    lddw r0, 0x100005820 --> b"\xbfW\x00\x00\x00\x00\x00\x00\xbfH\x00\x00\x00\x00\x00\x00\xbf9\x00\x00\x…        r0 load str located at 4294989856
    ja lbb_2772                                     if true { pc += 11 }
lbb_2761:
    lddw r0, 0x10001c388 --> b"\xbfW\x00\x00\x00\x00\x00\x00{J \xff\x00\x00\x00\x00\xbf9\x00\x00\x00\x00…        r0 load str located at 4295082888
    ja lbb_2772                                     if true { pc += 8 }
lbb_2764:
    lddw r0, 0x100017a80 --> b"\xbfW\x00\x00\x00\x00\x00\x00\xbfH\x00\x00\x00\x00\x00\x00\xbf9\x00\x00\x…        r0 load str located at 4295064192
    ja lbb_2772                                     if true { pc += 5 }
lbb_2767:
    lddw r0, 0x10001ab88 --> b"\xbfW\x00\x00\x00\x00\x00\x00\xbfH\x00\x00\x00\x00\x00\x00\xbf9\x00\x00\x…        r0 load str located at 4295076744
    ja lbb_2772                                     if true { pc += 2 }
lbb_2770:
    lddw r0, 0x10001add0 --> b"\xbfW\x00\x00\x00\x00\x00\x00\xbfI\x00\x00\x00\x00\x00\x00\xbf8\x00\x00\x…        r0 load str located at 4295077328
lbb_2772:
    stxdw [r10-0xff8], r1                   
    stxdw [r10-0x1000], r2                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -8                                    r1 += -8   ///  r1 = r1.wrapping_add(-8 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r9                                    r2 = r9
    mov64 r3, r8                                    r3 = r8
    ldxdw r4, [r10-0x10]                    
    callx r0                                
    ldxw r5, [r10-0x4]                      
    ldxw r3, [r10-0x8]                      
    ja lbb_2702                                     if true { pc += -82 }

function_2784:
    mov64 r7, r5                                    r7 = r5
    mov64 r8, r4                                    r8 = r4
    mov64 r9, r3                                    r9 = r3
    stxdw [r10-0xf8], r2                    
    stxdw [r10-0xf0], r1                    
    lddw r1, 0x10002d317 --> b"Instruction: swap"        r1 load str located at 4295152407
    mov64 r2, 17                                    r2 = 17 as i32 as i64 as u64
    syscall [invalid]                       
    ldxdw r6, [r7-0xff8]                    
    jeq r6, 0, lbb_2862                             if r6 == (0 as i32 as i64 as u64) { pc += 67 }
    jlt r6, 9, lbb_2801                             if r6 < (9 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, r6                                    r1 = r6
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    jeq r1, 8, lbb_2801                             if r1 == (8 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2815                                     if true { pc += 14 }
lbb_2801:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, r0                                    r1 = r0
    and64 r1, 3                                     r1 &= 3   ///  r1 = r1.and(3)
    jne r1, 1, lbb_2858                             if r1 != (1 as i32 as i64 as u64) { pc += 49 }
    ldxdw r1, [r0+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_2858                             if r2 == (0 as i32 as i64 as u64) { pc += 46 }
    ldxdw r1, [r0-0x1]                      
    callx r2                                
    ja lbb_2858                                     if true { pc += 43 }
lbb_2815:
    ldxdw r2, [r7-0x1000]                   
    ldxdw r1, [r2+0x1]                      
    stxdw [r10-0x100], r2                   
    ldxdw r2, [r2+0x9]                      
    stxdw [r10-0xd8], r2                    
    stxdw [r10-0xe0], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, r8                                    r3 = r8
    call function_14928                     
    ldxw r8, [r10-0x5c]                     
    ldxw r7, [r10-0x60]                     
    ldxdw r9, [r10-0x68]                    
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    jeq r9, r1, lbb_2858                            if r9 == r1 { pc += 26 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -192                                  r1 += -192   ///  r1 = r1.wrapping_add(-192 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -88                                   r2 += -88   ///  r2 = r2.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r3, 88                                    r3 = 88 as i32 as i64 as u64
    call function_21513                     
    stxw [r10-0xc4], r8                     
    stxw [r10-0xc8], r7                     
    stxdw [r10-0xd0], r9                    
    add64 r6, -17                                   r6 += -17   ///  r6 = r6.wrapping_add(-17 as i32 as i64 as u64)
    stxdw [r10-0xff8], r6                   
    ldxdw r1, [r10-0x100]                   
    add64 r1, 17                                    r1 += 17   ///  r1 = r1.wrapping_add(17 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    stdw [r10-0xff0], 0                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -232                                  r1 += -232   ///  r1 = r1.wrapping_add(-232 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -208                                  r3 += -208   ///  r3 = r3.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -224                                  r4 += -224   ///  r4 = r4.wrapping_add(-224 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0xf8]                    
    call function_7831                      
    ldxw r8, [r10-0xe4]                     
    ldxw r7, [r10-0xe8]                     
lbb_2858:
    ldxdw r1, [r10-0xf0]                    
    stxw [r1+0x4], r8                       
    stxw [r1+0x0], r7                       
    exit                                    
lbb_2862:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    lddw r3, 0x10002ec98 --> b"\x00\x00\x00\x00\xf9\xd2\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00{\x00\x00…        r3 load str located at 4295158936
    call function_20246                     
    mov64 r0, r4                                    r0 = r4
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r4, [r5-0xff8]                    
    jne r4, 104, lbb_2925                           if r4 != (104 as i32 as i64 as u64) { pc += 53 }
    ldxdw r4, [r5-0x1000]                   
    mov64 r5, r4                                    r5 = r4
    and64 r5, 7                                     r5 &= 7   ///  r5 = r5.and(7)
    jne r5, 0, lbb_2925                             if r5 != (0 as i32 as i64 as u64) { pc += 49 }
    mov64 r6, 10                                    r6 = 10 as i32 as i64 as u64
    jeq r0, 0, lbb_2923                             if r0 == (0 as i32 as i64 as u64) { pc += 45 }
    mov64 r6, 7                                     r6 = 7 as i32 as i64 as u64
    ldxdw r5, [r3+0x0]                      
    ldxb r5, [r5+0x1]                       
    jeq r5, 0, lbb_2923                             if r5 == (0 as i32 as i64 as u64) { pc += 41 }
    mov64 r6, 10                                    r6 = 10 as i32 as i64 as u64
    jeq r0, 1, lbb_2923                             if r0 == (1 as i32 as i64 as u64) { pc += 39 }
    lddw r6, 0x200000000                            r6 load str located at 8589934592
    ldxdw r5, [r3+0x8]                      
    ldxb r5, [r5+0x2]                       
    jeq r5, 0, lbb_2923                             if r5 == (0 as i32 as i64 as u64) { pc += 34 }
    mov64 r5, r0                                    r5 = r0
    lsh64 r5, 3                                     r5 <<= 3   ///  r5 = r5.wrapping_shl(3)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    add64 r5, -16                                   r5 += -16   ///  r5 = r5.wrapping_add(-16 as i32 as i64 as u64)
    lddw r7, 0x7ffffffffffffff8                     r7 load str located at 9223372036854775800
    jgt r5, r7, lbb_2918                            if r5 > r7 { pc += 22 }
    mov64 r7, 8                                     r7 = 8 as i32 as i64 as u64
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jeq r0, 2, lbb_2944                             if r0 == (2 as i32 as i64 as u64) { pc += 45 }
    lddw r0, 0x300000000                            r0 load str located at 12884901888
    ldxdw r0, [r0+0x0]                      
    lddw r6, 0x300008000                            r6 load str located at 12884934656
    jeq r0, 0, lbb_2906                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r0                                    r6 = r0
lbb_2906:
    mov64 r0, r6                                    r0 = r6
    sub64 r0, r5                                    r0 -= r5   ///  r0 = r0.wrapping_sub(r5)
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jgt r0, r6, lbb_2912                            if r0 > r6 { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_2912:
    jne r8, 0, lbb_2914                             if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, r0                                    r7 = r0
lbb_2914:
    mov64 r6, 8                                     r6 = 8 as i32 as i64 as u64
    lddw r0, 0x300000007                            r0 load str located at 12884901895
    jgt r7, r0, lbb_2928                            if r7 > r0 { pc += 10 }
lbb_2918:
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r5                                    r2 = r5
    lddw r3, 0x10002f560 --> b"\x00\x00\x00\x00j\xd0\x02\x00b\x00\x00\x00\x00\x00\x00\x00\xb3\x07\x00\x0…        r3 load str located at 4295161184
    call function_18348                     
lbb_2923:
    mov64 r7, r6                                    r7 = r6
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
lbb_2925:
    stxw [r1+0x4], r7                       
    stxw [r1+0x0], r6                       
    exit                                    
lbb_2928:
    mov64 r0, r3                                    r0 = r3
    add64 r0, 16                                    r0 += 16   ///  r0 = r0.wrapping_add(16 as i32 as i64 as u64)
    mov64 r6, r5                                    r6 = r5
    rsh64 r6, 3                                     r6 >>= 3   ///  r6 = r6.wrapping_shr(3)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
    lddw r8, 0x300000000                            r8 load str located at 12884901888
    stxdw [r8+0x0], r7                      
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r9, r7                                    r9 = r7
lbb_2938:
    stxdw [r9+0x0], r0                      
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
    add64 r5, -8                                    r5 += -8   ///  r5 = r5.wrapping_add(-8 as i32 as i64 as u64)
    jne r5, 0, lbb_2938                             if r5 != (0 as i32 as i64 as u64) { pc += -6 }
lbb_2944:
    mov64 r5, r3                                    r5 = r3
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x8], r5                     
    stxdw [r10-0x10], r3                    
    stxdw [r10-0x18], r8                    
    stxdw [r10-0x28], r6                    
    stxw [r10-0x20], r7                     
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    stxw [r10-0x1c], r7                     
    mov64 r5, r10                                   r5 = r10
    add64 r5, -48                                   r5 += -48   ///  r5 = r5.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -40                                   r3 += -40   ///  r3 = r3.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r5                                    r1 = r5
    call function_7419                      
    mov64 r1, r6                                    r1 = r6
    ldxw r7, [r10-0x2c]                     
    ldxw r6, [r10-0x30]                     
    ja lbb_2925                                     if true { pc += -39 }

function_2964:
    lddw r2, 0xc2a5a9d28e3490c9                     r2 load str located at -4420940737400237879
    stxdw [r10-0x8], r2                     
    lddw r2, 0xb45bdf8986f6d946                     r2 load str located at -5450517142234015418
    stxdw [r10-0x10], r2                    
    lddw r2, 0xc00987efbb8a4b6c                     r2 load str located at -4609003279923655828
    stxdw [r10-0x18], r2                    
    lddw r2, 0xaad838b236ba11b9                     r2 load str located at -6136092154218802759
    stxdw [r10-0x20], r2                    
    lddw r2, 0xeee96cf3ff21f1c6                     r2 load str located at -1231333227895852602
    stxdw [r10-0x28], r2                    
    lddw r2, 0x569c5dd5fe47a9e5                     r2 load str located at 6240966357304191461
    stxdw [r10-0x30], r2                    
    lddw r2, 0xbf3ee7e0c8afa641                     r2 load str located at -4666037211283610047
    stxdw [r10-0x38], r2                    
    lddw r2, 0x340696fdac79982a                     r2 load str located at 3748849756097058858
    stxdw [r10-0x40], r2                    
    lddw r2, 0x7865fc4f1fa1d59f                     r2 load str located at 8675617673962444191
    stxdw [r10-0x48], r2                    
    lddw r2, 0x4106c5c5d916e87c                     r2 load str located at 4685649915866966140
    stxdw [r10-0x50], r2                    
    lddw r2, 0xbe47d742b702c79b                     r2 load str located at -4735579801618561125
    stxdw [r10-0x58], r2                    
    lddw r2, 0x3396d17ef9481219                     r2 load str located at 3717388885719257625
    stxdw [r10-0x60], r2                    
    lddw r2, 0x7031e0e1f925639d                     r2 load str located at 8084490067258991517
    stxdw [r10-0x68], r2                    
    lddw r2, 0xd17b7711e376e102                     r2 load str located at -3351954578932834046
    stxdw [r10-0x70], r2                    
    lddw r2, 0x4b9bbecb8fd18da1                     r2 load str located at 5448157955735260577
    stxdw [r10-0x78], r2                    
    lddw r2, 0x7d53d4ea1e007042                     r2 load str located at 9030795779798757442
    stxdw [r10-0x80], r2                    
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r5, r10                                   r5 = r10
    add64 r5, -128                                  r5 += -128   ///  r5 = r5.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, 0                                     r3 += 0   ///  r3 = r3.wrapping_add(0 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -120                                  r4 += -120   ///  r4 = r4.wrapping_add(-120 as i32 as i64 as u64)
    ja lbb_3031                                     if true { pc += 11 }
lbb_3020:
    jeq r5, r3, lbb_3021                            if r5 == r3 { pc += 0 }
lbb_3021:
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r5, r0                                    r5 = r0
    jne r2, 672, lbb_3031                           if r2 != (672 as i32 as i64 as u64) { pc += 7 }
    stxdw [r10-0x80], r1                    
    stdw [r10-0x78], 672                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_18030                     
    exit                                    
lbb_3031:
    mov64 r0, r10                                   r0 = r10
    add64 r0, -128                                  r0 += -128   ///  r0 = r0.wrapping_add(-128 as i32 as i64 as u64)
    jeq r5, r3, lbb_3035                            if r5 == r3 { pc += 1 }
    mov64 r0, r5                                    r0 = r5
lbb_3035:
    ldxdw r0, [r0+0x0]                      
    mov64 r6, r1                                    r6 = r1
    add64 r6, r2                                    r6 += r2   ///  r6 = r6.wrapping_add(r2)
    ldxdw r7, [r6+0x0]                      
    xor64 r7, r0                                    r7 ^= r0   ///  r7 = r7.xor(r0)
    stxdw [r6+0x0], r7                      
    mov64 r0, r4                                    r0 = r4
    jeq r5, r3, lbb_3020                            if r5 == r3 { pc += -23 }
    mov64 r0, r5                                    r0 = r5
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_3020                                     if true { pc += -26 }

function_3046:
    lddw r2, 0xc2a5a9d28e3490c9                     r2 load str located at -4420940737400237879
    stxdw [r10-0x8], r2                     
    lddw r2, 0xb45bdf8986f6d946                     r2 load str located at -5450517142234015418
    stxdw [r10-0x10], r2                    
    lddw r2, 0xc00987efbb8a4b6c                     r2 load str located at -4609003279923655828
    stxdw [r10-0x18], r2                    
    lddw r2, 0xaad838b236ba11b9                     r2 load str located at -6136092154218802759
    stxdw [r10-0x20], r2                    
    lddw r2, 0xeee96cf3ff21f1c6                     r2 load str located at -1231333227895852602
    stxdw [r10-0x28], r2                    
    lddw r2, 0x569c5dd5fe47a9e5                     r2 load str located at 6240966357304191461
    stxdw [r10-0x30], r2                    
    lddw r2, 0xbf3ee7e0c8afa641                     r2 load str located at -4666037211283610047
    stxdw [r10-0x38], r2                    
    lddw r2, 0x340696fdac79982a                     r2 load str located at 3748849756097058858
    stxdw [r10-0x40], r2                    
    lddw r2, 0x7865fc4f1fa1d59f                     r2 load str located at 8675617673962444191
    stxdw [r10-0x48], r2                    
    lddw r2, 0x4106c5c5d916e87c                     r2 load str located at 4685649915866966140
    stxdw [r10-0x50], r2                    
    lddw r2, 0xbe47d742b702c79b                     r2 load str located at -4735579801618561125
    stxdw [r10-0x58], r2                    
    lddw r2, 0x3396d17ef9481219                     r2 load str located at 3717388885719257625
    stxdw [r10-0x60], r2                    
    lddw r2, 0x7031e0e1f925639d                     r2 load str located at 8084490067258991517
    stxdw [r10-0x68], r2                    
    lddw r2, 0xd17b7711e376e102                     r2 load str located at -3351954578932834046
    stxdw [r10-0x70], r2                    
    lddw r2, 0x4b9bbecb8fd18da1                     r2 load str located at 5448157955735260577
    stxdw [r10-0x78], r2                    
    lddw r2, 0x7d53d4ea1e007042                     r2 load str located at 9030795779798757442
    stxdw [r10-0x80], r2                    
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r5, r10                                   r5 = r10
    add64 r5, -128                                  r5 += -128   ///  r5 = r5.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, 0                                     r3 += 0   ///  r3 = r3.wrapping_add(0 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -120                                  r4 += -120   ///  r4 = r4.wrapping_add(-120 as i32 as i64 as u64)
    ja lbb_3113                                     if true { pc += 11 }
lbb_3102:
    jeq r5, r3, lbb_3103                            if r5 == r3 { pc += 0 }
lbb_3103:
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r5, r0                                    r5 = r0
    jne r2, 664, lbb_3113                           if r2 != (664 as i32 as i64 as u64) { pc += 7 }
    stxdw [r10-0x80], r1                    
    stdw [r10-0x78], 664                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_18030                     
    exit                                    
lbb_3113:
    mov64 r0, r10                                   r0 = r10
    add64 r0, -128                                  r0 += -128   ///  r0 = r0.wrapping_add(-128 as i32 as i64 as u64)
    jeq r5, r3, lbb_3117                            if r5 == r3 { pc += 1 }
    mov64 r0, r5                                    r0 = r5
lbb_3117:
    ldxdw r0, [r0+0x0]                      
    mov64 r6, r1                                    r6 = r1
    add64 r6, r2                                    r6 += r2   ///  r6 = r6.wrapping_add(r2)
    ldxdw r7, [r6+0x0]                      
    xor64 r7, r0                                    r7 ^= r0   ///  r7 = r7.xor(r0)
    stxdw [r6+0x0], r7                      
    mov64 r0, r4                                    r0 = r4
    jeq r5, r3, lbb_3102                            if r5 == r3 { pc += -23 }
    mov64 r0, r5                                    r0 = r5
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_3102                                     if true { pc += -26 }

function_3128:
    mov64 r8, r3                                    r8 = r3
    mov64 r7, r2                                    r7 = r2
    ldxb r2, [r7+0x746]                     
    lddw r3, 0x20000001a                            r3 load str located at 8589934618
    jeq r2, 2, lbb_3136                             if r2 == (2 as i32 as i64 as u64) { pc += 2 }
    lddw r3, 0xb00000000                            r3 load str located at 47244640256
lbb_3136:
    lddw r0, 0x10000001a --> b"\x01\x00\x00\x00\x00\x00@\x00\x00\x00\x00\x00\x00\x00\xa0.\x03\x00\x00\x0…        r0 load str located at 4294967322
    jeq r2, 1, lbb_3140                             if r2 == (1 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, r3                                    r0 = r3
lbb_3140:
    mov64 r6, r0                                    r6 = r0
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    mov64 r2, r0                                    r2 = r0
    and64 r2, 26                                    r2 &= 26   ///  r2 = r2.and(26)
    jne r2, 26, lbb_3651                            if r2 != (26 as i32 as i64 as u64) { pc += 506 }
    stxdw [r10-0x40], r4                    
    stxdw [r10-0x38], r1                    
    ldxdw r1, [r5-0xff8]                    
    stxdw [r10-0x48], r1                    
    ldxdw r9, [r5-0x1000]                   
    mov64 r3, r8                                    r3 = r8
    add64 r3, 136                                   r3 += 136   ///  r3 = r3.wrapping_add(136 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    add64 r2, 472                                   r2 += 472   ///  r2 = r2.wrapping_add(472 as i32 as i64 as u64)
    jeq r6, 1, lbb_3156                             if r6 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3385                                     if true { pc += 229 }
lbb_3156:
    ldxdw r1, [r7+0xb70]                    
    stxdw [r10-0x50], r1                    
    ldxdw r6, [r7+0xb78]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r4, r6                                    r4 = r6
    call function_3829                      
    ldxw r0, [r10-0x20]                     
    jne r0, 26, lbb_3643                            if r0 != (26 as i32 as i64 as u64) { pc += 478 }
    mov64 r2, r9                                    r2 = r9
    add64 r2, 472                                   r2 += 472   ///  r2 = r2.wrapping_add(472 as i32 as i64 as u64)
    mov64 r3, r9                                    r3 = r9
    add64 r3, 136                                   r3 += 136   ///  r3 = r3.wrapping_add(136 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    ldxdw r4, [r10-0x50]                    
    call function_3829                      
    ldxw r0, [r10-0x28]                     
    jeq r0, 26, lbb_3176                            if r0 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3647                                     if true { pc += 471 }
lbb_3176:
    mov64 r1, r7                                    r1 = r7
    add64 r1, 4000                                  r1 += 4000   ///  r1 = r1.wrapping_add(4000 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    add64 r2, 272                                   r2 += 272   ///  r2 = r2.wrapping_add(272 as i32 as i64 as u64)
    mov64 r3, r6                                    r3 = r6
    ldxdw r4, [r10-0x50]                    
    call function_4201                      
    ldxdw r1, [r10-0x50]                    
    stxdw [r7+0xb80], r1                    
    stxdw [r7+0xb88], r6                    
    mov64 r1, r8                                    r1 = r8
    ldxdw r2, [r10-0x40]                    
    call function_4128                      
    mov64 r1, r9                                    r1 = r9
    ldxdw r2, [r10-0x48]                    
    call function_4128                      
    ldxw r4, [r9+0x330]                     
    ldxw r3, [r8+0x330]                     
    ldxw r2, [r7+0xb9c]                     
    ldxw r1, [r7+0x344]                     
    ldxw r5, [r7+0x14b8]                    
    ldxw r0, [r7+0x14bc]                    
    stxdw [r10-0xff0], r0                   
    stxdw [r10-0xff8], r5                   
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    call function_16039                     
    ldxw r3, [r7+0x348]                     
    ldxw r1, [r10-0x2c]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    ldxw r2, [r10-0x30]                     
    stxw [r7+0x14a4], r1                    
    stxw [r7+0x14a0], r2                    
    ldxw r4, [r7+0x14b8]                    
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    jsle r4, r2, lbb_3237                           if (r4 as i64) <= (r2 as i64) { pc += 19 }
    mov64 r0, r3                                    r0 = r3
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    mov64 r5, r2                                    r5 = r2
    jsgt r2, r0, lbb_3224                           if (r2 as i64) > (r0 as i64) { pc += 1 }
    mov64 r5, r0                                    r5 = r0
lbb_3224:
    mov64 r0, r5                                    r0 = r5
    mul64 r0, 1000                                  r0 *= 1000   ///  r0 = r0.wrapping_mul(1000 as u64)
    mov64 r6, r0                                    r6 = r0
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    arsh64 r6, 32                                   r6 >>= 32 (signed)   ///  r6 = (r6 as i64).wrapping_shr(32)
    jne r6, r0, lbb_3230                            if r6 != r0 { pc += 0 }
lbb_3230:
    jne r6, r0, lbb_3673                            if r6 != r0 { pc += 442 }
    stxw [r7+0x14b0], r0                    
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    arsh64 r5, 32                                   r5 >>= 32 (signed)   ///  r5 = (r5 as i64).wrapping_shr(32)
    jslt r4, r5, lbb_3236                           if (r4 as i64) < (r5 as i64) { pc += 1 }
    mov64 r4, r5                                    r4 = r5
lbb_3236:
    stxw [r7+0x14b8], r4                    
lbb_3237:
    ldxw r4, [r7+0x14bc]                    
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    jsle r4, r1, lbb_3259                           if (r4 as i64) <= (r1 as i64) { pc += 18 }
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    mov64 r5, r1                                    r5 = r1
    jsgt r1, r3, lbb_3246                           if (r1 as i64) > (r3 as i64) { pc += 1 }
    mov64 r5, r3                                    r5 = r3
lbb_3246:
    mov64 r3, r5                                    r3 = r5
    mul64 r3, 1000                                  r3 *= 1000   ///  r3 = r3.wrapping_mul(1000 as u64)
    mov64 r0, r3                                    r0 = r3
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    jne r0, r3, lbb_3252                            if r0 != r3 { pc += 0 }
lbb_3252:
    jne r0, r3, lbb_3673                            if r0 != r3 { pc += 420 }
    stxw [r7+0x14b4], r3                    
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    arsh64 r5, 32                                   r5 >>= 32 (signed)   ///  r5 = (r5 as i64).wrapping_shr(32)
    jslt r4, r5, lbb_3258                           if (r4 as i64) < (r5 as i64) { pc += 1 }
    mov64 r4, r5                                    r4 = r5
lbb_3258:
    stxw [r7+0x14bc], r4                    
lbb_3259:
    ldxw r3, [r7+0x350]                     
    mov64 r4, r3                                    r4 = r3
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    mul64 r2, r4                                    r2 *= r4   ///  r2 = r2.wrapping_mul(r4)
    mov64 r4, r2                                    r4 = r2
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    jne r4, r2, lbb_3268                            if r4 != r2 { pc += 0 }
lbb_3268:
    jne r4, r2, lbb_3676                            if r4 != r2 { pc += 407 }
    ldxw r8, [r7+0x354]                     
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    arsh64 r8, 32                                   r8 >>= 32 (signed)   ///  r8 = (r8 as i64).wrapping_shr(32)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r6, r8                                    r6 = r8
    jsgt r2, r8, lbb_3285                           if (r2 as i64) > (r8 as i64) { pc += 9 }
    jeq r8, -2147483648, lbb_3688                   if r8 == (-2147483648 as i32 as i64 as u64) { pc += 411 }
    mov64 r4, r8                                    r4 = r8
    neg64 r4                                        r4 = -r4   ///  r4 = (r4 as i64).wrapping_neg() as u64
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    mov64 r6, r2                                    r6 = r2
    jslt r2, r4, lbb_3284                           if (r2 as i64) < (r4 as i64) { pc += 1 }
    ja lbb_3285                                     if true { pc += 1 }
lbb_3284:
    mov64 r6, r4                                    r6 = r4
lbb_3285:
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    mul64 r1, r3                                    r1 *= r3   ///  r1 = r1.wrapping_mul(r3)
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    jne r2, r1, lbb_3292                            if r2 != r1 { pc += 0 }
lbb_3292:
    jne r2, r1, lbb_3685                            if r2 != r1 { pc += 392 }
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jsgt r1, r8, lbb_3305                           if (r1 as i64) > (r8 as i64) { pc += 9 }
    jeq r8, -2147483648, lbb_3688                   if r8 == (-2147483648 as i32 as i64 as u64) { pc += 391 }
    neg64 r8                                        r8 = -r8   ///  r8 = (r8 as i64).wrapping_neg() as u64
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    mov64 r2, r8                                    r2 = r8
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r8, r1                                    r8 = r1
    jslt r1, r2, lbb_3304                           if (r1 as i64) < (r2 as i64) { pc += 1 }
    ja lbb_3305                                     if true { pc += 1 }
lbb_3304:
    mov64 r8, r2                                    r8 = r2
lbb_3305:
    ldxdw r1, [r7+0xfa0]                    
    mov64 r2, r6                                    r2 = r6
    call function_17482                     
    stxdw [r7+0xfa8], r0                    
    ldxdw r1, [r7+0xfe0]                    
    mov64 r2, r6                                    r2 = r6
    call function_17482                     
    stxdw [r7+0xfe8], r0                    
    ldxdw r1, [r7+0x1020]                   
    mov64 r2, r6                                    r2 = r6
    call function_17482                     
    stxdw [r7+0x1028], r0                   
    ldxdw r1, [r7+0x1060]                   
    mov64 r2, r6                                    r2 = r6
    call function_17482                     
    stxdw [r7+0x1068], r0                   
    ldxdw r1, [r7+0x10a0]                   
    mov64 r2, r6                                    r2 = r6
    call function_17482                     
    stxdw [r7+0x10a8], r0                   
    ldxdw r1, [r7+0x10e0]                   
    mov64 r2, r6                                    r2 = r6
    call function_17482                     
    stxdw [r7+0x10e8], r0                   
    ldxdw r1, [r7+0x1120]                   
    mov64 r2, r6                                    r2 = r6
    call function_17482                     
    stxdw [r7+0x1128], r0                   
    ldxdw r1, [r7+0x1160]                   
    mov64 r2, r6                                    r2 = r6
    call function_17482                     
    stxdw [r7+0x1168], r0                   
    ldxdw r1, [r7+0x11a0]                   
    mov64 r2, r6                                    r2 = r6
    call function_17482                     
    stxdw [r7+0x11a8], r0                   
    ldxdw r1, [r7+0x11e0]                   
    mov64 r2, r6                                    r2 = r6
    call function_17482                     
    stxdw [r7+0x11e8], r0                   
    ldxdw r1, [r7+0x1220]                   
    mov64 r2, r8                                    r2 = r8
    call function_17482                     
    stxdw [r7+0x1228], r0                   
    ldxdw r1, [r7+0x1260]                   
    mov64 r2, r8                                    r2 = r8
    call function_17482                     
    stxdw [r7+0x1268], r0                   
    ldxdw r1, [r7+0x12a0]                   
    mov64 r2, r8                                    r2 = r8
    call function_17482                     
    stxdw [r7+0x12a8], r0                   
    ldxdw r1, [r7+0x12e0]                   
    mov64 r2, r8                                    r2 = r8
    call function_17482                     
    stxdw [r7+0x12e8], r0                   
    ldxdw r1, [r7+0x1320]                   
    mov64 r2, r8                                    r2 = r8
    call function_17482                     
    stxdw [r7+0x1328], r0                   
    ldxdw r1, [r7+0x1360]                   
    mov64 r2, r8                                    r2 = r8
    call function_17482                     
    stxdw [r7+0x1368], r0                   
    ldxdw r1, [r7+0x13a0]                   
    mov64 r2, r8                                    r2 = r8
    call function_17482                     
    stxdw [r7+0x13a8], r0                   
    ldxdw r1, [r7+0x13e0]                   
    mov64 r2, r8                                    r2 = r8
    call function_17482                     
    stxdw [r7+0x13e8], r0                   
    ldxdw r1, [r7+0x1420]                   
    mov64 r2, r8                                    r2 = r8
    call function_17482                     
    stxdw [r7+0x1428], r0                   
    ldxdw r1, [r7+0x1460]                   
    mov64 r2, r8                                    r2 = r8
    call function_17482                     
    ja lbb_3640                                     if true { pc += 255 }
lbb_3385:
    ldxdw r1, [r7+0xb70]                    
    stxdw [r10-0x50], r1                    
    ldxdw r6, [r7+0xb78]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -8                                    r1 += -8   ///  r1 = r1.wrapping_add(-8 as i32 as i64 as u64)
    mov64 r4, r6                                    r4 = r6
    call function_4319                      
    ldxw r0, [r10-0x8]                      
    jne r0, 26, lbb_3645                            if r0 != (26 as i32 as i64 as u64) { pc += 251 }
    mov64 r2, r9                                    r2 = r9
    add64 r2, 472                                   r2 += 472   ///  r2 = r2.wrapping_add(472 as i32 as i64 as u64)
    mov64 r3, r9                                    r3 = r9
    add64 r3, 136                                   r3 += 136   ///  r3 = r3.wrapping_add(136 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    ldxdw r4, [r10-0x50]                    
    call function_4319                      
    ldxw r0, [r10-0x10]                     
    jeq r0, 26, lbb_3405                            if r0 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3649                                     if true { pc += 244 }
lbb_3405:
    mov64 r1, r7                                    r1 = r7
    add64 r1, 4000                                  r1 += 4000   ///  r1 = r1.wrapping_add(4000 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    add64 r2, 272                                   r2 += 272   ///  r2 = r2.wrapping_add(272 as i32 as i64 as u64)
    mov64 r3, r6                                    r3 = r6
    ldxdw r4, [r10-0x50]                    
    call function_4201                      
    ldxdw r1, [r10-0x50]                    
    stxdw [r7+0xb80], r1                    
    stxdw [r7+0xb88], r6                    
    mov64 r1, r8                                    r1 = r8
    ldxdw r2, [r10-0x40]                    
    call function_4621                      
    mov64 r1, r9                                    r1 = r9
    ldxdw r2, [r10-0x48]                    
    call function_4621                      
    ldxw r5, [r7+0x344]                     
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    arsh64 r5, 32                                   r5 >>= 32 (signed)   ///  r5 = (r5 as i64).wrapping_shr(32)
    mov64 r1, r5                                    r1 = r5
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r0, r1                                    r0 = r1
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    jne r0, r1, lbb_3430                            if r0 != r1 { pc += 0 }
lbb_3430:
    ldxw r2, [r7+0xb9c]                     
    ldxw r4, [r9+0x358]                     
    ldxw r3, [r8+0x358]                     
    jne r0, r1, lbb_3654                            if r0 != r1 { pc += 220 }
lbb_3434:
    ldxw r5, [r7+0x14c8]                    
    ldxw r0, [r7+0x14cc]                    
    stxdw [r10-0xff0], r0                   
    stxdw [r10-0xff8], r5                   
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    call function_16771                     
    ldxw r2, [r10-0x18]                     
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    ldxw r1, [r10-0x14]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    stxw [r7+0x14c4], r1                    
    stxw [r7+0x14c0], r2                    
    ldxw r4, [r7+0x348]                     
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    mov64 r3, r4                                    r3 = r4
    mul64 r3, 1000                                  r3 *= 1000   ///  r3 = r3.wrapping_mul(1000 as u64)
    mov64 r5, r3                                    r5 = r3
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    arsh64 r5, 32                                   r5 >>= 32 (signed)   ///  r5 = (r5 as i64).wrapping_shr(32)
    jne r5, r3, lbb_3460                            if r5 != r3 { pc += 0 }
lbb_3460:
    jne r5, r3, lbb_3658                            if r5 != r3 { pc += 197 }
lbb_3461:
    ldxw r4, [r7+0x14c8]                    
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    jsle r4, r2, lbb_3476                           if (r4 as i64) <= (r2 as i64) { pc += 11 }
    mov64 r5, r3                                    r5 = r3
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    arsh64 r5, 32                                   r5 >>= 32 (signed)   ///  r5 = (r5 as i64).wrapping_shr(32)
    mov64 r0, r2                                    r0 = r2
    jsgt r2, r5, lbb_3471                           if (r2 as i64) > (r5 as i64) { pc += 1 }
    mov64 r0, r5                                    r0 = r5
lbb_3471:
    mov64 r6, r0                                    r6 = r0
    jsgt r4, r5, lbb_3474                           if (r4 as i64) > (r5 as i64) { pc += 1 }
    mov64 r6, r4                                    r6 = r4
lbb_3474:
    stxw [r7+0x14b0], r0                    
    stxw [r7+0x14c8], r6                    
lbb_3476:
    ldxw r4, [r7+0x14cc]                    
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    jsle r4, r1, lbb_3490                           if (r4 as i64) <= (r1 as i64) { pc += 10 }
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    mov64 r5, r1                                    r5 = r1
    jsgt r1, r3, lbb_3485                           if (r1 as i64) > (r3 as i64) { pc += 1 }
    mov64 r5, r3                                    r5 = r3
lbb_3485:
    mov64 r0, r5                                    r0 = r5
    jsgt r4, r3, lbb_3488                           if (r4 as i64) > (r3 as i64) { pc += 1 }
    mov64 r0, r4                                    r0 = r4
lbb_3488:
    stxw [r7+0x14b4], r5                    
    stxw [r7+0x14cc], r0                    
lbb_3490:
    ldxw r4, [r7+0x350]                     
    mov64 r3, r4                                    r3 = r4
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    mul64 r2, r3                                    r2 *= r3   ///  r2 = r2.wrapping_mul(r3)
    mov64 r3, r2                                    r3 = r2
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    jne r3, r2, lbb_3499                            if r3 != r2 { pc += 0 }
lbb_3499:
    jne r3, r2, lbb_3670                            if r3 != r2 { pc += 170 }
    ldxw r3, [r7+0x354]                     
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    mov64 r8, r3                                    r8 = r3
    mul64 r8, 1000                                  r8 *= 1000   ///  r8 = r8.wrapping_mul(1000 as u64)
    mov64 r0, r8                                    r0 = r8
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r0, r8, lbb_3511                            if r0 != r8 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_3511:
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    jne r0, r8, lbb_3662                            if r0 != r8 { pc += 148 }
    mov64 r9, r8                                    r9 = r8
lbb_3515:
    mov64 r0, r9                                    r0 = r9
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    mov64 r6, r0                                    r6 = r0
    arsh64 r6, 32                                   r6 >>= 32 (signed)   ///  r6 = (r6 as i64).wrapping_shr(32)
    jslt r6, r2, lbb_3532                           if (r6 as i64) < (r2 as i64) { pc += 12 }
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    lddw r6, 0x80000000                             r6 load str located at 2147483648
    jeq r0, r6, lbb_3682                            if r0 == r6 { pc += 158 }
    neg64 r9                                        r9 = -r9   ///  r9 = (r9 as i64).wrapping_neg() as u64
    lsh64 r9, 32                                    r9 <<= 32   ///  r9 = r9.wrapping_shl(32)
    mov64 r0, r9                                    r0 = r9
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    mov64 r9, r2                                    r9 = r2
    jslt r2, r0, lbb_3531                           if (r2 as i64) < (r0 as i64) { pc += 1 }
    ja lbb_3532                                     if true { pc += 1 }
lbb_3531:
    mov64 r9, r0                                    r9 = r0
lbb_3532:
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    mul64 r1, r4                                    r1 *= r4   ///  r1 = r1.wrapping_mul(r4)
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    jne r2, r1, lbb_3539                            if r2 != r1 { pc += 0 }
lbb_3539:
    jne r2, r1, lbb_3679                            if r2 != r1 { pc += 139 }
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_3666                             if r5 != (0 as i32 as i64 as u64) { pc += 122 }
lbb_3544:
    mov64 r2, r8                                    r2 = r8
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    mov64 r3, r2                                    r3 = r2
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    jslt r3, r1, lbb_3561                           if (r3 as i64) < (r1 as i64) { pc += 12 }
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    lddw r3, 0x80000000                             r3 load str located at 2147483648
    jeq r2, r3, lbb_3682                            if r2 == r3 { pc += 129 }
    neg64 r8                                        r8 = -r8   ///  r8 = (r8 as i64).wrapping_neg() as u64
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    mov64 r2, r8                                    r2 = r8
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r8, r1                                    r8 = r1
    jslt r1, r2, lbb_3560                           if (r1 as i64) < (r2 as i64) { pc += 1 }
    ja lbb_3561                                     if true { pc += 1 }
lbb_3560:
    mov64 r8, r2                                    r8 = r2
lbb_3561:
    ldxdw r1, [r7+0xfa0]                    
    mov64 r2, r9                                    r2 = r9
    call function_17530                     
    stxdw [r7+0xfa8], r0                    
    ldxdw r1, [r7+0xfe0]                    
    mov64 r2, r9                                    r2 = r9
    call function_17530                     
    stxdw [r7+0xfe8], r0                    
    ldxdw r1, [r7+0x1020]                   
    mov64 r2, r9                                    r2 = r9
    call function_17530                     
    stxdw [r7+0x1028], r0                   
    ldxdw r1, [r7+0x1060]                   
    mov64 r2, r9                                    r2 = r9
    call function_17530                     
    stxdw [r7+0x1068], r0                   
    ldxdw r1, [r7+0x10a0]                   
    mov64 r2, r9                                    r2 = r9
    call function_17530                     
    stxdw [r7+0x10a8], r0                   
    ldxdw r1, [r7+0x10e0]                   
    mov64 r2, r9                                    r2 = r9
    call function_17530                     
    stxdw [r7+0x10e8], r0                   
    ldxdw r1, [r7+0x1120]                   
    mov64 r2, r9                                    r2 = r9
    call function_17530                     
    stxdw [r7+0x1128], r0                   
    ldxdw r1, [r7+0x1160]                   
    mov64 r2, r9                                    r2 = r9
    call function_17530                     
    stxdw [r7+0x1168], r0                   
    ldxdw r1, [r7+0x11a0]                   
    mov64 r2, r9                                    r2 = r9
    call function_17530                     
    stxdw [r7+0x11a8], r0                   
    ldxdw r1, [r7+0x11e0]                   
    mov64 r2, r9                                    r2 = r9
    call function_17530                     
    stxdw [r7+0x11e8], r0                   
    ldxdw r1, [r7+0x1220]                   
    mov64 r2, r8                                    r2 = r8
    call function_17530                     
    stxdw [r7+0x1228], r0                   
    ldxdw r1, [r7+0x1260]                   
    mov64 r2, r8                                    r2 = r8
    call function_17530                     
    stxdw [r7+0x1268], r0                   
    ldxdw r1, [r7+0x12a0]                   
    mov64 r2, r8                                    r2 = r8
    call function_17530                     
    stxdw [r7+0x12a8], r0                   
    ldxdw r1, [r7+0x12e0]                   
    mov64 r2, r8                                    r2 = r8
    call function_17530                     
    stxdw [r7+0x12e8], r0                   
    ldxdw r1, [r7+0x1320]                   
    mov64 r2, r8                                    r2 = r8
    call function_17530                     
    stxdw [r7+0x1328], r0                   
    ldxdw r1, [r7+0x1360]                   
    mov64 r2, r8                                    r2 = r8
    call function_17530                     
    stxdw [r7+0x1368], r0                   
    ldxdw r1, [r7+0x13a0]                   
    mov64 r2, r8                                    r2 = r8
    call function_17530                     
    stxdw [r7+0x13a8], r0                   
    ldxdw r1, [r7+0x13e0]                   
    mov64 r2, r8                                    r2 = r8
    call function_17530                     
    stxdw [r7+0x13e8], r0                   
    ldxdw r1, [r7+0x1420]                   
    mov64 r2, r8                                    r2 = r8
    call function_17530                     
    stxdw [r7+0x1428], r0                   
    ldxdw r1, [r7+0x1460]                   
    mov64 r2, r8                                    r2 = r8
    call function_17530                     
lbb_3640:
    stxdw [r7+0x1468], r0                   
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
    ja lbb_3650                                     if true { pc += 7 }
lbb_3643:
    ldxw r6, [r10-0x1c]                     
    ja lbb_3650                                     if true { pc += 5 }
lbb_3645:
    ldxw r6, [r10-0x4]                      
    ja lbb_3650                                     if true { pc += 3 }
lbb_3647:
    ldxw r6, [r10-0x24]                     
    ja lbb_3650                                     if true { pc += 1 }
lbb_3649:
    ldxw r6, [r10-0xc]                      
lbb_3650:
    ldxdw r1, [r10-0x38]                    
lbb_3651:
    stxw [r1+0x4], r6                       
    stxw [r1+0x0], r0                       
    exit                                    
lbb_3654:
    arsh64 r5, 31                                   r5 >>= 31 (signed)   ///  r5 = (r5 as i64).wrapping_shr(31)
    xor64 r5, 2147483647                            r5 ^= 2147483647   ///  r5 = r5.xor(2147483647)
    mov64 r1, r5                                    r1 = r5
    ja lbb_3434                                     if true { pc += -224 }
lbb_3658:
    arsh64 r4, 31                                   r4 >>= 31 (signed)   ///  r4 = (r4 as i64).wrapping_shr(31)
    xor64 r4, 2147483647                            r4 ^= 2147483647   ///  r4 = r4.xor(2147483647)
    mov64 r3, r4                                    r3 = r4
    ja lbb_3461                                     if true { pc += -201 }
lbb_3662:
    mov64 r9, r3                                    r9 = r3
    arsh64 r9, 31                                   r9 >>= 31 (signed)   ///  r9 = (r9 as i64).wrapping_shr(31)
    xor64 r9, 2147483647                            r9 ^= 2147483647   ///  r9 = r9.xor(2147483647)
    ja lbb_3515                                     if true { pc += -151 }
lbb_3666:
    arsh64 r3, 31                                   r3 >>= 31 (signed)   ///  r3 = (r3 as i64).wrapping_shr(31)
    xor64 r3, 2147483647                            r3 ^= 2147483647   ///  r3 = r3.xor(2147483647)
    mov64 r8, r3                                    r8 = r3
    ja lbb_3544                                     if true { pc += -126 }
lbb_3670:
    lddw r1, 0x10002ee08 --> b"\x00\x00\x00\x00`\xd3\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00o\x01\x00\x0…        r1 load str located at 4295159304
    call function_20968                     
lbb_3673:
    lddw r1, 0x10002ecc8 --> b"\x00\x00\x00\x00(\xd3\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00\xfe\x00\x00…        r1 load str located at 4295158984
    call function_20968                     
lbb_3676:
    lddw r1, 0x10002ed60 --> b"\x00\x00\x00\x00(\xd3\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00P\x01\x00\x0…        r1 load str located at 4295159136
    call function_20968                     
lbb_3679:
    lddw r1, 0x10002ee20 --> b"\x00\x00\x00\x00`\xd3\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00q\x01\x00\x0…        r1 load str located at 4295159328
    call function_20968                     
lbb_3682:
    lddw r1, 0x10002edf0 --> b"\x00\x00\x00\x00`\xd3\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00O\x01\x00\x0…        r1 load str located at 4295159280
    call function_20979                     
lbb_3685:
    lddw r1, 0x10002ed78 --> b"\x00\x00\x00\x00(\xd3\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00R\x01\x00\x0…        r1 load str located at 4295159160
    call function_20968                     
lbb_3688:
    lddw r1, 0x10002ed48 --> b"\x00\x00\x00\x00(\xd3\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x003\x01\x00\x0…        r1 load str located at 4295159112
    call function_20979                     

function_3691:
    ldxb r4, [r2+0x746]                     
    lddw r5, 0x20000001a                            r5 load str located at 8589934618
    jeq r4, 2, lbb_3697                             if r4 == (2 as i32 as i64 as u64) { pc += 2 }
    lddw r5, 0xb00000000                            r5 load str located at 47244640256
lbb_3697:
    lddw r3, 0x10000001a --> b"\x01\x00\x00\x00\x00\x00@\x00\x00\x00\x00\x00\x00\x00\xa0.\x03\x00\x00\x0…        r3 load str located at 4294967322
    jeq r4, 1, lbb_3701                             if r4 == (1 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r5                                    r3 = r5
lbb_3701:
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    mov64 r5, r3                                    r5 = r3
    and64 r5, 26                                    r5 &= 26   ///  r5 = r5.and(26)
    jne r5, 26, lbb_3822                            if r5 != (26 as i32 as i64 as u64) { pc += 116 }
    jeq r4, 1, lbb_3708                             if r4 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3782                                     if true { pc += 74 }
lbb_3708:
    ldxw r5, [r2+0x340]                     
    ldxw r4, [r2+0x14b4]                    
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    jsgt r4, -1, lbb_3723                           if (r4 as i64) > (-1 as i32 as i64) { pc += 10 }
    mov64 r0, r4                                    r0 = r4
    add64 r0, -1000                                 r0 += -1000   ///  r0 = r0.wrapping_add(-1000 as i32 as i64 as u64)
    mov64 r6, r0                                    r6 = r0
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    arsh64 r6, 32                                   r6 >>= 32 (signed)   ///  r6 = (r6 as i64).wrapping_shr(32)
    jne r6, r0, lbb_3719                            if r6 != r0 { pc += 0 }
lbb_3719:
    jeq r6, r0, lbb_3728                            if r6 == r0 { pc += 8 }
lbb_3720:
    lddw r1, 0x10002e9f0 --> b"\x00\x00\x00\x005\xd0\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00^\x00\x00\x0…        r1 load str located at 4295158256
    call function_20957                     
lbb_3723:
    mov64 r0, r4                                    r0 = r4
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    div64 r0, 1000                                  r0 /= 1000   ///  r0 = r0 / (1000 as u64)
    ja lbb_3733                                     if true { pc += 5 }
lbb_3728:
    xor64 r0, -1                                    r0 ^= -1   ///  r0 = r0.xor(-1)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    div64 r0, 1000                                  r0 /= 1000   ///  r0 = r0 / (1000 as u64)
    neg64 r0                                        r0 = -r0   ///  r0 = (r0 as i64).wrapping_neg() as u64
lbb_3733:
    mov64 r6, r5                                    r6 = r5
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    arsh64 r6, 32                                   r6 >>= 32 (signed)   ///  r6 = (r6 as i64).wrapping_shr(32)
    jslt r6, r0, lbb_3738                           if (r6 as i64) < (r0 as i64) { pc += 1 }
    mov64 r6, r0                                    r6 = r0
lbb_3738:
    stxw [r2+0x14bc], r6                    
    ldxw r0, [r2+0x14b0]                    
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    jsgt r0, -1, lbb_3751                           if (r0 as i64) > (-1 as i32 as i64) { pc += 8 }
    mov64 r6, r0                                    r6 = r0
    add64 r6, -1000                                 r6 += -1000   ///  r6 = r6.wrapping_add(-1000 as i32 as i64 as u64)
    mov64 r7, r6                                    r7 = r6
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    arsh64 r7, 32                                   r7 >>= 32 (signed)   ///  r7 = (r7 as i64).wrapping_shr(32)
    jne r7, r6, lbb_3749                            if r7 != r6 { pc += 0 }
lbb_3749:
    jeq r7, r6, lbb_3756                            if r7 == r6 { pc += 6 }
    ja lbb_3720                                     if true { pc += -31 }
lbb_3751:
    mov64 r6, r0                                    r6 = r0
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    div64 r6, 1000                                  r6 /= 1000   ///  r6 = r6 / (1000 as u64)
    ja lbb_3761                                     if true { pc += 5 }
lbb_3756:
    xor64 r6, -1                                    r6 ^= -1   ///  r6 = r6.xor(-1)
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    div64 r6, 1000                                  r6 /= 1000   ///  r6 = r6 / (1000 as u64)
    neg64 r6                                        r6 = -r6   ///  r6 = (r6 as i64).wrapping_neg() as u64
lbb_3761:
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    arsh64 r5, 32                                   r5 >>= 32 (signed)   ///  r5 = (r5 as i64).wrapping_shr(32)
    jslt r5, r6, lbb_3765                           if (r5 as i64) < (r6 as i64) { pc += 1 }
    mov64 r5, r6                                    r5 = r6
lbb_3765:
    stxw [r2+0x14b8], r5                    
    ldxw r5, [r2+0x34c]                     
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    arsh64 r5, 32                                   r5 >>= 32 (signed)   ///  r5 = (r5 as i64).wrapping_shr(32)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    jslt r4, 2147483647, lbb_3772                   if (r4 as i64) < (2147483647 as i32 as i64) { pc += 1 }
    mov64 r4, 2147483647                            r4 = 2147483647 as i32 as i64 as u64
lbb_3772:
    jsgt r4, -2147483648, lbb_3774                  if (r4 as i64) > (-2147483648 as i32 as i64) { pc += 1 }
    mov64 r4, -2147483648                           r4 = -2147483648 as i32 as i64 as u64
lbb_3774:
    stxw [r2+0x14b4], r4                    
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    jslt r0, 2147483647, lbb_3778                   if (r0 as i64) < (2147483647 as i32 as i64) { pc += 1 }
    mov64 r0, 2147483647                            r0 = 2147483647 as i32 as i64 as u64
lbb_3778:
    jsgt r0, -2147483648, lbb_3780                  if (r0 as i64) > (-2147483648 as i32 as i64) { pc += 1 }
    mov64 r0, -2147483648                           r0 = -2147483648 as i32 as i64 as u64
lbb_3780:
    stxw [r2+0x14b0], r0                    
    ja lbb_3822                                     if true { pc += 40 }
lbb_3782:
    ldxw r4, [r2+0x340]                     
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    mov64 r5, r4                                    r5 = r4
    mul64 r5, 1000                                  r5 *= 1000   ///  r5 = r5.wrapping_mul(1000 as u64)
    mov64 r0, r5                                    r0 = r5
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    jne r0, r5, lbb_3791                            if r0 != r5 { pc += 0 }
lbb_3791:
    jne r0, r5, lbb_3825                            if r0 != r5 { pc += 33 }
lbb_3792:
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    arsh64 r5, 32                                   r5 >>= 32 (signed)   ///  r5 = (r5 as i64).wrapping_shr(32)
    ldxw r4, [r2+0x14b4]                    
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    mov64 r0, r5                                    r0 = r5
    jslt r5, r4, lbb_3800                           if (r5 as i64) < (r4 as i64) { pc += 1 }
    mov64 r0, r4                                    r0 = r4
lbb_3800:
    stxw [r2+0x14cc], r0                    
    ldxw r0, [r2+0x14b0]                    
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    jslt r5, r0, lbb_3806                           if (r5 as i64) < (r0 as i64) { pc += 1 }
    mov64 r5, r0                                    r5 = r0
lbb_3806:
    stxw [r2+0x14c8], r5                    
    ldxw r5, [r2+0x34c]                     
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    arsh64 r5, 32                                   r5 >>= 32 (signed)   ///  r5 = (r5 as i64).wrapping_shr(32)
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    jslt r0, 2147483647, lbb_3813                   if (r0 as i64) < (2147483647 as i32 as i64) { pc += 1 }
    mov64 r0, 2147483647                            r0 = 2147483647 as i32 as i64 as u64
lbb_3813:
    jsgt r0, -2147483648, lbb_3815                  if (r0 as i64) > (-2147483648 as i32 as i64) { pc += 1 }
    mov64 r0, -2147483648                           r0 = -2147483648 as i32 as i64 as u64
lbb_3815:
    stxw [r2+0x14b0], r0                    
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    jslt r4, 2147483647, lbb_3819                   if (r4 as i64) < (2147483647 as i32 as i64) { pc += 1 }
    mov64 r4, 2147483647                            r4 = 2147483647 as i32 as i64 as u64
lbb_3819:
    jsgt r4, -2147483648, lbb_3821                  if (r4 as i64) > (-2147483648 as i32 as i64) { pc += 1 }
    mov64 r4, -2147483648                           r4 = -2147483648 as i32 as i64 as u64
lbb_3821:
    stxw [r2+0x14b4], r4                    
lbb_3822:
    stxw [r1+0x4], r4                       
    stxw [r1+0x0], r3                       
    exit                                    
lbb_3825:
    arsh64 r4, 31                                   r4 >>= 31 (signed)   ///  r4 = (r4 as i64).wrapping_shr(31)
    xor64 r4, 2147483647                            r4 ^= 2147483647   ///  r4 = r4.xor(2147483647)
    mov64 r5, r4                                    r5 = r4
    ja lbb_3792                                     if true { pc += -37 }

function_3829:
    mov64 r7, r2                                    r7 = r2
    mov64 r5, r1                                    r5 = r1
    stxdw [r7+0x50], r4                     
    ldxdw r8, [r3+0x0]                      
    stxdw [r10-0x8], r3                     
    ldxdw r6, [r3+0x8]                      
    jeq r6, -1, lbb_3850                            if r6 == (-1 as i32 as i64 as u64) { pc += 14 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 10                                    r3 = 10 as i32 as i64 as u64
    jeq r8, -1, lbb_3840                            if r8 == (-1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4122                                     if true { pc += 282 }
lbb_3840:
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r4                                    r2 = r4
    mov64 r8, r5                                    r8 = r5
    mov64 r9, r7                                    r9 = r7
    mov64 r7, r4                                    r7 = r4
    call function_17448                     
    mov64 r4, r7                                    r4 = r7
    mov64 r7, r9                                    r7 = r9
    mov64 r5, r8                                    r5 = r8
    mov64 r8, r0                                    r8 = r0
lbb_3850:
    stxdw [r7+0x0], r8                      
    ldxdw r1, [r10-0x8]                     
    ldxdw r9, [r1+0x10]                     
    ldxdw r1, [r1+0x18]                     
    jeq r1, -1, lbb_3878                            if r1 == (-1 as i32 as i64 as u64) { pc += 23 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 10                                    r3 = 10 as i32 as i64 as u64
    jeq r9, -1, lbb_3859                            if r9 == (-1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4122                                     if true { pc += 263 }
lbb_3859:
    jslt r1, 0, lbb_4125                            if (r1 as i64) < (0 as i32 as i64) { pc += 265 }
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    jeq r2, r6, lbb_3864                            if r2 == r6 { pc += 1 }
    ja lbb_3869                                     if true { pc += 5 }
lbb_3864:
    mov64 r9, r8                                    r9 = r8
    rsh64 r9, 1                                     r9 >>= 1   ///  r9 = r9.wrapping_shr(1)
    stxdw [r7+0x8], r9                      
    stxdw [r10-0x20], r9                    
    ja lbb_3885                                     if true { pc += 16 }
lbb_3869:
    mov64 r2, r4                                    r2 = r4
    mov64 r6, r5                                    r6 = r5
    mov64 r9, r7                                    r9 = r7
    mov64 r7, r4                                    r7 = r4
    call function_17448                     
    mov64 r4, r7                                    r4 = r7
    mov64 r7, r9                                    r7 = r9
    mov64 r5, r6                                    r5 = r6
    mov64 r9, r0                                    r9 = r0
lbb_3878:
    stxdw [r7+0x8], r9                      
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 6                                     r3 = 6 as i32 as i64 as u64
    jgt r9, r8, lbb_4122                            if r9 > r8 { pc += 240 }
    mov64 r1, r8                                    r1 = r8
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
    stxdw [r10-0x20], r1                    
lbb_3885:
    ldxdw r1, [r10-0x8]                     
    ldxdw r6, [r1+0x20]                     
    lddw r1, 0x9184e729fff                          r1 load str located at 9999999999999
    stxdw [r10-0x10], r5                    
    stxdw [r10-0x18], r4                    
    jgt r9, r1, lbb_3893                            if r9 > r1 { pc += 1 }
    jlt r6, 1000000, lbb_3908                       if r6 < (1000000 as i32 as i64 as u64) { pc += 15 }
lbb_3893:
    mov64 r1, r9                                    r1 = r9
    call function_22513                     
    stxdw [r10-0x28], r0                    
    mov64 r1, r6                                    r1 = r6
    call function_22513                     
    ldxdw r1, [r10-0x28]                    
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x412e848000000000                     r2 load str located at 4696837146684686336
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    call function_17383                     
    ja lbb_3911                                     if true { pc += 3 }
lbb_3908:
    mov64 r0, r6                                    r0 = r6
    mul64 r0, r9                                    r0 *= r9   ///  r0 = r0.wrapping_mul(r9)
    div64 r0, 1000000                               r0 /= 1000000   ///  r0 = r0 / (1000000 as u64)
lbb_3911:
    stxdw [r7+0x10], r0                     
    sub64 r8, r9                                    r8 -= r9   ///  r8 = r8.wrapping_sub(r9)
    lddw r1, 0x9184e729fff                          r1 load str located at 9999999999999
    jgt r8, r1, lbb_3917                            if r8 > r1 { pc += 1 }
    jlt r6, 1000000, lbb_3933                       if r6 < (1000000 as i32 as i64 as u64) { pc += 16 }
lbb_3917:
    mov64 r1, r8                                    r1 = r8
    call function_22513                     
    stxdw [r10-0x28], r0                    
    mov64 r1, r6                                    r1 = r6
    call function_22513                     
    ldxdw r1, [r10-0x28]                    
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x412e848000000000                     r2 load str located at 4696837146684686336
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    call function_17383                     
    mov64 r6, r0                                    r6 = r0
    ja lbb_3935                                     if true { pc += 2 }
lbb_3933:
    mul64 r6, r8                                    r6 *= r8   ///  r6 = r6.wrapping_mul(r8)
    div64 r6, 1000000                               r6 /= 1000000   ///  r6 = r6 / (1000000 as u64)
lbb_3935:
    ldxdw r5, [r10-0x10]                    
    ldxdw r2, [r10-0x18]                    
    stxdw [r7+0x18], r6                     
    ldxdw r1, [r10-0x8]                     
    ldxdw r6, [r1+0x30]                     
    ldxdw r1, [r1+0x28]                     
    jne r1, 0, lbb_3966                             if r1 != (0 as i32 as i64 as u64) { pc += 24 }
    lddw r1, 0x9184e729fff                          r1 load str located at 9999999999999
    jgt r9, r1, lbb_3946                            if r9 > r1 { pc += 1 }
    jlt r6, 1000000, lbb_3973                       if r6 < (1000000 as i32 as i64 as u64) { pc += 27 }
lbb_3946:
    mov64 r1, r9                                    r1 = r9
    stxdw [r10-0x28], r6                    
    call function_22513                     
    mov64 r6, r0                                    r6 = r0
    ldxdw r1, [r10-0x28]                    
    call function_22513                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x412e848000000000                     r2 load str located at 4696837146684686336
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    call function_17383                     
    ldxdw r6, [r10-0x28]                    
    stxdw [r7+0x20], r0                     
    ldxdw r1, [r10-0x20]                    
    jeq r9, r1, lbb_4001                            if r9 == r1 { pc += 36 }
    ja lbb_3979                                     if true { pc += 13 }
lbb_3966:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 10                                    r3 = 10 as i32 as i64 as u64
    jeq r6, 0, lbb_3970                             if r6 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4122                                     if true { pc += 152 }
lbb_3970:
    call function_17448                     
    stxdw [r7+0x20], r0                     
    ja lbb_4001                                     if true { pc += 28 }
lbb_3973:
    mov64 r0, r6                                    r0 = r6
    mul64 r0, r9                                    r0 *= r9   ///  r0 = r0.wrapping_mul(r9)
    div64 r0, 1000000                               r0 /= 1000000   ///  r0 = r0 / (1000000 as u64)
    stxdw [r7+0x20], r0                     
    ldxdw r1, [r10-0x20]                    
    jeq r9, r1, lbb_4001                            if r9 == r1 { pc += 22 }
lbb_3979:
    lddw r1, 0x9184e729fff                          r1 load str located at 9999999999999
    jgt r8, r1, lbb_3983                            if r8 > r1 { pc += 1 }
    jlt r6, 1000000, lbb_3998                       if r6 < (1000000 as i32 as i64 as u64) { pc += 15 }
lbb_3983:
    mov64 r1, r8                                    r1 = r8
    call function_22513                     
    stxdw [r10-0x28], r0                    
    mov64 r1, r6                                    r1 = r6
    call function_22513                     
    ldxdw r1, [r10-0x28]                    
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x412e848000000000                     r2 load str located at 4696837146684686336
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    call function_17383                     
    ja lbb_4001                                     if true { pc += 3 }
lbb_3998:
    mul64 r6, r8                                    r6 *= r8   ///  r6 = r6.wrapping_mul(r8)
    div64 r6, 1000000                               r6 /= 1000000   ///  r6 = r6 / (1000000 as u64)
    mov64 r0, r6                                    r0 = r6
lbb_4001:
    stxdw [r7+0x28], r0                     
    ldxdw r1, [r10-0x8]                     
    ldxdw r6, [r1+0x38]                     
    lddw r1, 0x9184e729fff                          r1 load str located at 9999999999999
    jgt r9, r1, lbb_4008                            if r9 > r1 { pc += 1 }
    jlt r6, 1000000, lbb_4023                       if r6 < (1000000 as i32 as i64 as u64) { pc += 15 }
lbb_4008:
    mov64 r1, r9                                    r1 = r9
    call function_22513                     
    stxdw [r10-0x28], r0                    
    mov64 r1, r6                                    r1 = r6
    call function_22513                     
    ldxdw r1, [r10-0x28]                    
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x412e848000000000                     r2 load str located at 4696837146684686336
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    call function_17383                     
    ja lbb_4026                                     if true { pc += 3 }
lbb_4023:
    mov64 r0, r6                                    r0 = r6
    mul64 r0, r9                                    r0 *= r9   ///  r0 = r0.wrapping_mul(r9)
    div64 r0, 1000000                               r0 /= 1000000   ///  r0 = r0 / (1000000 as u64)
lbb_4026:
    stxdw [r7+0x30], r0                     
    lddw r1, 0x9184e729fff                          r1 load str located at 9999999999999
    jgt r8, r1, lbb_4031                            if r8 > r1 { pc += 1 }
    jlt r6, 1000000, lbb_4047                       if r6 < (1000000 as i32 as i64 as u64) { pc += 16 }
lbb_4031:
    mov64 r1, r8                                    r1 = r8
    call function_22513                     
    stxdw [r10-0x28], r0                    
    mov64 r1, r6                                    r1 = r6
    call function_22513                     
    ldxdw r1, [r10-0x28]                    
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x412e848000000000                     r2 load str located at 4696837146684686336
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    call function_17383                     
    mov64 r6, r0                                    r6 = r0
    ja lbb_4049                                     if true { pc += 2 }
lbb_4047:
    mul64 r6, r8                                    r6 *= r8   ///  r6 = r6.wrapping_mul(r8)
    div64 r6, 1000000                               r6 /= 1000000   ///  r6 = r6 / (1000000 as u64)
lbb_4049:
    ldxdw r5, [r10-0x10]                    
    ldxdw r2, [r10-0x18]                    
    stxdw [r7+0x38], r6                     
    ldxdw r1, [r10-0x8]                     
    ldxdw r4, [r1+0x48]                     
    ldxdw r1, [r1+0x40]                     
    jne r1, 0, lbb_4080                             if r1 != (0 as i32 as i64 as u64) { pc += 24 }
    lddw r1, 0x9184e729fff                          r1 load str located at 9999999999999
    jgt r9, r1, lbb_4060                            if r9 > r1 { pc += 1 }
    jlt r4, 1000000, lbb_4089                       if r4 < (1000000 as i32 as i64 as u64) { pc += 29 }
lbb_4060:
    mov64 r1, r9                                    r1 = r9
    stxdw [r10-0x8], r9                     
    mov64 r9, r4                                    r9 = r4
    call function_22513                     
    mov64 r6, r0                                    r6 = r0
    mov64 r1, r9                                    r1 = r9
    call function_22513                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x412e848000000000                     r2 load str located at 4696837146684686336
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    call function_17383                     
    mov64 r4, r9                                    r4 = r9
    ldxdw r5, [r10-0x10]                    
    ldxdw r9, [r10-0x8]                     
    ja lbb_4092                                     if true { pc += 12 }
lbb_4080:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 10                                    r3 = 10 as i32 as i64 as u64
    jeq r4, 0, lbb_4084                             if r4 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4122                                     if true { pc += 38 }
lbb_4084:
    mov64 r6, r5                                    r6 = r5
    call function_17448                     
    mov64 r5, r6                                    r5 = r6
    stxdw [r7+0x40], r0                     
    ja lbb_4120                                     if true { pc += 31 }
lbb_4089:
    mov64 r0, r4                                    r0 = r4
    mul64 r0, r9                                    r0 *= r9   ///  r0 = r0.wrapping_mul(r9)
    div64 r0, 1000000                               r0 /= 1000000   ///  r0 = r0 / (1000000 as u64)
lbb_4092:
    stxdw [r7+0x40], r0                     
    ldxdw r1, [r10-0x20]                    
    jeq r9, r1, lbb_4120                            if r9 == r1 { pc += 25 }
    lddw r1, 0x9184e729fff                          r1 load str located at 9999999999999
    jgt r8, r1, lbb_4099                            if r8 > r1 { pc += 1 }
    jlt r4, 1000000, lbb_4117                       if r4 < (1000000 as i32 as i64 as u64) { pc += 18 }
lbb_4099:
    mov64 r1, r8                                    r1 = r8
    mov64 r8, r5                                    r8 = r5
    mov64 r9, r4                                    r9 = r4
    call function_22513                     
    mov64 r6, r0                                    r6 = r0
    mov64 r1, r9                                    r1 = r9
    call function_22513                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x412e848000000000                     r2 load str located at 4696837146684686336
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    call function_17383                     
    mov64 r5, r8                                    r5 = r8
    ja lbb_4120                                     if true { pc += 3 }
lbb_4117:
    mul64 r4, r8                                    r4 *= r8   ///  r4 = r4.wrapping_mul(r8)
    div64 r4, 1000000                               r4 /= 1000000   ///  r4 = r4 / (1000000 as u64)
    mov64 r0, r4                                    r0 = r4
lbb_4120:
    stxdw [r7+0x48], r0                     
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
lbb_4122:
    stxw [r5+0x4], r3                       
    stxw [r5+0x0], r0                       
    exit                                    
lbb_4125:
    lddw r1, 0x10002ecb0 --> b"\x00\x00\x00\x00(\xd3\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00\x9b\x00\x00…        r1 load str located at 4295158960
    call function_20968                     

function_4128:
    mov64 r0, r1                                    r0 = r1
    add64 r0, 472                                   r0 += 472   ///  r0 = r0.wrapping_add(472 as i32 as i64 as u64)
    stxdw [r1+0x328], r2                    
    ldxdw r3, [r1+0x1e0]                    
    jle r3, r2, lbb_4140                            if r3 <= r2 { pc += 7 }
    mov64 r4, r3                                    r4 = r3
    sub64 r4, r2                                    r4 -= r2   ///  r4 = r4.wrapping_sub(r2)
    mov64 r5, 64                                    r5 = 64 as i32 as i64 as u64
    mov64 r8, 32                                    r8 = 32 as i32 as i64 as u64
    mov64 r6, 48                                    r6 = 48 as i32 as i64 as u64
    mov64 r7, 16                                    r7 = 16 as i32 as i64 as u64
    ja lbb_4146                                     if true { pc += 6 }
lbb_4140:
    mov64 r4, r2                                    r4 = r2
    sub64 r4, r3                                    r4 -= r3   ///  r4 = r4.wrapping_sub(r3)
    mov64 r5, 72                                    r5 = 72 as i32 as i64 as u64
    mov64 r8, 40                                    r8 = 40 as i32 as i64 as u64
    mov64 r6, 56                                    r6 = 56 as i32 as i64 as u64
    mov64 r7, 24                                    r7 = 24 as i32 as i64 as u64
lbb_4146:
    mov64 r9, r0                                    r9 = r0
    add64 r9, r7                                    r9 += r7   ///  r9 = r9.wrapping_add(r7)
    mov64 r7, r0                                    r7 = r0
    add64 r7, r6                                    r7 += r6   ///  r7 = r7.wrapping_add(r6)
    mov64 r6, r0                                    r6 = r0
    add64 r6, r5                                    r6 += r5   ///  r6 = r6.wrapping_add(r5)
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r6, [r6+0x0]                      
    ldxdw r7, [r7+0x0]                      
    ldxdw r9, [r9+0x0]                      
    jle r4, r9, lbb_4160                            if r4 <= r9 { pc += 3 }
    add64 r0, r8                                    r0 += r8   ///  r0 = r0.wrapping_add(r8)
    ldxdw r0, [r0+0x0]                      
    jne r0, 0, lbb_4178                             if r0 != (0 as i32 as i64 as u64) { pc += 18 }
lbb_4160:
    jle r4, r7, lbb_4173                            if r4 <= r7 { pc += 12 }
    jne r6, 0, lbb_4163                             if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4173                                     if true { pc += 10 }
lbb_4163:
    sub64 r4, r7                                    r4 -= r7   ///  r4 = r4.wrapping_sub(r7)
    div64 r4, r6                                    r4 /= r6   ///  r4 = r4 / r6
    mov64 r0, r5                                    r0 = r5
    add64 r0, r4                                    r0 += r4   ///  r0 = r0.wrapping_add(r4)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r0, r5, lbb_4170                            if r0 < r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_4170:
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jne r4, 0, lbb_4198                             if r4 != (0 as i32 as i64 as u64) { pc += 26 }
    mov64 r5, r0                                    r5 = r0
lbb_4173:
    jgt r5, 2147483647, lbb_4188                    if r5 > (2147483647 as i32 as i64 as u64) { pc += 14 }
    jgt r3, r2, lbb_4176                            if r3 > r2 { pc += 1 }
    neg64 r5                                        r5 = -r5   ///  r5 = (r5 as i64).wrapping_neg() as u64
lbb_4176:
    stxw [r1+0x330], r5                     
    exit                                    
lbb_4178:
    jge r7, r9, lbb_4182                            if r7 >= r9 { pc += 3 }
    lddw r1, 0x10002ece0 --> b"\x00\x00\x00\x00(\xd3\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00\x1e\x01\x00…        r1 load str located at 4295159008
    call function_20957                     
lbb_4182:
    mov64 r5, r4                                    r5 = r4
    jlt r4, r7, lbb_4185                            if r4 < r7 { pc += 1 }
    mov64 r5, r7                                    r5 = r7
lbb_4185:
    sub64 r5, r9                                    r5 -= r9   ///  r5 = r5.wrapping_sub(r9)
    div64 r5, r0                                    r5 /= r0   ///  r5 = r5 / r0
    ja lbb_4160                                     if true { pc += -28 }
lbb_4188:
    mov64 r3, r10                                   r3 = r10
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    lddw r1, 0x10002d346 --> b"overflow is a config error"        r1 load str located at 4295152454
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    lddw r4, 0x10002ed10 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\…        r4 load str located at 4295159056
    lddw r5, 0x10002ed30 --> b"\x00\x00\x00\x00(\xd3\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00&\x01\x00\x0…        r5 load str located at 4295159088
    call function_18742                     
lbb_4198:
    lddw r1, 0x10002ecf8 --> b"\x00\x00\x00\x00(\xd3\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00#\x01\x00\x0…        r1 load str located at 4295159032
    call function_20946                     

function_4201:
    mov64 r7, r4                                    r7 = r4
    mov64 r9, r3                                    r9 = r3
    mov64 r8, r2                                    r8 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r8+0x0]                      
    stxdw [r10-0x8], r1                     
    mov64 r2, r9                                    r2 = r9
    call function_17448                     
    stxdw [r6+0x0], r0                      
    ldxdw r1, [r8+0x38]                     
    stxdw [r10-0x10], r1                    
    mov64 r2, r9                                    r2 = r9
    call function_17448                     
    stxdw [r6+0x40], r0                     
    ldxdw r1, [r8+0x70]                     
    stxdw [r10-0x18], r1                    
    mov64 r2, r9                                    r2 = r9
    call function_17448                     
    stxdw [r6+0x80], r0                     
    ldxdw r1, [r8+0xa8]                     
    stxdw [r10-0x20], r1                    
    mov64 r2, r9                                    r2 = r9
    call function_17448                     
    stxdw [r6+0xc0], r0                     
    ldxdw r1, [r8+0xe0]                     
    stxdw [r10-0x28], r1                    
    mov64 r2, r9                                    r2 = r9
    call function_17448                     
    stxdw [r6+0x100], r0                    
    ldxdw r1, [r8+0x118]                    
    stxdw [r10-0x30], r1                    
    mov64 r2, r9                                    r2 = r9
    call function_17448                     
    stxdw [r6+0x140], r0                    
    ldxdw r1, [r8+0x150]                    
    stxdw [r10-0x38], r1                    
    mov64 r2, r9                                    r2 = r9
    call function_17448                     
    stxdw [r6+0x180], r0                    
    ldxdw r1, [r8+0x188]                    
    stxdw [r10-0x40], r1                    
    mov64 r2, r9                                    r2 = r9
    call function_17448                     
    stxdw [r6+0x1c0], r0                    
    ldxdw r1, [r8+0x1c0]                    
    stxdw [r10-0x48], r1                    
    mov64 r2, r9                                    r2 = r9
    call function_17448                     
    stxdw [r6+0x200], r0                    
    ldxdw r8, [r8+0x1f8]                    
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r9                                    r2 = r9
    call function_17448                     
    stxdw [r6+0x240], r0                    
    ldxdw r1, [r10-0x8]                     
    mov64 r2, r7                                    r2 = r7
    call function_17448                     
    stxdw [r6+0x280], r0                    
    ldxdw r1, [r10-0x10]                    
    mov64 r2, r7                                    r2 = r7
    call function_17448                     
    stxdw [r6+0x2c0], r0                    
    ldxdw r1, [r10-0x18]                    
    mov64 r2, r7                                    r2 = r7
    call function_17448                     
    stxdw [r6+0x300], r0                    
    ldxdw r1, [r10-0x20]                    
    mov64 r2, r7                                    r2 = r7
    call function_17448                     
    stxdw [r6+0x340], r0                    
    ldxdw r1, [r10-0x28]                    
    mov64 r2, r7                                    r2 = r7
    call function_17448                     
    stxdw [r6+0x380], r0                    
    ldxdw r1, [r10-0x30]                    
    mov64 r2, r7                                    r2 = r7
    call function_17448                     
    stxdw [r6+0x3c0], r0                    
    ldxdw r1, [r10-0x38]                    
    mov64 r2, r7                                    r2 = r7
    call function_17448                     
    stxdw [r6+0x400], r0                    
    ldxdw r1, [r10-0x40]                    
    mov64 r2, r7                                    r2 = r7
    call function_17448                     
    stxdw [r6+0x440], r0                    
    ldxdw r1, [r10-0x48]                    
    mov64 r2, r7                                    r2 = r7
    call function_17448                     
    stxdw [r6+0x480], r0                    
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r7                                    r2 = r7
    call function_17448                     
    stxdw [r6+0x4c0], r0                    
    exit                                    

function_4296:
    mov64 r0, r2                                    r0 = r2
    lddw r2, 0x9184e729fff                          r2 load str located at 9999999999999
    jgt r1, r2, lbb_4301                            if r1 > r2 { pc += 1 }
    jlt r0, 1000000, lbb_4316                       if r0 < (1000000 as i32 as i64 as u64) { pc += 15 }
lbb_4301:
    mov64 r6, r0                                    r6 = r0
    call function_22513                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r6                                    r1 = r6
    call function_22513                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x412e848000000000                     r2 load str located at 4696837146684686336
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    call function_17383                     
    ja lbb_4318                                     if true { pc += 2 }
lbb_4316:
    mul64 r0, r1                                    r0 *= r1   ///  r0 = r0.wrapping_mul(r1)
    div64 r0, 1000000                               r0 /= 1000000   ///  r0 = r0 / (1000000 as u64)
lbb_4318:
    exit                                    

function_4319:
    mov64 r6, r3                                    r6 = r3
    mov64 r7, r2                                    r7 = r2
    mov64 r5, r1                                    r5 = r1
    stxdw [r10-0x8], r4                     
    stxdw [r7+0x50], r4                     
    ldxdw r8, [r6+0x0]                      
    ldxdw r9, [r6+0x8]                      
    jeq r9, -1, lbb_4337                            if r9 == (-1 as i32 as i64 as u64) { pc += 10 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 10                                    r3 = 10 as i32 as i64 as u64
    jeq r8, -1, lbb_4331                            if r8 == (-1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4615                                     if true { pc += 284 }
lbb_4331:
    mov64 r1, r9                                    r1 = r9
    ldxdw r2, [r10-0x8]                     
    mov64 r8, r5                                    r8 = r5
    call function_17448                     
    mov64 r5, r8                                    r5 = r8
    mov64 r8, r0                                    r8 = r0
lbb_4337:
    stxdw [r7+0x0], r8                      
    ldxdw r4, [r6+0x10]                     
    ldxdw r1, [r6+0x18]                     
    jeq r1, -1, lbb_4360                            if r1 == (-1 as i32 as i64 as u64) { pc += 19 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 10                                    r3 = 10 as i32 as i64 as u64
    jeq r4, -1, lbb_4345                            if r4 == (-1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4615                                     if true { pc += 270 }
lbb_4345:
    jslt r1, 0, lbb_4618                            if (r1 as i64) < (0 as i32 as i64) { pc += 272 }
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    jeq r2, r9, lbb_4350                            if r2 == r9 { pc += 1 }
    ja lbb_4355                                     if true { pc += 5 }
lbb_4350:
    mov64 r4, r8                                    r4 = r8
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    stxdw [r7+0x8], r4                      
    stxdw [r10-0x20], r4                    
    ja lbb_4367                                     if true { pc += 12 }
lbb_4355:
    ldxdw r2, [r10-0x8]                     
    mov64 r9, r5                                    r9 = r5
    call function_17448                     
    mov64 r5, r9                                    r5 = r9
    mov64 r4, r0                                    r4 = r0
lbb_4360:
    stxdw [r7+0x8], r4                      
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 6                                     r3 = 6 as i32 as i64 as u64
    jgt r4, r8, lbb_4615                            if r4 > r8 { pc += 251 }
    mov64 r1, r8                                    r1 = r8
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
    stxdw [r10-0x20], r1                    
lbb_4367:
    ldxdw r9, [r6+0x20]                     
    lddw r1, 0x9184e729fff                          r1 load str located at 9999999999999
    stxdw [r10-0x10], r4                    
    stxdw [r10-0x18], r5                    
    jgt r4, r1, lbb_4374                            if r4 > r1 { pc += 1 }
    jlt r9, 1000000, lbb_4390                       if r9 < (1000000 as i32 as i64 as u64) { pc += 16 }
lbb_4374:
    mov64 r1, r4                                    r1 = r4
    call function_22513                     
    stxdw [r10-0x28], r0                    
    mov64 r1, r9                                    r1 = r9
    call function_22513                     
    ldxdw r1, [r10-0x28]                    
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x412e848000000000                     r2 load str located at 4696837146684686336
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    call function_17383                     
    ldxdw r4, [r10-0x10]                    
    ja lbb_4393                                     if true { pc += 3 }
lbb_4390:
    mov64 r0, r9                                    r0 = r9
    mul64 r0, r4                                    r0 *= r4   ///  r0 = r0.wrapping_mul(r4)
    div64 r0, 1000000                               r0 /= 1000000   ///  r0 = r0 / (1000000 as u64)
lbb_4393:
    stxdw [r7+0x10], r0                     
    sub64 r8, r4                                    r8 -= r4   ///  r8 = r8.wrapping_sub(r4)
    lddw r1, 0x9184e729fff                          r1 load str located at 9999999999999
    jgt r8, r1, lbb_4399                            if r8 > r1 { pc += 1 }
    jlt r9, 1000000, lbb_4416                       if r9 < (1000000 as i32 as i64 as u64) { pc += 17 }
lbb_4399:
    mov64 r1, r8                                    r1 = r8
    call function_22513                     
    stxdw [r10-0x28], r0                    
    mov64 r1, r9                                    r1 = r9
    call function_22513                     
    ldxdw r1, [r10-0x28]                    
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x412e848000000000                     r2 load str located at 4696837146684686336
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    call function_17383                     
    ldxdw r4, [r10-0x10]                    
    mov64 r9, r0                                    r9 = r0
    ja lbb_4418                                     if true { pc += 2 }
lbb_4416:
    mul64 r9, r8                                    r9 *= r8   ///  r9 = r9.wrapping_mul(r8)
    div64 r9, 1000000                               r9 /= 1000000   ///  r9 = r9 / (1000000 as u64)
lbb_4418:
    ldxdw r5, [r10-0x18]                    
    stxdw [r7+0x18], r9                     
    ldxdw r9, [r6+0x30]                     
    ldxdw r1, [r6+0x28]                     
    jne r1, 0, lbb_4445                             if r1 != (0 as i32 as i64 as u64) { pc += 22 }
    jeq r9, 0, lbb_4459                             if r9 == (0 as i32 as i64 as u64) { pc += 35 }
    lddw r1, 0x9184e729fff                          r1 load str located at 9999999999999
    jgt r4, r1, lbb_4428                            if r4 > r1 { pc += 1 }
    jlt r9, 1000000, lbb_4462                       if r9 < (1000000 as i32 as i64 as u64) { pc += 34 }
lbb_4428:
    mov64 r1, r4                                    r1 = r4
    call function_22513                     
    stxdw [r10-0x28], r0                    
    mov64 r1, r9                                    r1 = r9
    call function_22513                     
    ldxdw r1, [r10-0x28]                    
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x412e848000000000                     r2 load str located at 4696837146684686336
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    call function_17383                     
    ldxdw r4, [r10-0x10]                    
    ldxdw r5, [r10-0x18]                    
    ja lbb_4465                                     if true { pc += 20 }
lbb_4445:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 10                                    r3 = 10 as i32 as i64 as u64
    jeq r9, 0, lbb_4449                             if r9 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4615                                     if true { pc += 166 }
lbb_4449:
    ldxdw r2, [r10-0x8]                     
    call function_17448                     
    mov64 r1, r0                                    r1 = r0
    div64 r1, 1000                                  r1 /= 1000   ///  r1 = r1 / (1000 as u64)
    jgt r0, 1999, lbb_4455                          if r0 > (1999 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_4455:
    stxdw [r7+0x60], r1                     
    stxdw [r7+0x58], r1                     
    ldxdw r4, [r10-0x10]                    
    ja lbb_4501                                     if true { pc += 42 }
lbb_4459:
    stdw [r7+0x60], 0                       
    stdw [r7+0x58], 0                       
    ja lbb_4501                                     if true { pc += 39 }
lbb_4462:
    mov64 r0, r9                                    r0 = r9
    mul64 r0, r4                                    r0 *= r4   ///  r0 = r0.wrapping_mul(r4)
    div64 r0, 1000000                               r0 /= 1000000   ///  r0 = r0 / (1000000 as u64)
lbb_4465:
    mov64 r1, r0                                    r1 = r0
    div64 r1, 1000                                  r1 /= 1000   ///  r1 = r1 / (1000 as u64)
    jgt r0, 1999, lbb_4469                          if r0 > (1999 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_4469:
    stxdw [r7+0x58], r1                     
    ldxdw r2, [r10-0x20]                    
    jeq r4, r2, lbb_4500                            if r4 == r2 { pc += 28 }
    lddw r1, 0x9184e729fff                          r1 load str located at 9999999999999
    jgt r8, r1, lbb_4476                            if r8 > r1 { pc += 1 }
    jlt r9, 1000000, lbb_4494                       if r9 < (1000000 as i32 as i64 as u64) { pc += 18 }
lbb_4476:
    mov64 r1, r8                                    r1 = r8
    call function_22513                     
    mov64 r1, r9                                    r1 = r9
    mov64 r9, r0                                    r9 = r0
    call function_22513                     
    mov64 r1, r9                                    r1 = r9
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x412e848000000000                     r2 load str located at 4696837146684686336
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    call function_17383                     
    ldxdw r4, [r10-0x10]                    
    ldxdw r5, [r10-0x18]                    
    mov64 r9, r0                                    r9 = r0
    ja lbb_4496                                     if true { pc += 2 }
lbb_4494:
    mul64 r9, r8                                    r9 *= r8   ///  r9 = r9.wrapping_mul(r8)
    div64 r9, 1000000                               r9 /= 1000000   ///  r9 = r9 / (1000000 as u64)
lbb_4496:
    mov64 r1, r9                                    r1 = r9
    div64 r1, 1000                                  r1 /= 1000   ///  r1 = r1 / (1000 as u64)
    jgt r9, 1999, lbb_4500                          if r9 > (1999 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_4500:
    stxdw [r7+0x60], r1                     
lbb_4501:
    ldxdw r9, [r6+0x38]                     
    lddw r1, 0x9184e729fff                          r1 load str located at 9999999999999
    jgt r4, r1, lbb_4506                            if r4 > r1 { pc += 1 }
    jlt r9, 1000000, lbb_4521                       if r9 < (1000000 as i32 as i64 as u64) { pc += 15 }
lbb_4506:
    mov64 r1, r4                                    r1 = r4
    call function_22513                     
    stxdw [r10-0x28], r0                    
    mov64 r1, r9                                    r1 = r9
    call function_22513                     
    ldxdw r1, [r10-0x28]                    
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x412e848000000000                     r2 load str located at 4696837146684686336
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    call function_17383                     
    ja lbb_4524                                     if true { pc += 3 }
lbb_4521:
    mov64 r0, r9                                    r0 = r9
    mul64 r0, r4                                    r0 *= r4   ///  r0 = r0.wrapping_mul(r4)
    div64 r0, 1000000                               r0 /= 1000000   ///  r0 = r0 / (1000000 as u64)
lbb_4524:
    stxdw [r7+0x30], r0                     
    lddw r1, 0x9184e729fff                          r1 load str located at 9999999999999
    jgt r8, r1, lbb_4529                            if r8 > r1 { pc += 1 }
    jlt r9, 1000000, lbb_4545                       if r9 < (1000000 as i32 as i64 as u64) { pc += 16 }
lbb_4529:
    mov64 r1, r8                                    r1 = r8
    call function_22513                     
    stxdw [r10-0x28], r0                    
    mov64 r1, r9                                    r1 = r9
    call function_22513                     
    ldxdw r1, [r10-0x28]                    
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x412e848000000000                     r2 load str located at 4696837146684686336
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    call function_17383                     
    mov64 r9, r0                                    r9 = r0
    ja lbb_4547                                     if true { pc += 2 }
lbb_4545:
    mul64 r9, r8                                    r9 *= r8   ///  r9 = r9.wrapping_mul(r8)
    div64 r9, 1000000                               r9 /= 1000000   ///  r9 = r9 / (1000000 as u64)
lbb_4547:
    ldxdw r5, [r10-0x18]                    
    ldxdw r4, [r10-0x10]                    
    stxdw [r7+0x38], r9                     
    ldxdw r9, [r6+0x48]                     
    ldxdw r1, [r6+0x40]                     
    jne r1, 0, lbb_4575                             if r1 != (0 as i32 as i64 as u64) { pc += 22 }
    jeq r9, 0, lbb_4591                             if r9 == (0 as i32 as i64 as u64) { pc += 37 }
    lddw r1, 0x9184e729fff                          r1 load str located at 9999999999999
    jgt r4, r1, lbb_4558                            if r4 > r1 { pc += 1 }
    jlt r9, 1000000, lbb_4594                       if r9 < (1000000 as i32 as i64 as u64) { pc += 36 }
lbb_4558:
    mov64 r1, r4                                    r1 = r4
    call function_22513                     
    mov64 r6, r0                                    r6 = r0
    mov64 r1, r9                                    r1 = r9
    call function_22513                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x412e848000000000                     r2 load str located at 4696837146684686336
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    call function_17383                     
    ldxdw r4, [r10-0x10]                    
    ldxdw r5, [r10-0x18]                    
    ja lbb_4597                                     if true { pc += 22 }
lbb_4575:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 10                                    r3 = 10 as i32 as i64 as u64
    jeq r9, 0, lbb_4579                             if r9 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4615                                     if true { pc += 36 }
lbb_4579:
    mov64 r6, r5                                    r6 = r5
    ldxdw r2, [r10-0x8]                     
    call function_17448                     
    mov64 r1, r0                                    r1 = r0
    div64 r1, 1000                                  r1 /= 1000   ///  r1 = r1 / (1000 as u64)
    jgt r0, 1999, lbb_4586                          if r0 > (1999 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_4586:
    stxdw [r7+0x70], r1                     
    stxdw [r7+0x68], r1                     
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
    mov64 r5, r6                                    r5 = r6
    ja lbb_4615                                     if true { pc += 24 }
lbb_4591:
    stdw [r7+0x70], 0                       
    stdw [r7+0x68], 0                       
    ja lbb_4614                                     if true { pc += 20 }
lbb_4594:
    mov64 r0, r9                                    r0 = r9
    mul64 r0, r4                                    r0 *= r4   ///  r0 = r0.wrapping_mul(r4)
    div64 r0, 1000000                               r0 /= 1000000   ///  r0 = r0 / (1000000 as u64)
lbb_4597:
    mov64 r1, r0                                    r1 = r0
    div64 r1, 1000                                  r1 /= 1000   ///  r1 = r1 / (1000 as u64)
    jgt r0, 1999, lbb_4601                          if r0 > (1999 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_4601:
    stxdw [r7+0x68], r1                     
    ldxdw r3, [r10-0x20]                    
    jeq r4, r3, lbb_4613                            if r4 == r3 { pc += 9 }
    mov64 r6, r5                                    r6 = r5
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r9                                    r2 = r9
    call function_4296                      
    mov64 r1, r0                                    r1 = r0
    div64 r1, 1000                                  r1 /= 1000   ///  r1 = r1 / (1000 as u64)
    jgt r0, 1999, lbb_4612                          if r0 > (1999 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_4612:
    mov64 r5, r6                                    r5 = r6
lbb_4613:
    stxdw [r7+0x70], r1                     
lbb_4614:
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
lbb_4615:
    stxw [r5+0x4], r3                       
    stxw [r5+0x0], r0                       
    exit                                    
lbb_4618:
    lddw r1, 0x10002ed90 --> b"\x00\x00\x00\x00`\xd3\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00\xa6\x00\x00…        r1 load str located at 4295159184
    call function_20968                     

function_4621:
    mov64 r0, r1                                    r0 = r1
    add64 r0, 472                                   r0 += 472   ///  r0 = r0.wrapping_add(472 as i32 as i64 as u64)
    stxdw [r1+0x328], r2                    
    ldxdw r3, [r1+0x1e0]                    
    jle r3, r2, lbb_4633                            if r3 <= r2 { pc += 7 }
    mov64 r4, r3                                    r4 = r3
    sub64 r4, r2                                    r4 -= r2   ///  r4 = r4.wrapping_sub(r2)
    mov64 r5, 104                                   r5 = 104 as i32 as i64 as u64
    mov64 r8, 88                                    r8 = 88 as i32 as i64 as u64
    mov64 r6, 48                                    r6 = 48 as i32 as i64 as u64
    mov64 r7, 16                                    r7 = 16 as i32 as i64 as u64
    ja lbb_4639                                     if true { pc += 6 }
lbb_4633:
    mov64 r4, r2                                    r4 = r2
    sub64 r4, r3                                    r4 -= r3   ///  r4 = r4.wrapping_sub(r3)
    mov64 r5, 112                                   r5 = 112 as i32 as i64 as u64
    mov64 r8, 96                                    r8 = 96 as i32 as i64 as u64
    mov64 r6, 56                                    r6 = 56 as i32 as i64 as u64
    mov64 r7, 24                                    r7 = 24 as i32 as i64 as u64
lbb_4639:
    mov64 r9, r0                                    r9 = r0
    add64 r9, r7                                    r9 += r7   ///  r9 = r9.wrapping_add(r7)
    mov64 r7, r0                                    r7 = r0
    add64 r7, r6                                    r7 += r6   ///  r7 = r7.wrapping_add(r6)
    mov64 r6, r0                                    r6 = r0
    add64 r6, r5                                    r6 += r5   ///  r6 = r6.wrapping_add(r5)
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r6, [r6+0x0]                      
    ldxdw r7, [r7+0x0]                      
    ldxdw r9, [r9+0x0]                      
    jle r4, r9, lbb_4653                            if r4 <= r9 { pc += 3 }
    add64 r0, r8                                    r0 += r8   ///  r0 = r0.wrapping_add(r8)
    ldxdw r0, [r0+0x0]                      
    jne r0, 0, lbb_4671                             if r0 != (0 as i32 as i64 as u64) { pc += 18 }
lbb_4653:
    jle r4, r7, lbb_4666                            if r4 <= r7 { pc += 12 }
    jne r6, 0, lbb_4656                             if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4666                                     if true { pc += 10 }
lbb_4656:
    sub64 r4, r7                                    r4 -= r7   ///  r4 = r4.wrapping_sub(r7)
    div64 r4, r6                                    r4 /= r6   ///  r4 = r4 / r6
    mov64 r0, r5                                    r0 = r5
    add64 r0, r4                                    r0 += r4   ///  r0 = r0.wrapping_add(r4)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r0, r5, lbb_4663                            if r0 < r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_4663:
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jne r4, 0, lbb_4691                             if r4 != (0 as i32 as i64 as u64) { pc += 26 }
    mov64 r5, r0                                    r5 = r0
lbb_4666:
    jgt r5, 2147483647, lbb_4681                    if r5 > (2147483647 as i32 as i64 as u64) { pc += 14 }
    jgt r3, r2, lbb_4669                            if r3 > r2 { pc += 1 }
    neg64 r5                                        r5 = -r5   ///  r5 = (r5 as i64).wrapping_neg() as u64
lbb_4669:
    stxw [r1+0x358], r5                     
    exit                                    
lbb_4671:
    jge r7, r9, lbb_4675                            if r7 >= r9 { pc += 3 }
    lddw r1, 0x10002eda8 --> b"\x00\x00\x00\x00`\xd3\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x005\x01\x00\x0…        r1 load str located at 4295159208
    call function_20957                     
lbb_4675:
    mov64 r5, r4                                    r5 = r4
    jlt r4, r7, lbb_4678                            if r4 < r7 { pc += 1 }
    mov64 r5, r7                                    r5 = r7
lbb_4678:
    sub64 r5, r9                                    r5 -= r9   ///  r5 = r5.wrapping_sub(r9)
    div64 r5, r0                                    r5 /= r0   ///  r5 = r5 / r0
    ja lbb_4653                                     if true { pc += -28 }
lbb_4681:
    mov64 r3, r10                                   r3 = r10
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    lddw r1, 0x10002d346 --> b"overflow is a config error"        r1 load str located at 4295152454
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    lddw r4, 0x10002ed10 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\…        r4 load str located at 4295159056
    lddw r5, 0x10002edd8 --> b"\x00\x00\x00\x00`\xd3\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00A\x01\x00\x0…        r5 load str located at 4295159256
    call function_18742                     
lbb_4691:
    lddw r1, 0x10002edc0 --> b"\x00\x00\x00\x00`\xd3\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00:\x01\x00\x0…        r1 load str located at 4295159232
    call function_20946                     

function_4694:
    mov64 r6, r5                                    r6 = r5
    mov64 r9, r4                                    r9 = r4
    mov64 r7, r3                                    r7 = r3
    stxdw [r10-0x108], r2                   
    mov64 r8, r1                                    r8 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -152                                  r1 += -152   ///  r1 = r1.wrapping_add(-152 as i32 as i64 as u64)
    call function_18076                     
    ldxw r1, [r10-0x98]                     
    jne r1, 0, lbb_4754                             if r1 != (0 as i32 as i64 as u64) { pc += 50 }
    ldxdw r1, [r6-0xff0]                    
    stxdw [r10-0x118], r1                   
    ldxdw r1, [r6-0xff8]                    
    stxdw [r10-0x120], r1                   
    ldxdw r1, [r6-0x1000]                   
    stxdw [r10-0x110], r1                   
    mov64 r2, r7                                    r2 = r7
    add64 r2, 128                                   r2 += 128   ///  r2 = r2.wrapping_add(128 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -256                                  r1 += -256   ///  r1 = r1.wrapping_add(-256 as i32 as i64 as u64)
    ldxdw r4, [r10-0x90]                    
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r1, [r10-0xf8]                    
    jne r1, 0, lbb_4722                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_4722:
    ldxdw r1, [r10-0x100]                   
    ldxdw r6, [r10-0x88]                    
    lddw r3, 0x4000000000000000                     r3 load str located at 4611686018427387904
    jeq r6, r3, lbb_4757                            if r6 == r3 { pc += 30 }
    stxdw [r10-0x128], r8                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_4872                             if r2 != (0 as i32 as i64 as u64) { pc += 142 }
    call function_22513                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r6, r0                                    r6 = r0
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x130], r1                   
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_22511                     
    mov64 r8, r0                                    r8 = r0
    mov64 r1, r6                                    r1 = r6
    call function_22905                     
    jslt r8, 0, lbb_4745                            if (r8 as i64) < (0 as i32 as i64) { pc += 1 }
    stxdw [r10-0x130], r0                   
lbb_4745:
    mov64 r1, r6                                    r1 = r6
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_22929                     
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
    ldxdw r8, [r10-0x128]                   
    jsgt r0, 0, lbb_4761                            if (r0 as i64) > (0 as i32 as i64) { pc += 9 }
    ldxdw r1, [r10-0x130]                   
    ja lbb_4761                                     if true { pc += 7 }
lbb_4754:
    ldxw r2, [r10-0x90]                     
    ldxw r1, [r10-0x94]                     
    ja lbb_4866                                     if true { pc += 109 }
lbb_4757:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_4869                             if r2 != (0 as i32 as i64 as u64) { pc += 110 }
    jslt r1, 0, lbb_4869                            if (r1 as i64) < (0 as i32 as i64) { pc += 109 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
lbb_4761:
    ldxdw r2, [r10-0x110]                   
    ldxdw r4, [r2+0x0]                      
    ldxdw r2, [r10-0x108]                   
    ldxdw r3, [r2+0x0]                      
    mov64 r2, r3                                    r2 = r3
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xe0], r2                    
    mov64 r5, r4                                    r5 = r4
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xf0], r5                    
    sth [r10-0xd8], 257                     
    sth [r10-0xe8], 257                     
    stxdw [r10-0xc0], r7                    
    stxdw [r10-0xc8], r1                    
    stw [r10-0xcc], 0                       
    ldxdw r1, [r9+0x0]                      
    stxdw [r10-0xb8], r1                    
    ldxdw r1, [r9+0x8]                      
    stxdw [r10-0xb0], r1                    
    ldxdw r1, [r9+0x10]                     
    stxdw [r10-0xa8], r1                    
    ldxdw r1, [r9+0x18]                     
    stxdw [r10-0xa0], r1                    
    ldxb r1, [r4+0x0]                       
    jne r1, 255, lbb_4865                           if r1 != (255 as i32 as i64 as u64) { pc += 79 }
    ldxb r6, [r4+0x1]                       
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r6, 0, lbb_4791                             if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_4791:
    ldxb r7, [r4+0x2]                       
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jne r7, 0, lbb_4795                             if r7 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_4795:
    mov64 r9, r8                                    r9 = r8
    ldxb r7, [r4+0x3]                       
    jne r7, 0, lbb_4799                             if r7 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_4799:
    ldxdw r7, [r4+0x50]                     
    mov64 r8, r4                                    r8 = r4
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r8                    
    mov64 r8, r4                                    r8 = r4
    add64 r8, 88                                    r8 += 88   ///  r8 = r8.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r8                    
    stxdw [r10-0x88], r7                    
    add64 r4, 72                                    r4 += 72   ///  r4 = r4.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r4                    
    stxdw [r10-0x98], r5                    
    stxb [r10-0x66], r0                     
    stxb [r10-0x67], r6                     
    stxb [r10-0x68], r1                     
    stdw [r10-0x70], 0                      
    ldxb r1, [r3+0x0]                       
    mov64 r8, r9                                    r8 = r9
    jne r1, 255, lbb_4865                           if r1 != (255 as i32 as i64 as u64) { pc += 48 }
    ldxb r5, [r3+0x1]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r5, 0, lbb_4822                             if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_4822:
    ldxb r0, [r3+0x2]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_4826                             if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_4826:
    ldxb r0, [r3+0x3]                       
    jne r0, 0, lbb_4829                             if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_4829:
    ldxdw r0, [r3+0x50]                     
    mov64 r6, r3                                    r6 = r3
    add64 r6, 40                                    r6 += 40   ///  r6 = r6.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r6                    
    mov64 r6, r3                                    r6 = r3
    add64 r6, 88                                    r6 += 88   ///  r6 = r6.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r6                    
    stxdw [r10-0x50], r0                    
    add64 r3, 72                                    r3 += 72   ///  r3 = r3.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r3                    
    stxdw [r10-0x60], r2                    
    stxb [r10-0x2e], r4                     
    stxb [r10-0x2f], r5                     
    stxb [r10-0x30], r1                     
    stdw [r10-0x38], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -204                                  r1 += -204   ///  r1 = r1.wrapping_add(-204 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -240                                  r1 += -240   ///  r1 = r1.wrapping_add(-240 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x10002cf48 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295151432
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 52                      
    stdw [r10-0x18], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -152                                  r2 += -152   ///  r2 = r2.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    ldxdw r4, [r10-0x120]                   
    ldxdw r5, [r10-0x118]                   
    syscall [invalid]                       
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ja lbb_4866                                     if true { pc += 1 }
lbb_4865:
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
lbb_4866:
    stxw [r8+0x4], r2                       
    stxw [r8+0x0], r1                       
    exit                                    
lbb_4869:
    lddw r1, 0x10002ec80 --> b"\x00\x00\x00\x00\x81\xd2\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xaf\x00\…        r1 load str located at 4295158912
    call function_20968                     
lbb_4872:
    lddw r1, 0x10002ec68 --> b"\x00\x00\x00\x00\x81\xd2\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xb2\x00\…        r1 load str located at 4295158888
    call function_20968                     

function_4875:
    stxdw [r10-0x298], r4                   
    mov64 r8, r3                                    r8 = r3
    stxdw [r10-0x288], r2                   
    stxdw [r10-0x280], r1                   
    ldxdw r7, [r8+0x38]                     
    ldxdw r1, [r7+0x0]                      
    stxdw [r10-0x290], r1                   
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x10002cf48 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r2 load str located at 4295151432
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r6, 8                                     r6 = 8 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_5244                             if r0 != (0 as i32 as i64 as u64) { pc += 353 }
    ldxdw r1, [r8+0x28]                     
    stxdw [r10-0x2a8], r1                   
    ldxdw r1, [r1+0x0]                      
    stxdw [r10-0x2a0], r1                   
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x10002cf48 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r2 load str located at 4295151432
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_5244                             if r0 != (0 as i32 as i64 as u64) { pc += 341 }
    ldxdw r1, [r8+0x48]                     
    ldxdw r1, [r1+0x0]                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x2b0], r1                   
    lddw r2, 0x10002cf08 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295151368
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r0, 0, lbb_5244                             if r0 != (0 as i32 as i64 as u64) { pc += 328 }
    ldxdw r1, [r8+0x40]                     
    stxdw [r10-0x2b8], r1                   
    ldxdw r1, [r1+0x0]                      
    stxdw [r10-0x2c0], r1                   
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    ldxdw r2, [r10-0x2b0]                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r6, 22                                    r6 = 22 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_5244                             if r0 != (0 as i32 as i64 as u64) { pc += 316 }
    ldxdw r1, [r8+0x50]                     
    ldxdw r1, [r1+0x0]                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x10002cf48 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r2 load str located at 4295151432
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r0, 0, lbb_5244                             if r0 != (0 as i32 as i64 as u64) { pc += 305 }
    ldxdw r1, [r10-0x2b0]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -536                                  r1 += -536   ///  r1 = r1.wrapping_add(-536 as i32 as i64 as u64)
    ldxdw r2, [r10-0x2b8]                   
    call function_0                         
    ldxw r9, [r10-0x20c]                    
    ldxw r6, [r10-0x210]                    
    ldxdw r1, [r10-0x218]                   
    jeq r1, 0, lbb_5244                             if r1 == (0 as i32 as i64 as u64) { pc += 296 }
    stxdw [r10-0x2e0], r1                   
    ldxdw r1, [r10-0x208]                   
    stxdw [r10-0x2d0], r1                   
    ldxdw r2, [r8+0x18]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -536                                  r1 += -536   ///  r1 = r1.wrapping_add(-536 as i32 as i64 as u64)
    ldxdw r3, [r10-0x288]                   
    call function_2453                      
    lsh64 r9, 32                                    r9 <<= 32   ///  r9 = r9.wrapping_shl(32)
    or64 r9, r6                                     r9 |= r6   ///  r9 = r9.or(r6)
    stxdw [r10-0x2c8], r9                   
    ldxw r9, [r10-0x20c]                    
    ldxw r6, [r10-0x210]                    
    ldxdw r2, [r10-0x218]                   
    jne r2, 0, lbb_4964                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5241                                     if true { pc += 277 }
lbb_4964:
    ldxdw r1, [r10-0x208]                   
    stxdw [r10-0x2d8], r1                   
    ldxdw r1, [r8+0x20]                     
    stxdw [r10-0x2e8], r1                   
    ldxdw r1, [r1+0x0]                      
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    ldxdw r2, [r10-0x2d8]                   
    lsh64 r9, 32                                    r9 <<= 32   ///  r9 = r9.wrapping_shl(32)
    or64 r9, r6                                     r9 |= r6   ///  r9 = r9.or(r6)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r1, r9                                    r1 = r9
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_5240                             if r0 != (0 as i32 as i64 as u64) { pc += 258 }
    ldxdw r3, [r10-0x2e8]                   
    stxdw [r10-0x2f0], r1                   
    ldxdw r3, [r10-0x2a0]                   
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -536                                  r1 += -536   ///  r1 = r1.wrapping_add(-536 as i32 as i64 as u64)
    ldxdw r2, [r10-0x288]                   
    stxdw [r10-0x2a0], r3                   
    call function_2218                      
    ldxdw r1, [r10-0x200]                   
    stxdw [r10-0x248], r1                   
    ldxdw r1, [r10-0x208]                   
    stxdw [r10-0x250], r1                   
    ldxdw r1, [r10-0x210]                   
    stxdw [r10-0x258], r1                   
    ldxdw r1, [r10-0x218]                   
    stxdw [r10-0x260], r1                   
    ldxb r1, [r10-0x1f8]                    
    stxdw [r10-0x2f8], r1                   
    ldxb r1, [r10-0x1d7]                    
    stxb [r10-0x220], r1                    
    ldxdw r1, [r10-0x1df]                   
    stxdw [r10-0x228], r1                   
    ldxdw r1, [r10-0x1e7]                   
    stxdw [r10-0x230], r1                   
    ldxdw r1, [r10-0x1ef]                   
    stxdw [r10-0x238], r1                   
    ldxdw r1, [r10-0x1f7]                   
    stxdw [r10-0x240], r1                   
    ldxdw r1, [r10-0x290]                   
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -608                                  r2 += -608   ///  r2 = r2.wrapping_add(-608 as i32 as i64 as u64)
    stxdw [r10-0x290], r1                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_5238                             if r0 != (0 as i32 as i64 as u64) { pc += 216 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -536                                  r1 += -536   ///  r1 = r1.wrapping_add(-536 as i32 as i64 as u64)
    stxdw [r10-0x170], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -544                                  r1 += -544   ///  r1 = r1.wrapping_add(-544 as i32 as i64 as u64)
    stxdw [r10-0x1f8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -576                                  r1 += -576   ///  r1 = r1.wrapping_add(-576 as i32 as i64 as u64)
    stxdw [r10-0x208], r1                   
    lddw r1, 0x10002d2e8 --> b"VaultInitSettingsprograms/amm/src/dispatcher.rsIns"        r1 load str located at 4295152360
    stxdw [r10-0x218], r1                   
    stdw [r10-0x168], 3                     
    stdw [r10-0x1f0], 1                     
    stdw [r10-0x200], 32                    
    stdw [r10-0x210], 5                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0x2e8]                   
    stxdw [r10-0x1000], r1                  
    stdw [r10-0xff0], 1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -616                                  r1 += -616   ///  r1 = r1.wrapping_add(-616 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 165                                   r3 = 165 as i32 as i64 as u64
    ldxdw r4, [r10-0x2b0]                   
    call function_4694                      
    ldxw r9, [r10-0x264]                    
    ldxw r6, [r10-0x268]                    
    jne r6, 26, lbb_5238                            if r6 != (26 as i32 as i64 as u64) { pc += 184 }
    ldxdw r1, [r10-0x290]                   
    stxdw [r10-0x208], r1                   
    ldxdw r1, [r10-0x2b8]                   
    stxdw [r10-0x210], r1                   
    stxdw [r10-0x218], r7                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -624                                  r1 += -624   ///  r1 = r1.wrapping_add(-624 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -536                                  r2 += -536   ///  r2 = r2.wrapping_add(-536 as i32 as i64 as u64)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_15413                     
    ldxw r9, [r10-0x26c]                    
    ldxw r6, [r10-0x270]                    
    jne r6, 26, lbb_5238                            if r6 != (26 as i32 as i64 as u64) { pc += 169 }
    ldxdw r1, [r10-0x2e8]                   
    stxdw [r10-0x1000], r1                  
    stdw [r10-0xff0], 0                     
    stdw [r10-0xff8], 8                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -632                                  r1 += -632   ///  r1 = r1.wrapping_add(-632 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0x2a8]                   
    mov64 r3, 1072                                  r3 = 1072 as i32 as i64 as u64
    ldxdw r4, [r10-0x288]                   
    call function_4694                      
    ldxw r9, [r10-0x274]                    
    ldxw r6, [r10-0x278]                    
    jne r6, 26, lbb_5238                            if r6 != (26 as i32 as i64 as u64) { pc += 155 }
    mov64 r6, 11                                    r6 = 11 as i32 as i64 as u64
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x2a8]                   
    ldxdw r7, [r1+0x0]                      
    ldxb r1, [r7+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    jne r2, 15, lbb_5238                            if r2 != (15 as i32 as i64 as u64) { pc += 147 }
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r7+0x0], r1                       
    ldxdw r2, [r7+0x50]                     
    mov64 r1, r7                                    r1 = r7
    add64 r1, 88                                    r1 += 88   ///  r1 = r1.wrapping_add(88 as i32 as i64 as u64)
    call function_2080                      
    ldxdw r1, [r8+0x30]                     
    ldxdw r8, [r1+0x0]                      
    ldxdw r1, [r8+0x20]                     
    stxdw [r10-0x200], r1                   
    ldxdw r1, [r8+0x18]                     
    stxdw [r10-0x208], r1                   
    ldxdw r1, [r8+0x10]                     
    stxdw [r10-0x210], r1                   
    ldxdw r1, [r8+0x8]                      
    stxdw [r10-0x218], r1                   
    ldxdw r2, [r10-0x290]                   
    ldxdw r1, [r2+0x18]                     
    stxdw [r10-0x158], r1                   
    ldxdw r1, [r2+0x10]                     
    stxdw [r10-0x160], r1                   
    ldxdw r1, [r2+0x8]                      
    stxdw [r10-0x168], r1                   
    ldxdw r1, [r2+0x0]                      
    stxdw [r10-0x170], r1                   
    ldxdw r2, [r10-0x2a0]                   
    ldxdw r1, [r2+0x18]                     
    stxdw [r10-0x198], r1                   
    ldxdw r1, [r2+0x10]                     
    stxdw [r10-0x1a0], r1                   
    ldxdw r1, [r2+0x8]                      
    stxdw [r10-0x1a8], r1                   
    ldxdw r1, [r2+0x0]                      
    stxdw [r10-0x1b0], r1                   
    ldxdw r2, [r10-0x2c0]                   
    ldxdw r1, [r2+0x20]                     
    stxdw [r10-0x178], r1                   
    ldxdw r1, [r2+0x18]                     
    stxdw [r10-0x180], r1                   
    ldxdw r1, [r2+0x10]                     
    stxdw [r10-0x188], r1                   
    ldxdw r1, [r2+0x8]                      
    stxdw [r10-0x190], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -256                                  r1 += -256   ///  r1 = r1.wrapping_add(-256 as i32 as i64 as u64)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r3, 256                                   r3 = 256 as i32 as i64 as u64
    call function_21517                     
    stdw [r10-0x150], -1                    
    stdw [r10-0x148], -1                    
    stdw [r10-0x140], -1                    
    stdw [r10-0x138], -1                    
    stdw [r10-0x130], 0                     
    stdw [r10-0x128], 0                     
    stdw [r10-0x120], 0                     
    stdw [r10-0x118], 0                     
    stdw [r10-0x110], 0                     
    stdw [r10-0x108], 0                     
    ldxdw r1, [r10-0x2e0]                   
    ldxb r6, [r1+0x2c]                      
    ldxdw r1, [r8+0x8]                      
    stxdw [r10-0x1d0], r1                   
    ldxdw r1, [r8+0x10]                     
    stxdw [r10-0x1c8], r1                   
    ldxdw r1, [r8+0x18]                     
    stxdw [r10-0x1c0], r1                   
    ldxdw r1, [r8+0x20]                     
    stxdw [r10-0x1b8], r1                   
    stdw [r7+0x58], 1                       
    ldxdw r1, [r10-0x200]                   
    stxdw [r7+0x78], r1                     
    ldxdw r1, [r10-0x208]                   
    stxdw [r7+0x70], r1                     
    ldxdw r1, [r10-0x210]                   
    stxdw [r7+0x68], r1                     
    ldxdw r1, [r10-0x218]                   
    stxdw [r7+0x60], r1                     
    ldxdw r1, [r10-0x158]                   
    stxdw [r7+0x98], r1                     
    ldxdw r1, [r10-0x160]                   
    stxdw [r7+0x90], r1                     
    ldxdw r1, [r10-0x168]                   
    stxdw [r7+0x88], r1                     
    ldxdw r1, [r10-0x170]                   
    stxdw [r7+0x80], r1                     
    ldxdw r1, [r10-0x190]                   
    stxdw [r7+0xa0], r1                     
    ldxdw r1, [r10-0x188]                   
    stxdw [r7+0xa8], r1                     
    ldxdw r1, [r10-0x180]                   
    stxdw [r7+0xb0], r1                     
    ldxdw r1, [r10-0x178]                   
    stxdw [r7+0xb8], r1                     
    ldxdw r1, [r10-0x1b0]                   
    stxdw [r7+0xc0], r1                     
    ldxdw r1, [r10-0x1a8]                   
    stxdw [r7+0xc8], r1                     
    ldxdw r1, [r10-0x1a0]                   
    stxdw [r7+0xd0], r1                     
    ldxdw r1, [r10-0x198]                   
    stxdw [r7+0xd8], r1                     
    mov64 r1, r7                                    r1 = r7
    add64 r1, 224                                   r1 += 224   ///  r1 = r1.wrapping_add(224 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -336                                  r2 += -336   ///  r2 = r2.wrapping_add(-336 as i32 as i64 as u64)
    mov64 r3, 336                                   r3 = 336 as i32 as i64 as u64
    call function_21513                     
    mov64 r1, r7                                    r1 = r7
    add64 r1, 560                                   r1 += 560   ///  r1 = r1.wrapping_add(560 as i32 as i64 as u64)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r3, 348                                   r3 = 348 as i32 as i64 as u64
    call function_21517                     
    stxb [r7+0x38e], r6                     
    ldxdw r1, [r10-0x2f8]                   
    stxb [r7+0x38c], r1                     
    stb [r7+0x38f], 0                       
    stb [r7+0x38d], 0                       
    ldxdw r1, [r10-0x1d0]                   
    stxdw [r7+0x390], r1                    
    ldxdw r1, [r10-0x1c8]                   
    stxdw [r7+0x398], r1                    
    ldxdw r1, [r10-0x1c0]                   
    stxdw [r7+0x3a0], r1                    
    ldxdw r1, [r10-0x1b8]                   
    stxdw [r7+0x3a8], r1                    
    mov64 r1, r7                                    r1 = r7
    add64 r1, 944                                   r1 += 944   ///  r1 = r1.wrapping_add(944 as i32 as i64 as u64)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r3, 216                                   r3 = 216 as i32 as i64 as u64
    call function_21517                     
    ldxdw r1, [r10-0x298]                   
    ldxdw r2, [r1+0x0]                      
    lddw r1, 0x3ff0000000000000                     r1 load str located at 4607182418800017408
    call function_21530                     
    stxdw [r7+0x280], r0                    
    ldxb r1, [r7+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r7+0x0], r1                       
    ldxdw r1, [r10-0x2f0]                   
    ldxdw r2, [r10-0x2d8]                   
    call function_167                       
    ldxdw r1, [r10-0x2c8]                   
    ldxdw r2, [r10-0x2d0]                   
    call function_167                       
    mov64 r6, 26                                    r6 = 26 as i32 as i64 as u64
    ja lbb_5244                                     if true { pc += 6 }
lbb_5238:
    ldxdw r1, [r10-0x2f0]                   
    ldxdw r2, [r10-0x2d8]                   
lbb_5240:
    call function_167                       
lbb_5241:
    ldxdw r1, [r10-0x2c8]                   
    ldxdw r2, [r10-0x2d0]                   
    call function_167                       
lbb_5244:
    ldxdw r1, [r10-0x280]                   
    stxw [r1+0x4], r9                       
    stxw [r1+0x0], r6                       
    exit                                    

function_5248:
    mov64 r8, r4                                    r8 = r4
    mov64 r7, r3                                    r7 = r3
    stxdw [r10-0x38], r1                    
    ldxdw r1, [r7+0x18]                     
    ldxdw r6, [r1+0x0]                      
    ldxdw r1, [r7+0x28]                     
    stxdw [r10-0x40], r1                    
    ldxdw r9, [r1+0x0]                      
    mov64 r1, r6                                    r1 = r6
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r4, r6                                    r4 = r6
    mov64 r6, 22                                    r6 = 22 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_5311                             if r0 != (0 as i32 as i64 as u64) { pc += 46 }
    lddw r6, 0x800000000                            r6 load str located at 34359738368
    ldxdw r1, [r4+0x50]                     
    jne r1, 1072, lbb_5311                          if r1 != (1072 as i32 as i64 as u64) { pc += 42 }
    lddw r6, 0x200000000                            r6 load str located at 8589934592
    ldxb r1, [r4+0x2]                       
    jeq r1, 0, lbb_5311                             if r1 == (0 as i32 as i64 as u64) { pc += 38 }
    mov64 r6, 11                                    r6 = 11 as i32 as i64 as u64
    ldxb r3, [r4+0x0]                       
    mov64 r1, r3                                    r1 = r3
    and64 r1, 15                                    r1 &= 15   ///  r1 = r1.and(15)
    jne r1, 15, lbb_5311                            if r1 != (15 as i32 as i64 as u64) { pc += 33 }
    mov64 r1, r3                                    r1 = r3
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r4+0x0], r1                       
    mov64 r2, r4                                    r2 = r4
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r2                                    r1 = r2
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jeq r1, 0, lbb_5291                             if r1 == (0 as i32 as i64 as u64) { pc += 5 }
    lddw r1, 0x10002d1f5 --> b"from_bytes_mut"        r1 load str located at 4295152117
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_2131                      
lbb_5291:
    lddw r6, 0x900000000                            r6 load str located at 38654705664
    stxdw [r10-0x50], r2                    
    ldxdw r1, [r2+0x0]                      
    jne r1, 1, lbb_5310                             if r1 != (1 as i32 as i64 as u64) { pc += 14 }
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    mov64 r2, r4                                    r2 = r4
    add64 r2, 128                                   r2 += 128   ///  r2 = r2.wrapping_add(128 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    mov64 r6, r3                                    r6 = r3
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    stxdw [r10-0x48], r4                    
    call function_21522                     
    mov64 r3, r6                                    r3 = r6
    ldxdw r4, [r10-0x48]                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_5317                             if r0 == (0 as i32 as i64 as u64) { pc += 7 }
lbb_5310:
    stxb [r4+0x0], r3                       
lbb_5311:
    mov64 r1, r6                                    r1 = r6
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
lbb_5313:
    ldxdw r2, [r10-0x38]                    
    stxw [r2+0x4], r1                       
    stxw [r2+0x0], r6                       
    exit                                    
lbb_5317:
    ldxdw r1, [r7+0x20]                     
    ldxdw r9, [r1+0x0]                      
    mov64 r7, r4                                    r7 = r4
    add64 r7, 912                                   r7 += 912   ///  r7 = r7.wrapping_add(912 as i32 as i64 as u64)
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    mov64 r6, r4                                    r6 = r4
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_5349                             if r0 == (0 as i32 as i64 as u64) { pc += 19 }
    mov64 r2, r6                                    r2 = r6
    add64 r2, 96                                    r2 += 96   ///  r2 = r2.wrapping_add(96 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r5, r6                                    r5 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_5345                             if r0 == (0 as i32 as i64 as u64) { pc += 4 }
lbb_5341:
    ldxb r2, [r5+0x0]                       
    or64 r2, 8                                      r2 |= 8   ///  r2 = r2.or(8)
    stxb [r5+0x0], r2                       
    ja lbb_5313                                     if true { pc += -32 }
lbb_5345:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    ldxb r2, [r8+0xd3]                      
    jne r2, 0, lbb_5341                             if r2 != (0 as i32 as i64 as u64) { pc += -7 }
    ja lbb_5352                                     if true { pc += 3 }
lbb_5349:
    ldxb r1, [r8+0xd3]                      
    mov64 r5, r6                                    r5 = r6
    jne r1, 0, lbb_5503                             if r1 != (0 as i32 as i64 as u64) { pc += 151 }
lbb_5352:
    ldxb r1, [r8+0xb2]                      
    jne r1, 0, lbb_5355                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5363                                     if true { pc += 8 }
lbb_5355:
    ldxdw r1, [r8+0xcb]                     
    stxdw [r5+0x78], r1                     
    ldxdw r1, [r8+0xc3]                     
    stxdw [r5+0x70], r1                     
    ldxdw r1, [r8+0xbb]                     
    stxdw [r5+0x68], r1                     
    ldxdw r1, [r8+0xb3]                     
    stxdw [r5+0x60], r1                     
lbb_5363:
    mov64 r9, r5                                    r9 = r5
    add64 r9, 224                                   r9 += 224   ///  r9 = r9.wrapping_add(224 as i32 as i64 as u64)
    ldxdw r1, [r8+0x0]                      
    ldxdw r7, [r10-0x40]                    
    jne r1, 1, lbb_5370                             if r1 != (1 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r8+0x8]                      
    stxdw [r9+0x0], r1                      
lbb_5370:
    ldxdw r1, [r8+0x10]                     
    jne r1, 1, lbb_5374                             if r1 != (1 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r8+0x18]                     
    stxdw [r5+0xe8], r1                     
lbb_5374:
    ldxdw r1, [r8+0x20]                     
    jne r1, 1, lbb_5378                             if r1 != (1 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r8+0x28]                     
    stxdw [r5+0xf0], r1                     
lbb_5378:
    ldxdw r1, [r8+0x30]                     
    jne r1, 1, lbb_5382                             if r1 != (1 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r8+0x38]                     
    stxdw [r5+0xf8], r1                     
lbb_5382:
    ldxdw r1, [r8+0x40]                     
    jne r1, 1, lbb_5386                             if r1 != (1 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r8+0x48]                     
    stxdw [r5+0x100], r1                    
lbb_5386:
    ldxdw r1, [r8+0x50]                     
    jne r1, 1, lbb_5390                             if r1 != (1 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r8+0x58]                     
    stxdw [r5+0x108], r1                    
lbb_5390:
    ldxdw r1, [r8+0x60]                     
    jne r1, 1, lbb_5394                             if r1 != (1 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r8+0x68]                     
    stxdw [r5+0x110], r1                    
lbb_5394:
    ldxdw r1, [r8+0x70]                     
    jne r1, 1, lbb_5398                             if r1 != (1 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r8+0x78]                     
    stxdw [r5+0x118], r1                    
lbb_5398:
    ldxdw r1, [r8+0x80]                     
    jne r1, 1, lbb_5402                             if r1 != (1 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r8+0x88]                     
    stxdw [r5+0x120], r1                    
lbb_5402:
    ldxdw r1, [r8+0x90]                     
    jne r1, 1, lbb_5406                             if r1 != (1 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r8+0x98]                     
    stxdw [r5+0x128], r1                    
lbb_5406:
    ldxdw r1, [r5+0x118]                    
    ldxdw r2, [r5+0x100]                    
    jle r2, r1, lbb_5415                            if r2 <= r1 { pc += 6 }
    lddw r1, 0x10002d37e --> b"assertion failed: config.phase0_start_micros <= config.phase1_start_micro…        r1 load str located at 4295152510
    mov64 r2, 74                                    r2 = 74 as i32 as i64 as u64
    lddw r3, 0x10002ee38 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xcf\x00\…        r3 load str located at 4295159352
    call function_18704                     
lbb_5415:
    ldxb r2, [r5+0x38d]                     
    ldxb r3, [r8+0xb0]                      
    mov64 r1, r2                                    r1 = r2
    jne r3, 1, lbb_5421                             if r3 != (1 as i32 as i64 as u64) { pc += 2 }
    ldxb r1, [r8+0xb1]                      
    stxb [r5+0x38d], r1                     
lbb_5421:
    jne r2, 1, lbb_5470                             if r2 != (1 as i32 as i64 as u64) { pc += 48 }
    jne r1, 2, lbb_5470                             if r1 != (2 as i32 as i64 as u64) { pc += 47 }
    ldxw r1, [r5+0x388]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    jne r2, r1, lbb_5431                            if r2 != r1 { pc += 0 }
lbb_5431:
    jne r2, r1, lbb_5566                            if r2 != r1 { pc += 134 }
    stxw [r5+0x3b0], r1                     
    stw [r5+0x388], 0                       
    ldxdw r2, [r5+0x250]                    
    jeq r2, 0, lbb_5441                             if r2 == (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, r2                                    r1 = r2
    div64 r1, 1000                                  r1 /= 1000   ///  r1 = r1 / (1000 as u64)
    jgt r2, 1999, lbb_5440                          if r2 > (1999 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_5440:
    stxdw [r5+0x288], r1                    
lbb_5441:
    ldxdw r2, [r5+0x258]                    
    jeq r2, 0, lbb_5448                             if r2 == (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, r2                                    r1 = r2
    div64 r1, 1000                                  r1 /= 1000   ///  r1 = r1 / (1000 as u64)
    jgt r2, 1999, lbb_5447                          if r2 > (1999 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_5447:
    stxdw [r5+0x290], r1                    
lbb_5448:
    ldxdw r2, [r5+0x270]                    
    jeq r2, 0, lbb_5455                             if r2 == (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, r2                                    r1 = r2
    div64 r1, 1000                                  r1 /= 1000   ///  r1 = r1 / (1000 as u64)
    jgt r2, 1999, lbb_5454                          if r2 > (1999 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_5454:
    stxdw [r5+0x298], r1                    
lbb_5455:
    mov64 r1, r5                                    r1 = r5
    add64 r1, 592                                   r1 += 592   ///  r1 = r1.wrapping_add(592 as i32 as i64 as u64)
    mov64 r2, r5                                    r2 = r5
    add64 r2, 624                                   r2 += 624   ///  r2 = r2.wrapping_add(624 as i32 as i64 as u64)
    ldxdw r4, [r5+0x278]                    
    jeq r4, 0, lbb_5466                             if r4 == (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r3, r4                                    r3 = r4
    div64 r3, 1000                                  r3 /= 1000   ///  r3 = r3 / (1000 as u64)
    jgt r4, 1999, lbb_5465                          if r4 > (1999 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_5465:
    stxdw [r5+0x2a0], r3                    
lbb_5466:
    stdw [r1+0x8], 0                        
    stdw [r1+0x0], 0                        
    stdw [r2+0x8], 0                        
    stdw [r2+0x0], 0                        
lbb_5470:
    ldxdw r1, [r8+0xa0]                     
    jne r1, 0, lbb_5473                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5479                                     if true { pc += 6 }
lbb_5473:
    ldxdw r2, [r8+0xa8]                     
    lddw r1, 0x3ff0000000000000                     r1 load str located at 4607182418800017408
    call function_21530                     
    ldxdw r5, [r10-0x48]                    
    stxdw [r5+0x280], r0                    
lbb_5479:
    mov64 r6, 3                                     r6 = 3 as i32 as i64 as u64
    ldxdw r1, [r7+0x0]                      
    ldxdw r2, [r1+0x50]                     
    jne r2, 165, lbb_5500                           if r2 != (165 as i32 as i64 as u64) { pc += 17 }
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x10002cf08 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295151368
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    ldxdw r5, [r10-0x48]                    
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_5500                             if r0 != (0 as i32 as i64 as u64) { pc += 8 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_18010                     
    ldxdw r1, [r10-0x20]                    
    jne r1, 0, lbb_5512                             if r1 != (0 as i32 as i64 as u64) { pc += 14 }
    ldxdw r6, [r10-0x18]                    
    ldxdw r5, [r10-0x48]                    
lbb_5500:
    mov64 r1, r6                                    r1 = r6
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    ja lbb_5341                                     if true { pc += -162 }
lbb_5503:
    ldxdw r1, [r8+0xec]                     
    stxdw [r7+0x18], r1                     
    ldxdw r1, [r8+0xe4]                     
    stxdw [r7+0x10], r1                     
    ldxdw r1, [r8+0xdc]                     
    stxdw [r7+0x8], r1                      
    ldxdw r1, [r8+0xd4]                     
    stxdw [r7+0x0], r1                      
    ja lbb_5352                                     if true { pc += -160 }
lbb_5512:
    ldxb r3, [r10-0x8]                      
    jgt r3, 7, lbb_5569                             if r3 > (7 as i32 as i64 as u64) { pc += 55 }
    ldxdw r2, [r10-0x10]                    
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    lsh64 r4, r3                                    r4 <<= r3   ///  r4 = r4.wrapping_shl(r3 as u32)
    and64 r4, 255                                   r4 &= 255   ///  r4 = r4.and(255)
    ldxb r3, [r2+0x0]                       
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    mov64 r4, r3                                    r4 = r3
    and64 r4, 255                                   r4 &= 255   ///  r4 = r4.and(255)
    ldxdw r5, [r10-0x48]                    
    jne r4, r3, lbb_5524                            if r4 != r3 { pc += 0 }
lbb_5524:
    jne r4, r3, lbb_5572                            if r4 != r3 { pc += 47 }
    ldxdw r7, [r1+0x40]                     
    stxb [r2+0x0], r3                       
    ldxdw r4, [r5+0x280]                    
    ldxb r2, [r5+0x38d]                     
    jeq r2, 2, lbb_5546                             if r2 == (2 as i32 as i64 as u64) { pc += 16 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    jne r2, 1, lbb_5341                             if r2 != (1 as i32 as i64 as u64) { pc += -192 }
    mov64 r2, r5                                    r2 = r5
    add64 r2, 560                                   r2 += 560   ///  r2 = r2.wrapping_add(560 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r3, r9                                    r3 = r9
    call function_3829                      
    ldxw r1, [r10-0x2c]                     
    ldxw r6, [r10-0x30]                     
    jne r6, 26, lbb_5564                            if r6 != (26 as i32 as i64 as u64) { pc += 22 }
    ldxdw r1, [r10-0x50]                    
    mov64 r2, r7                                    r2 = r7
    call function_4128                      
    ja lbb_5558                                     if true { pc += 12 }
lbb_5546:
    mov64 r2, r5                                    r2 = r5
    add64 r2, 560                                   r2 += 560   ///  r2 = r2.wrapping_add(560 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r3, r9                                    r3 = r9
    call function_4319                      
    ldxw r1, [r10-0x24]                     
    ldxw r6, [r10-0x28]                     
    jne r6, 26, lbb_5564                            if r6 != (26 as i32 as i64 as u64) { pc += 9 }
    ldxdw r1, [r10-0x50]                    
    mov64 r2, r7                                    r2 = r7
    call function_4621                      
lbb_5558:
    ldxdw r2, [r10-0x48]                    
    ldxb r1, [r2+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r2+0x0], r1                       
    mov64 r6, 26                                    r6 = 26 as i32 as i64 as u64
    ja lbb_5313                                     if true { pc += -251 }
lbb_5564:
    ldxdw r5, [r10-0x48]                    
    ja lbb_5341                                     if true { pc += -225 }
lbb_5566:
    lddw r1, 0x10002ee50 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xf3\x00\…        r1 load str located at 4295159376
    call function_20968                     
lbb_5569:
    lddw r1, 0x10002eb30 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158576
    call function_20990                     
lbb_5572:
    lddw r1, 0x10002eb48 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158600
    call function_20946                     

function_5575:
    mov64 r9, r3                                    r9 = r3
    mov64 r6, r2                                    r6 = r2
    stxdw [r10-0x28], r1                    
    ldxdw r8, [r9+0x28]                     
    ldxdw r1, [r8+0x0]                      
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x10002cf48 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r2 load str located at 4295151432
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r7, 8                                     r7 = 8 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_5879                             if r0 != (0 as i32 as i64 as u64) { pc += 290 }
    ldxdw r1, [r9+0x68]                     
    ldxdw r1, [r1+0x0]                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x10002cf48 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r2 load str located at 4295151432
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jne r0, 0, lbb_5879                             if r0 != (0 as i32 as i64 as u64) { pc += 278 }
    ldxdw r2, [r9+0x18]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r3, r6                                    r3 = r6
    call function_2453                      
    ldxw r3, [r10-0xc]                      
    ldxw r7, [r10-0x10]                     
    ldxdw r2, [r10-0x18]                    
    jeq r2, 0, lbb_5879                             if r2 == (0 as i32 as i64 as u64) { pc += 269 }
    ldxdw r1, [r10-0x8]                     
    stxdw [r10-0x38], r1                    
    ldxdw r1, [r9+0x20]                     
    stxdw [r10-0x40], r1                    
    ldxdw r1, [r1+0x0]                      
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x30], r3                    
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    ldxdw r1, [r10-0x38]                    
    ldxdw r3, [r10-0x30]                    
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    or64 r3, r7                                     r3 |= r7   ///  r3 = r3.or(r7)
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_5630                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5866                                     if true { pc += 236 }
lbb_5630:
    ldxdw r2, [r10-0x40]                    
    stxdw [r10-0x30], r3                    
    ldxdw r1, [r9+0x40]                     
    ldxdw r4, [r1+0x0]                      
    ldxdw r2, [r9+0x38]                     
    ldxdw r1, [r9+0x48]                     
    ldxdw r3, [r1+0x0]                      
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    add64 r4, 8                                     r4 += 8   ///  r4 = r4.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    stxdw [r10-0x58], r2                    
    stxdw [r10-0x60], r3                    
    mov64 r5, r6                                    r5 = r6
    call function_2269                      
    ldxw r1, [r10-0xc]                      
    ldxw r7, [r10-0x10]                     
    ldxdw r2, [r10-0x18]                    
    jne r2, 0, lbb_5653                             if r2 != (0 as i32 as i64 as u64) { pc += 4 }
    mov64 r4, r1                                    r4 = r1
    ldxdw r3, [r10-0x30]                    
    ldxdw r1, [r10-0x38]                    
    ja lbb_5866                                     if true { pc += 213 }
lbb_5653:
    stxdw [r10-0x70], r2                    
    ldxdw r2, [r10-0x8]                     
    stxdw [r10-0x50], r2                    
    stxdw [r10-0x48], r1                    
    ldxdw r1, [r9+0x58]                     
    ldxdw r4, [r1+0x0]                      
    ldxdw r2, [r9+0x50]                     
    ldxdw r1, [r9+0x60]                     
    ldxdw r3, [r1+0x0]                      
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    add64 r4, 8                                     r4 += 8   ///  r4 = r4.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    stxdw [r10-0x78], r2                    
    stxdw [r10-0x80], r3                    
    mov64 r5, r6                                    r5 = r6
    call function_2269                      
    ldxdw r1, [r10-0x48]                    
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    or64 r1, r7                                     r1 |= r7   ///  r1 = r1.or(r7)
    ldxw r3, [r10-0xc]                      
    ldxw r7, [r10-0x10]                     
    ldxdw r2, [r10-0x18]                    
    jne r2, 0, lbb_5679                             if r2 != (0 as i32 as i64 as u64) { pc += 2 }
    mov64 r6, r3                                    r6 = r3
    ja lbb_5861                                     if true { pc += 182 }
lbb_5679:
    stxdw [r10-0x88], r2                    
    stxdw [r10-0x48], r1                    
    ldxdw r1, [r10-0x8]                     
    stxdw [r10-0x68], r1                    
    ldxdw r1, [r10-0x40]                    
    stxdw [r10-0x1000], r1                  
    stdw [r10-0xff0], 0                     
    stdw [r10-0xff8], 8                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r8                                    r2 = r8
    stxdw [r10-0x40], r3                    
    mov64 r3, 7456                                  r3 = 7456 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    call function_4694                      
    ldxdw r3, [r10-0x40]                    
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    or64 r3, r7                                     r3 |= r7   ///  r3 = r3.or(r7)
    ldxw r6, [r10-0x1c]                     
    ldxw r7, [r10-0x20]                     
    jne r7, 26, lbb_5857                            if r7 != (26 as i32 as i64 as u64) { pc += 156 }
    mov64 r7, 11                                    r7 = 11 as i32 as i64 as u64
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxdw r8, [r8+0x0]                      
    ldxb r1, [r8+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    jne r2, 15, lbb_5857                            if r2 != (15 as i32 as i64 as u64) { pc += 149 }
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r8+0x0], r1                       
    ldxdw r2, [r8+0x50]                     
    mov64 r7, r8                                    r7 = r8
    add64 r7, 88                                    r7 += 88   ///  r7 = r7.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    mov64 r6, r3                                    r6 = r3
    call function_2097                      
    mov64 r1, r7                                    r1 = r7
    call function_5889                      
    stdw [r8+0x58], 4                       
    ldxdw r1, [r9+0x30]                     
    ldxdw r1, [r1+0x0]                      
    ldxdw r2, [r1+0x8]                      
    ldxdw r3, [r1+0x10]                     
    ldxdw r4, [r1+0x18]                     
    ldxdw r5, [r1+0x20]                     
    stxdw [r8+0x78], r5                     
    stxdw [r8+0x70], r4                     
    stxdw [r8+0x68], r3                     
    stxdw [r8+0x60], r2                     
    ldxdw r2, [r1+0x8]                      
    ldxdw r3, [r1+0x10]                     
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r1+0x20]                     
    stxdw [r8+0x98], r1                     
    stxdw [r8+0x90], r4                     
    stxdw [r8+0x88], r3                     
    stxdw [r8+0x80], r2                     
    ldxdw r4, [r10-0x60]                    
    ldxdw r1, [r4+0x0]                      
    ldxdw r2, [r4+0x8]                      
    ldxdw r3, [r4+0x10]                     
    ldxdw r4, [r4+0x18]                     
    stxdw [r8+0xf8], r4                     
    stxdw [r8+0xf0], r3                     
    stxdw [r8+0xe8], r2                     
    stxdw [r8+0xe0], r1                     
    ldxdw r1, [r10-0x58]                    
    ldxdw r1, [r1+0x0]                      
    ldxdw r2, [r1+0x8]                      
    ldxdw r3, [r1+0x10]                     
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r1+0x20]                     
    stxdw [r8+0x118], r1                    
    stxdw [r8+0x110], r4                    
    stxdw [r8+0x108], r3                    
    stxdw [r8+0x100], r2                    
    ldxdw r5, [r10-0x70]                    
    ldxb r1, [r5+0x334]                     
    stxb [r8+0x160], r1                     
    ldxdw r4, [r10-0x80]                    
    ldxdw r1, [r4+0x0]                      
    ldxdw r2, [r4+0x8]                      
    ldxdw r3, [r4+0x10]                     
    ldxdw r4, [r4+0x18]                     
    stxdw [r8+0x138], r4                    
    stxdw [r8+0x130], r3                    
    stxdw [r8+0x128], r2                    
    stxdw [r8+0x120], r1                    
    ldxdw r1, [r10-0x78]                    
    ldxdw r1, [r1+0x0]                      
    ldxdw r2, [r1+0x8]                      
    ldxdw r3, [r1+0x10]                     
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r1+0x20]                     
    stxdw [r8+0x158], r1                    
    stxdw [r8+0x150], r4                    
    stxdw [r8+0x148], r3                    
    stxdw [r8+0x140], r2                    
    ldxdw r0, [r10-0x88]                    
    ldxb r1, [r0+0x334]                     
    stxb [r8+0x161], r1                     
    ldxdw r1, [r5+0x48]                     
    ldxdw r2, [r5+0x50]                     
    ldxdw r3, [r5+0x58]                     
    ldxdw r4, [r5+0x60]                     
    stxdw [r8+0xb8], r4                     
    stxdw [r8+0xb0], r3                     
    stxdw [r8+0xa8], r2                     
    stxdw [r8+0xa0], r1                     
    ldxdw r1, [r0+0x48]                     
    ldxdw r2, [r0+0x50]                     
    ldxdw r3, [r0+0x58]                     
    ldxdw r4, [r0+0x60]                     
    stxdw [r8+0xd8], r4                     
    stxdw [r8+0xd0], r3                     
    stxdw [r8+0xc8], r2                     
    stxdw [r8+0xc0], r1                     
    ldxb r1, [r5+0x336]                     
    stxb [r8+0x162], r1                     
    ldxb r1, [r0+0x336]                     
    stxb [r8+0x163], r1                     
    stdw [r8+0x460], 1000                   
    stdw [r8+0x458], 1000                   
    stdw [r8+0x450], 1000                   
    stdw [r8+0x448], 1000                   
    stdw [r8+0x440], 1000                   
    stdw [r8+0x438], 1000                   
    stdw [r8+0x430], 1000                   
    stdw [r8+0x428], 1000                   
    stdw [r8+0x420], 1000                   
    stdw [r8+0x418], 1000                   
    stdw [r8+0x410], 1000                   
    stdw [r8+0x408], 1000                   
    stdw [r8+0x400], 1000                   
    stdw [r8+0x3f8], 1000                   
    stdw [r8+0x3f0], 1000                   
    stdw [r8+0x3e8], 1000                   
    stdw [r8+0x3e0], 1000                   
    stdw [r8+0x3d8], 1000                   
    stdw [r8+0x3d0], 1000                   
    stdw [r8+0x3c8], 1000                   
    ldxdw r7, [r5+0x228]                    
    stxdw [r8+0xbd0], r7                    
    ldxdw r9, [r0+0x228]                    
    ldxb r1, [r8+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r8+0x0], r1                       
    lddw r1, 0x3ff0000000000000                     r1 load str located at 4607182418800017408
    mov64 r2, r7                                    r2 = r7
    call function_21530                     
    stxdw [r8+0xba8], r0                    
    mov64 r1, r0                                    r1 = r0
    mov64 r2, r9                                    r2 = r9
    call function_22254                     
    stxdw [r8+0xbb8], r0                    
    stxdw [r8+0xbc8], r9                    
    lddw r1, 0x3ff0000000000000                     r1 load str located at 4607182418800017408
    mov64 r2, r9                                    r2 = r9
    call function_21530                     
    stxdw [r8+0xba0], r0                    
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    stxdw [r8+0xbc0], r0                    
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x68]                    
    call function_167                       
    ldxdw r1, [r10-0x48]                    
    ldxdw r2, [r10-0x50]                    
    call function_167                       
    ldxdw r1, [r10-0x30]                    
    ldxdw r2, [r10-0x38]                    
    call function_167                       
    mov64 r7, 26                                    r7 = 26 as i32 as i64 as u64
    ja lbb_5879                                     if true { pc += 22 }
lbb_5857:
    mov64 r1, r3                                    r1 = r3
    ldxdw r2, [r10-0x68]                    
    call function_167                       
    ldxdw r1, [r10-0x48]                    
lbb_5861:
    ldxdw r2, [r10-0x50]                    
    call function_167                       
    ldxdw r3, [r10-0x30]                    
    ldxdw r1, [r10-0x38]                    
    mov64 r4, r6                                    r4 = r6
lbb_5866:
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    jgt r1, 7, lbb_5883                             if r1 > (7 as i32 as i64 as u64) { pc += 15 }
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lsh64 r2, r1                                    r2 <<= r1   ///  r2 = r2.wrapping_shl(r1 as u32)
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    ldxb r1, [r3+0x0]                       
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, r1                                    r2 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jne r2, r1, lbb_5876                            if r2 != r1 { pc += 0 }
lbb_5876:
    jne r2, r1, lbb_5886                            if r2 != r1 { pc += 9 }
    stxb [r3+0x0], r1                       
    mov64 r3, r4                                    r3 = r4
lbb_5879:
    ldxdw r1, [r10-0x28]                    
    stxw [r1+0x4], r3                       
    stxw [r1+0x0], r7                       
    exit                                    
lbb_5883:
    lddw r1, 0x10002eb30 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158576
    call function_20990                     
lbb_5886:
    lddw r1, 0x10002eb48 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158600
    call function_20946                     

function_5889:
    mov64 r6, r1                                    r6 = r1
    mov64 r7, r10                                   r7 = r10
    add64 r7, -1568                                 r7 += -1568   ///  r7 = r7.wrapping_add(-1568 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r3, 560                                   r3 = 560 as i32 as i64 as u64
    call function_21517                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -960                                  r1 += -960   ///  r1 = r1.wrapping_add(-960 as i32 as i64 as u64)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r3, 960                                   r3 = 960 as i32 as i64 as u64
    call function_21517                     
    stdw [r10-0x3f0], 0                     
    stdw [r10-0x3e8], 0                     
    stdw [r10-0x3e0], 0                     
    stdw [r10-0x3d8], 0                     
    stdw [r10-0x3d0], 0                     
    stdw [r10-0x3c8], 0                     
    mov64 r1, r6                                    r1 = r6
    add64 r1, 272                                   r1 += 272   ///  r1 = r1.wrapping_add(272 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 1568                                  r3 = 1568 as i32 as i64 as u64
    call function_21513                     
    mov64 r1, r6                                    r1 = r6
    add64 r1, 1840                                  r1 += 1840   ///  r1 = r1.wrapping_add(1840 as i32 as i64 as u64)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r3, 1048                                  r3 = 1048 as i32 as i64 as u64
    call function_21517                     
    mov64 r1, r6                                    r1 = r6
    add64 r1, 2888                                  r1 += 2888   ///  r1 = r1.wrapping_add(2888 as i32 as i64 as u64)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r3, 1112                                  r3 = 1112 as i32 as i64 as u64
    call function_21517                     
    lddw r1, 0x8000000080000000                     r1 load str located at -9223372034707292160
    stxdw [r6+0x14b0], r1                   
    exit                                    

function_5926:
    stxdw [r10-0x48], r4                    
    mov64 r9, r3                                    r9 = r3
    mov64 r7, r2                                    r7 = r2
    stxdw [r10-0x40], r1                    
    ldxdw r1, [r9+0x28]                     
    ldxdw r6, [r1+0x0]                      
    ldxdw r1, [r9+0x30]                     
    stxdw [r10-0x50], r1                    
    ldxdw r8, [r1+0x0]                      
    mov64 r1, r6                                    r1 = r6
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r2, r6                                    r2 = r6
    mov64 r6, 22                                    r6 = 22 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_5991                             if r0 != (0 as i32 as i64 as u64) { pc += 47 }
    lddw r6, 0x800000000                            r6 load str located at 34359738368
    ldxdw r1, [r2+0x50]                     
    jne r1, 1072, lbb_5991                          if r1 != (1072 as i32 as i64 as u64) { pc += 43 }
    lddw r6, 0x200000000                            r6 load str located at 8589934592
    ldxb r1, [r2+0x2]                       
    jeq r1, 0, lbb_5991                             if r1 == (0 as i32 as i64 as u64) { pc += 39 }
    mov64 r6, 11                                    r6 = 11 as i32 as i64 as u64
    ldxb r4, [r2+0x0]                       
    mov64 r1, r4                                    r1 = r4
    and64 r1, 15                                    r1 &= 15   ///  r1 = r1.and(15)
    jne r1, 15, lbb_5991                            if r1 != (15 as i32 as i64 as u64) { pc += 34 }
    mov64 r1, r4                                    r1 = r4
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r2+0x0], r1                       
    mov64 r3, r2                                    r3 = r2
    add64 r3, 88                                    r3 += 88   ///  r3 = r3.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r3                                    r1 = r3
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jeq r1, 0, lbb_5970                             if r1 == (0 as i32 as i64 as u64) { pc += 5 }
lbb_5965:
    lddw r1, 0x10002d1f5 --> b"from_bytes_mut"        r1 load str located at 4295152117
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_2131                      
lbb_5970:
    lddw r6, 0x900000000                            r6 load str located at 38654705664
    stxdw [r10-0x60], r3                    
    ldxdw r1, [r3+0x0]                      
    jne r1, 1, lbb_5990                             if r1 != (1 as i32 as i64 as u64) { pc += 15 }
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x58], r2                    
    ldxdw r2, [r10-0x58]                    
    add64 r2, 128                                   r2 += 128   ///  r2 = r2.wrapping_add(128 as i32 as i64 as u64)
    stxdw [r10-0x68], r8                    
    mov64 r1, r8                                    r1 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    mov64 r6, r4                                    r6 = r4
    call function_21522                     
    mov64 r4, r6                                    r4 = r6
    ldxdw r2, [r10-0x58]                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_5997                             if r0 == (0 as i32 as i64 as u64) { pc += 7 }
lbb_5990:
    stxb [r2+0x0], r4                       
lbb_5991:
    mov64 r9, r6                                    r9 = r6
    rsh64 r9, 32                                    r9 >>= 32   ///  r9 = r9.wrapping_shr(32)
lbb_5993:
    ldxdw r1, [r10-0x40]                    
    stxw [r1+0x4], r9                       
    stxw [r1+0x0], r6                       
    exit                                    
lbb_5997:
    ldxdw r1, [r9+0x38]                     
    ldxdw r6, [r1+0x0]                      
    ldxdw r1, [r9+0x40]                     
    stxdw [r10-0x70], r1                    
    ldxdw r8, [r1+0x0]                      
    mov64 r1, r6                                    r1 = r6
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r2, r6                                    r2 = r6
    mov64 r6, 22                                    r6 = 22 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_6055                             if r0 != (0 as i32 as i64 as u64) { pc += 43 }
    lddw r6, 0x800000000                            r6 load str located at 34359738368
    ldxdw r1, [r2+0x50]                     
    jne r1, 1072, lbb_6055                          if r1 != (1072 as i32 as i64 as u64) { pc += 39 }
    lddw r6, 0x200000000                            r6 load str located at 8589934592
    ldxb r1, [r2+0x2]                       
    jeq r1, 0, lbb_6055                             if r1 == (0 as i32 as i64 as u64) { pc += 35 }
    mov64 r6, 11                                    r6 = 11 as i32 as i64 as u64
    ldxb r4, [r2+0x0]                       
    mov64 r1, r4                                    r1 = r4
    and64 r1, 15                                    r1 &= 15   ///  r1 = r1.and(15)
    jne r1, 15, lbb_6055                            if r1 != (15 as i32 as i64 as u64) { pc += 30 }
    mov64 r1, r4                                    r1 = r4
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r2+0x0], r1                       
    mov64 r3, r2                                    r3 = r2
    add64 r3, 88                                    r3 += 88   ///  r3 = r3.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r3                                    r1 = r3
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jeq r1, 0, lbb_6034                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5965                                     if true { pc += -69 }
lbb_6034:
    lddw r6, 0x900000000                            r6 load str located at 38654705664
    stxdw [r10-0x80], r3                    
    ldxdw r1, [r3+0x0]                      
    jne r1, 1, lbb_6054                             if r1 != (1 as i32 as i64 as u64) { pc += 15 }
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x78], r2                    
    ldxdw r2, [r10-0x78]                    
    add64 r2, 128                                   r2 += 128   ///  r2 = r2.wrapping_add(128 as i32 as i64 as u64)
    stxdw [r10-0x88], r8                    
    mov64 r1, r8                                    r1 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    mov64 r6, r4                                    r6 = r4
    call function_21522                     
    mov64 r4, r6                                    r4 = r6
    ldxdw r2, [r10-0x78]                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_6062                             if r0 == (0 as i32 as i64 as u64) { pc += 8 }
lbb_6054:
    stxb [r2+0x0], r4                       
lbb_6055:
    mov64 r9, r6                                    r9 = r6
    rsh64 r9, 32                                    r9 >>= 32   ///  r9 = r9.wrapping_shr(32)
lbb_6057:
    ldxdw r2, [r10-0x58]                    
    ldxb r1, [r2+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r2+0x0], r1                       
    ja lbb_5993                                     if true { pc += -69 }
lbb_6062:
    ldxdw r1, [r9+0x20]                     
    ldxdw r8, [r1+0x0]                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r6, 22                                    r6 = 22 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_6103                             if r0 != (0 as i32 as i64 as u64) { pc += 30 }
    lddw r6, 0x800000000                            r6 load str located at 34359738368
    ldxdw r1, [r8+0x50]                     
    jne r1, 7456, lbb_6103                          if r1 != (7456 as i32 as i64 as u64) { pc += 26 }
    lddw r6, 0x200000000                            r6 load str located at 8589934592
    ldxb r1, [r8+0x2]                       
    jeq r1, 0, lbb_6103                             if r1 == (0 as i32 as i64 as u64) { pc += 22 }
    mov64 r6, 11                                    r6 = 11 as i32 as i64 as u64
    ldxb r1, [r8+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    jne r2, 15, lbb_6103                            if r2 != (15 as i32 as i64 as u64) { pc += 17 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 247                                   r2 &= 247   ///  r2 = r2.and(247)
    stxb [r8+0x0], r2                       
    stxdw [r10-0x90], r8                    
    mov64 r2, r8                                    r2 = r8
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x98], r2                    
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    jeq r2, 0, lbb_6096                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5965                                     if true { pc += -131 }
lbb_6096:
    ldxdw r2, [r10-0x98]                    
    ldxdw r2, [r2+0x0]                      
    jeq r2, 4, lbb_6110                             if r2 == (4 as i32 as i64 as u64) { pc += 11 }
    ldxdw r2, [r10-0x90]                    
    stxb [r2+0x0], r1                       
    lddw r6, 0x900000000                            r6 load str located at 38654705664
lbb_6103:
    mov64 r9, r6                                    r9 = r6
    rsh64 r9, 32                                    r9 >>= 32   ///  r9 = r9.wrapping_shr(32)
lbb_6105:
    ldxdw r2, [r10-0x78]                    
    ldxb r1, [r2+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r2+0x0], r1                       
    ja lbb_6057                                     if true { pc += -53 }
lbb_6110:
    ldxdw r1, [r9+0x18]                     
    ldxdw r1, [r1+0x0]                      
    ldxdw r8, [r10-0x90]                    
    add64 r8, 96                                    r8 += 96   ///  r8 = r8.wrapping_add(96 as i32 as i64 as u64)
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_7151                             if r0 != (0 as i32 as i64 as u64) { pc += 1028 }
    ldxdw r2, [r10-0x90]                    
    add64 r2, 224                                   r2 += 224   ///  r2 = r2.wrapping_add(224 as i32 as i64 as u64)
    ldxdw r1, [r10-0x68]                    
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r9, 3                                     r9 = 3 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_7151                             if r0 != (0 as i32 as i64 as u64) { pc += 1019 }
    ldxdw r2, [r10-0x90]                    
    add64 r2, 288                                   r2 += 288   ///  r2 = r2.wrapping_add(288 as i32 as i64 as u64)
    ldxdw r1, [r10-0x88]                    
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_7151                             if r0 != (0 as i32 as i64 as u64) { pc += 1011 }
    ldxdw r1, [r10-0x48]                    
    ldxb r1, [r1+0x1fe]                     
    jeq r1, 2, lbb_6145                             if r1 == (2 as i32 as i64 as u64) { pc += 2 }
    ldxdw r2, [r10-0x90]                    
    stxb [r2+0x79c], r1                     
lbb_6145:
    ldxdw r1, [r10-0x48]                    
    ldxb r1, [r1+0x1ff]                     
    jeq r1, 2, lbb_6150                             if r1 == (2 as i32 as i64 as u64) { pc += 2 }
    ldxdw r2, [r10-0x90]                    
    stxb [r2+0x79d], r1                     
lbb_6150:
    ldxdw r1, [r10-0x48]                    
    ldxb r1, [r1+0x200]                     
    jeq r1, 2, lbb_6155                             if r1 == (2 as i32 as i64 as u64) { pc += 2 }
    ldxdw r2, [r10-0x90]                    
    stxb [r2+0x790], r1                     
lbb_6155:
    ldxdw r1, [r10-0x48]                    
    ldxb r1, [r1+0x201]                     
    jeq r1, 2, lbb_6160                             if r1 == (2 as i32 as i64 as u64) { pc += 2 }
    ldxdw r2, [r10-0x90]                    
    stxb [r2+0x791], r1                     
lbb_6160:
    ldxdw r1, [r10-0x90]                    
    ldxb r1, [r1+0x79e]                     
    ldxdw r2, [r10-0x48]                    
    ldxb r3, [r2+0x1b8]                     
    mov64 r2, r1                                    r2 = r1
    jne r3, 1, lbb_6170                             if r3 != (1 as i32 as i64 as u64) { pc += 4 }
    ldxdw r2, [r10-0x48]                    
    ldxb r2, [r2+0x1b9]                     
    ldxdw r3, [r10-0x90]                    
    stxb [r3+0x79e], r2                     
lbb_6170:
    ldxdw r3, [r10-0x48]                    
    ldxb r3, [r3+0x1ba]                     
    jne r3, 0, lbb_6176                             if r3 != (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r3, [r10-0x90]                    
    ldxb r7, [r3+0x79f]                     
    ja lbb_6180                                     if true { pc += 4 }
lbb_6176:
    ldxdw r3, [r10-0x48]                    
    ldxb r7, [r3+0x1bb]                     
    ldxdw r3, [r10-0x90]                    
    stxb [r3+0x79f], r7                     
lbb_6180:
    mov64 r3, r7                                    r3 = r7
    jeq r3, 2, lbb_6191                             if r3 == (2 as i32 as i64 as u64) { pc += 9 }
    jne r2, 2, lbb_6515                             if r2 != (2 as i32 as i64 as u64) { pc += 332 }
    mov64 r2, r7                                    r2 = r7
    jne r2, 1, lbb_6198                             if r2 != (1 as i32 as i64 as u64) { pc += 13 }
    lddw r1, 0x10002d3e5 --> b"assertion failed: quoting_version != QuotingVersion::V1 as u8"        r1 load str located at 4295152613
    mov64 r2, 61                                    r2 = 61 as i32 as i64 as u64
    lddw r3, 0x10002ee68 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xb5\x01\…        r3 load str located at 4295159400
    call function_18704                     
lbb_6191:
    jne r2, 1, lbb_6198                             if r2 != (1 as i32 as i64 as u64) { pc += 6 }
    lddw r1, 0x10002d422 --> b"assertion failed: market.config.prepare_market_version != PrepareMarketVe…        r1 load str located at 4295152674
    mov64 r2, 88                                    r2 = 88 as i32 as i64 as u64
    lddw r3, 0x10002ee80 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xb6\x01\…        r3 load str located at 4295159424
    call function_18704                     
lbb_6198:
    jne r1, 1, lbb_6515                             if r1 != (1 as i32 as i64 as u64) { pc += 316 }
    ldxdw r1, [r10-0x90]                    
    stdw [r1+0xbb0], 0                      
    ldxw r1, [r1+0xbf4]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    jne r2, r1, lbb_6209                            if r2 != r1 { pc += 0 }
lbb_6209:
    jne r2, r1, lbb_7395                            if r2 != r1 { pc += 1185 }
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0xbf4], r1                     
    ldxdw r1, [r2+0x3b0]                    
    jslt r1, 0, lbb_7398                            if (r1 as i64) < (0 as i32 as i64) { pc += 1184 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxdw [r2+0x3b0], r1                    
    ldxdw r2, [r2+0x3b8]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, 400                                   r4 = 400 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x18]                    
    jne r2, 0, lbb_6228                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_6228:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_7401                             if r1 != (0 as i32 as i64 as u64) { pc += 1171 }
    ldxdw r1, [r10-0x20]                    
    ldxdw r2, [r10-0x90]                    
    stxdw [r2+0x3b8], r1                    
    ldxdw r2, [r2+0x3c0]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, 400                                   r4 = 400 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x28]                    
    jne r2, 0, lbb_6244                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_6244:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_7404                             if r1 != (0 as i32 as i64 as u64) { pc += 1158 }
    ldxdw r1, [r10-0x30]                    
    ldxdw r2, [r10-0x90]                    
    stxdw [r2+0x3c0], r1                    
    ldxw r1, [r2+0x178]                     
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jne r2, 0, lbb_6254                             if r2 != (0 as i32 as i64 as u64) { pc += 0 }
lbb_6254:
    jne r2, 0, lbb_7383                             if r2 != (0 as i32 as i64 as u64) { pc += 1128 }
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x178], r1                     
    ldxw r1, [r2+0x170]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7386                            if (r1 as i64) < (0 as i32 as i64) { pc += 1125 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x170], r1                     
    ldxw r1, [r2+0x174]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7389                            if (r1 as i64) < (0 as i32 as i64) { pc += 1121 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x174], r1                     
    ldxw r1, [r2+0x1b0]                     
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jne r2, 0, lbb_6276                             if r2 != (0 as i32 as i64 as u64) { pc += 0 }
lbb_6276:
    jne r2, 0, lbb_7383                             if r2 != (0 as i32 as i64 as u64) { pc += 1106 }
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x1b0], r1                     
    ldxw r1, [r2+0x1a8]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7386                            if (r1 as i64) < (0 as i32 as i64) { pc += 1103 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x1a8], r1                     
    ldxw r1, [r2+0x1ac]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7389                            if (r1 as i64) < (0 as i32 as i64) { pc += 1099 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x1ac], r1                     
    ldxw r1, [r2+0x1e8]                     
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jne r2, 0, lbb_6298                             if r2 != (0 as i32 as i64 as u64) { pc += 0 }
lbb_6298:
    jne r2, 0, lbb_7383                             if r2 != (0 as i32 as i64 as u64) { pc += 1084 }
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x1e8], r1                     
    ldxw r1, [r2+0x1e0]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7386                            if (r1 as i64) < (0 as i32 as i64) { pc += 1081 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x1e0], r1                     
    ldxw r1, [r2+0x1e4]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7389                            if (r1 as i64) < (0 as i32 as i64) { pc += 1077 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x1e4], r1                     
    ldxw r1, [r2+0x220]                     
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jne r2, 0, lbb_6320                             if r2 != (0 as i32 as i64 as u64) { pc += 0 }
lbb_6320:
    jne r2, 0, lbb_7383                             if r2 != (0 as i32 as i64 as u64) { pc += 1062 }
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x220], r1                     
    ldxw r1, [r2+0x218]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7386                            if (r1 as i64) < (0 as i32 as i64) { pc += 1059 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x218], r1                     
    ldxw r1, [r2+0x21c]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7389                            if (r1 as i64) < (0 as i32 as i64) { pc += 1055 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x21c], r1                     
    ldxw r1, [r2+0x258]                     
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jne r2, 0, lbb_6342                             if r2 != (0 as i32 as i64 as u64) { pc += 0 }
lbb_6342:
    jne r2, 0, lbb_7383                             if r2 != (0 as i32 as i64 as u64) { pc += 1040 }
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x258], r1                     
    ldxw r1, [r2+0x250]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7386                            if (r1 as i64) < (0 as i32 as i64) { pc += 1037 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x250], r1                     
    ldxw r1, [r2+0x254]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7389                            if (r1 as i64) < (0 as i32 as i64) { pc += 1033 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x254], r1                     
    ldxw r1, [r2+0x290]                     
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jne r2, 0, lbb_6364                             if r2 != (0 as i32 as i64 as u64) { pc += 0 }
lbb_6364:
    jne r2, 0, lbb_7383                             if r2 != (0 as i32 as i64 as u64) { pc += 1018 }
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x290], r1                     
    ldxw r1, [r2+0x288]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7386                            if (r1 as i64) < (0 as i32 as i64) { pc += 1015 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x288], r1                     
    ldxw r1, [r2+0x28c]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7389                            if (r1 as i64) < (0 as i32 as i64) { pc += 1011 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x28c], r1                     
    ldxw r1, [r2+0x2c8]                     
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jne r2, 0, lbb_6386                             if r2 != (0 as i32 as i64 as u64) { pc += 0 }
lbb_6386:
    jne r2, 0, lbb_7383                             if r2 != (0 as i32 as i64 as u64) { pc += 996 }
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x2c8], r1                     
    ldxw r1, [r2+0x2c0]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7386                            if (r1 as i64) < (0 as i32 as i64) { pc += 993 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x2c0], r1                     
    ldxw r1, [r2+0x2c4]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7389                            if (r1 as i64) < (0 as i32 as i64) { pc += 989 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x2c4], r1                     
    ldxw r1, [r2+0x300]                     
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jne r2, 0, lbb_6408                             if r2 != (0 as i32 as i64 as u64) { pc += 0 }
lbb_6408:
    jne r2, 0, lbb_7383                             if r2 != (0 as i32 as i64 as u64) { pc += 974 }
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x300], r1                     
    ldxw r1, [r2+0x2f8]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7386                            if (r1 as i64) < (0 as i32 as i64) { pc += 971 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x2f8], r1                     
    ldxw r1, [r2+0x2fc]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7389                            if (r1 as i64) < (0 as i32 as i64) { pc += 967 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x2fc], r1                     
    ldxw r1, [r2+0x338]                     
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jne r2, 0, lbb_6430                             if r2 != (0 as i32 as i64 as u64) { pc += 0 }
lbb_6430:
    jne r2, 0, lbb_7383                             if r2 != (0 as i32 as i64 as u64) { pc += 952 }
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x338], r1                     
    ldxw r1, [r2+0x330]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7386                            if (r1 as i64) < (0 as i32 as i64) { pc += 949 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x330], r1                     
    ldxw r1, [r2+0x334]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7389                            if (r1 as i64) < (0 as i32 as i64) { pc += 945 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x334], r1                     
    ldxw r1, [r2+0x370]                     
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jne r2, 0, lbb_6452                             if r2 != (0 as i32 as i64 as u64) { pc += 0 }
lbb_6452:
    jne r2, 0, lbb_7383                             if r2 != (0 as i32 as i64 as u64) { pc += 930 }
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x370], r1                     
    ldxw r1, [r2+0x368]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7386                            if (r1 as i64) < (0 as i32 as i64) { pc += 927 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x368], r1                     
    ldxw r1, [r2+0x36c]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jslt r1, 0, lbb_7389                            if (r1 as i64) < (0 as i32 as i64) { pc += 923 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x36c], r1                     
    ldxw r1, [r2+0x14f8]                    
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    jne r2, r1, lbb_6477                            if r2 != r1 { pc += 0 }
lbb_6477:
    jne r2, r1, lbb_7407                            if r2 != r1 { pc += 929 }
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x1518], r1                    
    ldxw r1, [r2+0x14fc]                    
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    jne r2, r1, lbb_6488                            if r2 != r1 { pc += 0 }
lbb_6488:
    jne r2, r1, lbb_7410                            if r2 != r1 { pc += 921 }
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x151c], r1                    
    stdw [r2+0x14f8], 0                     
    ldxw r1, [r2+0x1510]                    
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    jne r2, r1, lbb_6500                            if r2 != r1 { pc += 0 }
lbb_6500:
    jne r2, r1, lbb_7413                            if r2 != r1 { pc += 912 }
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x1520], r1                    
    ldxw r1, [r2+0x1514]                    
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    jne r2, r1, lbb_6511                            if r2 != r1 { pc += 0 }
lbb_6511:
    jne r2, r1, lbb_7416                            if r2 != r1 { pc += 904 }
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x1524], r1                    
    stdw [r2+0x1510], 0                     
lbb_6515:
    ldxdw r1, [r10-0x48]                    
    ldxb r1, [r1+0x1bc]                     
    jne r1, 1, lbb_6527                             if r1 != (1 as i32 as i64 as u64) { pc += 9 }
    ldxdw r2, [r10-0x48]                    
    ldxdw r1, [r2+0x1d5]                    
    stxdw [r8+0x18], r1                     
    ldxdw r1, [r2+0x1cd]                    
    stxdw [r8+0x10], r1                     
    ldxdw r1, [r2+0x1c5]                    
    stxdw [r8+0x8], r1                      
    ldxdw r1, [r2+0x1bd]                    
    stxdw [r8+0x0], r1                      
lbb_6527:
    ldxdw r1, [r10-0x48]                    
    ldxb r1, [r1+0x1dd]                     
    jne r1, 1, lbb_6540                             if r1 != (1 as i32 as i64 as u64) { pc += 10 }
    ldxdw r3, [r10-0x48]                    
    ldxdw r1, [r3+0x1f6]                    
    ldxdw r2, [r10-0x90]                    
    stxdw [r2+0x98], r1                     
    ldxdw r1, [r3+0x1ee]                    
    stxdw [r2+0x90], r1                     
    ldxdw r1, [r3+0x1e6]                    
    stxdw [r2+0x88], r1                     
    ldxdw r1, [r3+0x1de]                    
    stxdw [r2+0x80], r1                     
lbb_6540:
    ldxdw r1, [r10-0x48]                    
    ldxdw r2, [r1+0x128]                    
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    stxdw [r10-0x68], r2                    
    jeq r2, r1, lbb_6580                            if r2 == r1 { pc += 34 }
    mov64 r2, r7                                    r2 = r7
    jeq r2, 1, lbb_6549                             if r2 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7377                                     if true { pc += 828 }
lbb_6549:
    ldxdw r2, [r10-0x48]                    
    ldxdw r2, [r2+0x138]                    
    jeq r2, 0, lbb_6565                             if r2 == (0 as i32 as i64 as u64) { pc += 13 }
    ldxdw r3, [r10-0x48]                    
    ldxdw r3, [r3+0x130]                    
    ldxdw r4, [r3+0x0]                      
    ldxdw r5, [r10-0x90]                    
    stxdw [r5+0x168], r4                    
    ldxw r4, [r3+0x8]                       
    stxw [r5+0x170], r4                     
    ldxw r4, [r3+0xc]                       
    stxw [r5+0x174], r4                     
    ldxw r4, [r3+0x10]                      
    stxw [r5+0x178], r4                     
    jeq r2, 1, lbb_6565                             if r2 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7156                                     if true { pc += 591 }
lbb_6565:
    ldxdw r3, [r10-0x90]                    
    add64 r3, 360                                   r3 += 360   ///  r3 = r3.wrapping_add(360 as i32 as i64 as u64)
    mul64 r2, 56                                    r2 *= 56   ///  r2 = r2.wrapping_mul(56 as u64)
lbb_6568:
    mov64 r4, r3                                    r4 = r3
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    stdw [r4+0x30], 0                       
    stdw [r4+0x28], 0                       
    stdw [r4+0x20], 0                       
    stdw [r4+0x18], 0                       
    stdw [r4+0x10], 0                       
    stdw [r4+0x8], 0                        
    stdw [r4+0x0], 0                        
    add64 r2, 56                                    r2 += 56   ///  r2 = r2.wrapping_add(56 as i32 as i64 as u64)
    jeq r2, 560, lbb_6580                           if r2 == (560 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6568                                     if true { pc += -12 }
lbb_6580:
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0x48]                    
    ldxdw r2, [r2+0x170]                    
    jeq r2, r1, lbb_6617                            if r2 == r1 { pc += 33 }
    mov64 r1, r7                                    r1 = r7
    jeq r1, 1, lbb_7365                             if r1 == (1 as i32 as i64 as u64) { pc += 779 }
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x180]                    
    jeq r1, 0, lbb_6601                             if r1 == (0 as i32 as i64 as u64) { pc += 12 }
    ldxdw r2, [r10-0x48]                    
    ldxdw r2, [r2+0x178]                    
    ldxdw r3, [r2+0x0]                      
    ldxdw r4, [r10-0x90]                    
    stxdw [r4+0x168], r3                    
    ldxw r3, [r2+0x8]                       
    stxw [r4+0x170], r3                     
    ldxw r3, [r2+0xc]                       
    stxw [r4+0x174], r3                     
    ldxw r3, [r2+0x10]                      
    stxw [r4+0x178], r3                     
    jne r1, 1, lbb_7251                             if r1 != (1 as i32 as i64 as u64) { pc += 650 }
lbb_6601:
    ldxdw r2, [r10-0x90]                    
    add64 r2, 360                                   r2 += 360   ///  r2 = r2.wrapping_add(360 as i32 as i64 as u64)
    mul64 r1, 56                                    r1 *= 56   ///  r1 = r1.wrapping_mul(56 as u64)
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
lbb_6605:
    mov64 r3, r2                                    r3 = r2
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    stdw [r3+0x30], 0                       
    stdw [r3+0x28], 0                       
    stdw [r3+0x20], 0                       
    stdw [r3+0x18], 0                       
    stdw [r3+0x10], 0                       
    stdw [r3+0x8], 0                        
    stdw [r3+0x0], 0                        
    add64 r1, 56                                    r1 += 56   ///  r1 = r1.wrapping_add(56 as i32 as i64 as u64)
    jeq r1, 560, lbb_6617                           if r1 == (560 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6605                                     if true { pc += -12 }
lbb_6617:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0xe8]                     
    jne r1, 0, lbb_6621                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6625                                     if true { pc += 4 }
lbb_6621:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0xf0]                     
    ldxdw r2, [r10-0x90]                    
    stxdw [r2+0xbf8], r1                    
lbb_6625:
    ldxdw r1, [r10-0x48]                    
    ldxw r1, [r1+0x188]                     
    jne r1, 1, lbb_6632                             if r1 != (1 as i32 as i64 as u64) { pc += 4 }
    ldxdw r1, [r10-0x48]                    
    ldxw r1, [r1+0x18c]                     
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x398], r1                     
lbb_6632:
    ldxdw r1, [r10-0x48]                    
    ldxw r1, [r1+0x190]                     
    jne r1, 1, lbb_6639                             if r1 != (1 as i32 as i64 as u64) { pc += 4 }
    ldxdw r1, [r10-0x48]                    
    ldxw r1, [r1+0x194]                     
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x39c], r1                     
lbb_6639:
    ldxdw r1, [r10-0x48]                    
    ldxw r1, [r1+0x198]                     
    jne r1, 1, lbb_6646                             if r1 != (1 as i32 as i64 as u64) { pc += 4 }
    ldxdw r1, [r10-0x48]                    
    ldxw r1, [r1+0x19c]                     
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x3a0], r1                     
lbb_6646:
    ldxdw r1, [r10-0x48]                    
    ldxw r1, [r1+0x1a0]                     
    jne r1, 1, lbb_6653                             if r1 != (1 as i32 as i64 as u64) { pc += 4 }
    ldxdw r1, [r10-0x48]                    
    ldxw r1, [r1+0x1a4]                     
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x3a4], r1                     
lbb_6653:
    ldxdw r1, [r10-0x48]                    
    ldxw r1, [r1+0x1a8]                     
    jne r1, 1, lbb_6660                             if r1 != (1 as i32 as i64 as u64) { pc += 4 }
    ldxdw r1, [r10-0x48]                    
    ldxw r1, [r1+0x1ac]                     
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x3a8], r1                     
lbb_6660:
    ldxdw r1, [r10-0x48]                    
    ldxw r1, [r1+0x1b0]                     
    jne r1, 1, lbb_6667                             if r1 != (1 as i32 as i64 as u64) { pc += 4 }
    ldxdw r1, [r10-0x48]                    
    ldxw r1, [r1+0x1b4]                     
    ldxdw r2, [r10-0x90]                    
    stxw [r2+0x3ac], r1                     
lbb_6667:
    mov64 r1, r7                                    r1 = r7
    jne r1, 1, lbb_6700                             if r1 != (1 as i32 as i64 as u64) { pc += 31 }
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x0]                      
    jne r1, 0, lbb_6673                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6677                                     if true { pc += 4 }
lbb_6673:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x8]                      
    ldxdw r2, [r10-0x90]                    
    stxdw [r2+0x3b0], r1                    
lbb_6677:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x10]                     
    jne r1, 1, lbb_6684                             if r1 != (1 as i32 as i64 as u64) { pc += 4 }
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x18]                     
    ldxdw r2, [r10-0x90]                    
    stxdw [r2+0x3b8], r1                    
lbb_6684:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x20]                     
    jne r1, 1, lbb_6691                             if r1 != (1 as i32 as i64 as u64) { pc += 4 }
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x28]                     
    ldxdw r2, [r10-0x90]                    
    stxdw [r2+0x3c0], r1                    
lbb_6691:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0xf8]                     
    jne r1, 1, lbb_6731                             if r1 != (1 as i32 as i64 as u64) { pc += 37 }
    lddw r1, 0x10002d4b7 --> b"assertion failed: args.order_full_expiry_slots.is_none()"        r1 load str located at 4295152823
    mov64 r2, 56                                    r2 = 56 as i32 as i64 as u64
    lddw r3, 0x10002f000 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x1a\x02\…        r3 load str located at 4295159808
    call function_18704                     
lbb_6700:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0xf8]                     
    jne r1, 0, lbb_6704                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6708                                     if true { pc += 4 }
lbb_6704:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x100]                    
    ldxdw r2, [r10-0x90]                    
    stxdw [r2+0x3b0], r1                    
lbb_6708:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x108]                    
    jne r1, 1, lbb_6715                             if r1 != (1 as i32 as i64 as u64) { pc += 4 }
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x110]                    
    ldxdw r2, [r10-0x90]                    
    stxdw [r2+0x3b8], r1                    
lbb_6715:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x118]                    
    jne r1, 1, lbb_6722                             if r1 != (1 as i32 as i64 as u64) { pc += 4 }
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x120]                    
    ldxdw r2, [r10-0x90]                    
    stxdw [r2+0x3c0], r1                    
lbb_6722:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x0]                      
    jne r1, 1, lbb_6740                             if r1 != (1 as i32 as i64 as u64) { pc += 15 }
    lddw r1, 0x10002d58f --> b"assertion failed: args.order_full_expiry_seconds.is_none()"        r1 load str located at 4295153039
    mov64 r2, 58                                    r2 = 58 as i32 as i64 as u64
    lddw r3, 0x10002f048 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00+\x02\x00…        r3 load str located at 4295159880
    call function_18704                     
lbb_6731:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x108]                    
    jne r1, 1, lbb_6749                             if r1 != (1 as i32 as i64 as u64) { pc += 15 }
    lddw r1, 0x10002d4ef --> b"assertion failed: args.order_expiry_slow_spread_growth_micros_per_slot.is…        r1 load str located at 4295152879
    mov64 r2, 80                                    r2 = 80 as i32 as i64 as u64
    lddw r3, 0x10002f018 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x1b\x02\…        r3 load str located at 4295159832
    call function_18704                     
lbb_6740:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x10]                     
    jne r1, 1, lbb_6758                             if r1 != (1 as i32 as i64 as u64) { pc += 15 }
    lddw r1, 0x10002d5c9 --> b"assertion failed: args.order_expiry_slow_spread_growth_millis_per_second.…        r1 load str located at 4295153097
    mov64 r2, 82                                    r2 = 82 as i32 as i64 as u64
    lddw r3, 0x10002f060 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00,\x02\x00…        r3 load str located at 4295159904
    call function_18704                     
lbb_6749:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x118]                    
    jne r1, 1, lbb_6767                             if r1 != (1 as i32 as i64 as u64) { pc += 15 }
    lddw r1, 0x10002d53f --> b"assertion failed: args.order_expiry_fast_spread_growth_micros_per_slot.is…        r1 load str located at 4295152959
    mov64 r2, 80                                    r2 = 80 as i32 as i64 as u64
    lddw r3, 0x10002f030 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x1e\x02\…        r3 load str located at 4295159856
    call function_18704                     
lbb_6758:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x20]                     
    jne r1, 1, lbb_6767                             if r1 != (1 as i32 as i64 as u64) { pc += 6 }
    lddw r1, 0x10002d61b --> b"assertion failed: args.order_expiry_fast_spread_growth_millis_per_second.…        r1 load str located at 4295153179
    mov64 r2, 82                                    r2 = 82 as i32 as i64 as u64
    lddw r3, 0x10002f078 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00/\x02\x00…        r3 load str located at 4295159928
    call function_18704                     
lbb_6767:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x30]                     
    jne r1, 0, lbb_6771                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6777                                     if true { pc += 6 }
lbb_6771:
    ldxdw r1, [r10-0x90]                    
    add64 r1, 968                                   r1 += 968   ///  r1 = r1.wrapping_add(968 as i32 as i64 as u64)
    ldxdw r2, [r10-0x48]                    
    add64 r2, 56                                    r2 += 56   ///  r2 = r2.wrapping_add(56 as i32 as i64 as u64)
    mov64 r3, 160                                   r3 = 160 as i32 as i64 as u64
    call function_21513                     
lbb_6777:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x140]                    
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    jeq r1, r2, lbb_6810                            if r1 == r2 { pc += 28 }
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x150]                    
    mov64 r6, r1                                    r6 = r1
    jlt r1, 21, lbb_6791                            if r1 < (21 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 20                                    r2 = 20 as i32 as i64 as u64
    lddw r3, 0x10002f090 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00:\x02\x00…        r3 load str located at 4295159952
    call function_20247                     
lbb_6791:
    ldxdw r1, [r10-0x48]                    
    ldxdw r2, [r1+0x148]                    
    ldxdw r7, [r10-0x90]                    
    add64 r7, 1288                                  r7 += 1288   ///  r7 = r7.wrapping_add(1288 as i32 as i64 as u64)
    mov64 r8, r6                                    r8 = r6
    lsh64 r8, 5                                     r8 <<= 5   ///  r8 = r8.wrapping_shl(5)
    mov64 r1, r7                                    r1 = r7
    mov64 r3, r8                                    r3 = r8
    call function_21513                     
    jeq r6, 20, lbb_6810                            if r6 == (20 as i32 as i64 as u64) { pc += 9 }
lbb_6801:
    mov64 r1, r7                                    r1 = r7
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    stdw [r1+0x18], 0                       
    stdw [r1+0x10], 0                       
    stdw [r1+0x8], 0                        
    stdw [r1+0x0], 0                        
    add64 r8, 32                                    r8 += 32   ///  r8 = r8.wrapping_add(32 as i32 as i64 as u64)
    jeq r8, 640, lbb_6810                           if r8 == (640 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6801                                     if true { pc += -9 }
lbb_6810:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x158]                    
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    jeq r1, r2, lbb_6844                            if r1 == r2 { pc += 29 }
    ldxdw r1, [r10-0x48]                    
    ldxdw r6, [r1+0x168]                    
    ldxdw r1, [r10-0x90]                    
    stxb [r1+0x79b], r6                     
    jlt r6, 11, lbb_6825                            if r6 < (11 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    lddw r3, 0x10002f0a8 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00A\x02\x00…        r3 load str located at 4295159976
    call function_20247                     
lbb_6825:
    ldxdw r1, [r10-0x48]                    
    ldxdw r2, [r1+0x160]                    
    ldxdw r7, [r10-0x90]                    
    add64 r7, 1952                                  r7 += 1952   ///  r7 = r7.wrapping_add(1952 as i32 as i64 as u64)
    mov64 r8, r6                                    r8 = r6
    lsh64 r8, 5                                     r8 <<= 5   ///  r8 = r8.wrapping_shl(5)
    mov64 r1, r7                                    r1 = r7
    mov64 r3, r8                                    r3 = r8
    call function_21513                     
    jeq r6, 10, lbb_6844                            if r6 == (10 as i32 as i64 as u64) { pc += 9 }
lbb_6835:
    mov64 r1, r7                                    r1 = r7
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    stdw [r1+0x18], 0                       
    stdw [r1+0x10], 0                       
    stdw [r1+0x8], 0                        
    stdw [r1+0x0], 0                        
    add64 r8, 32                                    r8 += 32   ///  r8 = r8.wrapping_add(32 as i32 as i64 as u64)
    jeq r8, 320, lbb_6844                           if r8 == (320 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6835                                     if true { pc += -9 }
lbb_6844:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0xd8]                     
    jne r1, 0, lbb_6848                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6852                                     if true { pc += 4 }
lbb_6848:
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0xe0]                     
    ldxdw r2, [r10-0x90]                    
    stxdw [r2+0x788], r1                    
lbb_6852:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    ldxdw r3, [r10-0x68]                    
    jeq r3, r2, lbb_6858                            if r3 == r2 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_6858:
    xor64 r9, 1                                     r9 ^= 1   ///  r9 = r9.xor(1)
    and64 r1, r9                                    r1 &= r9   ///  r1 = r1.and(r9)
    jne r1, 0, lbb_7030                             if r1 != (0 as i32 as i64 as u64) { pc += 169 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r3, [r10-0x90]                    
    ldxw r1, [r3+0x174]                     
    ldxw r2, [r3+0x170]                     
    ldxdw r3, [r3+0x168]                    
    jeq r3, 0, lbb_6869                             if r3 == (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r3, [r10-0x90]                    
    ldxw r4, [r3+0x178]                     
lbb_6869:
    jlt r1, r2, lbb_7347                            if r1 < r2 { pc += 477 }
    ldxdw r2, [r10-0x90]                    
    ldxw r1, [r2+0x370]                     
    stxdw [r10-0x160], r1                   
    ldxw r1, [r2+0x36c]                     
    stxdw [r10-0x120], r1                   
    ldxw r1, [r2+0x368]                     
    stxdw [r10-0x128], r1                   
    ldxdw r1, [r2+0x360]                    
    stxdw [r10-0x118], r1                   
    ldxw r1, [r2+0x338]                     
    stxdw [r10-0x158], r1                   
    ldxw r1, [r2+0x334]                     
    stxdw [r10-0x108], r1                   
    ldxw r1, [r2+0x330]                     
    stxdw [r10-0x110], r1                   
    ldxdw r1, [r2+0x328]                    
    stxdw [r10-0x100], r1                   
    ldxw r1, [r2+0x300]                     
    stxdw [r10-0x150], r1                   
    ldxw r1, [r2+0x2fc]                     
    stxdw [r10-0xf0], r1                    
    ldxw r1, [r2+0x2f8]                     
    stxdw [r10-0xf8], r1                    
    ldxdw r1, [r2+0x2f0]                    
    stxdw [r10-0xe8], r1                    
    ldxw r1, [r2+0x2c8]                     
    stxdw [r10-0x148], r1                   
    ldxw r1, [r2+0x2c4]                     
    stxdw [r10-0xd8], r1                    
    ldxw r1, [r2+0x2c0]                     
    stxdw [r10-0xe0], r1                    
    ldxdw r1, [r2+0x2b8]                    
    stxdw [r10-0xd0], r1                    
    ldxw r1, [r2+0x290]                     
    stxdw [r10-0x140], r1                   
    ldxw r1, [r2+0x28c]                     
    stxdw [r10-0xc0], r1                    
    ldxw r1, [r2+0x288]                     
    stxdw [r10-0xc8], r1                    
    ldxdw r1, [r2+0x280]                    
    stxdw [r10-0xb8], r1                    
    ldxw r1, [r2+0x258]                     
    stxdw [r10-0x138], r1                   
    ldxw r1, [r2+0x254]                     
    stxdw [r10-0xa8], r1                    
    ldxw r1, [r2+0x250]                     
    stxdw [r10-0xb0], r1                    
    ldxdw r1, [r2+0x248]                    
    stxdw [r10-0xa0], r1                    
    ldxw r1, [r2+0x220]                     
    stxdw [r10-0x130], r1                   
    ldxw r1, [r2+0x21c]                     
    stxdw [r10-0x68], r1                    
    ldxw r1, [r2+0x218]                     
    stxdw [r10-0x88], r1                    
    ldxdw r1, [r2+0x210]                    
    stxdw [r10-0x48], r1                    
    ldxw r0, [r2+0x1e8]                     
    ldxw r7, [r2+0x1e4]                     
    ldxw r5, [r2+0x1e0]                     
    ldxdw r8, [r2+0x1d8]                    
    ldxw r1, [r2+0x1ac]                     
    ldxw r9, [r2+0x1a8]                     
    ldxdw r6, [r2+0x1a0]                    
    jeq r6, 0, lbb_6943                             if r6 == (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r2, [r10-0x90]                    
    ldxw r6, [r2+0x1b0]                     
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    mov64 r2, r6                                    r2 = r6
    mov64 r4, r6                                    r4 = r6
    jlt r2, r3, lbb_7353                            if r2 < r3 { pc += 410 }
lbb_6943:
    mov64 r2, r9                                    r2 = r9
    jlt r1, r2, lbb_7347                            if r1 < r2 { pc += 402 }
    jeq r8, 0, lbb_6952                             if r8 == (0 as i32 as i64 as u64) { pc += 6 }
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r2, r0                                    r2 = r0
    mov64 r4, r0                                    r4 = r0
    jlt r2, r1, lbb_7353                            if r2 < r1 { pc += 401 }
lbb_6952:
    mov64 r1, r5                                    r1 = r5
    mov64 r2, r7                                    r2 = r7
    jlt r2, r1, lbb_7347                            if r2 < r1 { pc += 392 }
    ldxdw r1, [r10-0x48]                    
    jeq r1, 0, lbb_6963                             if r1 == (0 as i32 as i64 as u64) { pc += 6 }
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    ldxdw r4, [r10-0x130]                   
    mov64 r2, r4                                    r2 = r4
    jlt r2, r1, lbb_7353                            if r2 < r1 { pc += 390 }
lbb_6963:
    ldxdw r1, [r10-0x88]                    
    ldxdw r2, [r10-0x68]                    
    jlt r2, r1, lbb_7347                            if r2 < r1 { pc += 381 }
    ldxdw r1, [r10-0xa0]                    
    jeq r1, 0, lbb_6974                             if r1 == (0 as i32 as i64 as u64) { pc += 6 }
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    ldxdw r4, [r10-0x138]                   
    mov64 r2, r4                                    r2 = r4
    jlt r2, r1, lbb_7353                            if r2 < r1 { pc += 379 }
lbb_6974:
    ldxdw r1, [r10-0xb0]                    
    ldxdw r2, [r10-0xa8]                    
    jlt r2, r1, lbb_7347                            if r2 < r1 { pc += 370 }
    ldxdw r1, [r10-0xb8]                    
    jeq r1, 0, lbb_6985                             if r1 == (0 as i32 as i64 as u64) { pc += 6 }
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    ldxdw r4, [r10-0x140]                   
    mov64 r2, r4                                    r2 = r4
    jlt r2, r1, lbb_7353                            if r2 < r1 { pc += 368 }
lbb_6985:
    ldxdw r1, [r10-0xc8]                    
    ldxdw r2, [r10-0xc0]                    
    jlt r2, r1, lbb_7347                            if r2 < r1 { pc += 359 }
    ldxdw r1, [r10-0xd0]                    
    jeq r1, 0, lbb_6996                             if r1 == (0 as i32 as i64 as u64) { pc += 6 }
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    ldxdw r4, [r10-0x148]                   
    mov64 r2, r4                                    r2 = r4
    jlt r2, r1, lbb_7353                            if r2 < r1 { pc += 357 }
lbb_6996:
    ldxdw r1, [r10-0xe0]                    
    ldxdw r2, [r10-0xd8]                    
    jlt r2, r1, lbb_7347                            if r2 < r1 { pc += 348 }
    ldxdw r1, [r10-0xe8]                    
    jeq r1, 0, lbb_7007                             if r1 == (0 as i32 as i64 as u64) { pc += 6 }
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    ldxdw r4, [r10-0x150]                   
    mov64 r2, r4                                    r2 = r4
    jlt r2, r1, lbb_7353                            if r2 < r1 { pc += 346 }
lbb_7007:
    ldxdw r1, [r10-0xf8]                    
    ldxdw r2, [r10-0xf0]                    
    jlt r2, r1, lbb_7347                            if r2 < r1 { pc += 337 }
    ldxdw r1, [r10-0x100]                   
    jeq r1, 0, lbb_7018                             if r1 == (0 as i32 as i64 as u64) { pc += 6 }
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    ldxdw r4, [r10-0x158]                   
    mov64 r2, r4                                    r2 = r4
    jlt r2, r1, lbb_7353                            if r2 < r1 { pc += 335 }
lbb_7018:
    ldxdw r1, [r10-0x110]                   
    ldxdw r2, [r10-0x108]                   
    jlt r2, r1, lbb_7347                            if r2 < r1 { pc += 326 }
    ldxdw r1, [r10-0x118]                   
    jeq r1, 0, lbb_7027                             if r1 == (0 as i32 as i64 as u64) { pc += 4 }
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    ldxdw r1, [r10-0x160]                   
    jlt r1, r4, lbb_7353                            if r1 < r4 { pc += 326 }
lbb_7027:
    ldxdw r1, [r10-0x128]                   
    ldxdw r2, [r10-0x120]                   
    jlt r2, r1, lbb_7347                            if r2 < r1 { pc += 317 }
lbb_7030:
    ldxdw r1, [r10-0x90]                    
    ldxw r1, [r1+0x3ac]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jsgt r1, -1, lbb_7041                           if (r1 as i64) > (-1 as i32 as i64) { pc += 6 }
    lddw r1, 0x10002d66d --> b"assertion failed: config.quantity_skew_limit_bps >= 0"        r1 load str located at 4295153261
    mov64 r2, 53                                    r2 = 53 as i32 as i64 as u64
    lddw r3, 0x10002f0c0 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00V\x02\x00…        r3 load str located at 4295160000
    call function_18704                     
lbb_7041:
    ldxdw r1, [r10-0x90]                    
    ldxw r1, [r1+0x3a8]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jsgt r1, -1, lbb_7052                           if (r1 as i64) > (-1 as i32 as i64) { pc += 6 }
    lddw r1, 0x10002d6a2 --> b"assertion failed: config.quantity_skew_strength >= 0"        r1 load str located at 4295153314
    mov64 r2, 52                                    r2 = 52 as i32 as i64 as u64
    lddw r3, 0x10002f0d8 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00W\x02\x00…        r3 load str located at 4295160024
    call function_18704                     
lbb_7052:
    ldxdw r1, [r10-0x90]                    
    ldxw r1, [r1+0x39c]                     
    lddw r2, 0x80000000                             r2 load str located at 2147483648
    jeq r1, r2, lbb_7392                            if r1 == r2 { pc += 335 }
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    ldxdw r2, [r10-0x90]                    
    ldxw r2, [r2+0x398]                     
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    jslt r2, r1, lbb_7371                           if (r2 as i64) < (r1 as i64) { pc += 306 }
    ldxdw r1, [r10-0x90]                    
    ldxw r1, [r1+0x3a4]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    jsgt r1, -1, lbb_7076                           if (r1 as i64) > (-1 as i32 as i64) { pc += 6 }
    lddw r1, 0x10002d726 --> b"assertion failed: config.gradual_aggressive_skew_step_millibps >= 0"        r1 load str located at 4295153446
    mov64 r2, 67                                    r2 = 67 as i32 as i64 as u64
    lddw r3, 0x10002f120 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00Y\x02\x00…        r3 load str located at 4295160096
    call function_18704                     
lbb_7076:
    ldxdw r1, [r10-0x90]                    
    ldxw r1, [r1+0x3a0]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mul64 r1, 1000                                  r1 *= 1000   ///  r1 = r1.wrapping_mul(1000 as u64)
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    jne r2, r1, lbb_7085                            if r2 != r1 { pc += 0 }
lbb_7085:
    jeq r2, r1, lbb_7089                            if r2 == r1 { pc += 3 }
    lddw r1, 0x10002f138 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00]\x02\x00…        r1 load str located at 4295160120
    call function_20968                     
lbb_7089:
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    ldxdw r2, [r10-0x90]                    
    ldxw r2, [r2+0x1508]                    
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    jsgt r2, r1, lbb_7097                           if (r2 as i64) > (r1 as i64) { pc += 1 }
    mov64 r2, r1                                    r2 = r1
lbb_7097:
    ldxdw r3, [r10-0x90]                    
    stxw [r3+0x1508], r2                    
    ldxw r2, [r3+0x150c]                    
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    jsgt r2, r1, lbb_7104                           if (r2 as i64) > (r1 as i64) { pc += 1 }
    mov64 r2, r1                                    r2 = r1
lbb_7104:
    ldxdw r1, [r10-0x90]                    
    stxw [r1+0x150c], r2                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    ldxdw r2, [r10-0x50]                    
    call function_11320                     
    ldxw r9, [r10-0x8]                      
    ldxw r6, [r10-0xc]                      
    ldxw r1, [r10-0x10]                     
    jne r1, 0, lbb_7151                             if r1 != (0 as i32 as i64 as u64) { pc += 37 }
    ldxdw r7, [r10-0x8]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    ldxdw r2, [r10-0x70]                    
    call function_11320                     
    ldxw r9, [r10-0x8]                      
    ldxw r6, [r10-0xc]                      
    ldxw r1, [r10-0x10]                     
    jne r1, 0, lbb_7151                             if r1 != (0 as i32 as i64 as u64) { pc += 28 }
    ldxdw r1, [r10-0x8]                     
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0x80]                    
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -56                                   r1 += -56   ///  r1 = r1.wrapping_add(-56 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0x98]                    
    ldxdw r3, [r10-0x60]                    
    mov64 r4, r7                                    r4 = r7
    call function_3128                      
    ldxw r9, [r10-0x34]                     
    ldxw r6, [r10-0x38]                     
    jne r6, 26, lbb_7151                            if r6 != (26 as i32 as i64 as u64) { pc += 14 }
    ldxdw r2, [r10-0x90]                    
    ldxb r1, [r2+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r2+0x0], r1                       
    ldxdw r2, [r10-0x78]                    
    ldxb r1, [r2+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r2+0x0], r1                       
    ldxdw r2, [r10-0x58]                    
    ldxb r1, [r2+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r2+0x0], r1                       
    mov64 r6, 26                                    r6 = 26 as i32 as i64 as u64
    ja lbb_5993                                     if true { pc += -1158 }
lbb_7151:
    ldxdw r2, [r10-0x90]                    
    ldxb r1, [r2+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r2+0x0], r1                       
    ja lbb_6105                                     if true { pc += -1051 }
lbb_7156:
    ldxdw r4, [r3+0x18]                     
    ldxdw r5, [r10-0x90]                    
    stxdw [r5+0x1a0], r4                    
    ldxw r4, [r3+0x20]                      
    stxw [r5+0x1a8], r4                     
    ldxw r4, [r3+0x24]                      
    stxw [r5+0x1ac], r4                     
    ldxw r4, [r3+0x28]                      
    stxw [r5+0x1b0], r4                     
    jeq r2, 2, lbb_6565                             if r2 == (2 as i32 as i64 as u64) { pc += -601 }
    ldxdw r4, [r3+0x30]                     
    ldxdw r5, [r10-0x90]                    
    stxdw [r5+0x1d8], r4                    
    ldxw r4, [r3+0x38]                      
    stxw [r5+0x1e0], r4                     
    ldxw r4, [r3+0x3c]                      
    stxw [r5+0x1e4], r4                     
    ldxw r4, [r3+0x40]                      
    stxw [r5+0x1e8], r4                     
    jeq r2, 3, lbb_6565                             if r2 == (3 as i32 as i64 as u64) { pc += -611 }
    ldxdw r4, [r3+0x48]                     
    ldxdw r5, [r10-0x90]                    
    stxdw [r5+0x210], r4                    
    ldxw r4, [r3+0x50]                      
    stxw [r5+0x218], r4                     
    ldxw r4, [r3+0x54]                      
    stxw [r5+0x21c], r4                     
    ldxw r4, [r3+0x58]                      
    stxw [r5+0x220], r4                     
    jeq r2, 4, lbb_6565                             if r2 == (4 as i32 as i64 as u64) { pc += -621 }
    ldxdw r4, [r3+0x60]                     
    ldxdw r5, [r10-0x90]                    
    stxdw [r5+0x248], r4                    
    ldxw r4, [r3+0x68]                      
    stxw [r5+0x250], r4                     
    ldxw r4, [r3+0x6c]                      
    stxw [r5+0x254], r4                     
    ldxw r4, [r3+0x70]                      
    stxw [r5+0x258], r4                     
    jeq r2, 5, lbb_6565                             if r2 == (5 as i32 as i64 as u64) { pc += -631 }
    ldxdw r4, [r3+0x78]                     
    ldxdw r5, [r10-0x90]                    
    stxdw [r5+0x280], r4                    
    ldxw r4, [r3+0x80]                      
    stxw [r5+0x288], r4                     
    ldxw r4, [r3+0x84]                      
    stxw [r5+0x28c], r4                     
    ldxw r4, [r3+0x88]                      
    stxw [r5+0x290], r4                     
    jeq r2, 6, lbb_6565                             if r2 == (6 as i32 as i64 as u64) { pc += -641 }
    ldxdw r4, [r3+0x90]                     
    ldxdw r5, [r10-0x90]                    
    stxdw [r5+0x2b8], r4                    
    ldxw r4, [r3+0x98]                      
    stxw [r5+0x2c0], r4                     
    ldxw r4, [r3+0x9c]                      
    stxw [r5+0x2c4], r4                     
    ldxw r4, [r3+0xa0]                      
    stxw [r5+0x2c8], r4                     
    jeq r2, 7, lbb_6565                             if r2 == (7 as i32 as i64 as u64) { pc += -651 }
    ldxdw r4, [r3+0xa8]                     
    ldxdw r5, [r10-0x90]                    
    stxdw [r5+0x2f0], r4                    
    ldxw r4, [r3+0xb0]                      
    stxw [r5+0x2f8], r4                     
    ldxw r4, [r3+0xb4]                      
    stxw [r5+0x2fc], r4                     
    ldxw r4, [r3+0xb8]                      
    stxw [r5+0x300], r4                     
    jeq r2, 8, lbb_6565                             if r2 == (8 as i32 as i64 as u64) { pc += -661 }
    ldxdw r4, [r3+0xc0]                     
    ldxdw r5, [r10-0x90]                    
    stxdw [r5+0x328], r4                    
    ldxw r4, [r3+0xc8]                      
    stxw [r5+0x330], r4                     
    ldxw r4, [r3+0xcc]                      
    stxw [r5+0x334], r4                     
    ldxw r4, [r3+0xd0]                      
    stxw [r5+0x338], r4                     
    jeq r2, 9, lbb_6565                             if r2 == (9 as i32 as i64 as u64) { pc += -671 }
    ldxdw r1, [r3+0xd8]                     
    ldxdw r4, [r10-0x90]                    
    stxdw [r4+0x360], r1                    
    ldxw r1, [r3+0xe0]                      
    stxw [r4+0x368], r1                     
    ldxw r1, [r3+0xe4]                      
    stxw [r4+0x36c], r1                     
    ldxw r1, [r3+0xe8]                      
    stxw [r4+0x370], r1                     
    jeq r2, 10, lbb_7359                            if r2 == (10 as i32 as i64 as u64) { pc += 113 }
    mov64 r1, 10                                    r1 = 10 as i32 as i64 as u64
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    lddw r3, 0x10002efa0 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xe6\x01\…        r3 load str located at 4295159712
    call function_18717                     
lbb_7251:
    ldxdw r3, [r2+0x18]                     
    ldxdw r4, [r10-0x90]                    
    stxdw [r4+0x1a0], r3                    
    ldxw r3, [r2+0x20]                      
    stxw [r4+0x1a8], r3                     
    ldxw r3, [r2+0x24]                      
    stxw [r4+0x1ac], r3                     
    ldxw r3, [r2+0x28]                      
    stxw [r4+0x1b0], r3                     
    jeq r1, 2, lbb_6601                             if r1 == (2 as i32 as i64 as u64) { pc += -660 }
    ldxdw r3, [r2+0x30]                     
    ldxdw r4, [r10-0x90]                    
    stxdw [r4+0x1d8], r3                    
    ldxw r3, [r2+0x38]                      
    stxw [r4+0x1e0], r3                     
    ldxw r3, [r2+0x3c]                      
    stxw [r4+0x1e4], r3                     
    ldxw r3, [r2+0x40]                      
    stxw [r4+0x1e8], r3                     
    jeq r1, 3, lbb_6601                             if r1 == (3 as i32 as i64 as u64) { pc += -670 }
    ldxdw r3, [r2+0x48]                     
    ldxdw r4, [r10-0x90]                    
    stxdw [r4+0x210], r3                    
    ldxw r3, [r2+0x50]                      
    stxw [r4+0x218], r3                     
    ldxw r3, [r2+0x54]                      
    stxw [r4+0x21c], r3                     
    ldxw r3, [r2+0x58]                      
    stxw [r4+0x220], r3                     
    jeq r1, 4, lbb_6601                             if r1 == (4 as i32 as i64 as u64) { pc += -680 }
    ldxdw r3, [r2+0x60]                     
    ldxdw r4, [r10-0x90]                    
    stxdw [r4+0x248], r3                    
    ldxw r3, [r2+0x68]                      
    stxw [r4+0x250], r3                     
    ldxw r3, [r2+0x6c]                      
    stxw [r4+0x254], r3                     
    ldxw r3, [r2+0x70]                      
    stxw [r4+0x258], r3                     
    jeq r1, 5, lbb_6601                             if r1 == (5 as i32 as i64 as u64) { pc += -690 }
    ldxdw r3, [r2+0x78]                     
    ldxdw r4, [r10-0x90]                    
    stxdw [r4+0x280], r3                    
    ldxw r3, [r2+0x80]                      
    stxw [r4+0x288], r3                     
    ldxw r3, [r2+0x84]                      
    stxw [r4+0x28c], r3                     
    ldxw r3, [r2+0x88]                      
    stxw [r4+0x290], r3                     
    jeq r1, 6, lbb_6601                             if r1 == (6 as i32 as i64 as u64) { pc += -700 }
    ldxdw r3, [r2+0x90]                     
    ldxdw r4, [r10-0x90]                    
    stxdw [r4+0x2b8], r3                    
    ldxw r3, [r2+0x98]                      
    stxw [r4+0x2c0], r3                     
    ldxw r3, [r2+0x9c]                      
    stxw [r4+0x2c4], r3                     
    ldxw r3, [r2+0xa0]                      
    stxw [r4+0x2c8], r3                     
    jeq r1, 7, lbb_6601                             if r1 == (7 as i32 as i64 as u64) { pc += -710 }
    ldxdw r3, [r2+0xa8]                     
    ldxdw r4, [r10-0x90]                    
    stxdw [r4+0x2f0], r3                    
    ldxw r3, [r2+0xb0]                      
    stxw [r4+0x2f8], r3                     
    ldxw r3, [r2+0xb4]                      
    stxw [r4+0x2fc], r3                     
    ldxw r3, [r2+0xb8]                      
    stxw [r4+0x300], r3                     
    jeq r1, 8, lbb_6601                             if r1 == (8 as i32 as i64 as u64) { pc += -720 }
    ldxdw r3, [r2+0xc0]                     
    ldxdw r4, [r10-0x90]                    
    stxdw [r4+0x328], r3                    
    ldxw r3, [r2+0xc8]                      
    stxw [r4+0x330], r3                     
    ldxw r3, [r2+0xcc]                      
    stxw [r4+0x334], r3                     
    ldxw r3, [r2+0xd0]                      
    stxw [r4+0x338], r3                     
    jeq r1, 9, lbb_6601                             if r1 == (9 as i32 as i64 as u64) { pc += -730 }
    ldxdw r3, [r2+0xd8]                     
    ldxdw r4, [r10-0x90]                    
    stxdw [r4+0x360], r3                    
    ldxw r3, [r2+0xe0]                      
    stxw [r4+0x368], r3                     
    ldxw r3, [r2+0xe4]                      
    stxw [r4+0x36c], r3                     
    ldxw r2, [r2+0xe8]                      
    stxw [r4+0x370], r2                     
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jeq r1, 10, lbb_6617                            if r1 == (10 as i32 as i64 as u64) { pc += -725 }
    mov64 r1, 10                                    r1 = 10 as i32 as i64 as u64
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    lddw r3, 0x10002efe8 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xf4\x01\…        r3 load str located at 4295159784
    call function_18717                     
lbb_7347:
    lddw r1, 0x10002d796 --> b"assertion failed: layer.expiry_fast_start >= layer.expiry_slow_start"        r1 load str located at 4295153558
    mov64 r2, 68                                    r2 = 68 as i32 as i64 as u64
    lddw r3, 0x10002f168 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00S\x02\x00…        r3 load str located at 4295160168
    call function_18704                     
lbb_7353:
    lddw r1, 0x10002d769 --> b"assertion failed: layer.spread >= prev_spread"        r1 load str located at 4295153513
    mov64 r2, 45                                    r2 = 45 as i32 as i64 as u64
    lddw r3, 0x10002f150 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00P\x02\x00…        r3 load str located at 4295160144
    call function_18704                     
lbb_7359:
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x170]                    
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    jeq r1, r2, lbb_6617                            if r1 == r2 { pc += -748 }
lbb_7365:
    lddw r1, 0x10002d3e5 --> b"assertion failed: quoting_version != QuotingVersion::V1 as u8"        r1 load str located at 4295152613
    mov64 r2, 61                                    r2 = 61 as i32 as i64 as u64
    lddw r3, 0x10002efd0 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xf2\x01\…        r3 load str located at 4295159760
    call function_18704                     
lbb_7371:
    lddw r1, 0x10002d6d6 --> b"assertion failed: config.max_aggressive_skew_bps >= -config.max_passive_s…        r1 load str located at 4295153366
    mov64 r2, 80                                    r2 = 80 as i32 as i64 as u64
    lddw r3, 0x10002f108 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00X\x02\x00…        r3 load str located at 4295160072
    call function_18704                     
lbb_7377:
    lddw r1, 0x10002d47a --> b"assertion failed: quoting_version == QuotingVersion::V1 as u8"        r1 load str located at 4295152762
    mov64 r2, 61                                    r2 = 61 as i32 as i64 as u64
    lddw r3, 0x10002efb8 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xe4\x01\…        r3 load str located at 4295159736
    call function_18704                     
lbb_7383:
    lddw r1, 0x10002ef58 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xcb\x01\…        r1 load str located at 4295159640
    call function_20968                     
lbb_7386:
    lddw r1, 0x10002ef70 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xcd\x01\…        r1 load str located at 4295159664
    call function_20968                     
lbb_7389:
    lddw r1, 0x10002ef88 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xce\x01\…        r1 load str located at 4295159688
    call function_20968                     
lbb_7392:
    lddw r1, 0x10002f0f0 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00X\x02\x00…        r1 load str located at 4295160048
    call function_20979                     
lbb_7395:
    lddw r1, 0x10002ee98 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xbe\x01\…        r1 load str located at 4295159448
    call function_20968                     
lbb_7398:
    lddw r1, 0x10002eeb0 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xc3\x01\…        r1 load str located at 4295159472
    call function_20968                     
lbb_7401:
    lddw r1, 0x10002eec8 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xc6\x01\…        r1 load str located at 4295159496
    call function_20968                     
lbb_7404:
    lddw r1, 0x10002eee0 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xc7\x01\…        r1 load str located at 4295159520
    call function_20968                     
lbb_7407:
    lddw r1, 0x10002eef8 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xd3\x01\…        r1 load str located at 4295159544
    call function_20968                     
lbb_7410:
    lddw r1, 0x10002ef10 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xd4\x01\…        r1 load str located at 4295159568
    call function_20968                     
lbb_7413:
    lddw r1, 0x10002ef28 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xd9\x01\…        r1 load str located at 4295159592
    call function_20968                     
lbb_7416:
    lddw r1, 0x10002ef40 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xdb\x01\…        r1 load str located at 4295159616
    call function_20968                     

function_7419:
    mov64 r7, r4                                    r7 = r4
    mov64 r9, r3                                    r9 = r3
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r9+0x20]                     
    ldxdw r8, [r1+0x0]                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r1, 22                                    r1 = 22 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_7463                             if r0 != (0 as i32 as i64 as u64) { pc += 31 }
    lddw r1, 0x800000000                            r1 load str located at 34359738368
    ldxdw r2, [r8+0x50]                     
    jne r2, 7456, lbb_7463                          if r2 != (7456 as i32 as i64 as u64) { pc += 27 }
    lddw r1, 0x200000000                            r1 load str located at 8589934592
    ldxb r2, [r8+0x2]                       
    jeq r2, 0, lbb_7463                             if r2 == (0 as i32 as i64 as u64) { pc += 23 }
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    ldxb r2, [r8+0x0]                       
    mov64 r3, r2                                    r3 = r2
    and64 r3, 15                                    r3 &= 15   ///  r3 = r3.and(15)
    jne r3, 15, lbb_7463                            if r3 != (15 as i32 as i64 as u64) { pc += 18 }
    mov64 r4, r2                                    r4 = r2
    and64 r4, 247                                   r4 &= 247   ///  r4 = r4.and(247)
    stxb [r8+0x0], r4                       
    mov64 r3, r8                                    r3 = r8
    add64 r3, 88                                    r3 += 88   ///  r3 = r3.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r3                                    r1 = r3
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jeq r1, 0, lbb_7458                             if r1 == (0 as i32 as i64 as u64) { pc += 5 }
    lddw r1, 0x10002d1f5 --> b"from_bytes_mut"        r1 load str located at 4295152117
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_2131                      
lbb_7458:
    ldxdw r1, [r3+0x0]                      
    jeq r1, 4, lbb_7468                             if r1 == (4 as i32 as i64 as u64) { pc += 8 }
    stxb [r8+0x0], r2                       
    lddw r1, 0x900000000                            r1 load str located at 38654705664
lbb_7463:
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
lbb_7465:
    stxw [r6+0x4], r2                       
    stxw [r6+0x0], r1                       
    exit                                    
lbb_7468:
    stxdw [r10-0x10], r4                    
    stxdw [r10-0x18], r3                    
    ldxdw r1, [r9+0x18]                     
    ldxdw r1, [r1+0x0]                      
    mov64 r2, r8                                    r2 = r8
    add64 r2, 128                                   r2 += 128   ///  r2 = r2.wrapping_add(128 as i32 as i64 as u64)
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_7486                             if r0 == (0 as i32 as i64 as u64) { pc += 4 }
lbb_7482:
    ldxdw r3, [r10-0x10]                    
    or64 r3, 8                                      r3 |= 8   ///  r3 = r3.or(8)
    stxb [r8+0x0], r3                       
    ja lbb_7465                                     if true { pc += -21 }
lbb_7486:
    ldxdw r3, [r7+0x50]                     
    ldxdw r4, [r8+0xbf8]                    
    jlt r4, r3, lbb_7492                            if r4 < r3 { pc += 3 }
    mov64 r2, 7                                     r2 = 7 as i32 as i64 as u64
    jne r4, -1, lbb_7482                            if r4 != (-1 as i32 as i64 as u64) { pc += -9 }
    jne r3, 0, lbb_7482                             if r3 != (0 as i32 as i64 as u64) { pc += -10 }
lbb_7492:
    ldxdw r1, [r7+0x10]                     
    stxdw [r8+0xba0], r1                    
    ldxdw r1, [r7+0x8]                      
    stxdw [r8+0xba8], r1                    
    ldxdw r1, [r7+0x20]                     
    stxdw [r8+0xbc8], r1                    
    ldxdw r1, [r7+0x18]                     
    stxdw [r8+0xbd0], r1                    
    ldxdw r1, [r7+0x38]                     
    ldxb r2, [r8+0x79f]                     
    stxdw [r10-0x10], r2                    
    jne r2, 1, lbb_7507                             if r2 != (1 as i32 as i64 as u64) { pc += 3 }
    mov64 r2, r1                                    r2 = r1
    div64 r2, 1000                                  r2 /= 1000   ///  r2 = r2 / (1000 as u64)
    stxdw [r8+0xbb0], r2                    
lbb_7507:
    stxdw [r8+0xc18], r1                    
    ldxdw r1, [r7+0x60]                     
    stxdw [r8+0xbf8], r3                    
    stxdw [r8+0xc20], r1                    
    ldxw r9, [r7+0x58]                      
    mov64 r1, r9                                    r1 = r9
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r2, 1000                                  r2 = 1000 as i32 as i64 as u64
    call function_22231                     
    ldxdw r1, [r10-0x10]                    
    jeq r1, 1, lbb_7520                             if r1 == (1 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, r9                                    r0 = r9
lbb_7520:
    stxw [r8+0xbf4], r0                     
    ldxdw r1, [r7+0x40]                     
    stxdw [r8+0xbe8], r1                    
    ldxdw r1, [r7+0x48]                     
    stxdw [r8+0xc00], r1                    
    ldxdw r1, [r7+0x28]                     
    stxdw [r8+0xbb8], r1                    
    ldxdw r1, [r7+0x30]                     
    stxdw [r8+0xbc0], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -8                                    r1 += -8   ///  r1 = r1.wrapping_add(-8 as i32 as i64 as u64)
    ldxdw r2, [r10-0x18]                    
    call function_3691                      
    ldxw r1, [r10-0x8]                      
    jne r1, 26, lbb_7540                            if r1 != (26 as i32 as i64 as u64) { pc += 5 }
    ldxb r1, [r8+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r8+0x0], r1                       
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ja lbb_7465                                     if true { pc += -75 }
lbb_7540:
    ldxw r2, [r10-0x4]                      
    ldxb r3, [r8+0x0]                       
    stxdw [r10-0x10], r3                    
    ja lbb_7482                                     if true { pc += -62 }

function_7544:
    stxdw [r10-0x18], r5                    
    mov64 r7, r4                                    r7 = r4
    mov64 r9, r3                                    r9 = r3
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r7                                    r1 = r7
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r8, 22                                    r8 = 22 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_7587                             if r0 != (0 as i32 as i64 as u64) { pc += 31 }
    lddw r8, 0x800000000                            r8 load str located at 34359738368
    ldxdw r1, [r7+0x50]                     
    jne r1, 7456, lbb_7587                          if r1 != (7456 as i32 as i64 as u64) { pc += 27 }
    lddw r8, 0x200000000                            r8 load str located at 8589934592
    ldxb r1, [r7+0x2]                       
    jeq r1, 0, lbb_7587                             if r1 == (0 as i32 as i64 as u64) { pc += 23 }
    mov64 r8, 11                                    r8 = 11 as i32 as i64 as u64
    ldxb r1, [r7+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    jne r2, 15, lbb_7587                            if r2 != (15 as i32 as i64 as u64) { pc += 18 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 247                                   r2 &= 247   ///  r2 = r2.and(247)
    stxb [r7+0x0], r2                       
    mov64 r3, r7                                    r3 = r7
    add64 r3, 88                                    r3 += 88   ///  r3 = r3.wrapping_add(88 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    jeq r2, 0, lbb_7582                             if r2 == (0 as i32 as i64 as u64) { pc += 5 }
    lddw r1, 0x10002d1f5 --> b"from_bytes_mut"        r1 load str located at 4295152117
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_2131                      
lbb_7582:
    ldxdw r2, [r3+0x0]                      
    jeq r2, 4, lbb_7592                             if r2 == (4 as i32 as i64 as u64) { pc += 8 }
    stxb [r7+0x0], r1                       
    lddw r8, 0x900000000                            r8 load str located at 38654705664
lbb_7587:
    mov64 r1, r8                                    r1 = r8
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
lbb_7589:
    stxw [r6+0x4], r1                       
    stxw [r6+0x0], r8                       
    exit                                    
lbb_7592:
    stxdw [r10-0x20], r3                    
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    add64 r2, 128                                   r2 += 128   ///  r2 = r2.wrapping_add(128 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_7605                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7676                                     if true { pc += 71 }
lbb_7605:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -8                                    r1 += -8   ///  r1 = r1.wrapping_add(-8 as i32 as i64 as u64)
    syscall [invalid]                       
    jne r0, 0, lbb_7675                             if r0 != (0 as i32 as i64 as u64) { pc += 66 }
    ldxdw r1, [r10-0x8]                     
    stxdw [r7+0x1968], r1                   
    ldxdw r3, [r7+0xbf8]                    
    ldxdw r1, [r10-0x18]                    
    ldxdw r2, [r1+0x38]                     
    jlt r3, r2, lbb_7618                            if r3 < r2 { pc += 3 }
    mov64 r1, 7                                     r1 = 7 as i32 as i64 as u64
    jne r3, -1, lbb_7676                            if r3 != (-1 as i32 as i64 as u64) { pc += 59 }
    jne r2, 0, lbb_7676                             if r2 != (0 as i32 as i64 as u64) { pc += 58 }
lbb_7618:
    ldxdw r1, [r10-0x18]                    
    ldxdw r3, [r1+0x8]                      
    stxdw [r10-0x38], r3                    
    stxdw [r7+0xba0], r3                    
    ldxdw r3, [r1+0x0]                      
    stxdw [r10-0x28], r3                    
    stxdw [r7+0xba8], r3                    
    ldxdw r3, [r1+0x18]                     
    stxdw [r10-0x30], r3                    
    stxdw [r7+0xbc8], r3                    
    ldxdw r3, [r1+0x10]                     
    stxdw [r10-0x40], r3                    
    stxdw [r7+0xbd0], r3                    
    ldxdw r1, [r1+0x20]                     
    ldxb r8, [r7+0x79f]                     
    jne r8, 1, lbb_7637                             if r8 != (1 as i32 as i64 as u64) { pc += 3 }
    mov64 r3, r1                                    r3 = r1
    div64 r3, 1000                                  r3 /= 1000   ///  r3 = r3 / (1000 as u64)
    stxdw [r7+0xbb0], r3                    
lbb_7637:
    stxdw [r7+0xc18], r1                    
    ldxdw r3, [r10-0x18]                    
    ldxdw r1, [r3+0x40]                     
    stxdw [r7+0xbf8], r2                    
    stxdw [r7+0xc20], r1                    
    ldxw r9, [r3+0x48]                      
    mov64 r1, r9                                    r1 = r9
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r2, 1000                                  r2 = 1000 as i32 as i64 as u64
    call function_22231                     
    jeq r8, 1, lbb_7650                             if r8 == (1 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, r9                                    r0 = r9
lbb_7650:
    stxw [r7+0xbf4], r0                     
    ldxdw r8, [r10-0x18]                    
    ldxdw r1, [r8+0x28]                     
    stxdw [r7+0xbe8], r1                    
    ldxdw r1, [r10-0x38]                    
    ldxdw r2, [r10-0x40]                    
    call function_22254                     
    ldxdw r8, [r8+0x30]                     
    stxdw [r7+0xbc0], r0                    
    ldxdw r1, [r10-0x28]                    
    ldxdw r2, [r10-0x30]                    
    call function_22254                     
    stxdw [r7+0xbb8], r0                    
    stxdw [r7+0xc00], r8                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    ldxdw r2, [r10-0x20]                    
    call function_3691                      
    ldxw r8, [r10-0x10]                     
    jne r8, 26, lbb_7680                            if r8 != (26 as i32 as i64 as u64) { pc += 10 }
    ldxb r1, [r7+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r7+0x0], r1                       
    mov64 r8, 26                                    r8 = 26 as i32 as i64 as u64
    ja lbb_7589                                     if true { pc += -86 }
lbb_7675:
    mov64 r8, 16                                    r8 = 16 as i32 as i64 as u64
lbb_7676:
    ldxb r2, [r7+0x0]                       
    or64 r2, 8                                      r2 |= 8   ///  r2 = r2.or(8)
    stxb [r7+0x0], r2                       
    ja lbb_7589                                     if true { pc += -91 }
lbb_7680:
    ldxw r1, [r10-0xc]                      
    ja lbb_7676                                     if true { pc += -6 }

function_7682:
    mov64 r7, r4                                    r7 = r4
    mov64 r9, r3                                    r9 = r3
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r9+0x20]                     
    ldxdw r8, [r1+0x0]                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r1, 22                                    r1 = 22 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_7726                             if r0 != (0 as i32 as i64 as u64) { pc += 31 }
    lddw r1, 0x800000000                            r1 load str located at 34359738368
    ldxdw r2, [r8+0x50]                     
    jne r2, 7456, lbb_7726                          if r2 != (7456 as i32 as i64 as u64) { pc += 27 }
    lddw r1, 0x200000000                            r1 load str located at 8589934592
    ldxb r2, [r8+0x2]                       
    jeq r2, 0, lbb_7726                             if r2 == (0 as i32 as i64 as u64) { pc += 23 }
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    ldxb r2, [r8+0x0]                       
    mov64 r3, r2                                    r3 = r2
    and64 r3, 15                                    r3 &= 15   ///  r3 = r3.and(15)
    jne r3, 15, lbb_7726                            if r3 != (15 as i32 as i64 as u64) { pc += 18 }
    mov64 r1, r2                                    r1 = r2
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r8+0x0], r1                       
    mov64 r3, r8                                    r3 = r8
    add64 r3, 88                                    r3 += 88   ///  r3 = r3.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r3                                    r1 = r3
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jeq r1, 0, lbb_7721                             if r1 == (0 as i32 as i64 as u64) { pc += 5 }
    lddw r1, 0x10002d1f5 --> b"from_bytes_mut"        r1 load str located at 4295152117
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_2131                      
lbb_7721:
    ldxdw r1, [r3+0x0]                      
    jeq r1, 4, lbb_7731                             if r1 == (4 as i32 as i64 as u64) { pc += 8 }
    stxb [r8+0x0], r2                       
    lddw r1, 0x900000000                            r1 load str located at 38654705664
lbb_7726:
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
lbb_7728:
    stxw [r6+0x4], r2                       
    stxw [r6+0x0], r1                       
    exit                                    
lbb_7731:
    stxdw [r10-0x18], r3                    
    ldxdw r1, [r9+0x18]                     
    ldxdw r1, [r1+0x0]                      
    mov64 r2, r8                                    r2 = r8
    add64 r2, 128                                   r2 += 128   ///  r2 = r2.wrapping_add(128 as i32 as i64 as u64)
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_7745                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7773                                     if true { pc += 28 }
lbb_7745:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -8                                    r1 += -8   ///  r1 = r1.wrapping_add(-8 as i32 as i64 as u64)
    syscall [invalid]                       
    jne r0, 0, lbb_7772                             if r0 != (0 as i32 as i64 as u64) { pc += 23 }
    ldxdw r1, [r10-0x8]                     
    stxdw [r8+0x1968], r1                   
    ldxb r1, [r8+0xbf0]                     
    jeq r1, 0, lbb_7754                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7825                                     if true { pc += 71 }
lbb_7754:
    ldxdw r1, [r8+0xbe8]                    
    stxdw [r8+0xc08], r1                    
    ldxdw r1, [r8+0xc00]                    
    stxdw [r8+0xc10], r1                    
    stb [r8+0xbf0], 1                       
    ldxdw r1, [r7+0x28]                     
    stxdw [r8+0xbe8], r1                    
    ldxdw r1, [r7+0x30]                     
    stxdw [r8+0xc00], r1                    
    ldxdw r1, [r8+0xbf8]                    
    ldxdw r2, [r7+0x38]                     
    jlt r2, r1, lbb_7767                            if r2 < r1 { pc += 1 }
    ja lbb_7777                                     if true { pc += 10 }
lbb_7767:
    ldxb r1, [r8+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r8+0x0], r1                       
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ja lbb_7728                                     if true { pc += -44 }
lbb_7772:
    mov64 r1, 16                                    r1 = 16 as i32 as i64 as u64
lbb_7773:
    ldxb r3, [r8+0x0]                       
    or64 r3, 8                                      r3 |= 8   ///  r3 = r3.or(8)
    stxb [r8+0x0], r3                       
    ja lbb_7728                                     if true { pc += -49 }
lbb_7777:
    ldxdw r1, [r7+0x8]                      
    stxdw [r10-0x30], r1                    
    stxdw [r8+0xba0], r1                    
    ldxdw r1, [r7+0x0]                      
    stxdw [r10-0x20], r1                    
    stxdw [r8+0xba8], r1                    
    ldxdw r1, [r7+0x18]                     
    stxdw [r10-0x28], r1                    
    stxdw [r8+0xbc8], r1                    
    ldxdw r1, [r7+0x10]                     
    stxdw [r10-0x38], r1                    
    stxdw [r8+0xbd0], r1                    
    ldxdw r1, [r7+0x20]                     
    ldxb r9, [r8+0x79f]                     
    jeq r9, 1, lbb_7793                             if r9 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7796                                     if true { pc += 3 }
lbb_7793:
    mov64 r2, r1                                    r2 = r1
    div64 r2, 1000                                  r2 /= 1000   ///  r2 = r2 / (1000 as u64)
    stxdw [r8+0xbb0], r2                    
lbb_7796:
    stxdw [r8+0xc18], r1                    
    ldxdw r1, [r7+0x40]                     
    stxdw [r8+0xc20], r1                    
    ldxw r7, [r7+0x48]                      
    mov64 r1, r7                                    r1 = r7
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r2, 1000                                  r2 = 1000 as i32 as i64 as u64
    call function_22231                     
    jeq r9, 1, lbb_7807                             if r9 == (1 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, r7                                    r0 = r7
lbb_7807:
    ldxdw r1, [r10-0x30]                    
    ldxdw r2, [r10-0x38]                    
    mov64 r7, r0                                    r7 = r0
    call function_22254                     
    stxdw [r8+0xbc0], r0                    
    ldxdw r1, [r10-0x20]                    
    ldxdw r2, [r10-0x28]                    
    call function_22254                     
    stxdw [r8+0xbb8], r0                    
    stxw [r8+0xbf4], r7                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    ldxdw r2, [r10-0x18]                    
    call function_3691                      
    ldxw r1, [r10-0x10]                     
    jeq r1, 26, lbb_7767                            if r1 == (26 as i32 as i64 as u64) { pc += -56 }
    ldxw r2, [r10-0xc]                      
    ja lbb_7773                                     if true { pc += -52 }
lbb_7825:
    lddw r1, 0x10002d7da --> b"assertion failed: !state.has_saved_spread_modifiers.value()"        r1 load str located at 4295153626
    mov64 r2, 59                                    r2 = 59 as i32 as i64 as u64
    lddw r3, 0x10002f180 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x18\x03\…        r3 load str located at 4295160192
    call function_18704                     

function_7831:
    mov64 r8, r5                                    r8 = r5
    mov64 r6, r4                                    r6 = r4
    mov64 r9, r3                                    r9 = r3
    mov64 r7, r2                                    r7 = r2
    stxdw [r10-0x4f8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -672                                  r1 += -672   ///  r1 = r1.wrapping_add(-672 as i32 as i64 as u64)
    call function_18055                     
    ldxw r1, [r10-0x2a0]                    
    jne r1, 1, lbb_7847                             if r1 != (1 as i32 as i64 as u64) { pc += 6 }
    ldxw r8, [r10-0x298]                    
    ldxw r6, [r10-0x29c]                    
lbb_7843:
    ldxdw r1, [r10-0x4f8]                   
    stxw [r1+0x4], r8                       
    stxw [r1+0x0], r6                       
    exit                                    
lbb_7847:
    stxdw [r10-0x500], r7                   
    stxdw [r10-0x520], r6                   
    ldxdw r1, [r8-0xfe8]                    
    stxdw [r10-0x528], r1                   
    ldxdw r1, [r8-0xff0]                    
    stxdw [r10-0x508], r1                   
    ldxdw r1, [r8-0xff8]                    
    stxdw [r10-0x538], r1                   
    ldxdw r1, [r8-0x1000]                   
    stxdw [r10-0x530], r1                   
    ldxdw r1, [r10-0x278]                   
    stxdw [r10-0x510], r1                   
    ldxdw r1, [r10-0x298]                   
    stxdw [r10-0x518], r1                   
    ldxdw r1, [r9+0x58]                     
    ldxdw r1, [r1+0x0]                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x10002cf08 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295151368
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r0, 0, lbb_7843                             if r0 != (0 as i32 as i64 as u64) { pc += -30 }
    ldxdw r7, [r9+0x60]                     
    ldxdw r1, [r7+0x0]                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x10002ce68 --> b"\x06\xa7\xd5\x17\x18{\xd1f5\xda\xd4\x04U\xfd\xc2\xc0\xc1$\xc6\x8f!Vu\xa5\…        r2 load str located at 4295151208
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_7843                             if r0 != (0 as i32 as i64 as u64) { pc += -40 }
    stxdw [r10-0x548], r7                   
    ldxdw r1, [r9+0x20]                     
    ldxdw r8, [r1+0x0]                      
    stxdw [r10-0x540], r9                   
    ldxdw r9, [r9+0x28]                     
    ldxdw r7, [r9+0x0]                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    ldxdw r2, [r10-0x500]                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r6, 22                                    r6 = 22 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_7942                             if r0 != (0 as i32 as i64 as u64) { pc += 44 }
    lddw r6, 0x800000000                            r6 load str located at 34359738368
    ldxdw r1, [r8+0x50]                     
    jne r1, 1072, lbb_7942                          if r1 != (1072 as i32 as i64 as u64) { pc += 40 }
    lddw r6, 0x200000000                            r6 load str located at 8589934592
    ldxb r1, [r8+0x2]                       
    jeq r1, 0, lbb_7942                             if r1 == (0 as i32 as i64 as u64) { pc += 36 }
    mov64 r6, 11                                    r6 = 11 as i32 as i64 as u64
    ldxb r2, [r8+0x0]                       
    mov64 r1, r2                                    r1 = r2
    and64 r1, 15                                    r1 &= 15   ///  r1 = r1.and(15)
    jne r1, 15, lbb_7942                            if r1 != (15 as i32 as i64 as u64) { pc += 31 }
    mov64 r1, r2                                    r1 = r2
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r8+0x0], r1                       
    stxdw [r10-0x550], r8                   
    add64 r8, 88                                    r8 += 88   ///  r8 = r8.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jeq r1, 0, lbb_7924                             if r1 == (0 as i32 as i64 as u64) { pc += 5 }
lbb_7919:
    lddw r1, 0x10002d1f5 --> b"from_bytes_mut"        r1 load str located at 4295152117
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_2131                      
lbb_7924:
    lddw r6, 0x900000000                            r6 load str located at 38654705664
    ldxdw r1, [r8+0x0]                      
    jne r1, 1, lbb_7940                             if r1 != (1 as i32 as i64 as u64) { pc += 12 }
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    mov64 r6, r2                                    r6 = r2
    ldxdw r2, [r10-0x550]                   
    add64 r2, 128                                   r2 += 128   ///  r2 = r2.wrapping_add(128 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r2, r6                                    r2 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_7945                             if r0 == (0 as i32 as i64 as u64) { pc += 5 }
lbb_7940:
    ldxdw r1, [r10-0x550]                   
    stxb [r1+0x0], r2                       
lbb_7942:
    mov64 r8, r6                                    r8 = r6
    rsh64 r8, 32                                    r8 >>= 32   ///  r8 = r8.wrapping_shr(32)
    ja lbb_7843                                     if true { pc += -102 }
lbb_7945:
    stxdw [r10-0x560], r9                   
    stxdw [r10-0x4b8], r8                   
    ldxdw r2, [r10-0x540]                   
    ldxdw r1, [r2+0x30]                     
    ldxdw r9, [r1+0x0]                      
    ldxdw r1, [r2+0x38]                     
    stxdw [r10-0x558], r1                   
    ldxdw r8, [r1+0x0]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    ldxdw r2, [r10-0x500]                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r2, r9                                    r2 = r9
    mov64 r6, 22                                    r6 = 22 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_8004                             if r0 != (0 as i32 as i64 as u64) { pc += 41 }
    lddw r6, 0x800000000                            r6 load str located at 34359738368
    ldxdw r1, [r2+0x50]                     
    jne r1, 1072, lbb_8004                          if r1 != (1072 as i32 as i64 as u64) { pc += 37 }
    lddw r6, 0x200000000                            r6 load str located at 8589934592
    ldxb r1, [r2+0x2]                       
    jeq r1, 0, lbb_8004                             if r1 == (0 as i32 as i64 as u64) { pc += 33 }
    mov64 r6, 11                                    r6 = 11 as i32 as i64 as u64
    ldxb r3, [r2+0x0]                       
    mov64 r1, r3                                    r1 = r3
    and64 r1, 15                                    r1 &= 15   ///  r1 = r1.and(15)
    jne r1, 15, lbb_8004                            if r1 != (15 as i32 as i64 as u64) { pc += 28 }
    stxdw [r10-0x570], r7                   
    mov64 r7, r3                                    r7 = r3
    mov64 r1, r3                                    r1 = r3
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r2+0x0], r1                       
    stxdw [r10-0x568], r2                   
    mov64 r1, r2                                    r1 = r2
    add64 r1, 88                                    r1 += 88   ///  r1 = r1.wrapping_add(88 as i32 as i64 as u64)
    mov64 r9, r1                                    r9 = r1
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jeq r1, 0, lbb_7988                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7919                                     if true { pc += -69 }
lbb_7988:
    lddw r6, 0x900000000                            r6 load str located at 38654705664
    ldxdw r1, [r9+0x0]                      
    jne r1, 1, lbb_8002                             if r1 != (1 as i32 as i64 as u64) { pc += 10 }
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r2, [r10-0x568]                   
    add64 r2, 128                                   r2 += 128   ///  r2 = r2.wrapping_add(128 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8011                             if r0 == (0 as i32 as i64 as u64) { pc += 9 }
lbb_8002:
    ldxdw r1, [r10-0x568]                   
    stxb [r1+0x0], r7                       
lbb_8004:
    mov64 r8, r6                                    r8 = r6
    rsh64 r8, 32                                    r8 >>= 32   ///  r8 = r8.wrapping_shr(32)
lbb_8006:
    ldxdw r2, [r10-0x550]                   
    ldxb r1, [r2+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r2+0x0], r1                       
    ja lbb_7843                                     if true { pc += -168 }
lbb_8011:
    stxdw [r10-0x4b0], r9                   
    ldxdw r1, [r10-0x540]                   
    ldxdw r1, [r1+0x18]                     
    stxdw [r10-0x580], r1                   
    ldxdw r1, [r1+0x0]                      
    stxdw [r10-0x578], r1                   
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    ldxdw r2, [r10-0x500]                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r6, 22                                    r6 = 22 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_8058                             if r0 != (0 as i32 as i64 as u64) { pc += 33 }
    lddw r6, 0x800000000                            r6 load str located at 34359738368
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0x50]                     
    jne r1, 7456, lbb_8058                          if r1 != (7456 as i32 as i64 as u64) { pc += 28 }
    lddw r6, 0x200000000                            r6 load str located at 8589934592
    ldxdw r1, [r10-0x578]                   
    ldxb r1, [r1+0x2]                       
    jeq r1, 0, lbb_8058                             if r1 == (0 as i32 as i64 as u64) { pc += 23 }
    mov64 r6, 11                                    r6 = 11 as i32 as i64 as u64
    ldxdw r1, [r10-0x578]                   
    ldxb r1, [r1+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    jne r2, 15, lbb_8058                            if r2 != (15 as i32 as i64 as u64) { pc += 17 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 247                                   r2 &= 247   ///  r2 = r2.and(247)
    ldxdw r3, [r10-0x578]                   
    stxb [r3+0x0], r2                       
    mov64 r2, r3                                    r2 = r3
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x500], r2                   
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    jeq r2, 0, lbb_8051                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7919                                     if true { pc += -132 }
lbb_8051:
    ldxdw r2, [r10-0x500]                   
    ldxdw r2, [r2+0x0]                      
    jeq r2, 4, lbb_8065                             if r2 == (4 as i32 as i64 as u64) { pc += 11 }
    ldxdw r2, [r10-0x578]                   
    stxb [r2+0x0], r1                       
    lddw r6, 0x900000000                            r6 load str located at 38654705664
lbb_8058:
    mov64 r8, r6                                    r8 = r6
    rsh64 r8, 32                                    r8 >>= 32   ///  r8 = r8.wrapping_shr(32)
lbb_8060:
    ldxdw r2, [r10-0x568]                   
    ldxb r1, [r2+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r2+0x0], r1                       
    ja lbb_8006                                     if true { pc += -59 }
lbb_8065:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r8, 4                                     r8 = 4 as i32 as i64 as u64
    ldxdw r1, [r10-0x578]                   
    ldxb r1, [r1+0x79c]                     
    jeq r1, 0, lbb_8113                             if r1 == (0 as i32 as i64 as u64) { pc += 43 }
    ldxdw r1, [r10-0x508]                   
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_8074                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8077                                     if true { pc += 3 }
lbb_8074:
    ldxdw r1, [r10-0x578]                   
    ldxb r1, [r1+0x791]                     
    jeq r1, 0, lbb_8113                             if r1 == (0 as i32 as i64 as u64) { pc += 36 }
lbb_8077:
    ldxdw r1, [r10-0x578]                   
    ldxdw r7, [r1+0x1968]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -672                                  r1 += -672   ///  r1 = r1.wrapping_add(-672 as i32 as i64 as u64)
    syscall [invalid]                       
    jne r0, 0, lbb_8111                             if r0 != (0 as i32 as i64 as u64) { pc += 28 }
    mov64 r8, 5                                     r8 = 5 as i32 as i64 as u64
    ldxdw r1, [r10-0x2a0]                   
    jeq r7, r1, lbb_8087                            if r7 == r1 { pc += 1 }
    ja lbb_8113                                     if true { pc += 26 }
lbb_8087:
    ldxdw r7, [r10-0x578]                   
    add64 r7, 224                                   r7 += 224   ///  r7 = r7.wrapping_add(224 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    ldxdw r2, [r10-0x570]                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    stxdw [r10-0x598], r0                   
    mov64 r1, r0                                    r1 = r0
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jeq r1, 0, lbb_8118                             if r1 == (0 as i32 as i64 as u64) { pc += 20 }
    ldxdw r9, [r10-0x540]                   
    add64 r9, 40                                    r9 += 40   ///  r9 = r9.wrapping_add(40 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1200                                 r1 += -1200   ///  r1 = r1.wrapping_add(-1200 as i32 as i64 as u64)
    stxdw [r10-0x588], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1208                                 r1 += -1208   ///  r1 = r1.wrapping_add(-1208 as i32 as i64 as u64)
    stxdw [r10-0x590], r1                   
    mov64 r1, 256                                   r1 = 256 as i32 as i64 as u64
    ldxdw r2, [r10-0x578]                   
    add64 r2, 352                                   r2 += 352   ///  r2 = r2.wrapping_add(352 as i32 as i64 as u64)
    ldxdw r3, [r10-0x558]                   
    ja lbb_8130                                     if true { pc += 19 }
lbb_8111:
    mov64 r6, 16                                    r6 = 16 as i32 as i64 as u64
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_8113:
    ldxdw r2, [r10-0x578]                   
    ldxb r1, [r2+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r2+0x0], r1                       
    ja lbb_8060                                     if true { pc += -58 }
lbb_8118:
    ldxdw r9, [r10-0x540]                   
    add64 r9, 56                                    r9 += 56   ///  r9 = r9.wrapping_add(56 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1208                                 r1 += -1208   ///  r1 = r1.wrapping_add(-1208 as i32 as i64 as u64)
    stxdw [r10-0x588], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1200                                 r1 += -1200   ///  r1 = r1.wrapping_add(-1200 as i32 as i64 as u64)
    stxdw [r10-0x590], r1                   
    mov64 r1, 320                                   r1 = 320 as i32 as i64 as u64
    ldxdw r2, [r10-0x578]                   
    add64 r2, 353                                   r2 += 353   ///  r2 = r2.wrapping_add(353 as i32 as i64 as u64)
    ldxdw r3, [r10-0x560]                   
lbb_8130:
    stxdw [r10-0x570], r3                   
    ldxdw r3, [r10-0x578]                   
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    ldxb r1, [r2+0x0]                       
    ldxdw r2, [r3+0x18]                     
    stxdw [r10-0x490], r2                   
    ldxdw r2, [r3+0x10]                     
    stxdw [r10-0x498], r2                   
    ldxdw r2, [r3+0x8]                      
    stxdw [r10-0x4a0], r2                   
    ldxdw r2, [r3+0x0]                      
    stxdw [r10-0x4a8], r2                   
    stxb [r10-0x488], r1                    
    ldxdw r1, [r10-0x570]                   
    ldxdw r2, [r1+0x0]                      
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r8, 3                                     r8 = 3 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_8113                             if r0 != (0 as i32 as i64 as u64) { pc += -40 }
    ldxdw r1, [r9+0x0]                      
    stxdw [r10-0x5a0], r1                   
    ldxdw r2, [r1+0x0]                      
    ldxdw r1, [r10-0x578]                   
    add64 r1, 288                                   r1 += 288   ///  r1 = r1.wrapping_add(288 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_8113                             if r0 != (0 as i32 as i64 as u64) { pc += -51 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -672                                  r1 += -672   ///  r1 = r1.wrapping_add(-672 as i32 as i64 as u64)
    ldxdw r2, [r10-0x570]                   
    call function_11320                     
    ldxw r8, [r10-0x298]                    
    ldxw r1, [r10-0x2a0]                    
    stxdw [r10-0x5a8], r8                   
    jne r1, 0, lbb_8174                             if r1 != (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r2, [r10-0x298]                   
    stxdw [r10-0x5a8], r2                   
lbb_8174:
    ldxw r6, [r10-0x29c]                    
    jne r1, 0, lbb_8113                             if r1 != (0 as i32 as i64 as u64) { pc += -63 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -672                                  r1 += -672   ///  r1 = r1.wrapping_add(-672 as i32 as i64 as u64)
    ldxdw r2, [r10-0x5a0]                   
    call function_11320                     
    ldxw r8, [r10-0x298]                    
    ldxw r1, [r10-0x2a0]                    
    stxdw [r10-0x5b0], r8                   
    jne r1, 0, lbb_8186                             if r1 != (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r2, [r10-0x298]                   
    stxdw [r10-0x5b0], r2                   
lbb_8186:
    ldxw r6, [r10-0x29c]                    
    jne r1, 0, lbb_8113                             if r1 != (0 as i32 as i64 as u64) { pc += -75 }
    ldxdw r1, [r10-0x590]                   
    ldxdw r1, [r1+0x0]                      
    ldxw r2, [r1+0x358]                     
    stxdw [r10-0x5b8], r2                   
    ldxw r1, [r1+0x330]                     
    stxdw [r10-0x5c8], r1                   
    ldxdw r1, [r10-0x588]                   
    ldxdw r1, [r1+0x0]                      
    ldxw r2, [r1+0x358]                     
    stxdw [r10-0x5c0], r2                   
    ldxw r1, [r1+0x330]                     
    stxdw [r10-0x5d0], r1                   
    ldxdw r1, [r10-0x548]                   
    ldxdw r1, [r1+0x0]                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x10002ce68 --> b"\x06\xa7\xd5\x17\x18{\xd1f5\xda\xd4\x04U\xfd\xc2\xc0\xc1$\xc6\x8f!Vu\xa5\…        r2 load str located at 4295151208
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r6, 16                                    r6 = 16 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_8113                             if r0 != (0 as i32 as i64 as u64) { pc += -98 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -672                                  r1 += -672   ///  r1 = r1.wrapping_add(-672 as i32 as i64 as u64)
    ldxdw r2, [r10-0x548]                   
    call function_18010                     
    ldxw r8, [r10-0x294]                    
    ldxw r6, [r10-0x298]                    
    ldxdw r1, [r10-0x2a0]                   
    stxdw [r10-0x5d8], r1                   
    jne r1, 0, lbb_8221                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8113                                     if true { pc += -108 }
lbb_8221:
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    or64 r8, r6                                     r8 |= r6   ///  r8 = r8.or(r6)
    jgt r8, 1, lbb_8227                             if r8 > (1 as i32 as i64 as u64) { pc += 3 }
    lddw r1, 0x10002ec50 --> b"\x00\x00\x00\x00f\xd2\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x007\x00\x00\x0…        r1 load str located at 4295158864
    call function_20957                     
lbb_8227:
    ldxb r1, [r10-0x288]                    
    stxdw [r10-0x600], r1                   
    ldxdw r1, [r10-0x290]                   
    stxdw [r10-0x608], r1                   
    mov64 r6, 2                                     r6 = 2 as i32 as i64 as u64
    ldxdw r2, [r10-0x5d8]                   
    mov64 r1, r2                                    r1 = r2
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    ldxh r1, [r1-0x2]                       
    ldxh r2, [r2+0x0]                       
    jge r1, r2, lbb_9978                            if r1 >= r2 { pc += 1740 }
    ldxdw r2, [r10-0x578]                   
    ldxb r2, [r2+0x79b]                     
    stxdw [r10-0x5e8], r2                   
    jgt r2, 10, lbb_10195                           if r2 > (10 as i32 as i64 as u64) { pc += 1953 }
    ldxdw r2, [r10-0x578]                   
    mov64 r3, r2                                    r3 = r2
    add64 r3, 360                                   r3 += 360   ///  r3 = r3.wrapping_add(360 as i32 as i64 as u64)
    stxdw [r10-0x6b8], r3                   
    mov64 r3, r10                                   r3 = r10
    add64 r3, -1160                                 r3 += -1160   ///  r3 = r3.wrapping_add(-1160 as i32 as i64 as u64)
    stxdw [r10-0x6b0], r3                   
    add64 r2, 1952                                  r2 += 1952   ///  r2 = r2.wrapping_add(1952 as i32 as i64 as u64)
    stxdw [r10-0x620], r2                   
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r3, [r10-0x5d8]                   
    mov64 r2, r3                                    r2 = r3
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxh r2, [r2+0x2]                       
    mov64 r1, r3                                    r1 = r3
    stxdw [r10-0x5f8], r2                   
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    stxdw [r10-0x618], r1                   
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r1, r10                                   r1 = r10
    add64 r1, -671                                  r1 += -671   ///  r1 = r1.wrapping_add(-671 as i32 as i64 as u64)
    stxdw [r10-0x5f0], r1                   
    ldxdw r1, [r10-0x5e8]                   
    lsh64 r1, 5                                     r1 <<= 5   ///  r1 = r1.wrapping_shl(5)
    stxdw [r10-0x5e8], r1                   
    ja lbb_8275                                     if true { pc += 7 }
lbb_8268:
    mov64 r2, r9                                    r2 = r9
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x5e0], r1                   
    stxdw [r10-0x610], r1                   
    mov64 r9, r2                                    r9 = r2
    jeq r2, 9, lbb_8338                             if r2 == (9 as i32 as i64 as u64) { pc += 63 }
lbb_8275:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1120                                 r1 += -1120   ///  r1 = r1.wrapping_add(-1120 as i32 as i64 as u64)
    ldxdw r2, [r10-0x548]                   
    call function_18010                     
    ldxw r8, [r10-0x454]                    
    ldxw r6, [r10-0x458]                    
    ldxdw r3, [r10-0x460]                   
    jeq r3, 0, lbb_9978                             if r3 == (0 as i32 as i64 as u64) { pc += 1695 }
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    or64 r8, r6                                     r8 |= r6   ///  r8 = r8.or(r6)
    ldxb r6, [r10-0x448]                    
    ldxdw r7, [r10-0x450]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -672                                  r1 += -672   ///  r1 = r1.wrapping_add(-672 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r4, r8                                    r4 = r8
    call function_11367                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r6                                    r2 = r6
    call function_167                       
    ldxb r1, [r10-0x2a0]                    
    jne r1, 0, lbb_8298                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8268                                     if true { pc += -30 }
lbb_8298:
    ldxdw r1, [r10-0x5f0]                   
    lddw r2, 0x10002ce48 --> b"\x0a\xc3J\x96\xc1fqZ`\xc1#>\xca%\x8a\x0d\xf3\x0b\x1e\xc8X\xe0t\sj\x12bfcK…        r2 load str located at 4295151176
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r6, r0                                    r6 = r0
    mov64 r1, r6                                    r1 = r6
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    stxdw [r10-0x5e0], r2                   
    jne r1, 0, lbb_8312                             if r1 != (0 as i32 as i64 as u64) { pc += 2 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    stxdw [r10-0x5e0], r2                   
lbb_8312:
    jne r1, 0, lbb_8314                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8331                                     if true { pc += 17 }
lbb_8314:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1120                                 r1 += -1120   ///  r1 = r1.wrapping_add(-1120 as i32 as i64 as u64)
    ldxdw r2, [r10-0x5f0]                   
    call function_11264                     
    ldxdw r7, [r10-0x5e8]                   
    ldxdw r8, [r10-0x620]                   
lbb_8320:
    jeq r7, 0, lbb_8268                             if r7 == (0 as i32 as i64 as u64) { pc += -53 }
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1120                                 r2 += -1120   ///  r2 = r2.wrapping_add(-1120 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    add64 r7, -32                                   r7 += -32   ///  r7 = r7.wrapping_add(-32 as i32 as i64 as u64)
    add64 r8, 32                                    r8 += 32   ///  r8 = r8.wrapping_add(32 as i32 as i64 as u64)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_8320                             if r0 != (0 as i32 as i64 as u64) { pc += -11 }
lbb_8331:
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x610], r1                   
    jeq r6, 0, lbb_8338                             if r6 == (0 as i32 as i64 as u64) { pc += 2 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x610], r1                   
lbb_8338:
    ldxdw r2, [r10-0x5f8]                   
    ldxdw r1, [r10-0x5d8]                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x5d8], r1                   
    ldxdw r1, [r10-0x578]                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1896                                  r3 += 1896   ///  r3 = r3.wrapping_add(1896 as i32 as i64 as u64)
    stxdw [r10-0x6a8], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1864                                  r3 += 1864   ///  r3 = r3.wrapping_add(1864 as i32 as i64 as u64)
    stxdw [r10-0x6a0], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1832                                  r3 += 1832   ///  r3 = r3.wrapping_add(1832 as i32 as i64 as u64)
    stxdw [r10-0x698], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1800                                  r3 += 1800   ///  r3 = r3.wrapping_add(1800 as i32 as i64 as u64)
    stxdw [r10-0x690], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1768                                  r3 += 1768   ///  r3 = r3.wrapping_add(1768 as i32 as i64 as u64)
    stxdw [r10-0x688], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1736                                  r3 += 1736   ///  r3 = r3.wrapping_add(1736 as i32 as i64 as u64)
    stxdw [r10-0x680], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1704                                  r3 += 1704   ///  r3 = r3.wrapping_add(1704 as i32 as i64 as u64)
    stxdw [r10-0x678], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1672                                  r3 += 1672   ///  r3 = r3.wrapping_add(1672 as i32 as i64 as u64)
    stxdw [r10-0x670], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1640                                  r3 += 1640   ///  r3 = r3.wrapping_add(1640 as i32 as i64 as u64)
    stxdw [r10-0x668], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1608                                  r3 += 1608   ///  r3 = r3.wrapping_add(1608 as i32 as i64 as u64)
    stxdw [r10-0x660], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1576                                  r3 += 1576   ///  r3 = r3.wrapping_add(1576 as i32 as i64 as u64)
    stxdw [r10-0x658], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1544                                  r3 += 1544   ///  r3 = r3.wrapping_add(1544 as i32 as i64 as u64)
    stxdw [r10-0x650], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1512                                  r3 += 1512   ///  r3 = r3.wrapping_add(1512 as i32 as i64 as u64)
    stxdw [r10-0x648], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1480                                  r3 += 1480   ///  r3 = r3.wrapping_add(1480 as i32 as i64 as u64)
    stxdw [r10-0x640], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1448                                  r3 += 1448   ///  r3 = r3.wrapping_add(1448 as i32 as i64 as u64)
    stxdw [r10-0x638], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1416                                  r3 += 1416   ///  r3 = r3.wrapping_add(1416 as i32 as i64 as u64)
    stxdw [r10-0x630], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1384                                  r3 += 1384   ///  r3 = r3.wrapping_add(1384 as i32 as i64 as u64)
    stxdw [r10-0x628], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1352                                  r3 += 1352   ///  r3 = r3.wrapping_add(1352 as i32 as i64 as u64)
    stxdw [r10-0x620], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1320                                  r3 += 1320   ///  r3 = r3.wrapping_add(1320 as i32 as i64 as u64)
    stxdw [r10-0x5f0], r3                   
    add64 r1, 1288                                  r1 += 1288   ///  r1 = r1.wrapping_add(1288 as i32 as i64 as u64)
    stxdw [r10-0x5e8], r1                   
    ldxdw r1, [r10-0x618]                   
    ldxh r1, [r1+0x0]                       
    stxdw [r10-0x548], r1                   
    add64 r2, -30                                   r2 += -30   ///  r2 = r2.wrapping_add(-30 as i32 as i64 as u64)
    stxdw [r10-0x5f8], r2                   
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ja lbb_8415                                     if true { pc += 5 }
lbb_8410:
    ldxdw r1, [r10-0x5d8]                   
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    stxdw [r10-0x5d8], r2                   
    jeq r1, 0, lbb_8581                             if r1 == (0 as i32 as i64 as u64) { pc += 166 }
lbb_8415:
    mov64 r1, r7                                    r1 = r7
    mul64 r1, 33                                    r1 *= 33   ///  r1 = r1.wrapping_mul(33 as u64)
    ldxdw r6, [r10-0x5f8]                   
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    ja lbb_8576                                     if true { pc += 156 }
lbb_8420:
    ldxdw r1, [r10-0x548]                   
    jge r7, r1, lbb_8592                            if r7 >= r1 { pc += 170 }
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    ldxb r1, [r6+0x20]                      
    add64 r6, 33                                    r6 += 33   ///  r6 = r6.wrapping_add(33 as i32 as i64 as u64)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jeq r1, 0, lbb_8576                             if r1 == (0 as i32 as i64 as u64) { pc += 149 }
    ldxdw r1, [r10-0x5e8]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 133 }
    ldxdw r1, [r10-0x5f0]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 126 }
    ldxdw r1, [r10-0x620]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 119 }
    ldxdw r1, [r10-0x628]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 112 }
    ldxdw r1, [r10-0x630]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 105 }
    ldxdw r1, [r10-0x638]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 98 }
    ldxdw r1, [r10-0x640]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 91 }
    ldxdw r1, [r10-0x648]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 84 }
    ldxdw r1, [r10-0x650]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 77 }
    ldxdw r1, [r10-0x658]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 70 }
    ldxdw r1, [r10-0x660]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 63 }
    ldxdw r1, [r10-0x668]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 56 }
    ldxdw r1, [r10-0x670]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 49 }
    ldxdw r1, [r10-0x678]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 42 }
    ldxdw r1, [r10-0x680]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 35 }
    ldxdw r1, [r10-0x688]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 28 }
    ldxdw r1, [r10-0x690]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 21 }
    ldxdw r1, [r10-0x698]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 14 }
    ldxdw r1, [r10-0x6a0]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8567                             if r0 == (0 as i32 as i64 as u64) { pc += 7 }
    ldxdw r1, [r10-0x6a8]                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_8410                             if r0 != (0 as i32 as i64 as u64) { pc += -157 }
lbb_8567:
    ldxdw r1, [r6+0x18]                     
    stxdw [r10-0x2b8], r1                   
    ldxdw r1, [r6+0x10]                     
    stxdw [r10-0x2c0], r1                   
    ldxdw r1, [r6+0x8]                      
    stxdw [r10-0x2c8], r1                   
    ldxdw r1, [r6+0x0]                      
    stxdw [r10-0x2d0], r1                   
    ja lbb_8595                                     if true { pc += 19 }
lbb_8576:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    ldxdw r1, [r10-0x548]                   
    jlt r7, r1, lbb_8420                            if r7 < r1 { pc += -159 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ja lbb_8420                                     if true { pc += -161 }
lbb_8581:
    ldxdw r1, [r6+0x18]                     
    stxdw [r10-0x2b8], r1                   
    ldxdw r1, [r6+0x10]                     
    stxdw [r10-0x2c0], r1                   
    ldxdw r1, [r6+0x8]                      
    stxdw [r10-0x2c8], r1                   
    ldxdw r1, [r6+0x0]                      
    stxdw [r10-0x2d0], r1                   
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x5d8], r1                   
    ja lbb_8415                                     if true { pc += -177 }
lbb_8592:
    ldxdw r1, [r10-0x5d8]                   
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jeq r1, 0, lbb_8740                             if r1 == (0 as i32 as i64 as u64) { pc += 145 }
lbb_8595:
    ldxdw r1, [r10-0x2b8]                   
    stxdw [r10-0x468], r1                   
    ldxdw r1, [r10-0x2c0]                   
    stxdw [r10-0x470], r1                   
    ldxdw r1, [r10-0x2c8]                   
    stxdw [r10-0x478], r1                   
    ldxdw r1, [r10-0x2d0]                   
    stxdw [r10-0x480], r1                   
lbb_8603:
    ldxdw r1, [r10-0x548]                   
    mul64 r1, 33                                    r1 *= 33   ///  r1 = r1.wrapping_mul(33 as u64)
    ldxdw r7, [r10-0x618]                   
    add64 r7, r1                                    r7 += r1   ///  r7 = r7.wrapping_add(r1)
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0x1968]                   
    stxdw [r10-0x5d8], r1                   
    ldxdw r1, [r7+0x1a]                     
    stxdw [r10-0x448], r1                   
    ldxdw r1, [r7+0x12]                     
    stxdw [r10-0x450], r1                   
    ldxdw r1, [r7+0xa]                      
    stxdw [r10-0x458], r1                   
    ldxdw r1, [r7+0x2]                      
    stxdw [r10-0x460], r1                   
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    jne r8, 0, lbb_8737                             if r8 != (0 as i32 as i64 as u64) { pc += 117 }
    ldxdw r1, [r10-0x5e0]                   
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_8934                             if r1 != (0 as i32 as i64 as u64) { pc += 311 }
    ldxh r8, [r7+0x22]                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1120                                 r1 += -1120   ///  r1 = r1.wrapping_add(-1120 as i32 as i64 as u64)
    lddw r2, 0x10002ce48 --> b"\x0a\xc3J\x96\xc1fqZ`\xc1#>\xca%\x8a\x0d\xf3\x0b\x1e\xc8X\xe0t\sj\x12bfcK…        r2 load str located at 4295151176
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_9616                             if r0 == (0 as i32 as i64 as u64) { pc += 983 }
    mov64 r6, r7                                    r6 = r7
    add64 r6, 36                                    r6 += 36   ///  r6 = r6.wrapping_add(36 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1120                                 r1 += -1120   ///  r1 = r1.wrapping_add(-1120 as i32 as i64 as u64)
    lddw r2, 0x10002ce88 --> b"\x04y\xd5[\xf21\xc0n\xeet\xc5n\xceh\x15\x07\xfd\xb1\xb2\xde\xa3\xf4\x8eQ\…        r2 load str located at 4295151240
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8937                             if r0 == (0 as i32 as i64 as u64) { pc += 293 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1120                                 r1 += -1120   ///  r1 = r1.wrapping_add(-1120 as i32 as i64 as u64)
    lddw r2, 0x10002ce08 --> b"\xb5\xe3J\x14\xe2\xbcsHi\x0e\xe1\xf5\xaf]\xee\xd6U8@\xa3m\xaa\xb8`\xb0P`s…        r2 load str located at 4295151112
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8969                             if r0 == (0 as i32 as i64 as u64) { pc += 316 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1120                                 r1 += -1120   ///  r1 = r1.wrapping_add(-1120 as i32 as i64 as u64)
    lddw r2, 0x10002cf88 --> b"\x08A\xc4\xc6\xee7o\x17\xcaK\x03\x1a\x94\xf8\xb8*j\xb2\xae\x91\xae\x19\xe…        r2 load str located at 4295151496
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_9006                             if r0 == (0 as i32 as i64 as u64) { pc += 344 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1120                                 r1 += -1120   ///  r1 = r1.wrapping_add(-1120 as i32 as i64 as u64)
    lddw r2, 0x10002cdc8 --> b"U\x91V\xf1\xa2\m\x13O*\xf7\xe6\x0a\x9a\x0d4~\xc7\x91Vcdb\xd5\xd1\xad&\xf1…        r2 load str located at 4295151048
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_9014                             if r0 == (0 as i32 as i64 as u64) { pc += 343 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1120                                 r1 += -1120   ///  r1 = r1.wrapping_add(-1120 as i32 as i64 as u64)
    lddw r2, 0x10002cfc8 --> b"\x06\xa9\x9cn\x12\xe7\x0e\xbb5\x18\\x14LK\x90n\x18\xff\xb0\x0aGt/*^\x04\x…        r2 load str located at 4295151560
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_9034                             if r0 == (0 as i32 as i64 as u64) { pc += 354 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1120                                 r1 += -1120   ///  r1 = r1.wrapping_add(-1120 as i32 as i64 as u64)
    lddw r2, 0x10002cf28 --> b"\x05.\xea3l\xd36f\x0czB\xf6\x06D\xc7\xaf\x83\xbb\x1e\x1b\xe9\xed\x8c\x0dX…        r2 load str located at 4295151400
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_9031                             if r0 == (0 as i32 as i64 as u64) { pc += 342 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1120                                 r1 += -1120   ///  r1 = r1.wrapping_add(-1120 as i32 as i64 as u64)
    lddw r2, 0x10002d008 --> b"+\xdbt\xef\xd83j6\xc0Sc\xd9\xc7\xea\x19h(%4Xs\x87\x05*\xd0\x9c`\x1a3\xce\…        r2 load str located at 4295151624
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_9031                             if r0 == (0 as i32 as i64 as u64) { pc += 333 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1120                                 r1 += -1120   ///  r1 = r1.wrapping_add(-1120 as i32 as i64 as u64)
    lddw r2, 0x10002ce28 --> b"\x0c\xd6Tm@\xaeb\xd2\x16\xb8\x95t\xfe\xfd\xeeE\x85k\x94}\x9c\xf4\xb5\x0b)…        r2 load str located at 4295151144
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_9031                             if r0 == (0 as i32 as i64 as u64) { pc += 324 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1120                                 r1 += -1120   ///  r1 = r1.wrapping_add(-1120 as i32 as i64 as u64)
    lddw r2, 0x10002cf68 --> b"\xc6\x15\xe2NqH\x91\xcfO\x9b,\x07\xb8\x11p\x00\x11\x0c\|\xf2=\xce\xbe.\x9…        r2 load str located at 4295151464
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_9031                             if r0 == (0 as i32 as i64 as u64) { pc += 315 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1120                                 r1 += -1120   ///  r1 = r1.wrapping_add(-1120 as i32 as i64 as u64)
    lddw r2, 0x10002cfa8 --> b"\x15\xe5\xa9\xd7\x16\xfe!\xca\xd7\x7f\xa4\xb26N\x8e\xbc\xca3N\xb4dv\x92\x…        r2 load str located at 4295151528
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_9031                             if r0 == (0 as i32 as i64 as u64) { pc += 306 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1120                                 r1 += -1120   ///  r1 = r1.wrapping_add(-1120 as i32 as i64 as u64)
    lddw r2, 0x10002cfe8 --> b"\x06\x92^\x1e\x03p\xa1\xf3\xd6\xc6\x1f>\xddP\x8f\xdfF\x9dr\xc7\xef>\xd8\x…        r2 load str located at 4295151592
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_9031                             if r0 == (0 as i32 as i64 as u64) { pc += 297 }
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0x3e0]                    
    ja lbb_9618                                     if true { pc += 881 }
lbb_8737:
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0x788]                    
    ja lbb_9618                                     if true { pc += 878 }
lbb_8740:
    ldxdw r1, [r10-0x540]                   
    ldxdw r1, [r1+0x50]                     
    ldxdw r1, [r1+0x0]                      
    ldxdw r2, [r1+0x20]                     
    stxdw [r10-0x468], r2                   
    ldxdw r2, [r1+0x18]                     
    stxdw [r10-0x470], r2                   
    ldxdw r2, [r1+0x10]                     
    stxdw [r10-0x478], r2                   
    ldxdw r1, [r1+0x8]                      
    stxdw [r10-0x480], r1                   
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1288                                  r1 += 1288   ///  r1 = r1.wrapping_add(1288 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -158 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1320                                  r1 += 1320   ///  r1 = r1.wrapping_add(1320 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -167 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1352                                  r1 += 1352   ///  r1 = r1.wrapping_add(1352 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -176 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1384                                  r1 += 1384   ///  r1 = r1.wrapping_add(1384 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -185 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1416                                  r1 += 1416   ///  r1 = r1.wrapping_add(1416 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -194 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1448                                  r1 += 1448   ///  r1 = r1.wrapping_add(1448 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -203 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1480                                  r1 += 1480   ///  r1 = r1.wrapping_add(1480 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -212 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1512                                  r1 += 1512   ///  r1 = r1.wrapping_add(1512 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -221 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1544                                  r1 += 1544   ///  r1 = r1.wrapping_add(1544 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -230 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1576                                  r1 += 1576   ///  r1 = r1.wrapping_add(1576 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -239 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1608                                  r1 += 1608   ///  r1 = r1.wrapping_add(1608 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -248 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1640                                  r1 += 1640   ///  r1 = r1.wrapping_add(1640 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -257 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1672                                  r1 += 1672   ///  r1 = r1.wrapping_add(1672 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -266 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1704                                  r1 += 1704   ///  r1 = r1.wrapping_add(1704 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -275 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1736                                  r1 += 1736   ///  r1 = r1.wrapping_add(1736 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -284 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1768                                  r1 += 1768   ///  r1 = r1.wrapping_add(1768 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -293 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1800                                  r1 += 1800   ///  r1 = r1.wrapping_add(1800 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -302 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1832                                  r1 += 1832   ///  r1 = r1.wrapping_add(1832 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -311 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1864                                  r1 += 1864   ///  r1 = r1.wrapping_add(1864 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -320 }
    ldxdw r1, [r10-0x578]                   
    add64 r1, 1896                                  r1 += 1896   ///  r1 = r1.wrapping_add(1896 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1152                                 r2 += -1152   ///  r2 = r2.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8603                             if r0 == (0 as i32 as i64 as u64) { pc += -329 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ja lbb_8603                                     if true { pc += -331 }
lbb_8934:
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0x400]                    
    ja lbb_9618                                     if true { pc += 681 }
lbb_8937:
    ldxdw r1, [r10-0x538]                   
    jeq r1, 0, lbb_8942                             if r1 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r1, [r10-0x530]                   
    ldxb r1, [r1+0x0]                       
    jeq r1, 1, lbb_9009                             if r1 == (1 as i32 as i64 as u64) { pc += 67 }
lbb_8942:
    ldxdw r1, [r10-0x610]                   
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_9616                             if r1 != (0 as i32 as i64 as u64) { pc += 671 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    stxdw [r10-0x530], r1                   
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x538], r1                   
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    stxdw [r10-0x5e0], r1                   
    jgt r8, 7, lbb_8954                             if r8 > (7 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9299                                     if true { pc += 345 }
lbb_8954:
    ldxdw r1, [r6+0x0]                      
    ldxdw r2, [r10-0x548]                   
    jlt r2, 4, lbb_9056                             if r2 < (4 as i32 as i64 as u64) { pc += 99 }
    lddw r2, 0x2aade37a97cb17e5                     r2 load str located at 3075364236236101605
    jne r1, r2, lbb_9056                            if r1 != r2 { pc += 96 }
    mov64 r2, r8                                    r2 = r8
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r3, r8                                    r3 = r8
    add64 r3, -3                                    r3 += -3   ///  r3 = r3.wrapping_add(-3 as i32 as i64 as u64)
    jge r2, r3, lbb_9070                            if r2 >= r3 { pc += 105 }
    mov64 r1, r3                                    r1 = r3
    lddw r3, 0x10002f240 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00!\x04\x00…        r3 load str located at 4295160384
    call function_20248                     
lbb_8969:
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0x3f0]                    
    stxdw [r10-0x530], r1                   
    jgt r8, 7, lbb_8974                             if r8 > (7 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9619                                     if true { pc += 645 }
lbb_8974:
    ldxdw r3, [r6+0x0]                      
    lddw r1, 0x885b5beb4c3f4b41                     r1 load str located at -8621195995516023999
    jeq r3, r1, lbb_9037                            if r3 == r1 { pc += 59 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    lddw r0, 0xc88775e1919ec6f8                     r0 load str located at -3997096532596832520
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jne r3, r0, lbb_9179                            if r3 != r0 { pc += 195 }
    add64 r8, -8                                    r8 += -8   ///  r8 = r8.wrapping_add(-8 as i32 as i64 as u64)
    stxdw [r10-0x2a8], r8                   
    add64 r7, 44                                    r7 += 44   ///  r7 = r7.wrapping_add(44 as i32 as i64 as u64)
    stxdw [r10-0x2b0], r7                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -672                                  r1 += -672   ///  r1 = r1.wrapping_add(-672 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -688                                  r2 += -688   ///  r2 = r2.wrapping_add(-688 as i32 as i64 as u64)
    call function_15178                     
    ldxdw r7, [r10-0x298]                   
    ldxdw r8, [r10-0x2a0]                   
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    jeq r8, r1, lbb_8999                            if r8 == r1 { pc += 1 }
    ja lbb_9051                                     if true { pc += 52 }
lbb_8999:
    mov64 r4, r7                                    r4 = r7
    and64 r4, 3                                     r4 &= 3   ///  r4 = r4.and(3)
    jne r4, 1, lbb_9163                             if r4 != (1 as i32 as i64 as u64) { pc += 161 }
    ldxdw r1, [r7+0x7]                      
    ldxdw r4, [r1+0x0]                      
    jeq r4, 0, lbb_9163                             if r4 == (0 as i32 as i64 as u64) { pc += 158 }
    ja lbb_9161                                     if true { pc += 155 }
lbb_9006:
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0x3d0]                    
    ja lbb_9618                                     if true { pc += 609 }
lbb_9009:
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0x418]                    
    stxdw [r10-0x530], r1                   
    jeq r1, 0, lbb_8942                             if r1 == (0 as i32 as i64 as u64) { pc += -71 }
    ja lbb_9619                                     if true { pc += 605 }
lbb_9014:
    jgt r8, 31, lbb_9016                            if r8 > (31 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9028                                     if true { pc += 12 }
lbb_9016:
    ldxdw r1, [r6+0x0]                      
    lddw r2, 0xaff11fb02126e0f0                     r2 load str located at -5768794806353993488
    jeq r1, r2, lbb_9026                            if r1 == r2 { pc += 6 }
    lddw r2, 0x5e2d497249868051                     r2 load str located at 6786160968725856337
    jeq r1, r2, lbb_9026                            if r1 == r2 { pc += 3 }
    lddw r2, 0xc88775e1919ec6f8                     r2 load str located at -3997096532596832520
    jne r1, r2, lbb_9411                            if r1 != r2 { pc += 385 }
lbb_9026:
    ldxdw r1, [r10-0x548]                   
    jgt r1, 4, lbb_9416                             if r1 > (4 as i32 as i64 as u64) { pc += 388 }
lbb_9028:
    ldxdw r1, [r10-0x5d8]                   
    jeq r1, 0, lbb_9445                             if r1 == (0 as i32 as i64 as u64) { pc += 415 }
    ja lbb_9431                                     if true { pc += 400 }
lbb_9031:
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0x410]                    
    ja lbb_9618                                     if true { pc += 584 }
lbb_9034:
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0x3f8]                    
    ja lbb_9618                                     if true { pc += 581 }
lbb_9037:
    add64 r8, -8                                    r8 += -8   ///  r8 = r8.wrapping_add(-8 as i32 as i64 as u64)
    stxdw [r10-0x2a8], r8                   
    add64 r7, 44                                    r7 += 44   ///  r7 = r7.wrapping_add(44 as i32 as i64 as u64)
    stxdw [r10-0x2b0], r7                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -672                                  r1 += -672   ///  r1 = r1.wrapping_add(-672 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -688                                  r2 += -688   ///  r2 = r2.wrapping_add(-688 as i32 as i64 as u64)
    call function_15225                     
    ldxdw r7, [r10-0x298]                   
    ldxdw r8, [r10-0x2a0]                   
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    jeq r8, r1, lbb_9154                            if r8 == r1 { pc += 103 }
lbb_9051:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0x290]                   
    ldxdw r1, [r10-0x288]                   
    ldxh r2, [r10-0x280]                    
    ja lbb_9163                                     if true { pc += 107 }
lbb_9056:
    lddw r2, 0x819cd641339b20c1                     r2 load str located at -9107168770922962751
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    stxdw [r10-0x5e0], r3                   
    jne r1, r2, lbb_9299                            if r1 != r2 { pc += 238 }
    mov64 r2, r8                                    r2 = r8
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r3, r8                                    r3 = r8
    add64 r3, -3                                    r3 += -3   ///  r3 = r3.wrapping_add(-3 as i32 as i64 as u64)
    jge r2, r3, lbb_9227                            if r2 >= r3 { pc += 161 }
    mov64 r1, r3                                    r1 = r3
    lddw r3, 0x10002f1f8 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00M\x04\x00…        r3 load str located at 4295160312
    call function_20248                     
lbb_9070:
    jgt r8, 10, lbb_9074                            if r8 > (10 as i32 as i64 as u64) { pc += 3 }
    lddw r1, 0x10002f258 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00&\x04\x00…        r1 load str located at 4295160408
    call function_20957                     
lbb_9074:
    mov64 r1, r8                                    r1 = r8
    add64 r1, -11                                   r1 += -11   ///  r1 = r1.wrapping_add(-11 as i32 as i64 as u64)
    jge r3, r1, lbb_9081                            if r3 >= r1 { pc += 4 }
    mov64 r2, r3                                    r2 = r3
    lddw r3, 0x10002f270 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00&\x04\x00…        r3 load str located at 4295160432
    call function_20248                     
lbb_9081:
    mov64 r2, r6                                    r2 = r6
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    ldxh r2, [r2+0x0]                       
    stxdw [r10-0x5e8], r2                   
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    ldxdw r1, [r6+0x0]                      
    stxdw [r10-0x5f0], r1                   
    add64 r8, -8                                    r8 += -8   ///  r8 = r8.wrapping_add(-8 as i32 as i64 as u64)
    stxdw [r10-0x2a8], r8                   
    add64 r7, 44                                    r7 += 44   ///  r7 = r7.wrapping_add(44 as i32 as i64 as u64)
    stxdw [r10-0x2b0], r7                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -672                                  r1 += -672   ///  r1 = r1.wrapping_add(-672 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -688                                  r2 += -688   ///  r2 = r2.wrapping_add(-688 as i32 as i64 as u64)
    call function_15280                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x538], r1                   
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    stxdw [r10-0x530], r1                   
    ldxdw r1, [r10-0x298]                   
    ldxdw r2, [r10-0x2a0]                   
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    jeq r2, r3, lbb_9131                            if r2 == r3 { pc += 25 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0x290]                   
    mul64 r4, 24                                    r4 *= 24   ///  r4 = r4.wrapping_mul(24 as u64)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ja lbb_9125                                     if true { pc += 14 }
lbb_9111:
    mov64 r5, 2                                     r5 = 2 as i32 as i64 as u64
    stxdw [r10-0x530], r5                   
    jeq r4, r3, lbb_9120                            if r4 == r3 { pc += 6 }
    mov64 r5, r1                                    r5 = r1
    add64 r5, r3                                    r5 += r3   ///  r5 = r5.wrapping_add(r3)
    stxdw [r10-0x530], r0                   
    add64 r3, 24                                    r3 += 24   ///  r3 = r3.wrapping_add(24 as i32 as i64 as u64)
    ldxb r5, [r5+0x0]                       
    jeq r5, 4, lbb_9125                             if r5 == (4 as i32 as i64 as u64) { pc += 5 }
lbb_9120:
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    jeq r2, r3, lbb_9131                            if r2 == r3 { pc += 8 }
    jeq r2, 0, lbb_9139                             if r2 == (0 as i32 as i64 as u64) { pc += 15 }
    ja lbb_9139                                     if true { pc += 14 }
lbb_9125:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    stxdw [r10-0x538], r5                   
    jeq r4, r3, lbb_9111                            if r4 == r3 { pc += -17 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    stxdw [r10-0x538], r5                   
    ja lbb_9111                                     if true { pc += -20 }
lbb_9131:
    mov64 r2, r1                                    r2 = r1
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    jne r2, 1, lbb_9139                             if r2 != (1 as i32 as i64 as u64) { pc += 5 }
    ldxdw r2, [r1+0x7]                      
    ldxdw r2, [r2+0x0]                      
    jeq r2, 0, lbb_9139                             if r2 == (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r1-0x1]                      
    callx r2                                
lbb_9139:
    ldxdw r2, [r10-0x618]                   
    mov64 r1, r2                                    r1 = r2
    add64 r1, 69                                    r1 += 69   ///  r1 = r1.wrapping_add(69 as i32 as i64 as u64)
    add64 r2, 102                                   r2 += 102   ///  r2 = r2.wrapping_add(102 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x5e0], r1                   
    jeq r0, 0, lbb_9299                             if r0 == (0 as i32 as i64 as u64) { pc += 148 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x5e0], r1                   
    ja lbb_9299                                     if true { pc += 145 }
lbb_9154:
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r4, r7                                    r4 = r7
    and64 r4, 3                                     r4 &= 3   ///  r4 = r4.and(3)
    jne r4, 1, lbb_9163                             if r4 != (1 as i32 as i64 as u64) { pc += 5 }
    ldxdw r1, [r7+0x7]                      
    ldxdw r4, [r1+0x0]                      
    jeq r4, 0, lbb_9163                             if r4 == (0 as i32 as i64 as u64) { pc += 2 }
lbb_9161:
    ldxdw r1, [r7-0x1]                      
    callx r4                                
lbb_9163:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    lddw r0, 0x8000000000000000                     r0 load str located at -9223372036854775808
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jeq r8, r0, lbb_9170                            if r8 == r0 { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_9170:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ldxdw r8, [r10-0x5d8]                   
    jeq r8, 0, lbb_9174                             if r8 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_9174:
    or64 r0, r6                                     r0 |= r6   ///  r0 = r0.or(r6)
    mov64 r8, r0                                    r8 = r0
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    jne r8, 0, lbb_9179                             if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9200                                     if true { pc += 21 }
lbb_9179:
    ldxdw r3, [r10-0x5d8]                   
    jeq r3, 0, lbb_9186                             if r3 == (0 as i32 as i64 as u64) { pc += 5 }
    jeq r9, 0, lbb_9186                             if r9 == (0 as i32 as i64 as u64) { pc += 4 }
    and64 r2, 65535                                 r2 &= 65535   ///  r2 = r2.and(65535)
    jne r2, 0, lbb_9186                             if r2 != (0 as i32 as i64 as u64) { pc += 2 }
    jne r1, 0, lbb_9186                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    jne r5, 0, lbb_9189                             if r5 != (0 as i32 as i64 as u64) { pc += 3 }
lbb_9186:
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jne r4, 0, lbb_9619                             if r4 != (0 as i32 as i64 as u64) { pc += 431 }
    ja lbb_9619                                     if true { pc += 430 }
lbb_9189:
    ldxdw r1, [r10-0x578]                   
    ldxdw r2, [r1+0x3e8]                    
    mov64 r3, r2                                    r3 = r2
    add64 r3, 2                                     r3 += 2   ///  r3 = r3.wrapping_add(2 as i32 as i64 as u64)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x530], r3                   
    jlt r3, r2, lbb_9197                            if r3 < r2 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_9197:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_10200                            if r1 != (0 as i32 as i64 as u64) { pc += 1001 }
    ja lbb_9186                                     if true { pc += -14 }
lbb_9200:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jeq r3, 0, lbb_9179                             if r3 == (0 as i32 as i64 as u64) { pc += -24 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    lsh64 r3, 5                                     r3 <<= 5   ///  r3 = r3.wrapping_shl(5)
    ja lbb_9209                                     if true { pc += 3 }
lbb_9206:
    add64 r8, 32                                    r8 += 32   ///  r8 = r8.wrapping_add(32 as i32 as i64 as u64)
    mov64 r4, r0                                    r4 = r0
    jeq r3, r8, lbb_9179                            if r3 == r8 { pc += -30 }
lbb_9209:
    mov64 r4, r7                                    r4 = r7
    add64 r4, r8                                    r4 += r8   ///  r4 = r4.wrapping_add(r8)
    ldxdw r6, [r4+0x0]                      
    jeq r6, 3, lbb_9206                             if r6 == (3 as i32 as i64 as u64) { pc += -7 }
    mov64 r4, r0                                    r4 = r0
    jeq r6, 5, lbb_9179                             if r6 == (5 as i32 as i64 as u64) { pc += -36 }
    ldxdw r3, [r10-0x578]                   
    ldxdw r4, [r3+0x3e8]                    
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    stxdw [r10-0x530], r4                   
    jeq r4, 0, lbb_9222                             if r4 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_9222:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_10203                            if r3 != (0 as i32 as i64 as u64) { pc += 979 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r4, r0                                    r4 = r0
    ja lbb_9179                                     if true { pc += -48 }
lbb_9227:
    jgt r8, 10, lbb_9231                            if r8 > (10 as i32 as i64 as u64) { pc += 3 }
    lddw r1, 0x10002f210 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00R\x04\x00…        r1 load str located at 4295160336
    call function_20957                     
lbb_9231:
    mov64 r1, r8                                    r1 = r8
    add64 r1, -11                                   r1 += -11   ///  r1 = r1.wrapping_add(-11 as i32 as i64 as u64)
    jge r3, r1, lbb_9238                            if r3 >= r1 { pc += 4 }
    mov64 r2, r3                                    r2 = r3
    lddw r3, 0x10002f228 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00R\x04\x00…        r3 load str located at 4295160360
    call function_20248                     
lbb_9238:
    mov64 r2, r6                                    r2 = r6
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    ldxh r2, [r2+0x0]                       
    stxdw [r10-0x5e8], r2                   
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    ldxdw r1, [r6+0x0]                      
    stxdw [r10-0x5f0], r1                   
    add64 r8, -8                                    r8 += -8   ///  r8 = r8.wrapping_add(-8 as i32 as i64 as u64)
    stxdw [r10-0x2a8], r8                   
    add64 r7, 44                                    r7 += 44   ///  r7 = r7.wrapping_add(44 as i32 as i64 as u64)
    stxdw [r10-0x2b0], r7                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -672                                  r1 += -672   ///  r1 = r1.wrapping_add(-672 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -688                                  r2 += -688   ///  r2 = r2.wrapping_add(-688 as i32 as i64 as u64)
    call function_15337                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x538], r1                   
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    stxdw [r10-0x530], r1                   
    ldxdw r1, [r10-0x298]                   
    ldxdw r2, [r10-0x2a0]                   
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    jeq r2, r3, lbb_9288                            if r2 == r3 { pc += 25 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0x290]                   
    mul64 r4, 24                                    r4 *= 24   ///  r4 = r4.wrapping_mul(24 as u64)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ja lbb_9282                                     if true { pc += 14 }
lbb_9268:
    mov64 r5, 2                                     r5 = 2 as i32 as i64 as u64
    stxdw [r10-0x530], r5                   
    jeq r4, r3, lbb_9277                            if r4 == r3 { pc += 6 }
    mov64 r5, r1                                    r5 = r1
    add64 r5, r3                                    r5 += r3   ///  r5 = r5.wrapping_add(r3)
    stxdw [r10-0x530], r0                   
    add64 r3, 24                                    r3 += 24   ///  r3 = r3.wrapping_add(24 as i32 as i64 as u64)
    ldxb r5, [r5+0x0]                       
    jeq r5, 4, lbb_9282                             if r5 == (4 as i32 as i64 as u64) { pc += 5 }
lbb_9277:
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    jeq r2, r3, lbb_9288                            if r2 == r3 { pc += 8 }
    jeq r2, 0, lbb_9296                             if r2 == (0 as i32 as i64 as u64) { pc += 15 }
    ja lbb_9296                                     if true { pc += 14 }
lbb_9282:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    stxdw [r10-0x538], r5                   
    jeq r4, r3, lbb_9268                            if r4 == r3 { pc += -17 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    stxdw [r10-0x538], r5                   
    ja lbb_9268                                     if true { pc += -20 }
lbb_9288:
    mov64 r2, r1                                    r2 = r1
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    jne r2, 1, lbb_9296                             if r2 != (1 as i32 as i64 as u64) { pc += 5 }
    ldxdw r2, [r1+0x7]                      
    ldxdw r2, [r2+0x0]                      
    jeq r2, 0, lbb_9296                             if r2 == (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r1-0x1]                      
    callx r2                                
lbb_9296:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x5e0], r1                   
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
lbb_9299:
    ldxdw r1, [r10-0x5d8]                   
    jeq r1, 0, lbb_9616                             if r1 == (0 as i32 as i64 as u64) { pc += 315 }
    ldxdw r2, [r10-0x5e8]                   
    and64 r2, 65535                                 r2 &= 65535   ///  r2 = r2.and(65535)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_9306                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_9306:
    mov64 r2, r9                                    r2 = r9
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    jne r2, 1, lbb_9318                             if r2 != (1 as i32 as i64 as u64) { pc += 9 }
    ldxdw r1, [r10-0x5e0]                   
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    jeq r1, 2, lbb_9338                             if r1 == (2 as i32 as i64 as u64) { pc += 26 }
    ldxdw r1, [r10-0x5e0]                   
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_9338                             if r1 != (0 as i32 as i64 as u64) { pc += 23 }
lbb_9315:
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0x3e8]                    
    ja lbb_9618                                     if true { pc += 300 }
lbb_9318:
    ldxdw r2, [r10-0x5e8]                   
    and64 r2, 65535                                 r2 &= 65535   ///  r2 = r2.and(65535)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r2, 21, lbb_9323                            if r2 == (21 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_9323:
    mov64 r2, r9                                    r2 = r9
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    jne r2, 1, lbb_9366                             if r2 != (1 as i32 as i64 as u64) { pc += 40 }
    ldxdw r1, [r10-0x5e0]                   
    ldxdw r2, [r10-0x5f0]                   
    jeq r2, 0, lbb_9614                             if r2 == (0 as i32 as i64 as u64) { pc += 285 }
    ldxdw r1, [r10-0x5e0]                   
    mov64 r2, r1                                    r2 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jeq r2, 2, lbb_9614                             if r2 == (2 as i32 as i64 as u64) { pc += 281 }
    ldxdw r1, [r10-0x5e0]                   
    mov64 r2, r1                                    r2 = r1
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_9614                             if r2 != (0 as i32 as i64 as u64) { pc += 277 }
    ja lbb_9315                                     if true { pc += -23 }
lbb_9338:
    ldxdw r1, [r10-0x5e0]                   
    ldxdw r2, [r10-0x5f0]                   
    jne r2, 0, lbb_9614                             if r2 != (0 as i32 as i64 as u64) { pc += 273 }
    ldxdw r1, [r10-0x618]                   
    ldxh r2, [r1+0x0]                       
    ldxdw r1, [r10-0x5e0]                   
    jlt r2, 4, lbb_9614                             if r2 < (4 as i32 as i64 as u64) { pc += 269 }
    ldxdw r2, [r10-0x618]                   
    mov64 r1, r2                                    r1 = r2
    add64 r1, 36                                    r1 += 36   ///  r1 = r1.wrapping_add(36 as i32 as i64 as u64)
    add64 r2, 102                                   r2 += 102   ///  r2 = r2.wrapping_add(102 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    ldxdw r1, [r10-0x5e0]                   
    jne r0, 0, lbb_9614                             if r0 != (0 as i32 as i64 as u64) { pc += 259 }
    ldxdw r1, [r10-0x578]                   
    ldxdw r2, [r1+0x3e8]                    
    mov64 r3, r2                                    r3 = r2
    add64 r3, 2                                     r3 += 2   ///  r3 = r3.wrapping_add(2 as i32 as i64 as u64)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x530], r3                   
    jlt r3, r2, lbb_9363                            if r3 < r2 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_9363:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_10206                            if r1 != (0 as i32 as i64 as u64) { pc += 841 }
    ja lbb_9619                                     if true { pc += 253 }
lbb_9366:
    ldxdw r2, [r10-0x5e8]                   
    and64 r2, 65535                                 r2 &= 65535   ///  r2 = r2.and(65535)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r2, 50, lbb_9371                            if r2 == (50 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_9371:
    mov64 r2, r9                                    r2 = r9
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    jne r2, 1, lbb_9386                             if r2 != (1 as i32 as i64 as u64) { pc += 12 }
    ldxdw r1, [r10-0x5e0]                   
    ldxdw r2, [r10-0x5f0]                   
    jeq r2, 0, lbb_9614                             if r2 == (0 as i32 as i64 as u64) { pc += 237 }
    ldxdw r1, [r10-0x5e0]                   
    mov64 r2, r1                                    r2 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jeq r2, 2, lbb_9614                             if r2 == (2 as i32 as i64 as u64) { pc += 233 }
    ldxdw r1, [r10-0x5e0]                   
    mov64 r2, r1                                    r2 = r1
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_9614                             if r2 != (0 as i32 as i64 as u64) { pc += 229 }
    ja lbb_9315                                     if true { pc += -71 }
lbb_9386:
    ldxdw r2, [r10-0x5e8]                   
    and64 r2, 65535                                 r2 &= 65535   ///  r2 = r2.and(65535)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r2, 1, lbb_9391                             if r2 == (1 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_9391:
    mov64 r2, r9                                    r2 = r9
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    jne r2, 0, lbb_9448                             if r2 != (0 as i32 as i64 as u64) { pc += 54 }
    ldxdw r1, [r10-0x5e0]                   
lbb_9395:
    ldxdw r3, [r10-0x5e8]                   
    and64 r3, 65535                                 r3 &= 65535   ///  r3 = r3.and(65535)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jeq r3, 10000, lbb_9400                         if r3 == (10000 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_9400:
    and64 r9, r2                                    r9 &= r2   ///  r9 = r9.and(r2)
    jne r9, 1, lbb_9614                             if r9 != (1 as i32 as i64 as u64) { pc += 212 }
    ldxdw r2, [r10-0x5f0]                   
    jne r2, 0, lbb_9614                             if r2 != (0 as i32 as i64 as u64) { pc += 210 }
    ldxdw r2, [r10-0x5e0]                   
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jeq r2, 2, lbb_9614                             if r2 == (2 as i32 as i64 as u64) { pc += 207 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_9614                             if r2 != (0 as i32 as i64 as u64) { pc += 204 }
    ja lbb_9315                                     if true { pc += -96 }
lbb_9411:
    ldxdw r2, [r10-0x548]                   
    jlt r2, 5, lbb_9028                             if r2 < (5 as i32 as i64 as u64) { pc += -385 }
    lddw r2, 0x9de0e18ef62cbf0e                     r2 load str located at -7070403410839945458
    jne r1, r2, lbb_9028                            if r1 != r2 { pc += -388 }
lbb_9416:
    ldxdw r2, [r10-0x618]                   
    mov64 r1, r2                                    r1 = r2
    add64 r1, 102                                   r1 += 102   ///  r1 = r1.wrapping_add(102 as i32 as i64 as u64)
    add64 r2, 135                                   r2 += 135   ///  r2 = r2.wrapping_add(135 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    ldxdw r1, [r10-0x5d8]                   
    jeq r1, 0, lbb_9445                             if r1 == (0 as i32 as i64 as u64) { pc += 21 }
    ldxdw r1, [r7+0x3c]                     
    ldxdw r2, [r7+0x34]                     
    jne r2, r1, lbb_9442                            if r2 != r1 { pc += 15 }
    mov64 r1, r0                                    r1 = r0
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jeq r1, 0, lbb_9442                             if r1 == (0 as i32 as i64 as u64) { pc += 11 }
lbb_9431:
    ldxdw r1, [r10-0x578]                   
    ldxdw r2, [r1+0x3e8]                    
    mov64 r3, r2                                    r3 = r2
    add64 r3, 3                                     r3 += 3   ///  r3 = r3.wrapping_add(3 as i32 as i64 as u64)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x530], r3                   
    jlt r3, r2, lbb_9439                            if r3 < r2 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_9439:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_10209                            if r1 != (0 as i32 as i64 as u64) { pc += 768 }
    ja lbb_9619                                     if true { pc += 177 }
lbb_9442:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_9456                             if r0 == (0 as i32 as i64 as u64) { pc += 11 }
lbb_9445:
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0x3d8]                    
    ja lbb_9618                                     if true { pc += 170 }
lbb_9448:
    ldxdw r1, [r10-0x5f0]                   
    jne r1, 0, lbb_9470                             if r1 != (0 as i32 as i64 as u64) { pc += 20 }
    ldxdw r1, [r10-0x5e0]                   
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_9454                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9616                                     if true { pc += 162 }
lbb_9454:
    ldxdw r1, [r10-0x530]                   
    jne r1, 2, lbb_9459                             if r1 != (2 as i32 as i64 as u64) { pc += 3 }
lbb_9456:
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0x408]                    
    ja lbb_9618                                     if true { pc += 159 }
lbb_9459:
    ldxdw r1, [r10-0x578]                   
    ldxdw r2, [r1+0x3e8]                    
    mov64 r3, r2                                    r3 = r2
    add64 r3, 1003                                  r3 += 1003   ///  r3 = r3.wrapping_add(1003 as i32 as i64 as u64)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x530], r3                   
    jlt r3, r2, lbb_9467                            if r3 < r2 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_9467:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_10212                            if r1 != (0 as i32 as i64 as u64) { pc += 743 }
    ja lbb_9619                                     if true { pc += 149 }
lbb_9470:
    ldxdw r1, [r10-0x5e0]                   
    mov64 r2, r1                                    r2 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jeq r2, 2, lbb_9614                             if r2 == (2 as i32 as i64 as u64) { pc += 140 }
    ldxdw r2, [r10-0x538]                   
    xor64 r2, 1                                     r2 ^= 1   ///  r2 = r2.xor(1)
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    ldxdw r1, [r10-0x5e0]                   
    jne r2, 0, lbb_9614                             if r2 != (0 as i32 as i64 as u64) { pc += 135 }
    ldxdw r1, [r10-0x5e0]                   
    mov64 r2, r1                                    r2 = r1
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_9614                             if r2 != (0 as i32 as i64 as u64) { pc += 131 }
    ldxdw r1, [r10-0x580]                   
    ldxdw r7, [r1+0x0]                      
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    lddw r6, 0x10002d815 --> b"\xb8\x9a\x91\xb4\x9b\x92\xa6\xba\xc5\x8bz\xcfy\xed\x8f\x07E\x91 \x84\xbe\…        r6 load str located at 4295153685
    lddw r1, 0x10002d815 --> b"\xb8\x9a\x91\xb4\x9b\x92\xa6\xba\xc5\x8bz\xcfy\xed\x8f\x07E\x91 \x84\xbe\…        r1 load str located at 4295153685
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_9505                             if r0 == (0 as i32 as i64 as u64) { pc += 9 }
    add64 r6, 32                                    r6 += 32   ///  r6 = r6.wrapping_add(32 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_9395                             if r0 != (0 as i32 as i64 as u64) { pc += -110 }
lbb_9505:
    ldxdw r2, [r10-0x598]                   
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mov64 r1, 2984                                  r1 = 2984 as i32 as i64 as u64
    jeq r2, 0, lbb_9511                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 2976                                  r1 = 2976 as i32 as i64 as u64
lbb_9511:
    ldxdw r2, [r10-0x578]                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxdw r1, [r2+0x0]                      
    lddw r2, 0x412e848000000000                     r2 load str located at 4696837146684686336
    call function_22254                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_22511                     
    mov64 r8, r0                                    r8 = r0
    mov64 r1, r7                                    r1 = r7
    call function_22905                     
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jslt r8, 0, lbb_9527                            if (r8 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r6, r0                                    r6 = r0
lbb_9527:
    mov64 r1, r7                                    r1 = r7
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_22929                     
    mov64 r7, -1                                    r7 = -1 as i32 as i64 as u64
    mov64 r4, -1                                    r4 = -1 as i32 as i64 as u64
    jsgt r0, 0, lbb_9535                            if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r4, r6                                    r4 = r6
lbb_9535:
    ldxdw r1, [r10-0x520]                   
    ldxdw r2, [r1+0x0]                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1224                                 r1 += -1224   ///  r1 = r1.wrapping_add(-1224 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0x4c0]                   
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_9547                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_9547:
    jne r2, 0, lbb_9549                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    ldxdw r7, [r10-0x4c8]                   
lbb_9549:
    lddw r2, 0xfffe9433be170000                     r2 load str located at -400000000000000
    mov64 r3, r7                                    r3 = r7
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    lddw r4, 0x886c98c6a240                         r4 load str located at 150000001000000
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r3, r4, lbb_9558                            if r3 < r4 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_9558:
    lddw r3, 0xffc39a970ed18000                     r3 load str located at -17000000000000000
    mov64 r5, r7                                    r5 = r7
    add64 r5, r3                                    r5 += r3   ///  r5 = r5.wrapping_add(r3)
    lddw r3, 0xffeaaf0823590000                     r3 load str located at -6000000000000000
    mov64 r4, r7                                    r4 = r7
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    lddw r0, 0x38d7ea4d5c240                        r0 load str located at 1000000001000000
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r4, r0, lbb_9571                            if r4 < r0 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_9571:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r5, r0, lbb_9574                            if r5 < r0 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_9574:
    lddw r5, 0xffe3940ad9cc0000                     r5 load str located at -8000000000000000
    mov64 r0, r7                                    r0 = r7
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    lddw r5, 0x71afd499c4240                        r5 load str located at 2000000001000000
    jlt r0, r5, lbb_9582                            if r0 < r5 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_9582:
    lsh64 r1, 2                                     r1 <<= 2   ///  r1 = r1.wrapping_shl(2)
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    lsh64 r3, 1                                     r3 <<= 1   ///  r3 = r3.wrapping_shl(1)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0x3c8]                    
    stxdw [r10-0x530], r1                   
    jne r2, 0, lbb_9605                             if r2 != (0 as i32 as i64 as u64) { pc += 13 }
    lddw r1, 0xffbc7f99c5448000                     r1 load str located at -19000000000000000
    mov64 r2, r7                                    r2 = r7
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    lddw r1, 0x38d7ea4d5c240                        r1 load str located at 1000000001000000
    jlt r2, r1, lbb_9605                            if r2 < r1 { pc += 6 }
    lddw r1, 0xff98f8a755838000                     r1 load str located at -29000000000000000
    add64 r7, r1                                    r7 += r1   ///  r7 = r7.wrapping_add(r1)
    lddw r1, 0x71afd499c423f                        r1 load str located at 2000000000999999
    jgt r7, r1, lbb_9619                            if r7 > r1 { pc += 14 }
lbb_9605:
    ldxdw r3, [r10-0x530]                   
    mov64 r1, r3                                    r1 = r3
    add64 r1, 1001                                  r1 += 1001   ///  r1 = r1.wrapping_add(1001 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r3, lbb_9611                            if r1 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_9611:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_10215                            if r2 != (0 as i32 as i64 as u64) { pc += 602 }
    ja lbb_9618                                     if true { pc += 4 }
lbb_9614:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_9454                             if r1 != (0 as i32 as i64 as u64) { pc += -162 }
lbb_9616:
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0x3c8]                    
lbb_9618:
    stxdw [r10-0x530], r1                   
lbb_9619:
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0x1970]                   
    ldxdw r2, [r10-0x518]                   
    jeq r1, r2, lbb_9646                            if r1 == r2 { pc += 23 }
    ldxdw r1, [r10-0x578]                   
    ldxdw r2, [r10-0x518]                   
    stxdw [r1+0x1970], r2                   
    stdw [r1+0x14c8], 0                     
    stdw [r1+0x1488], 0                     
    stdw [r1+0x1448], 0                     
    stdw [r1+0x1408], 0                     
    stdw [r1+0x13c8], 0                     
    stdw [r1+0x1388], 0                     
    stdw [r1+0x1348], 0                     
    stdw [r1+0x1308], 0                     
    stdw [r1+0x12c8], 0                     
    stdw [r1+0x1288], 0                     
    stdw [r1+0x1248], 0                     
    stdw [r1+0x1208], 0                     
    stdw [r1+0x11c8], 0                     
    stdw [r1+0x1188], 0                     
    stdw [r1+0x1148], 0                     
    stdw [r1+0x1108], 0                     
    stdw [r1+0x10c8], 0                     
    stdw [r1+0x1088], 0                     
    stdw [r1+0x1048], 0                     
    stdw [r1+0x1008], 0                     
lbb_9646:
    stdw [r10-0x2d8], 0                     
    stdw [r10-0x2e0], 0                     
    stdw [r10-0x2e8], 0                     
    stdw [r10-0x2f0], 0                     
    stdw [r10-0x2f8], 0                     
    mov64 r8, r10                                   r8 = r10
    add64 r8, -1120                                 r8 += -1120   ///  r8 = r8.wrapping_add(-1120 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r3, 360                                   r3 = 360 as i32 as i64 as u64
    call function_21517                     
    ldxdw r1, [r10-0x590]                   
    ldxdw r4, [r1+0x0]                      
    ldxdw r1, [r10-0x588]                   
    ldxdw r3, [r1+0x0]                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -672                                  r7 += -672   ///  r7 = r7.wrapping_add(-672 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    ldxdw r2, [r10-0x500]                   
    call function_11156                     
    ldxdw r1, [r10-0x598]                   
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_9672                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_9672:
    ldxdw r1, [r10-0x520]                   
    ldxdw r4, [r1+0x0]                      
    stxdw [r10-0xfc0], r8                   
    ldxdw r1, [r10-0x5b0]                   
    stxdw [r10-0xfc8], r1                   
    ldxdw r1, [r10-0x5a8]                   
    stxdw [r10-0xfd0], r1                   
    ldxdw r1, [r10-0x518]                   
    stxdw [r10-0xfd8], r1                   
    ldxdw r1, [r10-0x510]                   
    stxdw [r10-0xfe0], r1                   
    ldxdw r1, [r10-0x530]                   
    stxdw [r10-0xfe8], r1                   
    ldxdw r1, [r10-0x528]                   
    stxdw [r10-0xff0], r1                   
    ldxdw r1, [r10-0x508]                   
    stxdw [r10-0xff8], r1                   
    stdw [r10-0x1000], -1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -720                                  r1 += -720   ///  r1 = r1.wrapping_add(-720 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r7                                    r2 = r7
    call function_10876                     
    mov64 r6, 26                                    r6 = 26 as i32 as i64 as u64
    ldxdw r1, [r10-0x2d0]                   
    stxdw [r10-0x508], r1                   
    jeq r1, 0, lbb_9978                             if r1 == (0 as i32 as i64 as u64) { pc += 279 }
    ldxw r1, [r10-0x2c0]                    
    stxdw [r10-0x528], r1                   
    ldxdw r1, [r10-0x2c8]                   
    stxdw [r10-0x520], r1                   
    call function_22513                     
    mov64 r9, r0                                    r9 = r0
    mov64 r1, r9                                    r1 = r9
    lddw r2, 0x3fe6666666666666                     r2 load str located at 4604480259023595110
    call function_22254                     
    mov64 r8, r0                                    r8 = r0
    ldxdw r1, [r10-0x508]                   
    call function_22513                     
    mov64 r7, r0                                    r7 = r0
    ldxdw r1, [r10-0x598]                   
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jeq r1, 0, lbb_9774                             if r1 == (0 as i32 as i64 as u64) { pc += 57 }
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0xbc0]                    
    mov64 r2, r7                                    r2 = r7
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    mov64 r2, r8                                    r2 = r8
    call function_22929                     
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r8, 12                                    r8 = 12 as i32 as i64 as u64
    jsgt r0, 0, lbb_9728                            if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    ja lbb_9978                                     if true { pc += 250 }
lbb_9728:
    ldxdw r2, [r10-0x578]                   
    ldxdw r1, [r2+0x1738]                   
    mov64 r3, r1                                    r3 = r1
    ldxdw r4, [r10-0x508]                   
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    ldxdw r0, [r2+0x1740]                   
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r3, r1, lbb_9738                            if r3 < r1 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_9738:
    mov64 r4, r0                                    r4 = r0
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    jlt r4, r0, lbb_9742                            if r4 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_9742:
    jge r3, r1, lbb_9744                            if r3 >= r1 { pc += 1 }
    mov64 r2, r5                                    r2 = r5
lbb_9744:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_10218                            if r2 != (0 as i32 as i64 as u64) { pc += 472 }
    ldxdw r1, [r10-0x578]                   
    ldxdw r0, [r1+0x1728]                   
    mov64 r1, r0                                    r1 = r0
    ldxdw r2, [r10-0x520]                   
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r1, r0, lbb_9755                            if r1 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_9755:
    ldxdw r2, [r10-0x578]                   
    ldxdw r6, [r2+0x1730]                   
    mov64 r2, r6                                    r2 = r6
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    jlt r2, r6, lbb_9761                            if r2 < r6 { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_9761:
    jge r1, r0, lbb_9763                            if r1 >= r0 { pc += 1 }
    mov64 r5, r8                                    r5 = r8
lbb_9763:
    ldxdw r0, [r10-0x578]                   
    stxdw [r0+0x1738], r3                   
    stxdw [r0+0x1740], r4                   
    mov64 r3, r0                                    r3 = r0
    add64 r3, 5928                                  r3 += 5928   ///  r3 = r3.wrapping_add(5928 as i32 as i64 as u64)
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_9771                             if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9828                                     if true { pc += 57 }
lbb_9771:
    lddw r1, 0x10002f2e8 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00n\x05\x00…        r1 load str located at 4295160552
    call function_20946                     
lbb_9774:
    ldxdw r1, [r10-0x578]                   
    ldxdw r1, [r1+0xbb8]                    
    mov64 r2, r7                                    r2 = r7
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    mov64 r2, r8                                    r2 = r8
    call function_22929                     
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r8, 12                                    r8 = 12 as i32 as i64 as u64
    jsgt r0, 0, lbb_9785                            if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    ja lbb_9978                                     if true { pc += 193 }
lbb_9785:
    ldxdw r2, [r10-0x578]                   
    ldxdw r1, [r2+0x1718]                   
    mov64 r3, r1                                    r3 = r1
    ldxdw r4, [r10-0x508]                   
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    ldxdw r0, [r2+0x1720]                   
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r3, r1, lbb_9795                            if r3 < r1 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_9795:
    mov64 r4, r0                                    r4 = r0
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    jlt r4, r0, lbb_9799                            if r4 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_9799:
    jge r3, r1, lbb_9801                            if r3 >= r1 { pc += 1 }
    mov64 r2, r5                                    r2 = r5
lbb_9801:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_10221                            if r2 != (0 as i32 as i64 as u64) { pc += 418 }
    ldxdw r1, [r10-0x578]                   
    ldxdw r0, [r1+0x1748]                   
    mov64 r1, r0                                    r1 = r0
    ldxdw r2, [r10-0x520]                   
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r1, r0, lbb_9812                            if r1 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_9812:
    ldxdw r2, [r10-0x578]                   
    ldxdw r8, [r2+0x1750]                   
    mov64 r2, r8                                    r2 = r8
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    jlt r2, r8, lbb_9818                            if r2 < r8 { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_9818:
    jge r1, r0, lbb_9820                            if r1 >= r0 { pc += 1 }
    mov64 r5, r7                                    r5 = r7
lbb_9820:
    ldxdw r0, [r10-0x578]                   
    stxdw [r0+0x1718], r3                   
    stxdw [r0+0x1720], r4                   
    mov64 r3, r0                                    r3 = r0
    add64 r3, 5960                                  r3 += 5960   ///  r3 = r3.wrapping_add(5960 as i32 as i64 as u64)
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    mov64 r7, r9                                    r7 = r9
    jne r5, 0, lbb_10224                            if r5 != (0 as i32 as i64 as u64) { pc += 396 }
lbb_9828:
    stxdw [r3+0x0], r1                      
    stxdw [r3+0x8], r2                      
    ldxdw r8, [r10-0x578]                   
    ldxdw r1, [r8+0xba0]                    
    mov64 r2, r7                                    r2 = r7
    call function_22254                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1240                                 r1 += -1240   ///  r1 = r1.wrapping_add(-1240 as i32 as i64 as u64)
    mov64 r2, r0                                    r2 = r0
    call function_17411                     
    ldxdw r3, [r8+0x1758]                   
    ldxdw r2, [r10-0x4d8]                   
    mov64 r1, r3                                    r1 = r3
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r3, lbb_9846                            if r1 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_9846:
    ldxdw r3, [r10-0x578]                   
    ldxdw r5, [r3+0x1760]                   
    ldxdw r0, [r10-0x4d0]                   
    mov64 r3, r5                                    r3 = r5
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    jlt r3, r5, lbb_9854                            if r3 < r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_9854:
    jeq r3, r5, lbb_9856                            if r3 == r5 { pc += 1 }
    mov64 r2, r4                                    r2 = r4
lbb_9856:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_10227                            if r2 != (0 as i32 as i64 as u64) { pc += 369 }
    ldxdw r2, [r10-0x578]                   
    stxdw [r2+0x1758], r1                   
    stxdw [r2+0x1760], r3                   
    ldxdw r2, [r10-0x540]                   
    ldxdw r1, [r2+0x40]                     
    ldxdw r2, [r2+0x50]                     
    ldxdw r3, [r10-0x508]                   
    stxdw [r10-0x288], r3                   
    stxdw [r10-0x290], r2                   
    ldxdw r2, [r10-0x560]                   
    stxdw [r10-0x298], r2                   
    stxdw [r10-0x2a0], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1248                                 r1 += -1248   ///  r1 = r1.wrapping_add(-1248 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -672                                  r2 += -672   ///  r2 = r2.wrapping_add(-672 as i32 as i64 as u64)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_15609                     
    ldxw r8, [r10-0x4dc]                    
    ldxw r6, [r10-0x4e0]                    
    jeq r6, 26, lbb_9881                            if r6 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9978                                     if true { pc += 97 }
lbb_9881:
    ldxdw r1, [r10-0x540]                   
    ldxdw r1, [r1+0x48]                     
    ldxdw r2, [r10-0x520]                   
    stxdw [r10-0x2b8], r2                   
    stxdw [r10-0x2c8], r1                   
    ldxdw r1, [r10-0x558]                   
    stxdw [r10-0x2c0], r1                   
    stxdw [r10-0x2d0], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -672                                  r1 += -672   ///  r1 = r1.wrapping_add(-672 as i32 as i64 as u64)
    stxdw [r10-0x2b0], r1                   
    ldxdw r1, [r10-0x6b0]                   
    stxdw [r10-0x280], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1192                                 r1 += -1192   ///  r1 = r1.wrapping_add(-1192 as i32 as i64 as u64)
    stxdw [r10-0x290], r1                   
    lddw r1, 0x10002d2e8 --> b"VaultInitSettingsprograms/amm/src/dispatcher.rsIns"        r1 load str located at 4295152360
    stxdw [r10-0x2a0], r1                   
    stdw [r10-0x2a8], 3                     
    stdw [r10-0x278], 1                     
    stdw [r10-0x288], 32                    
    stdw [r10-0x298], 5                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1256                                 r1 += -1256   ///  r1 = r1.wrapping_add(-1256 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -720                                  r2 += -720   ///  r2 = r2.wrapping_add(-720 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -688                                  r3 += -688   ///  r3 = r3.wrapping_add(-688 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_15609                     
    ldxw r8, [r10-0x4e4]                    
    ldxw r6, [r10-0x4e8]                    
    jne r6, 26, lbb_9978                            if r6 != (26 as i32 as i64 as u64) { pc += 63 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -672                                  r1 += -672   ///  r1 = r1.wrapping_add(-672 as i32 as i64 as u64)
    ldxdw r2, [r10-0x570]                   
    call function_11320                     
    ldxw r8, [r10-0x298]                    
    ldxw r6, [r10-0x29c]                    
    ldxw r1, [r10-0x2a0]                    
    jne r1, 0, lbb_9978                             if r1 != (0 as i32 as i64 as u64) { pc += 55 }
    ldxdw r7, [r10-0x298]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -672                                  r1 += -672   ///  r1 = r1.wrapping_add(-672 as i32 as i64 as u64)
    ldxdw r2, [r10-0x5a0]                   
    call function_11320                     
    ldxw r8, [r10-0x298]                    
    ldxw r6, [r10-0x29c]                    
    ldxw r1, [r10-0x2a0]                    
    jne r1, 0, lbb_9978                             if r1 != (0 as i32 as i64 as u64) { pc += 46 }
    ldxdw r1, [r10-0x298]                   
    ldxdw r2, [r10-0x588]                   
    ldxdw r3, [r2+0x0]                      
    ldxdw r2, [r10-0x590]                   
    ldxdw r2, [r2+0x0]                      
    stxdw [r10-0xff8], r1                   
    stxdw [r10-0x1000], r2                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1264                                 r1 += -1264   ///  r1 = r1.wrapping_add(-1264 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0x500]                   
    mov64 r4, r7                                    r4 = r7
    call function_3128                      
    ldxw r8, [r10-0x4ec]                    
    ldxw r6, [r10-0x4f0]                    
    jne r6, 26, lbb_9978                            if r6 != (26 as i32 as i64 as u64) { pc += 30 }
    ldxdw r1, [r10-0x578]                   
    ldxb r1, [r1+0x790]                     
    jne r1, 0, lbb_10004                            if r1 != (0 as i32 as i64 as u64) { pc += 53 }
    ldxdw r1, [r10-0x598]                   
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r2, 4728                                  r2 = 4728 as i32 as i64 as u64
    jeq r1, 0, lbb_9957                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 4088                                  r2 = 4088 as i32 as i64 as u64
lbb_9957:
    ldxdw r3, [r10-0x578]                   
    mov64 r1, r3                                    r1 = r3
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    ldxb r2, [r3+0x79f]                     
    jne r2, 1, lbb_9982                             if r2 != (1 as i32 as i64 as u64) { pc += 20 }
    ldxdw r3, [r10-0x598]                   
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    mov64 r2, 5372                                  r2 = 5372 as i32 as i64 as u64
    jeq r3, 0, lbb_9968                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 5368                                  r2 = 5368 as i32 as i64 as u64
lbb_9968:
    ldxdw r4, [r10-0x578]                   
    mov64 r3, r4                                    r3 = r4
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    ldxw r2, [r3+0x0]                       
    ldxdw r3, [r4+0xbb0]                    
    ldxdw r4, [r10-0x520]                   
    stxdw [r10-0xfe8], r4                   
    stxdw [r10-0xff0], r3                   
    ldxdw r3, [r10-0x510]                   
    ja lbb_9997                                     if true { pc += 19 }
lbb_9978:
    ldxdw r1, [r10-0x608]                   
    ldxdw r2, [r10-0x600]                   
    call function_167                       
    ja lbb_8113                                     if true { pc += -1869 }
lbb_9982:
    ldxdw r3, [r10-0x598]                   
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    mov64 r2, 5404                                  r2 = 5404 as i32 as i64 as u64
    jeq r3, 0, lbb_9988                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 5400                                  r2 = 5400 as i32 as i64 as u64
lbb_9988:
    ldxdw r4, [r10-0x578]                   
    mov64 r3, r4                                    r3 = r4
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    ldxw r2, [r3+0x0]                       
    ldxdw r3, [r4+0xc20]                    
    ldxdw r4, [r10-0x520]                   
    stxdw [r10-0xfe8], r4                   
    stxdw [r10-0xff0], r3                   
    ldxdw r3, [r10-0x518]                   
lbb_9997:
    stxdw [r10-0xff8], r3                   
    stxdw [r10-0x1000], r2                  
    mov64 r5, r10                                   r5 = r10
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    ldxdw r3, [r10-0x6b8]                   
    mov64 r4, 10                                    r4 = 10 as i32 as i64 as u64
    call function_11047                     
lbb_10004:
    ldxdw r1, [r10-0x578]                   
    ldxb r1, [r1+0x79f]                     
    jne r1, 1, lbb_10098                            if r1 != (1 as i32 as i64 as u64) { pc += 91 }
    ldxdw r1, [r10-0x580]                   
    ldxdw r1, [r1+0x0]                      
    ldxdw r2, [r1+0x8]                      
    ldxdw r3, [r1+0x10]                     
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r1+0x20]                     
    ldxdw r5, [r10-0x480]                   
    stxdw [r10-0x278], r5                   
    stxdw [r10-0x280], r1                   
    stxdw [r10-0x288], r4                   
    stxdw [r10-0x290], r3                   
    stxdw [r10-0x298], r2                   
    ldxdw r1, [r10-0x478]                   
    stxdw [r10-0x270], r1                   
    ldxdw r1, [r10-0x470]                   
    stxdw [r10-0x268], r1                   
    ldxdw r1, [r10-0x468]                   
    stxdw [r10-0x260], r1                   
    ldxdw r1, [r10-0x578]                   
    ldxdw r6, [r1+0xba8]                    
    ldxdw r2, [r1+0xba0]                    
    stxdw [r10-0x500], r2                   
    ldxdw r2, [r1+0xbb0]                    
    stxdw [r10-0x510], r2                   
    ldxw r2, [r1+0xbf4]                     
    stxdw [r10-0x518], r2                   
    ldxdw r8, [r1+0xbe8]                    
    ldxdw r9, [r1+0xc00]                    
    ldxdw r7, [r1+0x14f8]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -536                                  r1 += -536   ///  r1 = r1.wrapping_add(-536 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1120                                 r2 += -1120   ///  r2 = r2.wrapping_add(-1120 as i32 as i64 as u64)
    mov64 r3, 400                                   r3 = 400 as i32 as i64 as u64
    call function_21513                     
    ldxdw r2, [r10-0x598]                   
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_10048                            if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_10048:
    ldxdw r3, [r10-0x578]                   
    ldxdw r2, [r3+0xc20]                    
    ldxdw r3, [r3+0xc18]                    
    stxdw [r10-0x68], r3                    
    stxdw [r10-0x220], r7                   
    ldxdw r3, [r10-0x5d0]                   
    stxw [r10-0x88], r3                     
    ldxdw r3, [r10-0x5c8]                   
    stxw [r10-0x84], r3                     
    stxdw [r10-0x60], r2                    
    ldxdw r2, [r10-0x530]                   
    stxdw [r10-0x70], r2                    
    stxdw [r10-0x78], r9                    
    stxdw [r10-0x80], r8                    
    stxb [r10-0x224], r1                    
    ldxdw r1, [r10-0x528]                   
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r2, 1000                                  r2 = 1000 as i32 as i64 as u64
    call function_22231                     
    stxw [r10-0x228], r0                    
    ldxdw r1, [r10-0x518]                   
    stxw [r10-0x22c], r1                    
    ldxdw r1, [r10-0x510]                   
    stxdw [r10-0x238], r1                   
    ldxdw r1, [r10-0x500]                   
    stxdw [r10-0x240], r1                   
    stxdw [r10-0x248], r6                   
    ldxdw r1, [r10-0x520]                   
    stxdw [r10-0x250], r1                   
    ldxdw r1, [r10-0x508]                   
    stxdw [r10-0x258], r1                   
    stb [r10-0x221], 0                      
    sth [r10-0x223], 0                      
    stw [r10-0x230], 0                      
    stdw [r10-0x2a0], 1                     
    stdw [r10-0x10], 0                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 0                      
    stdw [r10-0x28], 0                      
    stdw [r10-0x30], 0                      
    stdw [r10-0x38], 0                      
    stdw [r10-0x40], 0                      
    stdw [r10-0x48], 0                      
    stdw [r10-0x50], 0                      
    stdw [r10-0x58], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -672                                  r1 += -672   ///  r1 = r1.wrapping_add(-672 as i32 as i64 as u64)
    call function_3046                      
    ja lbb_10178                                    if true { pc += 80 }
lbb_10098:
    ldxdw r1, [r10-0x580]                   
    ldxdw r1, [r1+0x0]                      
    ldxdw r2, [r1+0x8]                      
    ldxdw r3, [r1+0x10]                     
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r1+0x20]                     
    ldxdw r5, [r10-0x480]                   
    stxdw [r10-0x278], r5                   
    stxdw [r10-0x280], r1                   
    stxdw [r10-0x288], r4                   
    stxdw [r10-0x290], r3                   
    stxdw [r10-0x298], r2                   
    ldxdw r1, [r10-0x478]                   
    stxdw [r10-0x270], r1                   
    ldxdw r1, [r10-0x470]                   
    stxdw [r10-0x268], r1                   
    ldxdw r1, [r10-0x468]                   
    stxdw [r10-0x260], r1                   
    ldxdw r1, [r10-0x578]                   
    ldxdw r6, [r1+0xba8]                    
    ldxdw r2, [r1+0xba0]                    
    stxdw [r10-0x500], r2                   
    ldxdw r2, [r1+0xc18]                    
    stxdw [r10-0x510], r2                   
    ldxdw r2, [r1+0xc20]                    
    stxdw [r10-0x518], r2                   
    ldxw r2, [r1+0xbf4]                     
    stxdw [r10-0x538], r2                   
    ldxdw r9, [r1+0x1518]                   
    ldxdw r7, [r1+0xbe8]                    
    ldxdw r8, [r1+0xc00]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -528                                  r1 += -528   ///  r1 = r1.wrapping_add(-528 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1120                                 r2 += -1120   ///  r2 = r2.wrapping_add(-1120 as i32 as i64 as u64)
    mov64 r3, 400                                   r3 = 400 as i32 as i64 as u64
    call function_21513                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r3, 96                                    r3 = 96 as i32 as i64 as u64
    call function_21517                     
    ldxdw r2, [r10-0x598]                   
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_10146                            if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_10146:
    ldxdw r2, [r10-0x530]                   
    stxdw [r10-0x68], r2                    
    stxdw [r10-0x70], r8                    
    stxdw [r10-0x78], r7                    
    ldxdw r2, [r10-0x5b8]                   
    stxw [r10-0x7c], r2                     
    ldxdw r2, [r10-0x5c0]                   
    stxw [r10-0x80], r2                     
    stxdw [r10-0x218], r9                   
    stxb [r10-0x21c], r1                    
    ldxdw r1, [r10-0x528]                   
    stxw [r10-0x220], r1                    
    ldxdw r1, [r10-0x538]                   
    stxw [r10-0x224], r1                    
    ldxdw r1, [r10-0x518]                   
    stxdw [r10-0x230], r1                   
    ldxdw r1, [r10-0x510]                   
    stxdw [r10-0x238], r1                   
    ldxdw r1, [r10-0x500]                   
    stxdw [r10-0x240], r1                   
    stxdw [r10-0x248], r6                   
    ldxdw r1, [r10-0x520]                   
    stxdw [r10-0x250], r1                   
    ldxdw r1, [r10-0x508]                   
    stxdw [r10-0x258], r1                   
    stb [r10-0x219], 0                      
    sth [r10-0x21b], 0                      
    stw [r10-0x228], 0                      
    stdw [r10-0x2a0], 2                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -672                                  r1 += -672   ///  r1 = r1.wrapping_add(-672 as i32 as i64 as u64)
    call function_2964                      
lbb_10178:
    ldxdw r1, [r10-0x608]                   
    ldxdw r2, [r10-0x600]                   
    call function_167                       
    ldxdw r2, [r10-0x578]                   
    ldxb r1, [r2+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r2+0x0], r1                       
    ldxdw r2, [r10-0x568]                   
    ldxb r1, [r2+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r2+0x0], r1                       
    ldxdw r2, [r10-0x550]                   
    ldxb r1, [r2+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r2+0x0], r1                       
    mov64 r6, 26                                    r6 = 26 as i32 as i64 as u64
    ja lbb_7843                                     if true { pc += -2352 }
lbb_10195:
    ldxdw r1, [r10-0x5e8]                   
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    lddw r3, 0x10002f198 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xca\x03\…        r3 load str located at 4295160216
    call function_20247                     
lbb_10200:
    lddw r1, 0x10002f1e0 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xf1\x04\…        r1 load str located at 4295160288
    call function_20946                     
lbb_10203:
    lddw r1, 0x10002f1c8 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xe5\x04\…        r1 load str located at 4295160264
    call function_20946                     
lbb_10206:
    lddw r1, 0x10002f288 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x80\x04\…        r1 load str located at 4295160456
    call function_20946                     
lbb_10209:
    lddw r1, 0x10002f1b0 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x1b\x05\…        r1 load str located at 4295160240
    call function_20946                     
lbb_10212:
    lddw r1, 0x10002f2b8 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xbc\x04\…        r1 load str located at 4295160504
    call function_20946                     
lbb_10215:
    lddw r1, 0x10002f2a0 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xaa\x04\…        r1 load str located at 4295160480
    call function_20946                     
lbb_10218:
    lddw r1, 0x10002f2d0 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00m\x05\x00…        r1 load str located at 4295160528
    call function_20946                     
lbb_10221:
    lddw r1, 0x10002f300 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00i\x05\x00…        r1 load str located at 4295160576
    call function_20946                     
lbb_10224:
    lddw r1, 0x10002f318 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00j\x05\x00…        r1 load str located at 4295160600
    call function_20946                     
lbb_10227:
    lddw r1, 0x10002f330 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00r\x05\x00…        r1 load str located at 4295160624
    call function_20946                     

function_10230:
    mov64 r8, r4                                    r8 = r4
    mov64 r6, r3                                    r6 = r3
    mov64 r7, r2                                    r7 = r2
    mov64 r9, r1                                    r9 = r1
    ldxdw r1, [r6+0x38]                     
    ldxdw r1, [r1+0x0]                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x10002cf08 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295151368
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jne r0, 0, lbb_10346                            if r0 != (0 as i32 as i64 as u64) { pc += 100 }
    stxdw [r10-0xa8], r9                    
    ldxdw r1, [r6+0x20]                     
    ldxdw r4, [r1+0x0]                      
    ldxdw r2, [r6+0x18]                     
    ldxdw r9, [r6+0x28]                     
    ldxdw r3, [r9+0x0]                      
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    add64 r4, 8                                     r4 += 8   ///  r4 = r4.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r5, r7                                    r5 = r7
    call function_2375                      
    ldxw r3, [r10-0x24]                     
    ldxw r5, [r10-0x28]                     
    ldxdw r7, [r10-0x30]                    
    jeq r7, 0, lbb_10338                            if r7 == (0 as i32 as i64 as u64) { pc += 76 }
    ldxdw r1, [r10-0x20]                    
    stxdw [r10-0xb0], r1                    
    ldxb r1, [r7+0x334]                     
    ldxdw r2, [r7+0x80]                     
    stxdw [r10-0x70], r2                    
    ldxdw r2, [r7+0x78]                     
    stxdw [r10-0x78], r2                    
    ldxdw r2, [r7+0x70]                     
    stxdw [r10-0x80], r2                    
    ldxdw r2, [r7+0x68]                     
    stxdw [r10-0x88], r2                    
    stxb [r10-0x68], r1                     
    ldxdw r1, [r6+0x30]                     
    ldxdw r2, [r8+0x0]                      
    stxdw [r10-0x48], r2                    
    stxdw [r10-0x58], r1                    
    stxdw [r10-0x50], r9                    
    stxdw [r10-0x60], r9                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -136                                  r1 += -136   ///  r1 = r1.wrapping_add(-136 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x10002d2e8 --> b"VaultInitSettingsprograms/amm/src/dispatcher.rsIns"        r1 load str located at 4295152360
    stxdw [r10-0x30], r1                    
    stdw [r10-0x38], 3                      
    stdw [r10-0x8], 1                       
    stdw [r10-0x18], 32                     
    stdw [r10-0x28], 5                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -96                                   r2 += -96   ///  r2 = r2.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r6, r3                                    r6 = r3
    mov64 r3, r10                                   r3 = r10
    add64 r3, -64                                   r3 += -64   ///  r3 = r3.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r8, r5                                    r8 = r5
    call function_15609                     
    mov64 r3, r6                                    r3 = r6
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    or64 r3, r8                                     r3 |= r8   ///  r3 = r3.or(r8)
    ldxw r5, [r10-0x90]                     
    jeq r5, 26, lbb_10312                           if r5 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_10340                                    if true { pc += 28 }
lbb_10312:
    mov64 r5, 3                                     r5 = 3 as i32 as i64 as u64
    ldxdw r1, [r9+0x0]                      
    ldxdw r2, [r1+0x50]                     
    jne r2, 165, lbb_10335                          if r2 != (165 as i32 as i64 as u64) { pc += 19 }
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x10002cf08 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295151368
    mov64 r6, r3                                    r6 = r3
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r5, 3                                     r5 = 3 as i32 as i64 as u64
    mov64 r3, r6                                    r3 = r6
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_10335                            if r0 != (0 as i32 as i64 as u64) { pc += 8 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    call function_18010                     
    ldxdw r1, [r10-0x30]                    
    jne r1, 0, lbb_10349                            if r1 != (0 as i32 as i64 as u64) { pc += 16 }
    ldxdw r5, [r10-0x28]                    
    mov64 r3, r6                                    r3 = r6
lbb_10335:
    mov64 r1, r5                                    r1 = r5
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    ja lbb_10341                                    if true { pc += 3 }
lbb_10338:
    mov64 r1, r3                                    r1 = r3
    ja lbb_10345                                    if true { pc += 5 }
lbb_10340:
    ldxw r1, [r10-0x8c]                     
lbb_10341:
    ldxb r2, [r3+0x0]                       
    ldxdw r4, [r10-0xb0]                    
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    stxb [r3+0x0], r2                       
lbb_10345:
    ldxdw r9, [r10-0xa8]                    
lbb_10346:
    stxw [r9+0x4], r1                       
    stxw [r9+0x0], r5                       
    exit                                    
lbb_10349:
    ldxb r3, [r10-0x18]                     
    jgt r3, 7, lbb_10405                            if r3 > (7 as i32 as i64 as u64) { pc += 54 }
    ldxdw r2, [r10-0x20]                    
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    lsh64 r4, r3                                    r4 <<= r3   ///  r4 = r4.wrapping_shl(r3 as u32)
    and64 r4, 255                                   r4 &= 255   ///  r4 = r4.and(255)
    ldxb r3, [r2+0x0]                       
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    mov64 r4, r3                                    r4 = r3
    and64 r4, 255                                   r4 &= 255   ///  r4 = r4.and(255)
    jne r4, r3, lbb_10360                           if r4 != r3 { pc += 0 }
lbb_10360:
    jne r4, r3, lbb_10408                           if r4 != r3 { pc += 47 }
    ldxdw r8, [r1+0x40]                     
    stxb [r2+0x0], r3                       
    ldxdw r4, [r7+0x228]                    
    ldxb r2, [r7+0x335]                     
    jeq r2, 2, lbb_10384                            if r2 == (2 as i32 as i64 as u64) { pc += 18 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    mov64 r3, r6                                    r3 = r6
    jne r2, 1, lbb_10341                            if r2 != (1 as i32 as i64 as u64) { pc += -29 }
    mov64 r2, r7                                    r2 = r7
    add64 r2, 472                                   r2 += 472   ///  r2 = r2.wrapping_add(472 as i32 as i64 as u64)
    mov64 r3, r7                                    r3 = r7
    add64 r3, 136                                   r3 += 136   ///  r3 = r3.wrapping_add(136 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -160                                  r1 += -160   ///  r1 = r1.wrapping_add(-160 as i32 as i64 as u64)
    call function_3829                      
    ldxw r1, [r10-0x9c]                     
    ldxw r5, [r10-0xa0]                     
    jne r5, 26, lbb_10403                           if r5 != (26 as i32 as i64 as u64) { pc += 23 }
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r8                                    r2 = r8
    call function_4128                      
    ja lbb_10397                                    if true { pc += 13 }
lbb_10384:
    mov64 r2, r7                                    r2 = r7
    add64 r2, 472                                   r2 += 472   ///  r2 = r2.wrapping_add(472 as i32 as i64 as u64)
    mov64 r3, r7                                    r3 = r7
    add64 r3, 136                                   r3 += 136   ///  r3 = r3.wrapping_add(136 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -152                                  r1 += -152   ///  r1 = r1.wrapping_add(-152 as i32 as i64 as u64)
    call function_4319                      
    ldxw r1, [r10-0x94]                     
    ldxw r5, [r10-0x98]                     
    jne r5, 26, lbb_10403                           if r5 != (26 as i32 as i64 as u64) { pc += 9 }
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r8                                    r2 = r8
    call function_4621                      
lbb_10397:
    ldxb r1, [r6+0x0]                       
    ldxdw r2, [r10-0xb0]                    
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    stxb [r6+0x0], r1                       
    mov64 r5, 26                                    r5 = 26 as i32 as i64 as u64
    ja lbb_10345                                    if true { pc += -58 }
lbb_10403:
    mov64 r3, r6                                    r3 = r6
    ja lbb_10341                                    if true { pc += -64 }
lbb_10405:
    lddw r1, 0x10002eb30 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158576
    call function_20990                     
lbb_10408:
    lddw r1, 0x10002eb48 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158600
    call function_20946                     

function_10411:
    mov64 r6, r4                                    r6 = r4
    mov64 r7, r3                                    r7 = r3
    mov64 r8, r2                                    r8 = r2
    mov64 r9, r1                                    r9 = r1
    ldxdw r1, [r7+0x38]                     
    ldxdw r1, [r1+0x0]                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x10002cf08 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295151368
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jne r0, 0, lbb_10533                            if r0 != (0 as i32 as i64 as u64) { pc += 106 }
    stxdw [r10-0x40], r9                    
    stxdw [r10-0x50], r6                    
    ldxdw r1, [r7+0x18]                     
    ldxdw r9, [r1+0x0]                      
    ldxdw r1, [r7+0x20]                     
    stxdw [r10-0x48], r1                    
    ldxdw r6, [r1+0x0]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r3, r9                                    r3 = r9
    mov64 r5, 22                                    r5 = 22 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_10530                            if r0 != (0 as i32 as i64 as u64) { pc += 86 }
    lddw r5, 0x800000000                            r5 load str located at 34359738368
    ldxdw r1, [r3+0x50]                     
    jne r1, 1072, lbb_10530                         if r1 != (1072 as i32 as i64 as u64) { pc += 82 }
    lddw r5, 0x200000000                            r5 load str located at 8589934592
    ldxb r1, [r3+0x2]                       
    jeq r1, 0, lbb_10530                            if r1 == (0 as i32 as i64 as u64) { pc += 78 }
    mov64 r5, 11                                    r5 = 11 as i32 as i64 as u64
    ldxb r8, [r3+0x0]                       
    mov64 r1, r8                                    r1 = r8
    and64 r1, 15                                    r1 &= 15   ///  r1 = r1.and(15)
    jne r1, 15, lbb_10530                           if r1 != (15 as i32 as i64 as u64) { pc += 73 }
    mov64 r1, r8                                    r1 = r8
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r3+0x0], r1                       
    mov64 r2, r3                                    r2 = r3
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r2                                    r1 = r2
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jeq r1, 0, lbb_10470                            if r1 == (0 as i32 as i64 as u64) { pc += 5 }
    lddw r1, 0x10002d1f5 --> b"from_bytes_mut"        r1 load str located at 4295152117
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_2131                      
lbb_10470:
    lddw r5, 0x900000000                            r5 load str located at 38654705664
    stxdw [r10-0x60], r2                    
    ldxdw r1, [r2+0x0]                      
    jne r1, 1, lbb_10529                            if r1 != (1 as i32 as i64 as u64) { pc += 54 }
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    add64 r2, 128                                   r2 += 128   ///  r2 = r2.wrapping_add(128 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    stxdw [r10-0x58], r3                    
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    ldxdw r3, [r10-0x58]                    
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_10529                            if r0 != (0 as i32 as i64 as u64) { pc += 42 }
    ldxdw r1, [r7+0x28]                     
    ldxdw r2, [r7+0x30]                     
    ldxdw r3, [r10-0x50]                    
    ldxdw r3, [r3+0x0]                      
    stxdw [r10-0x8], r3                     
    stxdw [r10-0x10], r2                    
    ldxdw r7, [r10-0x48]                    
    stxdw [r10-0x18], r7                    
    stxdw [r10-0x20], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -32                                   r2 += -32   ///  r2 = r2.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_15609                     
    ldxw r5, [r10-0x28]                     
    jeq r5, 26, lbb_10506                           if r5 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_10536                                    if true { pc += 30 }
lbb_10506:
    mov64 r5, 3                                     r5 = 3 as i32 as i64 as u64
    ldxdw r1, [r7+0x0]                      
    ldxdw r2, [r1+0x50]                     
    jne r2, 165, lbb_10526                          if r2 != (165 as i32 as i64 as u64) { pc += 16 }
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x10002cf08 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295151368
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r5, 3                                     r5 = 3 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_10526                            if r0 != (0 as i32 as i64 as u64) { pc += 7 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_18010                     
    ldxdw r1, [r10-0x20]                    
    jne r1, 0, lbb_10542                            if r1 != (0 as i32 as i64 as u64) { pc += 17 }
    ldxdw r5, [r10-0x18]                    
lbb_10526:
    mov64 r1, r5                                    r1 = r5
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    ja lbb_10537                                    if true { pc += 8 }
lbb_10529:
    stxb [r3+0x0], r8                       
lbb_10530:
    mov64 r1, r5                                    r1 = r5
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
lbb_10532:
    ldxdw r9, [r10-0x40]                    
lbb_10533:
    stxw [r9+0x4], r1                       
    stxw [r9+0x0], r5                       
    exit                                    
lbb_10536:
    ldxw r1, [r10-0x24]                     
lbb_10537:
    ldxdw r3, [r10-0x58]                    
lbb_10538:
    ldxb r2, [r3+0x0]                       
    or64 r2, 8                                      r2 |= 8   ///  r2 = r2.or(8)
    stxb [r3+0x0], r2                       
    ja lbb_10532                                    if true { pc += -10 }
lbb_10542:
    ldxb r3, [r10-0x8]                      
    jgt r3, 7, lbb_10594                            if r3 > (7 as i32 as i64 as u64) { pc += 50 }
    ldxdw r2, [r10-0x10]                    
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    lsh64 r4, r3                                    r4 <<= r3   ///  r4 = r4.wrapping_shl(r3 as u32)
    and64 r4, 255                                   r4 &= 255   ///  r4 = r4.and(255)
    ldxb r3, [r2+0x0]                       
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    mov64 r4, r3                                    r4 = r3
    and64 r4, 255                                   r4 &= 255   ///  r4 = r4.and(255)
    jne r4, r3, lbb_10553                           if r4 != r3 { pc += 0 }
lbb_10553:
    jne r4, r3, lbb_10597                           if r4 != r3 { pc += 43 }
    ldxdw r7, [r1+0x40]                     
    stxb [r2+0x0], r3                       
    ldxdw r3, [r10-0x58]                    
    ldxdw r4, [r3+0x280]                    
    ldxb r2, [r3+0x38d]                     
    jeq r2, 2, lbb_10576                            if r2 == (2 as i32 as i64 as u64) { pc += 16 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    jne r2, 1, lbb_10538                            if r2 != (1 as i32 as i64 as u64) { pc += -25 }
    mov64 r2, r3                                    r2 = r3
    add64 r2, 560                                   r2 += 560   ///  r2 = r2.wrapping_add(560 as i32 as i64 as u64)
    add64 r3, 224                                   r3 += 224   ///  r3 = r3.wrapping_add(224 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -56                                   r1 += -56   ///  r1 = r1.wrapping_add(-56 as i32 as i64 as u64)
    call function_3829                      
    ldxw r1, [r10-0x34]                     
    ldxw r5, [r10-0x38]                     
    jne r5, 26, lbb_10537                           if r5 != (26 as i32 as i64 as u64) { pc += -35 }
    ldxdw r1, [r10-0x60]                    
    mov64 r2, r7                                    r2 = r7
    call function_4128                      
    ja lbb_10588                                    if true { pc += 12 }
lbb_10576:
    mov64 r2, r3                                    r2 = r3
    add64 r2, 560                                   r2 += 560   ///  r2 = r2.wrapping_add(560 as i32 as i64 as u64)
    add64 r3, 224                                   r3 += 224   ///  r3 = r3.wrapping_add(224 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_4319                      
    ldxw r1, [r10-0x2c]                     
    ldxw r5, [r10-0x30]                     
    jne r5, 26, lbb_10537                           if r5 != (26 as i32 as i64 as u64) { pc += -48 }
    ldxdw r1, [r10-0x60]                    
    mov64 r2, r7                                    r2 = r7
    call function_4621                      
lbb_10588:
    ldxdw r2, [r10-0x58]                    
    ldxb r1, [r2+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r2+0x0], r1                       
    mov64 r5, 26                                    r5 = 26 as i32 as i64 as u64
    ja lbb_10532                                    if true { pc += -62 }
lbb_10594:
    lddw r1, 0x10002eb30 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158576
    call function_20990                     
lbb_10597:
    lddw r1, 0x10002eb48 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158600
    call function_20946                     

function_10600:
    mov64 r8, r4                                    r8 = r4
    mov64 r7, r3                                    r7 = r3
    mov64 r3, r2                                    r3 = r2
    mov64 r6, r1                                    r6 = r1
    lddw r1, 0x10002d2ed --> b"InitSettingsprograms/amm/src/dispatcher.rsInstruct"        r1 load str located at 4295152365
    stxdw [r10-0x48], r1                    
    stdw [r10-0x40], 12                     
    stb [r10-0x31], 255                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -48                                   r4 += -48   ///  r4 = r4.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -49                                   r5 += -49   ///  r5 = r5.wrapping_add(-49 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    stxdw [r10-0x80], r3                    
    syscall [invalid]                       
    jeq r0, 0, lbb_10631                            if r0 == (0 as i32 as i64 as u64) { pc += 12 }
    lddw r1, 0x10002ec28 --> b"\x00\x00\x00\x00(\xd2\x02\x001\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295158824
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    lddw r2, 0x10002ec38 --> b"\x00\x00\x00\x00Y\xd2\x02\x00\x0d\x00\x00\x00\x00\x00\x00\x00m\x00\x00\x0…        r2 load str located at 4295158840
    call function_18698                     
lbb_10631:
    stxdw [r10-0x90], r8                    
    stxdw [r10-0x78], r6                    
    ldxdw r1, [r10-0x18]                    
    stxdw [r10-0x50], r1                    
    ldxdw r1, [r10-0x20]                    
    stxdw [r10-0x58], r1                    
    ldxdw r1, [r10-0x28]                    
    stxdw [r10-0x60], r1                    
    ldxdw r1, [r10-0x30]                    
    stxdw [r10-0x68], r1                    
    ldxb r1, [r10-0x31]                     
    stxb [r10-0x31], r1                     
    ldxdw r1, [r7+0x18]                     
    stxdw [r10-0x88], r1                    
    ldxdw r8, [r1+0x0]                      
    mov64 r2, r10                                   r2 = r10
    add64 r2, -104                                  r2 += -104   ///  r2 = r2.wrapping_add(-104 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jne r0, 0, lbb_10788                            if r0 != (0 as i32 as i64 as u64) { pc += 131 }
    ldxdw r1, [r7+0x28]                     
    ldxdw r1, [r1+0x0]                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x10002cf48 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r2 load str located at 4295151432
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_10788                            if r0 != (0 as i32 as i64 as u64) { pc += 121 }
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0x10002cf48 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r2 load str located at 4295151432
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_10713                            if r0 == (0 as i32 as i64 as u64) { pc += 37 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    ldxdw r2, [r10-0x88]                    
    ldxdw r3, [r10-0x80]                    
    call function_2453                      
    ldxw r9, [r10-0x24]                     
    ldxw r1, [r10-0x28]                     
    ldxdw r2, [r10-0x30]                    
    jeq r2, 0, lbb_10785                            if r2 == (0 as i32 as i64 as u64) { pc += 100 }
    stxdw [r10-0x98], r1                    
    ldxdw r8, [r10-0x20]                    
    ldxdw r1, [r7+0x20]                     
    ldxdw r1, [r1+0x0]                      
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    and64 r8, 255                                   r8 &= 255   ///  r8 = r8.and(255)
    jgt r8, 7, lbb_10792                            if r8 > (7 as i32 as i64 as u64) { pc += 97 }
    lsh64 r9, 32                                    r9 <<= 32   ///  r9 = r9.wrapping_shl(32)
    ldxdw r1, [r10-0x98]                    
    or64 r9, r1                                     r9 |= r1   ///  r9 = r9.or(r1)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lsh64 r2, r8                                    r2 <<= r8   ///  r2 = r2.wrapping_shl(r8 as u32)
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    ldxb r1, [r9+0x0]                       
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, r1                                    r2 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jne r2, r1, lbb_10706                           if r2 != r1 { pc += 0 }
lbb_10706:
    jne r2, r1, lbb_10795                           if r2 != r1 { pc += 88 }
    stxb [r9+0x0], r1                       
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_10759                            if r0 == (0 as i32 as i64 as u64) { pc += 47 }
    ja lbb_10788                                    if true { pc += 75 }
lbb_10713:
    ldxdw r1, [r7+0x20]                     
    mov64 r2, r10                                   r2 = r10
    add64 r2, -48                                   r2 += -48   ///  r2 = r2.wrapping_add(-48 as i32 as i64 as u64)
    stxdw [r10-0x48], r2                    
    mov64 r2, r10                                   r2 = r10
    add64 r2, -49                                   r2 += -49   ///  r2 = r2.wrapping_add(-49 as i32 as i64 as u64)
    stxdw [r10-0x20], r2                    
    lddw r2, 0x10002d2ed --> b"InitSettingsprograms/amm/src/dispatcher.rsInstruct"        r2 load str located at 4295152365
    stxdw [r10-0x30], r2                    
    stdw [r10-0x40], 2                      
    stdw [r10-0x18], 1                      
    stdw [r10-0x28], 12                     
    mov64 r2, r10                                   r2 = r10
    add64 r2, -72                                   r2 += -72   ///  r2 = r2.wrapping_add(-72 as i32 as i64 as u64)
    stxdw [r10-0xff8], r2                   
    stxdw [r10-0x1000], r1                  
    stdw [r10-0xff0], 1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r7, [r10-0x88]                    
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 296                                   r3 = 296 as i32 as i64 as u64
    ldxdw r4, [r10-0x80]                    
    call function_4694                      
    ldxw r6, [r10-0x70]                     
    jeq r6, 26, lbb_10742                           if r6 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_10787                                    if true { pc += 45 }
lbb_10742:
    mov64 r6, 11                                    r6 = 11 as i32 as i64 as u64
    ldxdw r7, [r7+0x0]                      
    ldxb r1, [r7+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    jeq r2, 15, lbb_10749                           if r2 == (15 as i32 as i64 as u64) { pc += 1 }
    ja lbb_10788                                    if true { pc += 39 }
lbb_10749:
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r7+0x0], r1                       
    ldxdw r2, [r7+0x50]                     
    mov64 r1, r7                                    r1 = r7
    add64 r1, 88                                    r1 += 88   ///  r1 = r1.wrapping_add(88 as i32 as i64 as u64)
    call function_2114                      
    ldxb r1, [r7+0x0]                       
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    stxb [r7+0x0], r1                       
    stdw [r7+0x58], 3                       
lbb_10759:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    ldxdw r2, [r10-0x88]                    
    ldxdw r3, [r10-0x80]                    
    call function_2578                      
    ldxw r9, [r10-0x24]                     
    ldxw r6, [r10-0x28]                     
    ldxdw r1, [r10-0x30]                    
    jeq r1, 0, lbb_10788                            if r1 == (0 as i32 as i64 as u64) { pc += 20 }
    ldxdw r2, [r10-0x20]                    
    ldxdw r4, [r10-0x90]                    
    ldxdw r3, [r4+0x18]                     
    stxdw [r1+0x20], r3                     
    ldxdw r3, [r4+0x10]                     
    stxdw [r1+0x18], r3                     
    ldxdw r3, [r4+0x8]                      
    stxdw [r1+0x10], r3                     
    ldxdw r3, [r4+0x0]                      
    stxdw [r1+0x8], r3                      
    lsh64 r9, 32                                    r9 <<= 32   ///  r9 = r9.wrapping_shl(32)
    or64 r9, r6                                     r9 |= r6   ///  r9 = r9.or(r6)
    ldxb r1, [r9+0x0]                       
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    stxb [r9+0x0], r1                       
    mov64 r6, 26                                    r6 = 26 as i32 as i64 as u64
    ja lbb_10788                                    if true { pc += 3 }
lbb_10785:
    mov64 r6, r1                                    r6 = r1
    ja lbb_10788                                    if true { pc += 1 }
lbb_10787:
    ldxw r9, [r10-0x6c]                     
lbb_10788:
    ldxdw r1, [r10-0x78]                    
    stxw [r1+0x4], r9                       
    stxw [r1+0x0], r6                       
    exit                                    
lbb_10792:
    lddw r1, 0x10002eb30 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158576
    call function_20990                     
lbb_10795:
    lddw r1, 0x10002eb48 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158600
    call function_20946                     

function_10798:
    mov64 r7, r3                                    r7 = r3
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r7+0x20]                     
    ldxdw r8, [r1+0x0]                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r1, 22                                    r1 = 22 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_10841                            if r0 != (0 as i32 as i64 as u64) { pc += 31 }
    lddw r1, 0x800000000                            r1 load str located at 34359738368
    ldxdw r2, [r8+0x50]                     
    jne r2, 7456, lbb_10841                         if r2 != (7456 as i32 as i64 as u64) { pc += 27 }
    lddw r1, 0x200000000                            r1 load str located at 8589934592
    ldxb r2, [r8+0x2]                       
    jeq r2, 0, lbb_10841                            if r2 == (0 as i32 as i64 as u64) { pc += 23 }
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    ldxb r9, [r8+0x0]                       
    mov64 r2, r9                                    r2 = r9
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    jne r2, 15, lbb_10841                           if r2 != (15 as i32 as i64 as u64) { pc += 18 }
    mov64 r1, r9                                    r1 = r9
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r8+0x0], r1                       
    mov64 r1, r8                                    r1 = r8
    add64 r1, 88                                    r1 += 88   ///  r1 = r1.wrapping_add(88 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    jeq r2, 0, lbb_10836                            if r2 == (0 as i32 as i64 as u64) { pc += 5 }
    lddw r1, 0x10002d1f5 --> b"from_bytes_mut"        r1 load str located at 4295152117
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_2131                      
lbb_10836:
    ldxdw r1, [r1+0x0]                      
    jeq r1, 4, lbb_10846                            if r1 == (4 as i32 as i64 as u64) { pc += 8 }
    stxb [r8+0x0], r9                       
    lddw r1, 0x900000000                            r1 load str located at 38654705664
lbb_10841:
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
lbb_10843:
    stxw [r6+0x4], r2                       
    stxw [r6+0x0], r1                       
    exit                                    
lbb_10846:
    ldxdw r1, [r7+0x18]                     
    ldxdw r1, [r1+0x0]                      
    mov64 r2, r8                                    r2 = r8
    add64 r2, 128                                   r2 += 128   ///  r2 = r2.wrapping_add(128 as i32 as i64 as u64)
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_10860                            if r0 == (0 as i32 as i64 as u64) { pc += 4 }
    stxb [r8+0x0], r9                       
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ja lbb_10843                                    if true { pc += -17 }
lbb_10860:
    ldxb r1, [r8+0xbf0]                     
    jeq r1, 0, lbb_10870                            if r1 == (0 as i32 as i64 as u64) { pc += 8 }
    stxb [r8+0x0], r9                       
    ldxdw r1, [r8+0xc08]                    
    stxdw [r8+0xbe8], r1                    
    ldxdw r1, [r8+0xc10]                    
    stxdw [r8+0xc00], r1                    
    stb [r8+0xbf0], 0                       
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ja lbb_10843                                    if true { pc += -27 }
lbb_10870:
    lddw r1, 0x10002d855 --> b"assertion failed: state.has_saved_spread_modifiers.value()"        r1 load str located at 4295153749
    mov64 r2, 58                                    r2 = 58 as i32 as i64 as u64
    lddw r3, 0x10002f348 --> b"\x00\x00\x00\x00\xc8\xd3\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00|\x06\x00…        r3 load str located at 4295160648
    call function_18704                     

function_10876:
    stxdw [r10-0x50], r3                    
    mov64 r3, r2                                    r3 = r2
    ldxb r2, [r3+0x151]                     
    jne r2, 0, lbb_10884                            if r2 != (0 as i32 as i64 as u64) { pc += 4 }
    stw [r1+0x10], 0                        
    stdw [r1+0x8], 0                        
    stdw [r1+0x0], 0                        
    ja lbb_11046                                    if true { pc += 162 }
lbb_10884:
    stxdw [r10-0x80], r1                    
    ldxdw r9, [r5-0xfc0]                    
    ldxdw r1, [r5-0xfc8]                    
    stxdw [r10-0x78], r1                    
    ldxdw r1, [r5-0xfd0]                    
    stxdw [r10-0x70], r1                    
    ldxdw r1, [r5-0xfd8]                    
    stxdw [r10-0x68], r1                    
    stxdw [r10-0x58], r4                    
    ldxdw r4, [r5-0xfe0]                    
    ldxdw r6, [r5-0xfe8]                    
    ldxdw r7, [r5-0xff0]                    
    ldxdw r0, [r5-0xff8]                    
    ldxdw r1, [r5-0x1000]                   
    stxdw [r10-0x60], r1                    
    ldxdw r8, [r3+0x38]                     
    ldxdw r1, [r3+0x40]                     
    ldxdw r5, [r3+0x30]                     
    mov64 r2, r3                                    r2 = r3
    add64 r2, 24                                    r2 += 24   ///  r2 = r2.wrapping_add(24 as i32 as i64 as u64)
    stxdw [r10-0x8], r2                     
    stxdw [r10-0x10], r3                    
    add64 r3, 72                                    r3 += 72   ///  r3 = r3.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x18], r3                    
    stxdw [r10-0x30], r5                    
    stxdw [r10-0x88], r1                    
    stxdw [r10-0x20], r1                    
    stxdw [r10-0x28], r8                    
    ldxdw r1, [r10-0x78]                    
    stxdw [r10-0xfc0], r1                   
    ldxdw r1, [r10-0x70]                    
    stxdw [r10-0xfc8], r1                   
    ldxdw r1, [r10-0x68]                    
    stxdw [r10-0xfd0], r1                   
    stxdw [r10-0xfd8], r4                   
    stxdw [r10-0xfe0], r6                   
    stxdw [r10-0xff0], r7                   
    stxdw [r10-0xff8], r0                   
    ldxdw r1, [r10-0x60]                    
    stxdw [r10-0x1000], r1                  
    stxdw [r10-0xfb8], r9                   
    stdw [r10-0xfe8], 0                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -48                                   r2 += -48   ///  r2 = r2.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r7, [r10-0x50]                    
    mov64 r3, r7                                    r3 = r7
    ldxdw r4, [r10-0x58]                    
    call function_15753                     
    jne r7, 0, lbb_10937                            if r7 != (0 as i32 as i64 as u64) { pc += 1 }
    ldxdw r8, [r10-0x88]                    
lbb_10937:
    ldxdw r1, [r9+0x0]                      
    ldxdw r2, [r8+0x10]                     
    mov64 r4, r2                                    r4 = r2
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r4, r2, lbb_10944                           if r4 < r2 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_10944:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_11038                            if r1 != (0 as i32 as i64 as u64) { pc += 92 }
    ldxdw r1, [r10-0x48]                    
    ldxdw r2, [r10-0x40]                    
    ldxw r3, [r10-0x38]                     
    stxdw [r8+0x10], r4                     
    ldxdw r0, [r8+0x50]                     
    ldxdw r5, [r9+0x28]                     
    mov64 r4, r0                                    r4 = r0
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r4, r0, lbb_10957                           if r4 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_10957:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_11038                            if r5 != (0 as i32 as i64 as u64) { pc += 79 }
    stxdw [r8+0x50], r4                     
    ldxdw r0, [r8+0x90]                     
    ldxdw r5, [r9+0x50]                     
    mov64 r4, r0                                    r4 = r0
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r4, r0, lbb_10967                           if r4 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_10967:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_11038                            if r5 != (0 as i32 as i64 as u64) { pc += 69 }
    stxdw [r8+0x90], r4                     
    ldxdw r0, [r8+0xd0]                     
    ldxdw r5, [r9+0x78]                     
    mov64 r4, r0                                    r4 = r0
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r4, r0, lbb_10977                           if r4 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_10977:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_11038                            if r5 != (0 as i32 as i64 as u64) { pc += 59 }
    stxdw [r8+0xd0], r4                     
    ldxdw r0, [r8+0x110]                    
    ldxdw r5, [r9+0xa0]                     
    mov64 r4, r0                                    r4 = r0
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r4, r0, lbb_10987                           if r4 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_10987:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_11038                            if r5 != (0 as i32 as i64 as u64) { pc += 49 }
    stxdw [r8+0x110], r4                    
    ldxdw r0, [r8+0x150]                    
    ldxdw r5, [r9+0xc8]                     
    mov64 r4, r0                                    r4 = r0
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r4, r0, lbb_10997                           if r4 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_10997:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_11038                            if r5 != (0 as i32 as i64 as u64) { pc += 39 }
    stxdw [r8+0x150], r4                    
    ldxdw r0, [r8+0x190]                    
    ldxdw r5, [r9+0xf0]                     
    mov64 r4, r0                                    r4 = r0
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r4, r0, lbb_11007                           if r4 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_11007:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_11038                            if r5 != (0 as i32 as i64 as u64) { pc += 29 }
    stxdw [r8+0x190], r4                    
    ldxdw r0, [r8+0x1d0]                    
    ldxdw r5, [r9+0x118]                    
    mov64 r4, r0                                    r4 = r0
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r4, r0, lbb_11017                           if r4 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_11017:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_11038                            if r5 != (0 as i32 as i64 as u64) { pc += 19 }
    stxdw [r8+0x1d0], r4                    
    ldxdw r0, [r8+0x210]                    
    ldxdw r5, [r9+0x140]                    
    mov64 r4, r0                                    r4 = r0
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r4, r0, lbb_11027                           if r4 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_11027:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_11038                            if r5 != (0 as i32 as i64 as u64) { pc += 9 }
    stxdw [r8+0x210], r4                    
    ldxdw r0, [r8+0x250]                    
    ldxdw r5, [r9+0x168]                    
    mov64 r4, r0                                    r4 = r0
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r4, r0, lbb_11037                           if r4 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_11037:
    jne r5, 1, lbb_11041                            if r5 != (1 as i32 as i64 as u64) { pc += 3 }
lbb_11038:
    lddw r1, 0x10002f360 --> b"\x00\x00\x00\x00\x8f\xd8\x02\x00\x1f\x00\x00\x00\x00\x00\x00\x00R\x00\x00…        r1 load str located at 4295160672
    call function_20946                     
lbb_11041:
    stxdw [r8+0x250], r4                    
    ldxdw r4, [r10-0x80]                    
    stxw [r4+0x10], r3                      
    stxdw [r4+0x8], r2                      
    stxdw [r4+0x0], r1                      
lbb_11046:
    exit                                    

function_11047:
    stxdw [r10-0x8], r3                     
    stxdw [r10-0x10], r1                    
    jlt r2, r4, lbb_11051                           if r2 < r4 { pc += 1 }
    mov64 r2, r4                                    r2 = r4
lbb_11051:
    jeq r2, 0, lbb_11155                            if r2 == (0 as i32 as i64 as u64) { pc += 103 }
    ldxdw r1, [r5-0xfe8]                    
    stxdw [r10-0x18], r1                    
    ldxdw r4, [r5-0xff0]                    
    ldxdw r1, [r5-0xff8]                    
    ldxdw r7, [r5-0x1000]                   
    mov64 r3, r7                                    r3 = r7
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    jsgt r3, 0, lbb_11104                           if (r3 as i64) > (0 as i32 as i64) { pc += 43 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r3, [r10-0x10]                    
    add64 r3, 16                                    r3 += 16   ///  r3 = r3.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x10], r3                    
    ldxdw r3, [r10-0x8]                     
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x8], r3                     
lbb_11068:
    mov64 r3, r7                                    r3 = r7
    lsh64 r3, 6                                     r3 <<= 6   ///  r3 = r3.wrapping_shl(6)
    ldxdw r5, [r10-0x10]                    
    add64 r5, r3                                    r5 += r3   ///  r5 = r5.wrapping_add(r3)
    mov64 r3, r7                                    r3 = r7
    mul64 r3, 56                                    r3 *= 56   ///  r3 = r3.wrapping_mul(56 as u64)
    ldxdw r8, [r10-0x8]                     
    add64 r8, r3                                    r8 += r3   ///  r8 = r8.wrapping_add(r3)
    ja lbb_11092                                    if true { pc += 15 }
lbb_11077:
    jle r3, r1, lbb_11087                           if r3 <= r1 { pc += 9 }
    ldxdw r9, [r5+0x0]                      
    ldxdw r3, [r10-0x18]                    
    jle r3, r9, lbb_11153                           if r3 <= r9 { pc += 72 }
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    stdw [r5+0x0], 0                        
    sub64 r3, r9                                    r3 -= r9   ///  r3 = r3.wrapping_sub(r9)
    stxdw [r10-0x18], r3                    
    jlt r7, r2, lbb_11068                           if r7 < r2 { pc += -18 }
    ja lbb_11155                                    if true { pc += 68 }
lbb_11087:
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    add64 r5, 64                                    r5 += 64   ///  r5 = r5.wrapping_add(64 as i32 as i64 as u64)
    add64 r8, 56                                    r8 += 56   ///  r8 = r8.wrapping_add(56 as i32 as i64 as u64)
    jlt r7, r2, lbb_11092                           if r7 < r2 { pc += 1 }
    ja lbb_11155                                    if true { pc += 63 }
lbb_11092:
    ldxdw r3, [r5-0x8]                      
    jeq r3, 0, lbb_11155                            if r3 == (0 as i32 as i64 as u64) { pc += 61 }
    ldxw r3, [r8+0x0]                       
    mov64 r9, r4                                    r9 = r4
    add64 r9, r3                                    r9 += r3   ///  r9 = r9.wrapping_add(r3)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r9, r4, lbb_11100                           if r9 < r4 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_11100:
    mov64 r3, -1                                    r3 = -1 as i32 as i64 as u64
    jne r0, 0, lbb_11077                            if r0 != (0 as i32 as i64 as u64) { pc += -25 }
    mov64 r3, r9                                    r3 = r9
    ja lbb_11077                                    if true { pc += -27 }
lbb_11104:
    lsh64 r7, 1                                     r7 <<= 1   ///  r7 = r7.wrapping_shl(1)
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r3, [r10-0x10]                    
    add64 r3, 16                                    r3 += 16   ///  r3 = r3.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x10], r3                    
    ldxdw r3, [r10-0x8]                     
    add64 r3, 16                                    r3 += 16   ///  r3 = r3.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x8], r3                     
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
lbb_11114:
    mov64 r3, r8                                    r3 = r8
    lsh64 r3, 6                                     r3 <<= 6   ///  r3 = r3.wrapping_shl(6)
    ldxdw r5, [r10-0x10]                    
    add64 r5, r3                                    r5 += r3   ///  r5 = r5.wrapping_add(r3)
    mov64 r3, r8                                    r3 = r8
    mul64 r3, 56                                    r3 *= 56   ///  r3 = r3.wrapping_mul(56 as u64)
    ldxdw r9, [r10-0x8]                     
    add64 r9, r3                                    r9 += r3   ///  r9 = r9.wrapping_add(r3)
    ja lbb_11129                                    if true { pc += 6 }
lbb_11123:
    jgt r3, r1, lbb_11143                           if r3 > r1 { pc += 19 }
lbb_11124:
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
    add64 r5, 64                                    r5 += 64   ///  r5 = r5.wrapping_add(64 as i32 as i64 as u64)
    add64 r9, 56                                    r9 += 56   ///  r9 = r9.wrapping_add(56 as i32 as i64 as u64)
    jlt r8, r2, lbb_11129                           if r8 < r2 { pc += 1 }
    ja lbb_11155                                    if true { pc += 26 }
lbb_11129:
    ldxdw r3, [r5-0x8]                      
    jeq r3, 0, lbb_11155                            if r3 == (0 as i32 as i64 as u64) { pc += 24 }
    ldxw r3, [r9+0x0]                       
    jgt r7, r3, lbb_11124                           if r7 > r3 { pc += -9 }
    ldxw r3, [r9-0x8]                       
    mov64 r0, r4                                    r0 = r4
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jlt r0, r4, lbb_11139                           if r0 < r4 { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_11139:
    mov64 r3, -1                                    r3 = -1 as i32 as i64 as u64
    jne r6, 0, lbb_11123                            if r6 != (0 as i32 as i64 as u64) { pc += -18 }
    mov64 r3, r0                                    r3 = r0
    ja lbb_11123                                    if true { pc += -20 }
lbb_11143:
    ldxdw r9, [r5+0x0]                      
    ldxdw r3, [r10-0x18]                    
    jgt r3, r9, lbb_11147                           if r3 > r9 { pc += 1 }
    ja lbb_11153                                    if true { pc += 6 }
lbb_11147:
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
    stdw [r5+0x0], 0                        
    sub64 r3, r9                                    r3 -= r9   ///  r3 = r3.wrapping_sub(r9)
    stxdw [r10-0x18], r3                    
    jlt r8, r2, lbb_11114                           if r8 < r2 { pc += -38 }
    ja lbb_11155                                    if true { pc += 2 }
lbb_11153:
    sub64 r9, r3                                    r9 -= r3   ///  r9 = r9.wrapping_sub(r3)
    stxdw [r5+0x0], r9                      
lbb_11155:
    exit                                    

function_11156:
    mov64 r8, r4                                    r8 = r4
    mov64 r9, r3                                    r9 = r3
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r7+0x358]                    
    stxdw [r10-0x8], r1                     
    ldxdw r1, [r7+0x360]                    
    stxdw [r10-0x18], r1                    
    ldxdw r1, [r7+0x368]                    
    stxdw [r10-0x20], r1                    
    mov64 r1, r6                                    r1 = r6
    add64 r1, 72                                    r1 += 72   ///  r1 = r1.wrapping_add(72 as i32 as i64 as u64)
    add64 r2, 880                                   r2 += 880   ///  r2 = r2.wrapping_add(880 as i32 as i64 as u64)
    mov64 r3, 160                                   r3 = 160 as i32 as i64 as u64
    call function_21513                     
    ldxb r1, [r7+0x744]                     
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_11176                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11176:
    stxdw [r10-0x10], r2                    
    ldxb r1, [r7+0x745]                     
    jne r1, 0, lbb_11180                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_11180:
    stxdw [r10-0x68], r3                    
    ldxdw r1, [r9+0x1d8]                    
    stxdw [r10-0x28], r1                    
    ldxdw r1, [r9+0x1e0]                    
    stxdw [r10-0x30], r1                    
    ldxw r1, [r9+0x330]                     
    stxdw [r10-0x38], r1                    
    ldxw r1, [r9+0x358]                     
    stxdw [r10-0x40], r1                    
    ldxdw r1, [r8+0x1d8]                    
    stxdw [r10-0x48], r1                    
    ldxdw r1, [r8+0x1e0]                    
    stxdw [r10-0x50], r1                    
    ldxw r1, [r8+0x330]                     
    stxdw [r10-0x58], r1                    
    ldxw r1, [r8+0x358]                     
    stxdw [r10-0x60], r1                    
    ldxdw r1, [r7+0xb60]                    
    stxdw [r10-0x70], r1                    
    ldxdw r1, [r7+0xb68]                    
    stxdw [r10-0x78], r1                    
    ldxdw r1, [r7+0xb58]                    
    stxdw [r10-0x80], r1                    
    ldxdw r1, [r7+0xbc0]                    
    stxdw [r10-0x88], r1                    
    ldxdw r8, [r7+0xbc8]                    
    ldxdw r0, [r7+0xb90]                    
    ldxdw r5, [r7+0xba8]                    
    ldxw r4, [r7+0x344]                     
    ldxw r3, [r7+0xb9c]                     
    ldxdw r2, [r7+0x14b8]                   
    ldxdw r1, [r7+0x14c8]                   
    ldxb r9, [r7+0x747]                     
    stxb [r6+0x150], r9                     
    stxdw [r6+0x148], r1                    
    stxdw [r6+0x140], r2                    
    stxw [r6+0x13c], r3                     
    stxw [r6+0x138], r4                     
    ldxdw r1, [r10-0x20]                    
    stxdw [r6+0x130], r1                    
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x128], r1                    
    ldxdw r1, [r10-0x8]                     
    stxdw [r6+0x120], r1                    
    stxdw [r6+0x118], r5                    
    stxdw [r6+0x110], r0                    
    stxdw [r6+0x108], r8                    
    ldxdw r1, [r10-0x88]                    
    stxdw [r6+0x100], r1                    
    ldxdw r1, [r10-0x80]                    
    stxdw [r6+0xf8], r1                     
    ldxdw r1, [r10-0x78]                    
    stxdw [r6+0xf0], r1                     
    ldxdw r1, [r10-0x70]                    
    stxdw [r6+0xe8], r1                     
    mov64 r1, r7                                    r1 = r7
    add64 r1, 4640                                  r1 += 4640   ///  r1 = r1.wrapping_add(4640 as i32 as i64 as u64)
    stxdw [r6+0x40], r1                     
    mov64 r1, r7                                    r1 = r7
    add64 r1, 4000                                  r1 += 4000   ///  r1 = r1.wrapping_add(4000 as i32 as i64 as u64)
    stxdw [r6+0x38], r1                     
    add64 r7, 272                                   r7 += 272   ///  r7 = r7.wrapping_add(272 as i32 as i64 as u64)
    stxdw [r6+0x30], r7                     
    ldxdw r1, [r10-0x68]                    
    stxb [r6+0x152], r1                     
    ldxdw r1, [r10-0x10]                    
    stxb [r6+0x151], r1                     
    ldxdw r1, [r10-0x60]                    
    stxw [r6+0x2c], r1                      
    ldxdw r1, [r10-0x58]                    
    stxw [r6+0x28], r1                      
    ldxdw r1, [r10-0x50]                    
    stxdw [r6+0x20], r1                     
    ldxdw r1, [r10-0x48]                    
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x40]                    
    stxw [r6+0x14], r1                      
    ldxdw r1, [r10-0x38]                    
    stxw [r6+0x10], r1                      
    ldxdw r1, [r10-0x30]                    
    stxdw [r6+0x8], r1                      
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x0], r1                      
    exit                                    

function_11264:
    ldxdw r3, [r2+0x18]                     
    stxdw [r10-0x8], r3                     
    ldxdw r3, [r2+0x10]                     
    stxdw [r10-0x10], r3                    
    ldxdw r3, [r2+0x8]                      
    stxdw [r10-0x18], r3                    
    ldxdw r2, [r2+0x0]                      
    stxdw [r10-0x20], r2                    
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jne r2, 0, lbb_11315                            if r2 != (0 as i32 as i64 as u64) { pc += 41 }
    lddw r2, 0xabcdef01                             r2 load str located at 2882400001
    ldxw r3, [r10-0x14]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    lddw r2, 0xa35dcf11                             r2 load str located at 2740834065
    ldxw r4, [r10-0x1c]                     
    xor64 r4, r2                                    r4 ^= r2   ///  r4 = r4.xor(r2)
    ldxw r2, [r10-0x18]                     
    xor64 r2, 1436099174                            r2 ^= 1436099174   ///  r2 = r2.xor(1436099174)
    stxw [r10-0x18], r2                     
    stxw [r10-0x14], r3                     
    lddw r2, 0xdcba9876                             r2 load str located at 3703216246
    ldxw r3, [r10-0x8]                      
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    lddw r2, 0xabcdeffe                             r2 load str located at 2882400254
    ldxw r5, [r10-0xc]                      
    xor64 r5, r2                                    r5 ^= r2   ///  r5 = r5.xor(r2)
    ldxw r2, [r10-0x20]                     
    xor64 r2, 1108838659                            r2 ^= 1108838659   ///  r2 = r2.xor(1108838659)
    stxw [r10-0x20], r2                     
    stxw [r10-0x1c], r4                     
    ldxw r2, [r10-0x10]                     
    xor64 r2, 591751049                             r2 ^= 591751049   ///  r2 = r2.xor(591751049)
    stxw [r10-0x10], r2                     
    stxw [r10-0xc], r5                      
    stxw [r10-0x8], r3                      
    ldxw r2, [r10-0x4]                      
    xor64 r2, 1412567295                            r2 ^= 1412567295   ///  r2 = r2.xor(1412567295)
    stxw [r10-0x4], r2                      
    ldxdw r2, [r10-0x18]                    
    stxdw [r1+0x8], r2                      
    ldxdw r2, [r10-0x8]                     
    stxdw [r1+0x18], r2                     
    ldxdw r2, [r10-0x10]                    
    stxdw [r1+0x10], r2                     
    ldxdw r2, [r10-0x20]                    
    stxdw [r1+0x0], r2                      
    exit                                    
lbb_11315:
    lddw r1, 0x10002d1f5 --> b"from_bytes_mut"        r1 load str located at 4295152117
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_2131                      

function_11320:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r8, 3                                     r8 = 3 as i32 as i64 as u64
    ldxdw r1, [r7+0x0]                      
    ldxdw r2, [r1+0x50]                     
    jne r2, 165, lbb_11341                          if r2 != (165 as i32 as i64 as u64) { pc += 15 }
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x10002cf08 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295151368
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_11341                            if r0 != (0 as i32 as i64 as u64) { pc += 7 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_18010                     
    ldxdw r3, [r10-0x20]                    
    jne r3, 0, lbb_11344                            if r3 != (0 as i32 as i64 as u64) { pc += 4 }
    ldxdw r8, [r10-0x18]                    
lbb_11341:
    stxdw [r6+0x4], r8                      
    stw [r6+0x0], 1                         
lbb_11343:
    exit                                    
lbb_11344:
    ldxb r2, [r10-0x8]                      
    ldxdw r1, [r10-0x10]                    
    ldxdw r3, [r3+0x40]                     
    stxdw [r6+0x8], r3                      
    stw [r6+0x0], 0                         
    jgt r2, 7, lbb_11361                            if r2 > (7 as i32 as i64 as u64) { pc += 11 }
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    lsh64 r3, r2                                    r3 <<= r2   ///  r3 = r3.wrapping_shl(r2 as u32)
    and64 r3, 255                                   r3 &= 255   ///  r3 = r3.and(255)
    ldxb r2, [r1+0x0]                       
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    mov64 r3, r2                                    r3 = r2
    and64 r3, 255                                   r3 &= 255   ///  r3 = r3.and(255)
    jne r3, r2, lbb_11358                           if r3 != r2 { pc += 0 }
lbb_11358:
    jne r3, r2, lbb_11364                           if r3 != r2 { pc += 5 }
    stxb [r1+0x0], r2                       
    ja lbb_11343                                    if true { pc += -18 }
lbb_11361:
    lddw r1, 0x10002eb30 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158576
    call function_20990                     
lbb_11364:
    lddw r1, 0x10002eb48 --> b"\x00\x00\x00\x00\xd8\xd1\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xdd\x02\…        r1 load str located at 4295158600
    call function_20946                     

function_11367:
    jeq r4, 1, lbb_11430                            if r4 == (1 as i32 as i64 as u64) { pc += 62 }
    jne r4, 0, lbb_11374                            if r4 != (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    lddw r3, 0x10002f398 --> b"\x00\x00\x00\x00\xd6\xd8\x02\x00\x19\x00\x00\x00\x00\x00\x00\x00]\x00\x00…        r3 load str located at 4295160728
    call function_18717                     
lbb_11374:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxh r0, [r3+0x0]                       
    jgt r0, r2, lbb_11378                           if r0 > r2 { pc += 1 }
    ja lbb_11428                                    if true { pc += 50 }
lbb_11378:
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    mov64 r5, r2                                    r5 = r2
    add64 r5, 2                                     r5 += 2   ///  r5 = r5.wrapping_add(2 as i32 as i64 as u64)
    jge r5, r4, lbb_11435                           if r5 >= r4 { pc += 53 }
    add64 r2, 3                                     r2 += 3   ///  r2 = r2.wrapping_add(3 as i32 as i64 as u64)
    jlt r2, r4, lbb_11385                           if r2 < r4 { pc += 1 }
    ja lbb_11440                                    if true { pc += 55 }
lbb_11385:
    mov64 r0, r3                                    r0 = r3
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    mov64 r2, r3                                    r2 = r3
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    ldxb r5, [r2+0x0]                       
    ldxb r2, [r0+0x0]                       
    lsh64 r2, 8                                     r2 <<= 8   ///  r2 = r2.wrapping_shl(8)
    or64 r2, r5                                     r2 |= r5   ///  r2 = r2.or(r5)
    jlt r2, r4, lbb_11395                           if r2 < r4 { pc += 1 }
    ja lbb_11445                                    if true { pc += 50 }
lbb_11395:
    mov64 r5, r2                                    r5 = r2
    add64 r5, 1                                     r5 += 1   ///  r5 = r5.wrapping_add(1 as i32 as i64 as u64)
    jlt r5, r4, lbb_11399                           if r5 < r4 { pc += 1 }
    ja lbb_11450                                    if true { pc += 51 }
lbb_11399:
    mov64 r0, r3                                    r0 = r3
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    mov64 r5, r3                                    r5 = r3
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    ldxb r5, [r5+0x0]                       
    ldxb r0, [r0+0x0]                       
    lsh64 r0, 8                                     r0 <<= 8   ///  r0 = r0.wrapping_shl(8)
    or64 r0, r5                                     r0 |= r5   ///  r0 = r0.or(r5)
    mul64 r0, 33                                    r0 *= 33   ///  r0 = r0.wrapping_mul(33 as u64)
    add64 r2, r0                                    r2 += r0   ///  r2 = r2.wrapping_add(r0)
    mov64 r5, r2                                    r5 = r2
    add64 r5, 34                                    r5 += 34   ///  r5 = r5.wrapping_add(34 as i32 as i64 as u64)
    jle r5, r4, lbb_11417                           if r5 <= r4 { pc += 5 }
    mov64 r1, r5                                    r1 = r5
    mov64 r2, r4                                    r2 = r4
    lddw r3, 0x10002f428 --> b"\x00\x00\x00\x00\xd6\xd8\x02\x00\x19\x00\x00\x00\x00\x00\x00\x00q\x00\x00…        r3 load str located at 4295160872
    call function_20247                     
lbb_11417:
    add64 r2, 2                                     r2 += 2   ///  r2 = r2.wrapping_add(2 as i32 as i64 as u64)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    ldxdw r2, [r3+0x18]                     
    stxdw [r1+0x19], r2                     
    ldxdw r2, [r3+0x10]                     
    stxdw [r1+0x11], r2                     
    ldxdw r2, [r3+0x8]                      
    stxdw [r1+0x9], r2                      
    ldxdw r2, [r3+0x0]                      
    stxdw [r1+0x1], r2                      
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_11428:
    stxb [r1+0x0], r5                       
    exit                                    
lbb_11430:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x10002f3b0 --> b"\x00\x00\x00\x00\xd6\xd8\x02\x00\x19\x00\x00\x00\x00\x00\x00\x00]\x00\x00…        r3 load str located at 4295160752
    call function_18717                     
lbb_11435:
    mov64 r1, r5                                    r1 = r5
    mov64 r2, r4                                    r2 = r4
    lddw r3, 0x10002f3c8 --> b"\x00\x00\x00\x00\xd6\xd8\x02\x00\x19\x00\x00\x00\x00\x00\x00\x00e\x00\x00…        r3 load str located at 4295160776
    call function_18717                     
lbb_11440:
    mov64 r1, r2                                    r1 = r2
    mov64 r2, r4                                    r2 = r4
    lddw r3, 0x10002f3e0 --> b"\x00\x00\x00\x00\xd6\xd8\x02\x00\x19\x00\x00\x00\x00\x00\x00\x00f\x00\x00…        r3 load str located at 4295160800
    call function_18717                     
lbb_11445:
    mov64 r1, r2                                    r1 = r2
    mov64 r2, r4                                    r2 = r4
    lddw r3, 0x10002f3f8 --> b"\x00\x00\x00\x00\xd6\xd8\x02\x00\x19\x00\x00\x00\x00\x00\x00\x00k\x00\x00…        r3 load str located at 4295160824
    call function_18717                     
lbb_11450:
    mov64 r1, r5                                    r1 = r5
    mov64 r2, r4                                    r2 = r4
    lddw r3, 0x10002f410 --> b"\x00\x00\x00\x00\xd6\xd8\x02\x00\x19\x00\x00\x00\x00\x00\x00\x00k\x00\x00…        r3 load str located at 4295160848
    call function_18717                     

entrypoint:
    mov64 r2, r1                                    r2 = r1
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r4, [r1+0x0]                      
    jeq r4, 0, lbb_11755                            if r4 == (0 as i32 as i64 as u64) { pc += 296 }
    stxdw [r10-0x850], r2                   
    ldxdw r2, [r1+0x58]                     
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r3, r1                                    r3 = r1
    add64 r3, 10344                                 r3 += 10344   ///  r3 = r3.wrapping_add(10344 as i32 as i64 as u64)
    add64 r1, 10351                                 r1 += 10351   ///  r1 = r1.wrapping_add(10351 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r3, lbb_11468                           if r1 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11468:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11985                            if r2 != (0 as i32 as i64 as u64) { pc += 515 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    mov64 r2, r1                                    r2 = r1
    jeq r4, 1, lbb_11755                            if r4 == (1 as i32 as i64 as u64) { pc += 282 }
    mov64 r3, r10                                   r3 = r10
    add64 r3, -2128                                 r3 += -2128   ///  r3 = r3.wrapping_add(-2128 as i32 as i64 as u64)
    mov64 r5, r4                                    r5 = r4
    jlt r4, 6, lbb_11596                            if r4 < (6 as i32 as i64 as u64) { pc += 119 }
    mov64 r3, r10                                   r3 = r10
    add64 r3, -2128                                 r3 += -2128   ///  r3 = r3.wrapping_add(-2128 as i32 as i64 as u64)
    mov64 r5, r4                                    r5 = r4
lbb_11480:
    ldxb r2, [r1+0x0]                       
    jeq r2, 255, lbb_11483                          if r2 == (255 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11555                                    if true { pc += 72 }
lbb_11483:
    stxdw [r3+0x8], r1                      
    ldxdw r2, [r1+0x50]                     
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r0, r1                                    r0 = r1
    add64 r0, 10336                                 r0 += 10336   ///  r0 = r0.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r0, lbb_11492                           if r1 < r0 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11492:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11982                            if r2 != (0 as i32 as i64 as u64) { pc += 488 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_11495:
    ldxb r2, [r1+0x0]                       
    jne r2, 255, lbb_11563                          if r2 != (255 as i32 as i64 as u64) { pc += 66 }
    stxdw [r3+0x10], r1                     
    ldxdw r2, [r1+0x50]                     
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r0, r1                                    r0 = r1
    add64 r0, 10336                                 r0 += 10336   ///  r0 = r0.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r0, lbb_11506                           if r1 < r0 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11506:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11982                            if r2 != (0 as i32 as i64 as u64) { pc += 474 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_11509:
    ldxb r2, [r1+0x0]                       
    jne r2, 255, lbb_11571                          if r2 != (255 as i32 as i64 as u64) { pc += 60 }
    stxdw [r3+0x18], r1                     
    ldxdw r2, [r1+0x50]                     
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r0, r1                                    r0 = r1
    add64 r0, 10336                                 r0 += 10336   ///  r0 = r0.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r0, lbb_11520                           if r1 < r0 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11520:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11982                            if r2 != (0 as i32 as i64 as u64) { pc += 460 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_11523:
    ldxb r2, [r1+0x0]                       
    jne r2, 255, lbb_11579                          if r2 != (255 as i32 as i64 as u64) { pc += 54 }
    stxdw [r3+0x20], r1                     
    ldxdw r2, [r1+0x50]                     
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r0, r1                                    r0 = r1
    add64 r0, 10336                                 r0 += 10336   ///  r0 = r0.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r0, lbb_11534                           if r1 < r0 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11534:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11982                            if r2 != (0 as i32 as i64 as u64) { pc += 446 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_11537:
    add64 r3, 40                                    r3 += 40   ///  r3 = r3.wrapping_add(40 as i32 as i64 as u64)
    ldxb r2, [r1+0x0]                       
    jne r2, 255, lbb_11587                          if r2 != (255 as i32 as i64 as u64) { pc += 47 }
    stxdw [r3+0x0], r1                      
    ldxdw r2, [r1+0x50]                     
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r0, r1                                    r0 = r1
    add64 r0, 10336                                 r0 += 10336   ///  r0 = r0.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r0, lbb_11549                           if r1 < r0 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11549:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11982                            if r2 != (0 as i32 as i64 as u64) { pc += 431 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    add64 r5, -5                                    r5 += -5   ///  r5 = r5.wrapping_add(-5 as i32 as i64 as u64)
    jgt r5, 5, lbb_11480                            if r5 > (5 as i32 as i64 as u64) { pc += -74 }
    ja lbb_11596                                    if true { pc += 41 }
lbb_11555:
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r0, r10                                   r0 = r10
    add64 r0, -2128                                 r0 += -2128   ///  r0 = r0.wrapping_add(-2128 as i32 as i64 as u64)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    ldxdw r2, [r0+0x0]                      
    stxdw [r3+0x8], r2                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_11495                                    if true { pc += -68 }
lbb_11563:
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r0, r10                                   r0 = r10
    add64 r0, -2128                                 r0 += -2128   ///  r0 = r0.wrapping_add(-2128 as i32 as i64 as u64)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    ldxdw r2, [r0+0x0]                      
    stxdw [r3+0x10], r2                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_11509                                    if true { pc += -62 }
lbb_11571:
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r0, r10                                   r0 = r10
    add64 r0, -2128                                 r0 += -2128   ///  r0 = r0.wrapping_add(-2128 as i32 as i64 as u64)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    ldxdw r2, [r0+0x0]                      
    stxdw [r3+0x18], r2                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_11523                                    if true { pc += -56 }
lbb_11579:
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r0, r10                                   r0 = r10
    add64 r0, -2128                                 r0 += -2128   ///  r0 = r0.wrapping_add(-2128 as i32 as i64 as u64)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    ldxdw r2, [r0+0x0]                      
    stxdw [r3+0x20], r2                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_11537                                    if true { pc += -50 }
lbb_11587:
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r0, r10                                   r0 = r10
    add64 r0, -2128                                 r0 += -2128   ///  r0 = r0.wrapping_add(-2128 as i32 as i64 as u64)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    ldxdw r2, [r0+0x0]                      
    stxdw [r3+0x0], r2                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    add64 r5, -5                                    r5 += -5   ///  r5 = r5.wrapping_add(-5 as i32 as i64 as u64)
    jgt r5, 5, lbb_11480                            if r5 > (5 as i32 as i64 as u64) { pc += -116 }
lbb_11596:
    jsgt r5, 2, lbb_11617                           if (r5 as i64) > (2 as i32 as i64) { pc += 20 }
    mov64 r2, r1                                    r2 = r1
    jeq r5, 1, lbb_11755                            if r5 == (1 as i32 as i64 as u64) { pc += 156 }
    ldxb r2, [r1+0x0]                       
    jeq r2, 255, lbb_11602                          if r2 == (255 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11926                                    if true { pc += 324 }
lbb_11602:
    stxdw [r3+0x8], r1                      
    ldxdw r2, [r1+0x50]                     
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r3, r1                                    r3 = r1
    add64 r3, 10336                                 r3 += 10336   ///  r3 = r3.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r3, lbb_11611                           if r1 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11611:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11614                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11753                                    if true { pc += 139 }
lbb_11614:
    lddw r1, 0x10002ebf8 --> b"\x00\x00\x00\x00\x13\xd2\x02\x00\x15\x00\x00\x00\x00\x00\x00\x00v\x01\x00…        r1 load str located at 4295158776
    call function_20946                     
lbb_11617:
    jeq r5, 3, lbb_11679                            if r5 == (3 as i32 as i64 as u64) { pc += 61 }
    jeq r5, 4, lbb_11711                            if r5 == (4 as i32 as i64 as u64) { pc += 92 }
    ldxb r2, [r1+0x0]                       
    jeq r2, 255, lbb_11622                          if r2 == (255 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11910                                    if true { pc += 288 }
lbb_11622:
    stxdw [r3+0x8], r1                      
    ldxdw r2, [r1+0x50]                     
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r5, r1                                    r5 = r1
    add64 r5, 10336                                 r5 += 10336   ///  r5 = r5.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r5, lbb_11631                           if r1 < r5 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11631:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11676                            if r2 != (0 as i32 as i64 as u64) { pc += 43 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_11634:
    ldxb r2, [r1+0x0]                       
    jne r2, 255, lbb_11942                          if r2 != (255 as i32 as i64 as u64) { pc += 306 }
    stxdw [r3+0x10], r1                     
    ldxdw r2, [r1+0x50]                     
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r5, r1                                    r5 = r1
    add64 r5, 10336                                 r5 += 10336   ///  r5 = r5.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r5, lbb_11645                           if r1 < r5 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11645:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11676                            if r2 != (0 as i32 as i64 as u64) { pc += 29 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_11648:
    ldxb r2, [r1+0x0]                       
    jne r2, 255, lbb_11958                          if r2 != (255 as i32 as i64 as u64) { pc += 308 }
    stxdw [r3+0x18], r1                     
    ldxdw r2, [r1+0x50]                     
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r5, r1                                    r5 = r1
    add64 r5, 10336                                 r5 += 10336   ///  r5 = r5.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r5, lbb_11659                           if r1 < r5 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11659:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11676                            if r2 != (0 as i32 as i64 as u64) { pc += 15 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_11662:
    ldxb r2, [r1+0x0]                       
    jne r2, 255, lbb_11974                          if r2 != (255 as i32 as i64 as u64) { pc += 310 }
    stxdw [r3+0x20], r1                     
    ldxdw r2, [r1+0x50]                     
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r3, r1                                    r3 = r1
    add64 r3, 10336                                 r3 += 10336   ///  r3 = r3.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r3, lbb_11673                           if r1 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11673:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11676                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11753                                    if true { pc += 77 }
lbb_11676:
    lddw r1, 0x10002ebb0 --> b"\x00\x00\x00\x00\x13\xd2\x02\x00\x15\x00\x00\x00\x00\x00\x00\x00m\x01\x00…        r1 load str located at 4295158704
    call function_20946                     
lbb_11679:
    ldxb r2, [r1+0x0]                       
    jeq r2, 255, lbb_11682                          if r2 == (255 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11902                                    if true { pc += 220 }
lbb_11682:
    stxdw [r3+0x8], r1                      
    ldxdw r2, [r1+0x50]                     
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r5, r1                                    r5 = r1
    add64 r5, 10336                                 r5 += 10336   ///  r5 = r5.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r5, lbb_11691                           if r1 < r5 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11691:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11708                            if r2 != (0 as i32 as i64 as u64) { pc += 15 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_11694:
    ldxb r2, [r1+0x0]                       
    jne r2, 255, lbb_11934                          if r2 != (255 as i32 as i64 as u64) { pc += 238 }
    stxdw [r3+0x10], r1                     
    ldxdw r2, [r1+0x50]                     
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r3, r1                                    r3 = r1
    add64 r3, 10336                                 r3 += 10336   ///  r3 = r3.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r3, lbb_11705                           if r1 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11705:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11708                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11753                                    if true { pc += 45 }
lbb_11708:
    lddw r1, 0x10002ebe0 --> b"\x00\x00\x00\x00\x13\xd2\x02\x00\x15\x00\x00\x00\x00\x00\x00\x00s\x01\x00…        r1 load str located at 4295158752
    call function_20946                     
lbb_11711:
    ldxb r2, [r1+0x0]                       
    jeq r2, 255, lbb_11714                          if r2 == (255 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11918                                    if true { pc += 204 }
lbb_11714:
    stxdw [r3+0x8], r1                      
    ldxdw r2, [r1+0x50]                     
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r5, r1                                    r5 = r1
    add64 r5, 10336                                 r5 += 10336   ///  r5 = r5.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r5, lbb_11723                           if r1 < r5 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11723:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11988                            if r2 != (0 as i32 as i64 as u64) { pc += 263 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_11726:
    ldxb r2, [r1+0x0]                       
    jne r2, 255, lbb_11950                          if r2 != (255 as i32 as i64 as u64) { pc += 222 }
    stxdw [r3+0x10], r1                     
    ldxdw r2, [r1+0x50]                     
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r5, r1                                    r5 = r1
    add64 r5, 10336                                 r5 += 10336   ///  r5 = r5.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r5, lbb_11737                           if r1 < r5 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11737:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11988                            if r2 != (0 as i32 as i64 as u64) { pc += 249 }
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_11740:
    ldxb r2, [r1+0x0]                       
    jne r2, 255, lbb_11966                          if r2 != (255 as i32 as i64 as u64) { pc += 224 }
    stxdw [r3+0x18], r1                     
    ldxdw r2, [r1+0x50]                     
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r3, r1                                    r3 = r1
    add64 r3, 10336                                 r3 += 10336   ///  r3 = r3.wrapping_add(10336 as i32 as i64 as u64)
    add64 r1, 10343                                 r1 += 10343   ///  r1 = r1.wrapping_add(10343 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r3, lbb_11751                           if r1 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11751:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11988                            if r2 != (0 as i32 as i64 as u64) { pc += 235 }
lbb_11753:
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
lbb_11754:
    mov64 r2, r1                                    r2 = r1
lbb_11755:
    ldxdw r1, [r2+0x0]                      
    stxdw [r10-0xff8], r1                   
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x1000], r2                  
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2136                                 r1 += -2136   ///  r1 = r1.wrapping_add(-2136 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -2128                                 r3 += -2128   ///  r3 = r3.wrapping_add(-2128 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    call function_2685                      
    ldxw r1, [r10-0x858]                    
    jne r1, 26, lbb_11770                           if r1 != (26 as i32 as i64 as u64) { pc += 2 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_11769:
    exit                                    
lbb_11770:
    ldxw r2, [r10-0x854]                    
    stxw [r10-0x60], r1                     
    stxw [r10-0x5c], r2                     
    lddw r1, 0x10002d2c8 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00012345678…        r1 load str located at 4295152328
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x1000234a0 --> b"{\x1a\xb8\xff\x00\x00\x00\x00\x18\x01\x00\x00\xc8\xd2\x02\x00\x00\x00\x00…        r1 load str located at 4295111840
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    call function_18371                     
    ldxdw r1, [r10-0x50]                    
    ldxdw r2, [r10-0x48]                    
    syscall [invalid]                       
    ldxw r1, [r10-0x60]                     
    jsgt r1, 12, lbb_11807                          if (r1 as i64) > (12 as i32 as i64) { pc += 9 }
    jsgt r1, 5, lbb_11814                           if (r1 as i64) > (5 as i32 as i64) { pc += 15 }
    jsgt r1, 2, lbb_11836                           if (r1 as i64) > (2 as i32 as i64) { pc += 36 }
    jeq r1, 0, lbb_11860                            if r1 == (0 as i32 as i64 as u64) { pc += 59 }
    lddw r0, 0x200000000                            r0 load str located at 8589934592
    jeq r1, 1, lbb_11769                            if r1 == (1 as i32 as i64 as u64) { pc += -35 }
    lddw r0, 0x300000000                            r0 load str located at 12884901888
    ja lbb_11769                                    if true { pc += -38 }
lbb_11807:
    jsgt r1, 18, lbb_11820                          if (r1 as i64) > (18 as i32 as i64) { pc += 12 }
    jsgt r1, 15, lbb_11841                          if (r1 as i64) > (15 as i32 as i64) { pc += 32 }
    jeq r1, 13, lbb_11866                           if r1 == (13 as i32 as i64 as u64) { pc += 56 }
    jeq r1, 14, lbb_11893                           if r1 == (14 as i32 as i64 as u64) { pc += 82 }
    lddw r0, 0x1000000000                           r0 load str located at 68719476736
    ja lbb_11769                                    if true { pc += -45 }
lbb_11814:
    jsgt r1, 8, lbb_11826                           if (r1 as i64) > (8 as i32 as i64) { pc += 11 }
    jeq r1, 6, lbb_11854                            if r1 == (6 as i32 as i64 as u64) { pc += 38 }
    jeq r1, 7, lbb_11875                            if r1 == (7 as i32 as i64 as u64) { pc += 58 }
    lddw r0, 0x900000000                            r0 load str located at 38654705664
    ja lbb_11769                                    if true { pc += -51 }
lbb_11820:
    jsgt r1, 21, lbb_11831                          if (r1 as i64) > (21 as i32 as i64) { pc += 10 }
    jeq r1, 19, lbb_11857                           if r1 == (19 as i32 as i64 as u64) { pc += 35 }
    jeq r1, 20, lbb_11878                           if r1 == (20 as i32 as i64 as u64) { pc += 55 }
    lddw r0, 0x1600000000                           r0 load str located at 94489280512
    ja lbb_11769                                    if true { pc += -57 }
lbb_11826:
    jsgt r1, 10, lbb_11846                          if (r1 as i64) > (10 as i32 as i64) { pc += 19 }
    jeq r1, 9, lbb_11881                            if r1 == (9 as i32 as i64 as u64) { pc += 53 }
    lddw r0, 0xb00000000                            r0 load str located at 47244640256
    ja lbb_11769                                    if true { pc += -62 }
lbb_11831:
    jsgt r1, 23, lbb_11850                          if (r1 as i64) > (23 as i32 as i64) { pc += 18 }
    jeq r1, 22, lbb_11884                           if r1 == (22 as i32 as i64 as u64) { pc += 51 }
    lddw r0, 0x1800000000                           r0 load str located at 103079215104
    ja lbb_11769                                    if true { pc += -67 }
lbb_11836:
    jeq r1, 3, lbb_11869                            if r1 == (3 as i32 as i64 as u64) { pc += 32 }
    jeq r1, 4, lbb_11896                            if r1 == (4 as i32 as i64 as u64) { pc += 58 }
    lddw r0, 0x600000000                            r0 load str located at 25769803776
    ja lbb_11769                                    if true { pc += -72 }
lbb_11841:
    jeq r1, 16, lbb_11872                           if r1 == (16 as i32 as i64 as u64) { pc += 30 }
    jeq r1, 17, lbb_11899                           if r1 == (17 as i32 as i64 as u64) { pc += 56 }
    lddw r0, 0x1300000000                           r0 load str located at 81604378624
    ja lbb_11769                                    if true { pc += -77 }
lbb_11846:
    jeq r1, 11, lbb_11887                           if r1 == (11 as i32 as i64 as u64) { pc += 40 }
    lddw r0, 0xd00000000                            r0 load str located at 55834574848
    ja lbb_11769                                    if true { pc += -81 }
lbb_11850:
    jeq r1, 24, lbb_11890                           if r1 == (24 as i32 as i64 as u64) { pc += 39 }
    lddw r0, 0x1a00000000                           r0 load str located at 111669149696
    ja lbb_11769                                    if true { pc += -85 }
lbb_11854:
    lddw r0, 0x700000000                            r0 load str located at 30064771072
    ja lbb_11769                                    if true { pc += -88 }
lbb_11857:
    lddw r0, 0x1400000000                           r0 load str located at 85899345920
    ja lbb_11769                                    if true { pc += -91 }
lbb_11860:
    ldxw r1, [r10-0x5c]                     
    lddw r0, 0x100000000                            r0 load str located at 4294967296
    jeq r1, 0, lbb_11769                            if r1 == (0 as i32 as i64 as u64) { pc += -95 }
    mov64 r0, r1                                    r0 = r1
    ja lbb_11769                                    if true { pc += -97 }
lbb_11866:
    lddw r0, 0xe00000000                            r0 load str located at 60129542144
    ja lbb_11769                                    if true { pc += -100 }
lbb_11869:
    lddw r0, 0x400000000                            r0 load str located at 17179869184
    ja lbb_11769                                    if true { pc += -103 }
lbb_11872:
    lddw r0, 0x1100000000                           r0 load str located at 73014444032
    ja lbb_11769                                    if true { pc += -106 }
lbb_11875:
    lddw r0, 0x800000000                            r0 load str located at 34359738368
    ja lbb_11769                                    if true { pc += -109 }
lbb_11878:
    lddw r0, 0x1500000000                           r0 load str located at 90194313216
    ja lbb_11769                                    if true { pc += -112 }
lbb_11881:
    lddw r0, 0xa00000000                            r0 load str located at 42949672960
    ja lbb_11769                                    if true { pc += -115 }
lbb_11884:
    lddw r0, 0x1700000000                           r0 load str located at 98784247808
    ja lbb_11769                                    if true { pc += -118 }
lbb_11887:
    lddw r0, 0xc00000000                            r0 load str located at 51539607552
    ja lbb_11769                                    if true { pc += -121 }
lbb_11890:
    lddw r0, 0x1900000000                           r0 load str located at 107374182400
    ja lbb_11769                                    if true { pc += -124 }
lbb_11893:
    lddw r0, 0xf00000000                            r0 load str located at 64424509440
    ja lbb_11769                                    if true { pc += -127 }
lbb_11896:
    lddw r0, 0x500000000                            r0 load str located at 21474836480
    ja lbb_11769                                    if true { pc += -130 }
lbb_11899:
    lddw r0, 0x1200000000                           r0 load str located at 77309411328
    ja lbb_11769                                    if true { pc += -133 }
lbb_11902:
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -2128                                 r5 += -2128   ///  r5 = r5.wrapping_add(-2128 as i32 as i64 as u64)
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    ldxdw r2, [r5+0x0]                      
    stxdw [r3+0x8], r2                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_11694                                    if true { pc += -216 }
lbb_11910:
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -2128                                 r5 += -2128   ///  r5 = r5.wrapping_add(-2128 as i32 as i64 as u64)
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    ldxdw r2, [r5+0x0]                      
    stxdw [r3+0x8], r2                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_11634                                    if true { pc += -284 }
lbb_11918:
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -2128                                 r5 += -2128   ///  r5 = r5.wrapping_add(-2128 as i32 as i64 as u64)
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    ldxdw r2, [r5+0x0]                      
    stxdw [r3+0x8], r2                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_11726                                    if true { pc += -200 }
lbb_11926:
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -2128                                 r5 += -2128   ///  r5 = r5.wrapping_add(-2128 as i32 as i64 as u64)
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    ldxdw r2, [r5+0x0]                      
    stxdw [r3+0x8], r2                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_11754                                    if true { pc += -180 }
lbb_11934:
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -2128                                 r5 += -2128   ///  r5 = r5.wrapping_add(-2128 as i32 as i64 as u64)
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    ldxdw r2, [r5+0x0]                      
    stxdw [r3+0x10], r2                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_11754                                    if true { pc += -188 }
lbb_11942:
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -2128                                 r5 += -2128   ///  r5 = r5.wrapping_add(-2128 as i32 as i64 as u64)
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    ldxdw r2, [r5+0x0]                      
    stxdw [r3+0x10], r2                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_11648                                    if true { pc += -302 }
lbb_11950:
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -2128                                 r5 += -2128   ///  r5 = r5.wrapping_add(-2128 as i32 as i64 as u64)
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    ldxdw r2, [r5+0x0]                      
    stxdw [r3+0x10], r2                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_11740                                    if true { pc += -218 }
lbb_11958:
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -2128                                 r5 += -2128   ///  r5 = r5.wrapping_add(-2128 as i32 as i64 as u64)
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    ldxdw r2, [r5+0x0]                      
    stxdw [r3+0x18], r2                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_11662                                    if true { pc += -304 }
lbb_11966:
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -2128                                 r5 += -2128   ///  r5 = r5.wrapping_add(-2128 as i32 as i64 as u64)
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    ldxdw r2, [r5+0x0]                      
    stxdw [r3+0x18], r2                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_11754                                    if true { pc += -220 }
lbb_11974:
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -2128                                 r5 += -2128   ///  r5 = r5.wrapping_add(-2128 as i32 as i64 as u64)
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    ldxdw r2, [r5+0x0]                      
    stxdw [r3+0x20], r2                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_11754                                    if true { pc += -228 }
lbb_11982:
    lddw r1, 0x10002ec10 --> b"\x00\x00\x00\x00\x13\xd2\x02\x00\x15\x00\x00\x00\x00\x00\x00\x00f\x01\x00…        r1 load str located at 4295158800
    call function_20946                     
lbb_11985:
    lddw r1, 0x10002eb98 --> b"\x00\x00\x00\x00\x13\xd2\x02\x00\x15\x00\x00\x00\x00\x00\x00\x00K\x01\x00…        r1 load str located at 4295158680
    call function_20946                     
lbb_11988:
    lddw r1, 0x10002ebc8 --> b"\x00\x00\x00\x00\x13\xd2\x02\x00\x15\x00\x00\x00\x00\x00\x00\x00p\x01\x00…        r1 load str located at 4295158728
    call function_20946                     

function_11991:
    lddw r3, 0x300000000                            r3 load str located at 12884901888
    ldxdw r3, [r3+0x0]                      
    lddw r4, 0x300008000                            r4 load str located at 12884934656
    jeq r3, 0, lbb_11998                            if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r3                                    r4 = r3
lbb_11998:
    mov64 r3, r4                                    r3 = r4
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jgt r3, r4, lbb_12004                           if r3 > r4 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_12004:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jne r5, 0, lbb_12007                            if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r3                                    r1 = r3
lbb_12007:
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    lddw r2, 0x300000008                            r2 load str located at 12884901896
    jlt r1, r2, lbb_12016                           if r1 < r2 { pc += 4 }
    lddw r2, 0x300000000                            r2 load str located at 12884901888
    stxdw [r2+0x0], r1                      
    mov64 r0, r1                                    r0 = r1
lbb_12016:
    exit                                    

function_12017:
    exit                                    

function_12018:
    mov64 r5, r2                                    r5 = r2
    mov64 r2, r1                                    r2 = r1
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r6, 0x300008000                            r6 load str located at 12884934656
    jeq r1, 0, lbb_12027                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r1                                    r6 = r1
lbb_12027:
    mov64 r1, r6                                    r1 = r6
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jgt r1, r6, lbb_12033                           if r1 > r6 { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_12033:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 0, lbb_12036                            if r7 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r1                                    r6 = r1
lbb_12036:
    neg64 r3                                        r3 = -r3   ///  r3 = (r3 as i64).wrapping_neg() as u64
    and64 r6, r3                                    r6 &= r3   ///  r6 = r6.and(r3)
    lddw r1, 0x300000008                            r1 load str located at 12884901896
    jlt r6, r1, lbb_12050                           if r6 < r1 { pc += 9 }
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    stxdw [r1+0x0], r6                      
    jlt r5, r4, lbb_12046                           if r5 < r4 { pc += 1 }
    mov64 r5, r4                                    r5 = r4
lbb_12046:
    mov64 r1, r6                                    r1 = r6
    mov64 r3, r5                                    r3 = r5
    call function_21513                     
    mov64 r0, r6                                    r0 = r6
lbb_12050:
    exit                                    

custom_panic:
    stxdw [r10-0x60], r1                    
    lddw r1, 0x10002d2c8 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00012345678…        r1 load str located at 4295152328
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x1000005b0 --> b"y\x11\x00\x00\x00\x00\x00\x00\x85\x10\x00\x008H\x00\x00\x95\x00\x00\x00\x…        r1 load str located at 4294968752
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    call function_18371                     
    ldxdw r1, [r10-0x50]                    
    ldxdw r2, [r10-0x48]                    
    syscall [invalid]                       
    exit                                    

function_12076:
    mov64 r7, r5                                    r7 = r5
    mov64 r8, r4                                    r8 = r4
    mov64 r9, r3                                    r9 = r3
    stxdw [r10-0xd0], r2                    
    stxdw [r10-0xc8], r1                    
    lddw r1, 0x10002d8ef --> b"Instruction: init_vault"        r1 load str located at 4295153903
    mov64 r2, 23                                    r2 = 23 as i32 as i64 as u64
    syscall [invalid]                       
    ldxdw r6, [r7-0xff8]                    
    jeq r6, 0, lbb_12149                            if r6 == (0 as i32 as i64 as u64) { pc += 62 }
    jlt r6, 9, lbb_12100                            if r6 < (9 as i32 as i64 as u64) { pc += 12 }
    ldxdw r1, [r7-0x1000]                   
    ldxdw r7, [r1+0x1]                      
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r7                                    r2 = r7
    call function_22497                     
    jne r0, 0, lbb_12154                            if r0 != (0 as i32 as i64 as u64) { pc += 60 }
    jeq r6, 9, lbb_12114                            if r6 == (9 as i32 as i64 as u64) { pc += 19 }
    lddw r1, 0x10002d179 --> b"Not all bytes read"        r1 load str located at 4295151993
    mov64 r2, 18                                    r2 = 18 as i32 as i64 as u64
    call function_71                        
    ja lbb_12103                                    if true { pc += 3 }
lbb_12100:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
lbb_12103:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, r0                                    r1 = r0
    and64 r1, 3                                     r1 &= 3   ///  r1 = r1.and(3)
    jne r1, 1, lbb_12145                            if r1 != (1 as i32 as i64 as u64) { pc += 37 }
    ldxdw r1, [r0+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_12145                            if r2 == (0 as i32 as i64 as u64) { pc += 34 }
    ldxdw r1, [r0-0x1]                      
    callx r2                                
    ja lbb_12145                                    if true { pc += 31 }
lbb_12114:
    stxdw [r10-0xb8], r7                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, r8                                    r3 = r8
    call function_14511                     
    ldxw r8, [r10-0x4c]                     
    ldxw r7, [r10-0x50]                     
    ldxdw r6, [r10-0x58]                    
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    jeq r6, r1, lbb_12145                           if r6 == r1 { pc += 19 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -160                                  r1 += -160   ///  r1 = r1.wrapping_add(-160 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -72                                   r2 += -72   ///  r2 = r2.wrapping_add(-72 as i32 as i64 as u64)
    mov64 r3, 72                                    r3 = 72 as i32 as i64 as u64
    call function_21513                     
    stxw [r10-0xa4], r8                     
    stxw [r10-0xa8], r7                     
    stxdw [r10-0xb0], r6                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -192                                  r1 += -192   ///  r1 = r1.wrapping_add(-192 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -176                                  r3 += -176   ///  r3 = r3.wrapping_add(-176 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -184                                  r4 += -184   ///  r4 = r4.wrapping_add(-184 as i32 as i64 as u64)
    ldxdw r2, [r10-0xd0]                    
    call function_4875                      
    ldxw r8, [r10-0xbc]                     
    ldxw r7, [r10-0xc0]                     
lbb_12145:
    ldxdw r1, [r10-0xc8]                    
    stxw [r1+0x4], r8                       
    stxw [r1+0x0], r7                       
    exit                                    
lbb_12149:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    lddw r3, 0x10002f440 --> b"\x00\x00\x00\x00\xf9\xd2\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00K\x00\x00…        r3 load str located at 4295160896
    call function_20246                     
lbb_12154:
    lddw r1, 0x10002d0cc --> b"For portability reasons we do not allow to deserialize NaNs."        r1 load str located at 4295151820
    mov64 r2, 60                                    r2 = 60 as i32 as i64 as u64
    call function_71                        
    ja lbb_12103                                    if true { pc += -56 }

function_12159:
    mov64 r7, r5                                    r7 = r5
    mov64 r9, r4                                    r9 = r4
    stxdw [r10-0x1e0], r3                   
    mov64 r8, r2                                    r8 = r2
    mov64 r6, r1                                    r6 = r1
    lddw r1, 0x10002cea8 --> b"Instruction: update_vault_config"        r1 load str located at 4295151272
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    syscall [invalid]                       
    ldxdw r2, [r7-0xff8]                    
    jeq r2, 0, lbb_12526                            if r2 == (0 as i32 as i64 as u64) { pc += 356 }
    jeq r2, 1, lbb_12474                            if r2 == (1 as i32 as i64 as u64) { pc += 303 }
    stxdw [r10-0x1e8], r8                   
    ldxdw r4, [r7-0x1000]                   
    mov64 r3, r2                                    r3 = r2
    add64 r3, -2                                    r3 += -2   ///  r3 = r3.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r1, r4                                    r1 = r4
    add64 r1, 2                                     r1 += 2   ///  r1 = r1.wrapping_add(2 as i32 as i64 as u64)
    ldxb r8, [r4+0x1]                       
    stxb [r10-0x59], r8                     
    jeq r8, 0, lbb_12207                            if r8 == (0 as i32 as i64 as u64) { pc += 27 }
    jeq r8, 1, lbb_12182                            if r8 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12478                                    if true { pc += 296 }
lbb_12182:
    jlt r2, 34, lbb_12474                           if r2 < (34 as i32 as i64 as u64) { pc += 291 }
    ldxdw r3, [r4+0x8]                      
    ldxb r5, [r4+0x10]                      
    stxb [r10-0x38], r5                     
    ldxw r5, [r1+0x0]                       
    stxw [r10-0x90], r5                     
    ldxh r1, [r1+0x4]                       
    stxh [r10-0x8c], r1                     
    stxdw [r10-0x40], r3                    
    ldxdw r7, [r10-0x3f]                    
    ldxb r1, [r4+0x21]                      
    ldxdw r5, [r4+0x19]                     
    ldxdw r0, [r4+0x11]                     
    stxb [r10-0x70], r3                     
    stxdw [r10-0xa8], r0                    
    stxdw [r10-0xa0], r5                    
    stxb [r10-0x98], r1                     
    add64 r2, -34                                   r2 += -34   ///  r2 = r2.wrapping_add(-34 as i32 as i64 as u64)
    add64 r4, 34                                    r4 += 34   ///  r4 = r4.wrapping_add(34 as i32 as i64 as u64)
    stxdw [r10-0x6f], r7                    
    rsh64 r7, 56                                    r7 >>= 56   ///  r7 = r7.wrapping_shr(56)
    ldxdw r1, [r10-0x70]                    
    stxdw [r10-0x1f0], r1                   
    mov64 r1, r4                                    r1 = r4
    mov64 r3, r2                                    r3 = r2
lbb_12207:
    jeq r3, 0, lbb_12474                            if r3 == (0 as i32 as i64 as u64) { pc += 266 }
    mov64 r2, r3                                    r2 = r3
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r4, r1                                    r4 = r1
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    ldxb r0, [r1+0x0]                       
    stxb [r10-0x70], r0                     
    jeq r0, 0, lbb_12226                            if r0 == (0 as i32 as i64 as u64) { pc += 11 }
    jne r0, 1, lbb_12490                            if r0 != (1 as i32 as i64 as u64) { pc += 274 }
    jlt r3, 9, lbb_12474                            if r3 < (9 as i32 as i64 as u64) { pc += 257 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ldxdw r2, [r1+0x1]                      
    stxdw [r10-0x200], r2                   
    add64 r3, -9                                    r3 += -9   ///  r3 = r3.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r10-0xd0], r3                    
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r10-0xd8], r1                    
    mov64 r4, r1                                    r4 = r1
    mov64 r2, r3                                    r2 = r3
lbb_12226:
    jeq r2, 0, lbb_12474                            if r2 == (0 as i32 as i64 as u64) { pc += 247 }
    stxdw [r10-0x1f8], r7                   
    mov64 r1, r2                                    r1 = r2
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r3, r4                                    r3 = r4
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    ldxb r7, [r4+0x0]                       
    stxb [r10-0x70], r7                     
    jeq r7, 0, lbb_12244                            if r7 == (0 as i32 as i64 as u64) { pc += 9 }
    jne r7, 1, lbb_12490                            if r7 != (1 as i32 as i64 as u64) { pc += 254 }
    jlt r2, 9, lbb_12474                            if r2 < (9 as i32 as i64 as u64) { pc += 237 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ldxdw r1, [r4+0x1]                      
    stxdw [r10-0x210], r1                   
    add64 r2, -9                                    r2 += -9   ///  r2 = r2.wrapping_add(-9 as i32 as i64 as u64)
    add64 r4, 9                                     r4 += 9   ///  r4 = r4.wrapping_add(9 as i32 as i64 as u64)
    mov64 r3, r4                                    r3 = r4
    mov64 r1, r2                                    r1 = r2
lbb_12244:
    jeq r1, 0, lbb_12474                            if r1 == (0 as i32 as i64 as u64) { pc += 229 }
    stxdw [r10-0x208], r7                   
    mov64 r2, r1                                    r2 = r1
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r4, r3                                    r4 = r3
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    ldxb r7, [r3+0x0]                       
    stxb [r10-0x70], r7                     
    jeq r7, 0, lbb_12264                            if r7 == (0 as i32 as i64 as u64) { pc += 11 }
    jne r7, 1, lbb_12490                            if r7 != (1 as i32 as i64 as u64) { pc += 236 }
    jlt r1, 9, lbb_12474                            if r1 < (9 as i32 as i64 as u64) { pc += 219 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ldxdw r2, [r3+0x1]                      
    stxdw [r10-0x220], r2                   
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r10-0xd0], r1                    
    add64 r3, 9                                     r3 += 9   ///  r3 = r3.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r10-0xd8], r3                    
    mov64 r4, r3                                    r4 = r3
    mov64 r2, r1                                    r2 = r1
lbb_12264:
    stxdw [r10-0x218], r7                   
    ldxdw r1, [r10-0x1f8]                   
    jeq r2, 0, lbb_12474                            if r2 == (0 as i32 as i64 as u64) { pc += 207 }
    mov64 r1, r2                                    r1 = r2
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r3, r4                                    r3 = r4
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    ldxb r7, [r4+0x0]                       
    stxb [r10-0x70], r7                     
    jeq r7, 0, lbb_12283                            if r7 == (0 as i32 as i64 as u64) { pc += 9 }
    jne r7, 1, lbb_12490                            if r7 != (1 as i32 as i64 as u64) { pc += 215 }
    jlt r2, 9, lbb_12474                            if r2 < (9 as i32 as i64 as u64) { pc += 198 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ldxdw r1, [r4+0x1]                      
    stxdw [r10-0x230], r1                   
    add64 r2, -9                                    r2 += -9   ///  r2 = r2.wrapping_add(-9 as i32 as i64 as u64)
    add64 r4, 9                                     r4 += 9   ///  r4 = r4.wrapping_add(9 as i32 as i64 as u64)
    mov64 r3, r4                                    r3 = r4
    mov64 r1, r2                                    r1 = r2
lbb_12283:
    jeq r1, 0, lbb_12474                            if r1 == (0 as i32 as i64 as u64) { pc += 190 }
    stxdw [r10-0x228], r7                   
    mov64 r4, r1                                    r4 = r1
    add64 r4, -1                                    r4 += -1   ///  r4 = r4.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    ldxb r7, [r3+0x0]                       
    stxb [r10-0x70], r7                     
    ldxdw r5, [r10-0x208]                   
    jeq r7, 0, lbb_12303                            if r7 == (0 as i32 as i64 as u64) { pc += 10 }
    jne r7, 1, lbb_12490                            if r7 != (1 as i32 as i64 as u64) { pc += 196 }
    jlt r1, 9, lbb_12474                            if r1 < (9 as i32 as i64 as u64) { pc += 179 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ldxdw r2, [r3+0x1]                      
    stxdw [r10-0x238], r2                   
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    add64 r3, 9                                     r3 += 9   ///  r3 = r3.wrapping_add(9 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    mov64 r4, r1                                    r4 = r1
    ldxdw r5, [r10-0x208]                   
lbb_12303:
    jeq r4, 0, lbb_12474                            if r4 == (0 as i32 as i64 as u64) { pc += 170 }
    mov64 r1, r4                                    r1 = r4
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r3, [r2+0x0]                       
    stxdw [r10-0xd0], r1                    
    mov64 r1, r2                                    r1 = r2
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0xd8], r1                    
    stxb [r10-0x70], r3                     
    jeq r3, 0, lbb_12322                            if r3 == (0 as i32 as i64 as u64) { pc += 9 }
    jne r3, 1, lbb_12490                            if r3 != (1 as i32 as i64 as u64) { pc += 176 }
    jlt r4, 9, lbb_12474                            if r4 < (9 as i32 as i64 as u64) { pc += 159 }
    add64 r4, -9                                    r4 += -9   ///  r4 = r4.wrapping_add(-9 as i32 as i64 as u64)
    ldxdw r1, [r2+0x1]                      
    stxdw [r10-0x278], r1                   
    stxdw [r10-0xd0], r4                    
    add64 r2, 9                                     r2 += 9   ///  r2 = r2.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r10-0xd8], r2                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_12322:
    stxdw [r10-0x250], r3                   
    stxdw [r10-0x248], r7                   
    ldxdw r1, [r10-0x228]                   
    ldxdw r1, [r10-0x218]                   
    stxdw [r10-0x240], r0                   
    ldxdw r1, [r10-0x1f0]                   
    stxdw [r10-0x1f0], r1                   
    ldxdw r1, [r10-0x1f8]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -216                                  r2 += -216   ///  r2 = r2.wrapping_add(-216 as i32 as i64 as u64)
    call function_2014                      
    ldxdw r7, [r10-0x38]                    
    ldxdw r1, [r10-0x40]                    
    stxdw [r10-0x260], r7                   
    stxdw [r10-0x258], r1                   
    jeq r1, 2, lbb_12514                            if r1 == (2 as i32 as i64 as u64) { pc += 174 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -216                                  r2 += -216   ///  r2 = r2.wrapping_add(-216 as i32 as i64 as u64)
    call function_2014                      
    ldxdw r7, [r10-0x38]                    
    ldxdw r1, [r10-0x40]                    
    stxdw [r10-0x270], r7                   
    stxdw [r10-0x268], r1                   
    jeq r1, 2, lbb_12514                            if r1 == (2 as i32 as i64 as u64) { pc += 164 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -216                                  r2 += -216   ///  r2 = r2.wrapping_add(-216 as i32 as i64 as u64)
    call function_2014                      
    ldxdw r7, [r10-0x38]                    
    ldxdw r1, [r10-0x40]                    
    stxdw [r10-0x288], r7                   
    stxdw [r10-0x280], r1                   
    jeq r1, 2, lbb_12514                            if r1 == (2 as i32 as i64 as u64) { pc += 154 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -216                                  r2 += -216   ///  r2 = r2.wrapping_add(-216 as i32 as i64 as u64)
    call function_2014                      
    ldxdw r7, [r10-0x38]                    
    ldxdw r1, [r10-0x40]                    
    stxdw [r10-0x290], r1                   
    jeq r1, 2, lbb_12514                            if r1 == (2 as i32 as i64 as u64) { pc += 145 }
    ldxdw r1, [r10-0xd0]                    
    jeq r1, 0, lbb_12474                            if r1 == (0 as i32 as i64 as u64) { pc += 103 }
    ldxdw r2, [r10-0xd8]                    
    mov64 r3, r1                                    r3 = r1
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    stxdw [r10-0x298], r3                   
    mov64 r3, r2                                    r3 = r2
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    mov64 r0, r3                                    r0 = r3
    ldxb r3, [r2+0x0]                       
    stxb [r10-0x70], r3                     
    stxdw [r10-0x2a0], r3                   
    jeq r3, 0, lbb_12392                            if r3 == (0 as i32 as i64 as u64) { pc += 10 }
    ldxdw r3, [r10-0x2a0]                   
    jne r3, 1, lbb_12490                            if r3 != (1 as i32 as i64 as u64) { pc += 106 }
    ldxdw r3, [r10-0x298]                   
    jeq r3, 0, lbb_12474                            if r3 == (0 as i32 as i64 as u64) { pc += 88 }
    ldxb r3, [r2+0x1]                       
    stxdw [r10-0x2b0], r3                   
    add64 r1, -2                                    r1 += -2   ///  r1 = r1.wrapping_add(-2 as i32 as i64 as u64)
    add64 r2, 2                                     r2 += 2   ///  r2 = r2.wrapping_add(2 as i32 as i64 as u64)
    mov64 r0, r2                                    r0 = r2
    stxdw [r10-0x298], r1                   
lbb_12392:
    ldxdw r1, [r10-0x298]                   
    jeq r1, 0, lbb_12474                            if r1 == (0 as i32 as i64 as u64) { pc += 80 }
    ldxdw r4, [r10-0x298]                   
    add64 r4, -1                                    r4 += -1   ///  r4 = r4.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r0                                    r1 = r0
    mov64 r5, r1                                    r5 = r1
    add64 r5, 1                                     r5 += 1   ///  r5 = r5.wrapping_add(1 as i32 as i64 as u64)
    ldxb r1, [r1+0x0]                       
    stxb [r10-0x70], r1                     
    stxdw [r10-0x2a8], r1                   
    jeq r1, 0, lbb_12420                            if r1 == (0 as i32 as i64 as u64) { pc += 17 }
    ldxdw r1, [r10-0x2a8]                   
    jeq r1, 1, lbb_12406                            if r1 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12490                                    if true { pc += 84 }
lbb_12406:
    ldxdw r1, [r10-0x298]                   
    jlt r1, 9, lbb_12474                            if r1 < (9 as i32 as i64 as u64) { pc += 66 }
    stxdw [r10-0x2b8], r0                   
    ldxdw r1, [r0+0x1]                      
    stxdw [r10-0x2c0], r1                   
    mov64 r2, r1                                    r2 = r1
    call function_22497                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x2a8], r1                   
    ldxdw r4, [r10-0x298]                   
    add64 r4, -9                                    r4 += -9   ///  r4 = r4.wrapping_add(-9 as i32 as i64 as u64)
    ldxdw r5, [r10-0x2b8]                   
    add64 r5, 9                                     r5 += 9   ///  r5 = r5.wrapping_add(9 as i32 as i64 as u64)
    jne r0, 0, lbb_12704                            if r0 != (0 as i32 as i64 as u64) { pc += 284 }
lbb_12420:
    jeq r4, 0, lbb_12474                            if r4 == (0 as i32 as i64 as u64) { pc += 53 }
    mov64 r0, r4                                    r0 = r4
    add64 r0, -1                                    r0 += -1   ///  r0 = r0.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r1, [r5+0x0]                       
    stxdw [r10-0xd0], r0                    
    mov64 r3, r5                                    r3 = r5
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0xd8], r3                    
    stxb [r10-0x59], r1                     
    stxdw [r10-0x298], r1                   
    jeq r1, 0, lbb_12458                            if r1 == (0 as i32 as i64 as u64) { pc += 27 }
    ldxdw r2, [r10-0x298]                   
    jeq r2, 1, lbb_12434                            if r2 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12478                                    if true { pc += 44 }
lbb_12434:
    jlt r4, 33, lbb_12474                           if r4 < (33 as i32 as i64 as u64) { pc += 39 }
    ldxdw r1, [r5+0x7]                      
    ldxb r0, [r5+0xf]                       
    stxb [r10-0x38], r0                     
    ldxw r0, [r3+0x0]                       
    stxw [r10-0x60], r0                     
    ldxh r3, [r3+0x4]                       
    stxh [r10-0x5c], r3                     
    stxdw [r10-0x40], r1                    
    ldxdw r2, [r10-0x3f]                    
    ldxb r3, [r5+0x20]                      
    ldxdw r0, [r5+0x18]                     
    ldxdw r5, [r5+0x10]                     
    stxb [r10-0x70], r1                     
    stxdw [r10-0x88], r5                    
    stxdw [r10-0x80], r0                    
    stxb [r10-0x78], r3                     
    add64 r4, -33                                   r4 += -33   ///  r4 = r4.wrapping_add(-33 as i32 as i64 as u64)
    stxdw [r10-0x6f], r2                    
    rsh64 r2, 56                                    r2 >>= 56   ///  r2 = r2.wrapping_shr(56)
    stxdw [r10-0x2b8], r2                   
    ldxdw r1, [r10-0x70]                    
    stxdw [r10-0x2c8], r1                   
    mov64 r0, r4                                    r0 = r4
lbb_12458:
    ldxw r3, [r10-0x90]                     
    stxw [r10-0xb0], r3                     
    ldxh r3, [r10-0x8c]                     
    stxh [r10-0xac], r3                     
    ldxdw r3, [r10-0xa8]                    
    stxdw [r10-0xc8], r3                    
    ldxdw r3, [r10-0xa0]                    
    stxdw [r10-0xc0], r3                    
    ldxb r3, [r10-0x98]                     
    stxb [r10-0xb8], r3                     
    jeq r0, 0, lbb_12531                            if r0 == (0 as i32 as i64 as u64) { pc += 62 }
    lddw r1, 0x10002d179 --> b"Not all bytes read"        r1 load str located at 4295151993
    mov64 r2, 18                                    r2 = 18 as i32 as i64 as u64
lbb_12472:
    call function_71                        
    ja lbb_12513                                    if true { pc += 39 }
lbb_12474:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    ja lbb_12513                                    if true { pc += 35 }
lbb_12478:
    lddw r1, 0x10002eb10 --> b"\x00\x00\x00\x00\x9a\xd1\x02\x00\x1f\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295158544
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100029418 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295136280
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    ja lbb_12501                                    if true { pc += 11 }
lbb_12490:
    lddw r1, 0x10002eb10 --> b"\x00\x00\x00\x00\x9a\xd1\x02\x00\x1f\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295158544
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100029418 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295136280
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
lbb_12501:
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 2                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_18371                     
    mov64 r1, r7                                    r1 = r7
    call function_35                        
lbb_12513:
    mov64 r7, r0                                    r7 = r0
lbb_12514:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r2, r7                                    r2 = r7
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    jne r2, 1, lbb_12523                            if r2 != (1 as i32 as i64 as u64) { pc += 5 }
    ldxdw r1, [r7+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_12523                            if r2 == (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r7-0x1]                      
    callx r2                                
lbb_12523:
    stxw [r6+0x4], r1                       
    stxw [r6+0x0], r8                       
    exit                                    
lbb_12526:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    lddw r3, 0x10002f458 --> b"\x00\x00\x00\x00\xf9\xd2\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00S\x00\x00…        r3 load str located at 4295160920
    call function_20246                     
lbb_12531:
    ldxh r3, [r10-0xac]                     
    stxh [r10-0x119], r3                    
    ldxw r3, [r10-0xb0]                     
    stxw [r10-0x11d], r3                    
    ldxdw r3, [r10-0xc8]                    
    stxdw [r10-0x10e], r3                   
    ldxdw r3, [r10-0xc0]                    
    stxdw [r10-0x106], r3                   
    ldxb r3, [r10-0xb8]                     
    stxb [r10-0xfe], r3                     
    ldxw r3, [r10-0x60]                     
    stxw [r10-0xfc], r3                     
    ldxh r3, [r10-0x5c]                     
    stxh [r10-0xf8], r3                     
    ldxdw r3, [r10-0x88]                    
    stxdw [r10-0xed], r3                    
    ldxdw r3, [r10-0x80]                    
    stxdw [r10-0xe5], r3                    
    ldxb r3, [r10-0x78]                     
    stxb [r10-0xdd], r3                     
    ldxdw r3, [r10-0x200]                   
    stxw [r10-0x1c8], r3                    
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    stxw [r10-0x1c4], r3                    
    ldxdw r3, [r10-0x2c0]                   
    stxdw [r10-0x128], r3                   
    ldxdw r3, [r10-0x2a0]                   
    stxb [r10-0x120], r3                    
    ldxdw r3, [r10-0x2b0]                   
    stxb [r10-0x11f], r3                    
    stxb [r10-0x11e], r8                    
    ldxdw r3, [r10-0x1f8]                   
    stxb [r10-0x10f], r3                    
    ldxdw r2, [r10-0x298]                   
    stxb [r10-0xfd], r2                     
    ldxdw r2, [r10-0x2b8]                   
    stxb [r10-0xee], r2                     
    ldxdw r1, [r10-0x2c8]                   
    stxdw [r10-0xf6], r1                    
    ldxdw r2, [r10-0x1f0]                   
    stxdw [r10-0x117], r2                   
    ldxdw r1, [r10-0x2a8]                   
    stxdw [r10-0x130], r1                   
    stxdw [r10-0x138], r7                   
    ldxdw r1, [r10-0x290]                   
    stxdw [r10-0x140], r1                   
    ldxdw r1, [r10-0x288]                   
    stxdw [r10-0x148], r1                   
    ldxdw r1, [r10-0x280]                   
    stxdw [r10-0x150], r1                   
    ldxdw r1, [r10-0x270]                   
    stxdw [r10-0x158], r1                   
    ldxdw r1, [r10-0x268]                   
    stxdw [r10-0x160], r1                   
    ldxdw r1, [r10-0x260]                   
    stxdw [r10-0x168], r1                   
    ldxdw r1, [r10-0x258]                   
    stxdw [r10-0x170], r1                   
    ldxdw r1, [r10-0x278]                   
    stxdw [r10-0x178], r1                   
    ldxdw r1, [r10-0x250]                   
    stxdw [r10-0x180], r1                   
    ldxdw r1, [r10-0x238]                   
    stxdw [r10-0x188], r1                   
    ldxdw r1, [r10-0x248]                   
    stxdw [r10-0x190], r1                   
    ldxdw r1, [r10-0x230]                   
    stxdw [r10-0x198], r1                   
    ldxdw r1, [r10-0x228]                   
    stxdw [r10-0x1a0], r1                   
    ldxdw r1, [r10-0x220]                   
    stxdw [r10-0x1a8], r1                   
    ldxdw r1, [r10-0x218]                   
    stxdw [r10-0x1b0], r1                   
    ldxdw r1, [r10-0x210]                   
    stxdw [r10-0x1b8], r1                   
    ldxdw r1, [r10-0x208]                   
    stxdw [r10-0x1c0], r1                   
    ldxdw r1, [r10-0x240]                   
    stxdw [r10-0x1d0], r1                   
    mov64 r8, 10                                    r8 = 10 as i32 as i64 as u64
    jeq r9, 0, lbb_12660                            if r9 == (0 as i32 as i64 as u64) { pc += 47 }
    lddw r8, 0x200000000                            r8 load str located at 8589934592
    ldxdw r1, [r10-0x1e0]                   
    ldxdw r1, [r1+0x0]                      
    ldxb r1, [r1+0x2]                       
    jeq r1, 0, lbb_12660                            if r1 == (0 as i32 as i64 as u64) { pc += 41 }
    mov64 r8, 10                                    r8 = 10 as i32 as i64 as u64
    jeq r9, 1, lbb_12660                            if r9 == (1 as i32 as i64 as u64) { pc += 39 }
    mov64 r8, 7                                     r8 = 7 as i32 as i64 as u64
    ldxdw r1, [r10-0x1e0]                   
    ldxdw r1, [r1+0x8]                      
    ldxb r1, [r1+0x1]                       
    jeq r1, 0, lbb_12660                            if r1 == (0 as i32 as i64 as u64) { pc += 34 }
    mov64 r8, 10                                    r8 = 10 as i32 as i64 as u64
    jeq r9, 2, lbb_12660                            if r9 == (2 as i32 as i64 as u64) { pc += 32 }
    mov64 r2, r9                                    r2 = r9
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    add64 r2, -24                                   r2 += -24   ///  r2 = r2.wrapping_add(-24 as i32 as i64 as u64)
    lddw r3, 0x7ffffffffffffff8                     r3 load str located at 9223372036854775800
    jgt r2, r3, lbb_12657                           if r2 > r3 { pc += 22 }
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jeq r9, 3, lbb_12680                            if r9 == (3 as i32 as i64 as u64) { pc += 42 }
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r4, 0x300008000                            r4 load str located at 12884934656
    jeq r1, 0, lbb_12645                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r1                                    r4 = r1
lbb_12645:
    mov64 r1, r4                                    r1 = r4
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jgt r1, r4, lbb_12651                           if r1 > r4 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_12651:
    jne r5, 0, lbb_12653                            if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r1                                    r3 = r1
lbb_12653:
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    lddw r4, 0x300000007                            r4 load str located at 12884901895
    jgt r3, r4, lbb_12663                           if r3 > r4 { pc += 6 }
lbb_12657:
    lddw r3, 0x10002f560 --> b"\x00\x00\x00\x00j\xd0\x02\x00b\x00\x00\x00\x00\x00\x00\x00\xb3\x07\x00\x0…        r3 load str located at 4295161184
    call function_18348                     
lbb_12660:
    mov64 r1, r8                                    r1 = r8
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    ja lbb_12523                                    if true { pc += -140 }
lbb_12663:
    ldxdw r5, [r10-0x1e0]                   
    add64 r5, 24                                    r5 += 24   ///  r5 = r5.wrapping_add(24 as i32 as i64 as u64)
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 3                                     r1 >>= 3   ///  r1 = r1.wrapping_shr(3)
    and64 r3, -8                                    r3 &= -8   ///  r3 = r3.and(-8)
    lddw r4, 0x300000000                            r4 load str located at 12884901888
    stxdw [r4+0x0], r3                      
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r0, r3                                    r0 = r3
lbb_12673:
    stxdw [r0+0x0], r5                      
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, -8                                    r2 += -8   ///  r2 = r2.wrapping_add(-8 as i32 as i64 as u64)
    jeq r2, 0, lbb_12680                            if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12673                                    if true { pc += -7 }
lbb_12680:
    ldxdw r0, [r10-0x1e0]                   
    mov64 r2, r0                                    r2 = r0
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r5, r0                                    r5 = r0
    add64 r5, 16                                    r5 += 16   ///  r5 = r5.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x18], r5                    
    stxdw [r10-0x20], r2                    
    stxdw [r10-0x28], r0                    
    stxdw [r10-0x30], r4                    
    stxdw [r10-0x40], r1                    
    stxw [r10-0x38], r3                     
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    stxw [r10-0x34], r3                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -472                                  r1 += -472   ///  r1 = r1.wrapping_add(-472 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -64                                   r3 += -64   ///  r3 = r3.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -464                                  r4 += -464   ///  r4 = r4.wrapping_add(-464 as i32 as i64 as u64)
    ldxdw r2, [r10-0x1e8]                   
    call function_5248                      
    ldxw r1, [r10-0x1d4]                    
    ldxw r8, [r10-0x1d8]                    
    ja lbb_12523                                    if true { pc += -181 }
lbb_12704:
    lddw r1, 0x10002d0cc --> b"For portability reasons we do not allow to deserialize NaNs."        r1 load str located at 4295151820
    mov64 r2, 60                                    r2 = 60 as i32 as i64 as u64
    ja lbb_12472                                    if true { pc += -236 }

function_12708:
    mov64 r7, r5                                    r7 = r5
    mov64 r8, r4                                    r8 = r4
    mov64 r9, r3                                    r9 = r3
    mov64 r6, r2                                    r6 = r2
    stxdw [r10-0xf0], r1                    
    lddw r1, 0x10002d906 --> b"Instruction: init_market"        r1 load str located at 4295153926
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    syscall [invalid]                       
    ldxdw r1, [r7-0xff8]                    
    jeq r1, 1, lbb_12739                            if r1 == (1 as i32 as i64 as u64) { pc += 20 }
    jne r1, 0, lbb_12725                            if r1 != (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    lddw r3, 0x10002f470 --> b"\x00\x00\x00\x00\xf9\xd2\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00[\x00\x00…        r3 load str located at 4295160944
    call function_20246                     
lbb_12725:
    lddw r1, 0x10002d179 --> b"Not all bytes read"        r1 load str located at 4295151993
    mov64 r2, 18                                    r2 = 18 as i32 as i64 as u64
    call function_71                        
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r1, r0                                    r1 = r0
    and64 r1, 3                                     r1 &= 3   ///  r1 = r1.and(3)
    jne r1, 1, lbb_12768                            if r1 != (1 as i32 as i64 as u64) { pc += 35 }
    ldxdw r1, [r0+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_12768                            if r2 == (0 as i32 as i64 as u64) { pc += 32 }
    ldxdw r1, [r0-0x1]                      
    callx r2                                
    ja lbb_12768                                    if true { pc += 29 }
lbb_12739:
    stxdw [r10-0xf8], r6                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, r8                                    r3 = r8
    call function_14592                     
    ldxw r8, [r10-0x64]                     
    ldxw r7, [r10-0x68]                     
    ldxdw r9, [r10-0x70]                    
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    jeq r9, r1, lbb_12768                           if r9 == r1 { pc += 17 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -96                                   r2 += -96   ///  r2 = r2.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r3, 96                                    r3 = 96 as i32 as i64 as u64
    call function_21513                     
    stxw [r10-0xd4], r8                     
    stxw [r10-0xd8], r7                     
    stxdw [r10-0xe0], r9                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -232                                  r1 += -232   ///  r1 = r1.wrapping_add(-232 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -224                                  r3 += -224   ///  r3 = r3.wrapping_add(-224 as i32 as i64 as u64)
    ldxdw r2, [r10-0xf8]                    
    call function_5575                      
    ldxw r8, [r10-0xe4]                     
    ldxw r7, [r10-0xe8]                     
lbb_12768:
    ldxdw r1, [r10-0xf0]                    
    stxw [r1+0x4], r8                       
    stxw [r1+0x0], r7                       
    exit                                    

function_12772:
    mov64 r8, r4                                    r8 = r4
    mov64 r9, r3                                    r9 = r3
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r5-0xff8]                    
    jeq r1, 0, lbb_13003                            if r1 == (0 as i32 as i64 as u64) { pc += 226 }
    stxdw [r10-0x4a8], r2                   
    ldxdw r2, [r5-0x1000]                   
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxdw [r10-0x288], r1                   
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x290], r2                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_1421                      
    ldxdw r7, [r10-0x168]                   
    ldxb r1, [r10-0x170]                    
    jne r1, 0, lbb_12979                            if r1 != (0 as i32 as i64 as u64) { pc += 188 }
    ldxb r1, [r10-0x16f]                    
    stxdw [r10-0x4b0], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_1421                      
    ldxdw r7, [r10-0x168]                   
    ldxb r1, [r10-0x170]                    
    jne r1, 0, lbb_12979                            if r1 != (0 as i32 as i64 as u64) { pc += 178 }
    ldxb r1, [r10-0x16f]                    
    stxdw [r10-0x4b8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_1421                      
    ldxdw r7, [r10-0x168]                   
    ldxb r1, [r10-0x170]                    
    jne r1, 0, lbb_12979                            if r1 != (0 as i32 as i64 as u64) { pc += 168 }
    ldxdw r1, [r10-0x288]                   
    jeq r1, 0, lbb_12951                            if r1 == (0 as i32 as i64 as u64) { pc += 138 }
    ldxb r2, [r10-0x16f]                    
    stxdw [r10-0x4c0], r2                   
    ldxdw r2, [r10-0x290]                   
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r3, r1                                    r3 = r1
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r4, r2                                    r4 = r2
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    ldxb r5, [r2+0x0]                       
    stxb [r10-0x190], r5                    
    jeq r5, 0, lbb_12834                            if r5 == (0 as i32 as i64 as u64) { pc += 10 }
    jne r5, 1, lbb_12955                            if r5 != (1 as i32 as i64 as u64) { pc += 130 }
    jeq r3, 0, lbb_12951                            if r3 == (0 as i32 as i64 as u64) { pc += 125 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ldxb r3, [r2+0x1]                       
    stxdw [r10-0x4d8], r3                   
    add64 r1, -2                                    r1 += -2   ///  r1 = r1.wrapping_add(-2 as i32 as i64 as u64)
    add64 r2, 2                                     r2 += 2   ///  r2 = r2.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r10-0x290], r2                   
    mov64 r4, r2                                    r4 = r2
    mov64 r3, r1                                    r3 = r1
lbb_12834:
    jeq r3, 0, lbb_12951                            if r3 == (0 as i32 as i64 as u64) { pc += 116 }
    stxdw [r10-0x4c8], r7                   
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r2, r3                                    r2 = r3
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r4                                    r1 = r4
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    ldxb r5, [r4+0x0]                       
    stxb [r10-0x190], r5                    
    jeq r5, 0, lbb_12853                            if r5 == (0 as i32 as i64 as u64) { pc += 9 }
    jne r5, 1, lbb_12955                            if r5 != (1 as i32 as i64 as u64) { pc += 110 }
    jeq r2, 0, lbb_12951                            if r2 == (0 as i32 as i64 as u64) { pc += 105 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ldxb r1, [r4+0x1]                       
    stxdw [r10-0x4e0], r1                   
    add64 r3, -2                                    r3 += -2   ///  r3 = r3.wrapping_add(-2 as i32 as i64 as u64)
    add64 r4, 2                                     r4 += 2   ///  r4 = r4.wrapping_add(2 as i32 as i64 as u64)
    mov64 r1, r4                                    r1 = r4
    mov64 r2, r3                                    r2 = r3
lbb_12853:
    stxdw [r10-0x4d0], r7                   
    jeq r2, 0, lbb_12951                            if r2 == (0 as i32 as i64 as u64) { pc += 96 }
    mov64 r3, r2                                    r3 = r2
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r0, [r1+0x0]                       
    stxdw [r10-0x288], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x290], r3                   
    stxb [r10-0x29], r0                     
    ldxdw r7, [r10-0x4c8]                   
    jeq r0, 0, lbb_12894                            if r0 == (0 as i32 as i64 as u64) { pc += 29 }
    jeq r0, 1, lbb_12867                            if r0 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12991                                    if true { pc += 124 }
lbb_12867:
    jlt r2, 33, lbb_12951                           if r2 < (33 as i32 as i64 as u64) { pc += 83 }
    ldxdw r4, [r1+0x7]                      
    ldxb r5, [r1+0xf]                       
    stxb [r10-0x168], r5                    
    ldxw r5, [r3+0x0]                       
    stxw [r10-0x180], r5                    
    ldxh r3, [r3+0x4]                       
    stxh [r10-0x17c], r3                    
    add64 r2, -33                                   r2 += -33   ///  r2 = r2.wrapping_add(-33 as i32 as i64 as u64)
    stxdw [r10-0x288], r2                   
    mov64 r2, r1                                    r2 = r1
    add64 r2, 33                                    r2 += 33   ///  r2 = r2.wrapping_add(33 as i32 as i64 as u64)
    stxdw [r10-0x290], r2                   
    stxdw [r10-0x170], r4                   
    ldxdw r5, [r10-0x16f]                   
    ldxb r2, [r1+0x20]                      
    ldxdw r3, [r1+0x18]                     
    ldxdw r1, [r1+0x10]                     
    stxb [r10-0x190], r4                    
    stxdw [r10-0x1a8], r1                   
    stxdw [r10-0x1a0], r3                   
    stxb [r10-0x198], r2                    
    stxdw [r10-0x18f], r5                   
    rsh64 r5, 56                                    r5 >>= 56   ///  r5 = r5.wrapping_shr(56)
    stxdw [r10-0x4f8], r5                   
    ldxdw r1, [r10-0x190]                   
    stxdw [r10-0x500], r1                   
lbb_12894:
    stxdw [r10-0x4e8], r0                   
    ldxdw r1, [r10-0x4d0]                   
    ldxdw r1, [r10-0x4c0]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_1162                      
    ldxb r1, [r10-0x170]                    
    jne r1, 0, lbb_13170                            if r1 != (0 as i32 as i64 as u64) { pc += 266 }
    ldxw r1, [r10-0x16c]                    
    stxw [r10-0x175], r1                    
    ldxw r1, [r10-0x16f]                    
    stxw [r10-0x178], r1                    
    ldxdw r1, [r10-0x160]                   
    stxdw [r10-0x28], r1                    
    ldxdw r1, [r10-0x158]                   
    stxdw [r10-0x20], r1                    
    ldxh r1, [r10-0x150]                    
    stxh [r10-0x18], r1                     
    ldxdw r1, [r10-0x168]                   
    stxdw [r10-0x4f0], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_1248                      
    ldxdw r7, [r10-0x168]                   
    ldxdw r2, [r10-0x170]                   
    lddw r1, 0x8000000000000001                     r1 load str located at -9223372036854775807
    jeq r2, r1, lbb_12979                           if r2 == r1 { pc += 53 }
    stxdw [r10-0x508], r2                   
    ldxdw r1, [r10-0x160]                   
    stxdw [r10-0x510], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_1717                      
    ldxdw r2, [r10-0x168]                   
    ldxw r1, [r10-0x170]                    
    jne r1, 0, lbb_12949                            if r1 != (0 as i32 as i64 as u64) { pc += 12 }
    stxdw [r10-0x518], r2                   
    ldxw r1, [r10-0x16c]                    
    stxdw [r10-0x520], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_1717                      
    ldxdw r2, [r10-0x168]                   
    ldxw r1, [r10-0x170]                    
    jne r1, 0, lbb_12949                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_13008                                    if true { pc += 59 }
lbb_12949:
    mov64 r7, r2                                    r7 = r2
    ja lbb_12979                                    if true { pc += 28 }
lbb_12951:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    ja lbb_12978                                    if true { pc += 23 }
lbb_12955:
    lddw r1, 0x10002eb10 --> b"\x00\x00\x00\x00\x9a\xd1\x02\x00\x1f\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295158544
    stxdw [r10-0x170], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x160], r1                   
    lddw r1, 0x100029418 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295136280
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -400                                  r1 += -400   ///  r1 = r1.wrapping_add(-400 as i32 as i64 as u64)
lbb_12966:
    stxdw [r10-0x10], r1                    
    stdw [r10-0x150], 0                     
    stdw [r10-0x168], 2                     
    stdw [r10-0x158], 1                     
    mov64 r7, r10                                   r7 = r10
    add64 r7, -40                                   r7 += -40   ///  r7 = r7.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -368                                  r2 += -368   ///  r2 = r2.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_18371                     
    mov64 r1, r7                                    r1 = r7
    call function_35                        
lbb_12978:
    mov64 r7, r0                                    r7 = r0
lbb_12979:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r1, r7                                    r1 = r7
    and64 r1, 3                                     r1 &= 3   ///  r1 = r1.and(3)
    jne r1, 1, lbb_12988                            if r1 != (1 as i32 as i64 as u64) { pc += 5 }
    ldxdw r1, [r7+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_12988                            if r2 == (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r7-0x1]                      
    callx r2                                
lbb_12988:
    stxw [r6+0x4], r9                       
    stxw [r6+0x0], r8                       
    exit                                    
lbb_12991:
    lddw r1, 0x10002eb10 --> b"\x00\x00\x00\x00\x9a\xd1\x02\x00\x1f\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295158544
    stxdw [r10-0x170], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x160], r1                   
    lddw r1, 0x100029418 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295136280
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -41                                   r1 += -41   ///  r1 = r1.wrapping_add(-41 as i32 as i64 as u64)
    ja lbb_12966                                    if true { pc += -37 }
lbb_13003:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    lddw r3, 0x10002f488 --> b"\x00\x00\x00\x00\xf9\xd2\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00c\x00\x00…        r3 load str located at 4295160968
    call function_20246                     
lbb_13008:
    stxdw [r10-0x528], r2                   
    ldxw r1, [r10-0x16c]                    
    stxdw [r10-0x530], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_1717                      
    ldxdw r2, [r10-0x168]                   
    ldxw r1, [r10-0x170]                    
    jne r1, 0, lbb_12949                            if r1 != (0 as i32 as i64 as u64) { pc += -70 }
    stxdw [r10-0x538], r2                   
    ldxw r1, [r10-0x16c]                    
    stxdw [r10-0x540], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_1717                      
    ldxdw r2, [r10-0x168]                   
    ldxw r1, [r10-0x170]                    
    jne r1, 0, lbb_12949                            if r1 != (0 as i32 as i64 as u64) { pc += -81 }
    stxdw [r10-0x550], r2                   
    ldxw r1, [r10-0x16c]                    
    stxdw [r10-0x558], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_1717                      
    ldxdw r1, [r10-0x168]                   
    stxdw [r10-0x548], r1                   
    ldxw r1, [r10-0x170]                    
    jne r1, 0, lbb_13043                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_13045                                    if true { pc += 2 }
lbb_13043:
    ldxdw r7, [r10-0x548]                   
    ja lbb_12979                                    if true { pc += -66 }
lbb_13045:
    ldxw r1, [r10-0x16c]                    
    stxdw [r10-0x568], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_1717                      
    ldxdw r1, [r10-0x168]                   
    stxdw [r10-0x560], r1                   
    ldxw r1, [r10-0x170]                    
    jne r1, 0, lbb_13057                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_13059                                    if true { pc += 2 }
lbb_13057:
    ldxdw r7, [r10-0x560]                   
    ja lbb_12979                                    if true { pc += -80 }
lbb_13059:
    ldxw r1, [r10-0x16c]                    
    stxdw [r10-0x580], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_2014                      
    ldxdw r1, [r10-0x168]                   
    stxdw [r10-0x570], r1                   
    ldxdw r1, [r10-0x170]                   
    stxdw [r10-0x578], r1                   
    jeq r1, 2, lbb_13072                            if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_13074                                    if true { pc += 2 }
lbb_13072:
    ldxdw r7, [r10-0x570]                   
    ja lbb_12979                                    if true { pc += -95 }
lbb_13074:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_2014                      
    ldxdw r1, [r10-0x168]                   
    stxdw [r10-0x588], r1                   
    ldxdw r1, [r10-0x170]                   
    stxdw [r10-0x590], r1                   
    jeq r1, 2, lbb_13085                            if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_13087                                    if true { pc += 2 }
lbb_13085:
    ldxdw r7, [r10-0x588]                   
    ja lbb_12979                                    if true { pc += -108 }
lbb_13087:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_2014                      
    ldxdw r1, [r10-0x168]                   
    stxdw [r10-0x598], r1                   
    ldxdw r1, [r10-0x170]                   
    stxdw [r10-0x5a0], r1                   
    jeq r1, 2, lbb_13098                            if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_13100                                    if true { pc += 2 }
lbb_13098:
    ldxdw r7, [r10-0x598]                   
    ja lbb_12979                                    if true { pc += -121 }
lbb_13100:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_1787                      
    ldxdw r1, [r10-0x168]                   
    stxdw [r10-0x5a8], r1                   
    ldxdw r1, [r10-0x170]                   
    stxdw [r10-0x5b0], r1                   
    jeq r1, 2, lbb_13111                            if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_13113                                    if true { pc += 2 }
lbb_13111:
    ldxdw r7, [r10-0x5a8]                   
    ja lbb_12979                                    if true { pc += -134 }
lbb_13113:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -200                                  r1 += -200   ///  r1 = r1.wrapping_add(-200 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -352                                  r2 += -352   ///  r2 = r2.wrapping_add(-352 as i32 as i64 as u64)
    mov64 r3, 152                                   r3 = 152 as i32 as i64 as u64
    call function_21513                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_1514                      
    ldxdw r1, [r10-0x168]                   
    stxdw [r10-0x5b8], r1                   
    ldxdw r2, [r10-0x170]                   
    lddw r1, 0x8000000000000001                     r1 load str located at -9223372036854775807
    stxdw [r10-0x5c0], r2                   
    jeq r2, r1, lbb_13132                           if r2 == r1 { pc += 1 }
    ja lbb_13134                                    if true { pc += 2 }
lbb_13132:
    ldxdw r7, [r10-0x5b8]                   
    ja lbb_12979                                    if true { pc += -155 }
lbb_13134:
    ldxdw r1, [r10-0x160]                   
    stxdw [r10-0x5d8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_2014                      
    ldxdw r1, [r10-0x168]                   
    stxdw [r10-0x5c8], r1                   
    ldxdw r1, [r10-0x170]                   
    stxdw [r10-0x5d0], r1                   
    jeq r1, 2, lbb_13147                            if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_13149                                    if true { pc += 2 }
lbb_13147:
    ldxdw r7, [r10-0x5c8]                   
    ja lbb_12979                                    if true { pc += -170 }
lbb_13149:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_2014                      
    ldxdw r1, [r10-0x168]                   
    stxdw [r10-0x5e0], r1                   
    ldxdw r1, [r10-0x170]                   
    stxdw [r10-0x5e8], r1                   
    jeq r1, 2, lbb_13160                            if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_13162                                    if true { pc += 2 }
lbb_13160:
    ldxdw r7, [r10-0x5e0]                   
    ja lbb_12979                                    if true { pc += -183 }
lbb_13162:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_1421                      
    ldxb r1, [r10-0x170]                    
    jne r1, 0, lbb_13170                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_13172                                    if true { pc += 2 }
lbb_13170:
    ldxdw r7, [r10-0x168]                   
    ja lbb_12979                                    if true { pc += -193 }
lbb_13172:
    ldxb r1, [r10-0x16f]                    
    stxdw [r10-0x600], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_1514                      
    ldxdw r1, [r10-0x168]                   
    stxdw [r10-0x5f0], r1                   
    ldxdw r1, [r10-0x170]                   
    lddw r2, 0x8000000000000001                     r2 load str located at -9223372036854775807
    stxdw [r10-0x5f8], r1                   
    jeq r1, r2, lbb_13187                           if r1 == r2 { pc += 1 }
    ja lbb_13189                                    if true { pc += 2 }
lbb_13187:
    ldxdw r7, [r10-0x5f0]                   
    ja lbb_12979                                    if true { pc += -210 }
lbb_13189:
    ldxdw r1, [r10-0x160]                   
    stxdw [r10-0x610], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_1248                      
    ldxdw r1, [r10-0x168]                   
    stxdw [r10-0x608], r1                   
    ldxdw r1, [r10-0x170]                   
    stxdw [r10-0x618], r1                   
    lddw r2, 0x8000000000000001                     r2 load str located at -9223372036854775807
    jeq r1, r2, lbb_13204                           if r1 == r2 { pc += 1 }
    ja lbb_13206                                    if true { pc += 2 }
lbb_13204:
    ldxdw r7, [r10-0x608]                   
    ja lbb_12979                                    if true { pc += -227 }
lbb_13206:
    ldxdw r1, [r10-0x160]                   
    stxdw [r10-0x630], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_2014                      
    ldxdw r1, [r10-0x168]                   
    stxdw [r10-0x620], r1                   
    ldxdw r1, [r10-0x170]                   
    stxdw [r10-0x628], r1                   
    jeq r1, 2, lbb_13219                            if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_13221                                    if true { pc += 2 }
lbb_13219:
    ldxdw r7, [r10-0x620]                   
    ja lbb_12979                                    if true { pc += -242 }
lbb_13221:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_2014                      
    ldxdw r1, [r10-0x168]                   
    stxdw [r10-0x638], r1                   
    ldxdw r1, [r10-0x170]                   
    stxdw [r10-0x640], r1                   
    jeq r1, 2, lbb_13232                            if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_13234                                    if true { pc += 2 }
lbb_13232:
    ldxdw r7, [r10-0x638]                   
    ja lbb_12979                                    if true { pc += -255 }
lbb_13234:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -656                                  r2 += -656   ///  r2 = r2.wrapping_add(-656 as i32 as i64 as u64)
    call function_2014                      
    ldxdw r1, [r10-0x168]                   
    stxdw [r10-0x648], r1                   
    ldxdw r1, [r10-0x170]                   
    stxdw [r10-0x650], r1                   
    jeq r1, 2, lbb_13245                            if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_13247                                    if true { pc += 2 }
lbb_13245:
    ldxdw r7, [r10-0x648]                   
    ja lbb_12979                                    if true { pc += -268 }
lbb_13247:
    ldxh r1, [r10-0x17c]                    
    stxh [r10-0x244], r1                    
    ldxw r1, [r10-0x180]                    
    stxw [r10-0x248], r1                    
    ldxdw r1, [r10-0x1a8]                   
    stxdw [r10-0x260], r1                   
    ldxdw r1, [r10-0x1a0]                   
    stxdw [r10-0x258], r1                   
    ldxb r1, [r10-0x198]                    
    stxb [r10-0x250], r1                    
    ldxw r1, [r10-0x178]                    
    stxw [r10-0x268], r1                    
    ldxw r1, [r10-0x175]                    
    stxw [r10-0x265], r1                    
    ldxdw r1, [r10-0x28]                    
    stxdw [r10-0x280], r1                   
    ldxdw r1, [r10-0x20]                    
    stxdw [r10-0x278], r1                   
    ldxh r1, [r10-0x18]                     
    stxh [r10-0x270], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -576                                  r1 += -576   ///  r1 = r1.wrapping_add(-576 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -200                                  r2 += -200   ///  r2 = r2.wrapping_add(-200 as i32 as i64 as u64)
    mov64 r3, 152                                   r3 = 152 as i32 as i64 as u64
    call function_21513                     
    ldxdw r1, [r10-0x288]                   
    jeq r1, 0, lbb_13280                            if r1 == (0 as i32 as i64 as u64) { pc += 5 }
    lddw r1, 0x10002d179 --> b"Not all bytes read"        r1 load str located at 4295151993
    mov64 r2, 18                                    r2 = 18 as i32 as i64 as u64
    call function_71                        
    ja lbb_12978                                    if true { pc += -302 }
lbb_13280:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1112                                 r1 += -1112   ///  r1 = r1.wrapping_add(-1112 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -576                                  r2 += -576   ///  r2 = r2.wrapping_add(-576 as i32 as i64 as u64)
    mov64 r3, 152                                   r3 = 152 as i32 as i64 as u64
    call function_21513                     
    ldxw r1, [r10-0x248]                    
    stxw [r10-0x2db], r1                    
    ldxh r1, [r10-0x244]                    
    stxh [r10-0x2d7], r1                    
    ldxdw r1, [r10-0x260]                   
    stxdw [r10-0x2cc], r1                   
    ldxdw r1, [r10-0x258]                   
    stxdw [r10-0x2c4], r1                   
    ldxb r1, [r10-0x250]                    
    stxb [r10-0x2bc], r1                    
    ldxw r1, [r10-0x268]                    
    stxw [r10-0x2bb], r1                    
    ldxw r1, [r10-0x265]                    
    stxw [r10-0x2b8], r1                    
    ldxdw r1, [r10-0x280]                   
    stxdw [r10-0x2ac], r1                   
    ldxdw r1, [r10-0x278]                   
    stxdw [r10-0x2a4], r1                   
    ldxh r1, [r10-0x270]                    
    stxh [r10-0x29c], r1                    
    ldxdw r1, [r10-0x600]                   
    stxb [r10-0x297], r1                    
    ldxdw r1, [r10-0x4c0]                   
    stxb [r10-0x298], r1                    
    ldxdw r1, [r10-0x4b8]                   
    stxb [r10-0x299], r1                    
    ldxdw r1, [r10-0x4b0]                   
    stxb [r10-0x29a], r1                    
    ldxdw r1, [r10-0x4f0]                   
    stxdw [r10-0x2b4], r1                   
    ldxdw r1, [r10-0x4f8]                   
    stxb [r10-0x2cd], r1                    
    ldxdw r1, [r10-0x500]                   
    stxdw [r10-0x2d5], r1                   
    ldxdw r1, [r10-0x4e8]                   
    stxb [r10-0x2dc], r1                    
    ldxdw r1, [r10-0x4e0]                   
    stxb [r10-0x2dd], r1                    
    ldxdw r1, [r10-0x4d0]                   
    stxb [r10-0x2de], r1                    
    ldxdw r1, [r10-0x4d8]                   
    stxb [r10-0x2df], r1                    
    ldxdw r1, [r10-0x560]                   
    stxw [r10-0x2e4], r1                    
    ldxdw r1, [r10-0x580]                   
    stxw [r10-0x2e8], r1                    
    ldxdw r1, [r10-0x548]                   
    stxw [r10-0x2ec], r1                    
    ldxdw r1, [r10-0x568]                   
    stxw [r10-0x2f0], r1                    
    ldxdw r1, [r10-0x550]                   
    stxw [r10-0x2f4], r1                    
    ldxdw r1, [r10-0x558]                   
    stxw [r10-0x2f8], r1                    
    ldxdw r1, [r10-0x538]                   
    stxw [r10-0x2fc], r1                    
    ldxdw r1, [r10-0x540]                   
    stxw [r10-0x300], r1                    
    ldxdw r1, [r10-0x528]                   
    stxw [r10-0x304], r1                    
    ldxdw r1, [r10-0x530]                   
    stxw [r10-0x308], r1                    
    ldxdw r1, [r10-0x518]                   
    stxw [r10-0x30c], r1                    
    ldxdw r1, [r10-0x520]                   
    stxw [r10-0x310], r1                    
    ldxdw r1, [r10-0x630]                   
    stxdw [r10-0x318], r1                   
    ldxdw r1, [r10-0x608]                   
    stxdw [r10-0x320], r1                   
    ldxdw r1, [r10-0x618]                   
    stxdw [r10-0x328], r1                   
    ldxdw r1, [r10-0x610]                   
    stxdw [r10-0x330], r1                   
    ldxdw r1, [r10-0x5f0]                   
    stxdw [r10-0x338], r1                   
    ldxdw r1, [r10-0x5f8]                   
    stxdw [r10-0x340], r1                   
    ldxdw r1, [r10-0x5d8]                   
    stxdw [r10-0x348], r1                   
    ldxdw r1, [r10-0x5b8]                   
    stxdw [r10-0x350], r1                   
    ldxdw r1, [r10-0x5c0]                   
    stxdw [r10-0x358], r1                   
    ldxdw r1, [r10-0x510]                   
    stxdw [r10-0x360], r1                   
    stxdw [r10-0x368], r7                   
    ldxdw r1, [r10-0x508]                   
    stxdw [r10-0x370], r1                   
    ldxdw r1, [r10-0x648]                   
    stxdw [r10-0x378], r1                   
    ldxdw r1, [r10-0x650]                   
    stxdw [r10-0x380], r1                   
    ldxdw r1, [r10-0x638]                   
    stxdw [r10-0x388], r1                   
    ldxdw r1, [r10-0x640]                   
    stxdw [r10-0x390], r1                   
    ldxdw r1, [r10-0x620]                   
    stxdw [r10-0x398], r1                   
    ldxdw r1, [r10-0x628]                   
    stxdw [r10-0x3a0], r1                   
    ldxdw r1, [r10-0x5e0]                   
    stxdw [r10-0x3a8], r1                   
    ldxdw r1, [r10-0x5e8]                   
    stxdw [r10-0x3b0], r1                   
    ldxdw r1, [r10-0x5c8]                   
    stxdw [r10-0x3b8], r1                   
    ldxdw r1, [r10-0x5d0]                   
    stxdw [r10-0x3c0], r1                   
    ldxdw r1, [r10-0x5a8]                   
    stxdw [r10-0x460], r1                   
    ldxdw r1, [r10-0x5b0]                   
    stxdw [r10-0x468], r1                   
    ldxdw r1, [r10-0x598]                   
    stxdw [r10-0x470], r1                   
    ldxdw r1, [r10-0x5a0]                   
    stxdw [r10-0x478], r1                   
    ldxdw r1, [r10-0x588]                   
    stxdw [r10-0x480], r1                   
    ldxdw r1, [r10-0x590]                   
    stxdw [r10-0x488], r1                   
    ldxdw r2, [r10-0x570]                   
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    stxw [r10-0x48c], r1                    
    stxw [r10-0x490], r2                    
    ldxdw r1, [r10-0x578]                   
    stxdw [r10-0x498], r1                   
    ldxdw r1, [r10-0x4c8]                   
    stxb [r10-0x2e0], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, r8                                    r3 = r8
    call function_14697                     
    ldxw r9, [r10-0x164]                    
    ldxw r8, [r10-0x168]                    
    ldxdw r7, [r10-0x170]                   
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    jeq r7, r1, lbb_12988                           if r7 == r1 { pc += -439 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -184                                  r1 += -184   ///  r1 = r1.wrapping_add(-184 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -352                                  r2 += -352   ///  r2 = r2.wrapping_add(-352 as i32 as i64 as u64)
    mov64 r3, 56                                    r3 = 56 as i32 as i64 as u64
    call function_21513                     
    stxw [r10-0xbc], r9                     
    stxw [r10-0xc0], r8                     
    stxdw [r10-0xc8], r7                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1184                                 r1 += -1184   ///  r1 = r1.wrapping_add(-1184 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -200                                  r3 += -200   ///  r3 = r3.wrapping_add(-200 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -1176                                 r4 += -1176   ///  r4 = r4.wrapping_add(-1176 as i32 as i64 as u64)
    ldxdw r2, [r10-0x4a8]                   
    call function_5926                      
    ldxw r9, [r10-0x49c]                    
    ldxw r8, [r10-0x4a0]                    
    ja lbb_12988                                    if true { pc += -459 }

function_13447:
    mov64 r9, r4                                    r9 = r4
    mov64 r8, r2                                    r8 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r5-0xff8]                    
    jeq r1, 0, lbb_13567                            if r1 == (0 as i32 as i64 as u64) { pc += 115 }
    stxdw [r10-0x110], r3                   
    ldxdw r2, [r5-0x1000]                   
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxdw [r10-0xa8], r1                    
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0xb0], r2                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -176                                  r2 += -176   ///  r2 = r2.wrapping_add(-176 as i32 as i64 as u64)
    call function_14766                     
    ldxdw r7, [r10-0x50]                    
    ldxdw r1, [r10-0x58]                    
    jne r1, 0, lbb_13479                            if r1 != (0 as i32 as i64 as u64) { pc += 13 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -160                                  r1 += -160   ///  r1 = r1.wrapping_add(-160 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -72                                   r2 += -72   ///  r2 = r2.wrapping_add(-72 as i32 as i64 as u64)
    mov64 r3, 72                                    r3 = 72 as i32 as i64 as u64
    call function_21513                     
    ldxdw r1, [r10-0xa8]                    
    jeq r1, 0, lbb_13489                            if r1 == (0 as i32 as i64 as u64) { pc += 15 }
    lddw r1, 0x10002d179 --> b"Not all bytes read"        r1 load str located at 4295151993
    mov64 r2, 18                                    r2 = 18 as i32 as i64 as u64
    call function_71                        
    mov64 r7, r0                                    r7 = r0
lbb_13479:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r2, r7                                    r2 = r7
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    jne r2, 1, lbb_13544                            if r2 != (1 as i32 as i64 as u64) { pc += 61 }
    ldxdw r1, [r7+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_13544                            if r2 == (0 as i32 as i64 as u64) { pc += 58 }
    ldxdw r1, [r7-0x1]                      
    callx r2                                
    ja lbb_13544                                    if true { pc += 55 }
lbb_13489:
    stxdw [r10-0x118], r8                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -248                                  r1 += -248   ///  r1 = r1.wrapping_add(-248 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -160                                  r2 += -160   ///  r2 = r2.wrapping_add(-160 as i32 as i64 as u64)
    mov64 r3, 72                                    r3 = 72 as i32 as i64 as u64
    call function_21513                     
    stxdw [r10-0x100], r7                   
    mov64 r8, 10                                    r8 = 10 as i32 as i64 as u64
    ldxdw r2, [r10-0x110]                   
    jeq r9, 0, lbb_13542                            if r9 == (0 as i32 as i64 as u64) { pc += 42 }
    mov64 r8, 7                                     r8 = 7 as i32 as i64 as u64
    ldxdw r3, [r2+0x0]                      
    ldxb r1, [r3+0x1]                       
    jeq r1, 0, lbb_13542                            if r1 == (0 as i32 as i64 as u64) { pc += 38 }
    mov64 r8, 10                                    r8 = 10 as i32 as i64 as u64
    jeq r9, 1, lbb_13542                            if r9 == (1 as i32 as i64 as u64) { pc += 36 }
    lddw r8, 0x200000000                            r8 load str located at 8589934592
    ldxdw r4, [r2+0x8]                      
    ldxb r1, [r4+0x2]                       
    jeq r1, 0, lbb_13542                            if r1 == (0 as i32 as i64 as u64) { pc += 31 }
    mov64 r2, r9                                    r2 = r9
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    add64 r2, -16                                   r2 += -16   ///  r2 = r2.wrapping_add(-16 as i32 as i64 as u64)
    lddw r5, 0x7ffffffffffffff8                     r5 load str located at 9223372036854775800
    jgt r2, r5, lbb_13539                           if r2 > r5 { pc += 21 }
    jeq r9, 2, lbb_13558                            if r9 == (2 as i32 as i64 as u64) { pc += 39 }
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r0, 0x300008000                            r0 load str located at 12884934656
    jeq r1, 0, lbb_13526                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, r1                                    r0 = r1
lbb_13526:
    mov64 r1, r0                                    r1 = r0
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jgt r1, r0, lbb_13532                           if r1 > r0 { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_13532:
    jne r7, 0, lbb_13534                            if r7 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, r1                                    r5 = r1
lbb_13534:
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    lddw r0, 0x300000007                            r0 load str located at 12884901895
    ldxdw r7, [r10-0x110]                   
    jgt r5, r0, lbb_13547                           if r5 > r0 { pc += 8 }
lbb_13539:
    lddw r3, 0x10002f560 --> b"\x00\x00\x00\x00j\xd0\x02\x00b\x00\x00\x00\x00\x00\x00\x00\xb3\x07\x00\x0…        r3 load str located at 4295161184
    call function_18348                     
lbb_13542:
    mov64 r1, r8                                    r1 = r8
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
lbb_13544:
    stxw [r6+0x4], r1                       
    stxw [r6+0x0], r8                       
    exit                                    
lbb_13547:
    add64 r7, 16                                    r7 += 16   ///  r7 = r7.wrapping_add(16 as i32 as i64 as u64)
    and64 r5, -8                                    r5 &= -8   ///  r5 = r5.and(-8)
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    stxdw [r1+0x0], r5                      
lbb_13552:
    stxdw [r5+0x0], r7                      
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, -8                                    r2 += -8   ///  r2 = r2.wrapping_add(-8 as i32 as i64 as u64)
    jeq r2, 0, lbb_13558                            if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_13552                                    if true { pc += -6 }
lbb_13558:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -264                                  r1 += -264   ///  r1 = r1.wrapping_add(-264 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -256                                  r5 += -256   ///  r5 = r5.wrapping_add(-256 as i32 as i64 as u64)
    ldxdw r2, [r10-0x118]                   
    call function_7544                      
    ldxw r1, [r10-0x104]                    
    ldxw r8, [r10-0x108]                    
    ja lbb_13544                                    if true { pc += -23 }
lbb_13567:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    lddw r3, 0x10002f4a0 --> b"\x00\x00\x00\x00\xf9\xd2\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00k\x00\x00…        r3 load str located at 4295160992
    call function_20246                     
    mov64 r7, r5                                    r7 = r5
    mov64 r8, r4                                    r8 = r4
    mov64 r9, r3                                    r9 = r3
    stxdw [r10-0x98], r2                    
    mov64 r6, r1                                    r6 = r1
    lddw r1, 0x10002d91e --> b"Instruction: withdraw"        r1 load str located at 4295153950
    mov64 r2, 21                                    r2 = 21 as i32 as i64 as u64
    syscall [invalid]                       
    ldxdw r1, [r7-0xff8]                    
    jeq r1, 0, lbb_13640                            if r1 == (0 as i32 as i64 as u64) { pc += 57 }
    jgt r1, 8, lbb_13588                            if r1 > (8 as i32 as i64 as u64) { pc += 4 }
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    ja lbb_13593                                    if true { pc += 5 }
lbb_13588:
    jeq r1, 9, lbb_13604                            if r1 == (9 as i32 as i64 as u64) { pc += 15 }
    lddw r1, 0x10002d179 --> b"Not all bytes read"        r1 load str located at 4295151993
    mov64 r2, 18                                    r2 = 18 as i32 as i64 as u64
    call function_71                        
lbb_13593:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, r0                                    r1 = r0
    and64 r1, 3                                     r1 &= 3   ///  r1 = r1.and(3)
    jne r1, 1, lbb_13637                            if r1 != (1 as i32 as i64 as u64) { pc += 39 }
    ldxdw r1, [r0+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_13637                            if r2 == (0 as i32 as i64 as u64) { pc += 36 }
    ldxdw r1, [r0-0x1]                      
    callx r2                                
    ja lbb_13637                                    if true { pc += 33 }
lbb_13604:
    ldxdw r1, [r7-0x1000]                   
    ldxdw r1, [r1+0x1]                      
    stxdw [r10-0x88], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, r8                                    r3 = r8
    call function_15051                     
    ldxw r8, [r10-0x34]                     
    ldxw r7, [r10-0x38]                     
    ldxdw r9, [r10-0x40]                    
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    jeq r9, r1, lbb_13637                           if r9 == r1 { pc += 19 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -48                                   r2 += -48   ///  r2 = r2.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r3, 48                                    r3 = 48 as i32 as i64 as u64
    call function_21513                     
    stxw [r10-0x74], r8                     
    stxw [r10-0x78], r7                     
    stxdw [r10-0x80], r9                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -128                                  r3 += -128   ///  r3 = r3.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -136                                  r4 += -136   ///  r4 = r4.wrapping_add(-136 as i32 as i64 as u64)
    ldxdw r2, [r10-0x98]                    
    call function_10230                     
    ldxw r8, [r10-0x8c]                     
    ldxw r7, [r10-0x90]                     
lbb_13637:
    stxw [r6+0x4], r8                       
    stxw [r6+0x0], r7                       
    exit                                    
lbb_13640:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    lddw r3, 0x10002f4b8 --> b"\x00\x00\x00\x00\xf9\xd2\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00\x82\x00\…        r3 load str located at 4295161016
    call function_20246                     
    mov64 r7, r5                                    r7 = r5
    mov64 r8, r4                                    r8 = r4
    mov64 r9, r3                                    r9 = r3
    stxdw [r10-0x98], r2                    
    mov64 r6, r1                                    r6 = r1
    lddw r1, 0x10002d933 --> b"Instruction: deposit"        r1 load str located at 4295153971
    mov64 r2, 20                                    r2 = 20 as i32 as i64 as u64
    syscall [invalid]                       
    ldxdw r1, [r7-0xff8]                    
    jeq r1, 0, lbb_13713                            if r1 == (0 as i32 as i64 as u64) { pc += 57 }
    jgt r1, 8, lbb_13661                            if r1 > (8 as i32 as i64 as u64) { pc += 4 }
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    ja lbb_13666                                    if true { pc += 5 }
lbb_13661:
    jeq r1, 9, lbb_13677                            if r1 == (9 as i32 as i64 as u64) { pc += 15 }
    lddw r1, 0x10002d179 --> b"Not all bytes read"        r1 load str located at 4295151993
    mov64 r2, 18                                    r2 = 18 as i32 as i64 as u64
    call function_71                        
lbb_13666:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, r0                                    r1 = r0
    and64 r1, 3                                     r1 &= 3   ///  r1 = r1.and(3)
    jne r1, 1, lbb_13710                            if r1 != (1 as i32 as i64 as u64) { pc += 39 }
    ldxdw r1, [r0+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_13710                            if r2 == (0 as i32 as i64 as u64) { pc += 36 }
    ldxdw r1, [r0-0x1]                      
    callx r2                                
    ja lbb_13710                                    if true { pc += 33 }
lbb_13677:
    ldxdw r1, [r7-0x1000]                   
    ldxdw r1, [r1+0x1]                      
    stxdw [r10-0x88], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, r8                                    r3 = r8
    call function_15114                     
    ldxw r8, [r10-0x34]                     
    ldxw r7, [r10-0x38]                     
    ldxdw r9, [r10-0x40]                    
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    jeq r9, r1, lbb_13710                           if r9 == r1 { pc += 19 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -48                                   r2 += -48   ///  r2 = r2.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r3, 48                                    r3 = 48 as i32 as i64 as u64
    call function_21513                     
    stxw [r10-0x74], r8                     
    stxw [r10-0x78], r7                     
    stxdw [r10-0x80], r9                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -128                                  r3 += -128   ///  r3 = r3.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -136                                  r4 += -136   ///  r4 = r4.wrapping_add(-136 as i32 as i64 as u64)
    ldxdw r2, [r10-0x98]                    
    call function_10411                     
    ldxw r8, [r10-0x8c]                     
    ldxw r7, [r10-0x90]                     
lbb_13710:
    stxw [r6+0x4], r8                       
    stxw [r6+0x0], r7                       
    exit                                    
lbb_13713:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    lddw r3, 0x10002f4d0 --> b"\x00\x00\x00\x00\xf9\xd2\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00\x8a\x00\…        r3 load str located at 4295161040
    call function_20246                     
    mov64 r7, r5                                    r7 = r5
    mov64 r9, r4                                    r9 = r4
    mov64 r8, r3                                    r8 = r3
    stxdw [r10-0x60], r2                    
    mov64 r6, r1                                    r6 = r1
    lddw r1, 0x10002d947 --> b"Instruction: create_or_set_init_settings"        r1 load str located at 4295153991
    mov64 r2, 40                                    r2 = 40 as i32 as i64 as u64
    syscall [invalid]                       
    ldxdw r3, [r7-0xff8]                    
    jeq r3, 0, lbb_13874                            if r3 == (0 as i32 as i64 as u64) { pc += 145 }
    jlt r3, 33, lbb_13741                           if r3 < (33 as i32 as i64 as u64) { pc += 11 }
    ldxdw r1, [r7-0x1000]                   
    ldxdw r2, [r1+0x7]                      
    ldxb r4, [r1+0xf]                       
    stxb [r10-0x28], r4                     
    stxdw [r10-0x30], r2                    
    jeq r3, 33, lbb_13754                           if r3 == (33 as i32 as i64 as u64) { pc += 18 }
    lddw r1, 0x10002d179 --> b"Not all bytes read"        r1 load str located at 4295151993
    mov64 r2, 18                                    r2 = 18 as i32 as i64 as u64
    call function_71                        
    ja lbb_13744                                    if true { pc += 3 }
lbb_13741:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
lbb_13744:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r2, r0                                    r2 = r0
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    jne r2, 1, lbb_13799                            if r2 != (1 as i32 as i64 as u64) { pc += 51 }
    ldxdw r1, [r0+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_13799                            if r2 == (0 as i32 as i64 as u64) { pc += 48 }
    ldxdw r1, [r0-0x1]                      
    callx r2                                
    ja lbb_13799                                    if true { pc += 45 }
lbb_13754:
    mov64 r3, r1                                    r3 = r1
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    ldxdw r4, [r10-0x2f]                    
    ldxb r5, [r3+0x2]                       
    stxb [r10-0x4e], r5                     
    ldxh r3, [r3+0x0]                       
    stxh [r10-0x50], r3                     
    ldxh r3, [r1+0x4]                       
    ldxb r5, [r1+0x6]                       
    stxw [r10-0x49], r4                     
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    stxw [r10-0x45], r4                     
    lsh64 r5, 16                                    r5 <<= 16   ///  r5 = r5.wrapping_shl(16)
    or64 r3, r5                                     r3 |= r5   ///  r3 = r3.or(r5)
    lsh64 r2, 24                                    r2 <<= 24   ///  r2 = r2.wrapping_shl(24)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    stxw [r10-0x4d], r2                     
    ldxdw r2, [r1+0x10]                     
    stxdw [r10-0x41], r2                    
    ldxdw r2, [r1+0x18]                     
    stxdw [r10-0x39], r2                    
    ldxb r1, [r1+0x20]                      
    stxb [r10-0x31], r1                     
    mov64 r7, 10                                    r7 = 10 as i32 as i64 as u64
    jeq r9, 0, lbb_13797                            if r9 == (0 as i32 as i64 as u64) { pc += 18 }
    ldxdw r1, [r8+0x0]                      
    ldxb r1, [r1+0x2]                       
    lddw r7, 0x200000000                            r7 load str located at 8589934592
    jeq r1, 0, lbb_13797                            if r1 == (0 as i32 as i64 as u64) { pc += 13 }
    mov64 r7, 10                                    r7 = 10 as i32 as i64 as u64
    jeq r9, 1, lbb_13797                            if r9 == (1 as i32 as i64 as u64) { pc += 11 }
    ldxdw r1, [r8+0x8]                      
    ldxb r2, [r1+0x2]                       
    lddw r7, 0x200000000                            r7 load str located at 8589934592
    jeq r2, 0, lbb_13797                            if r2 == (0 as i32 as i64 as u64) { pc += 6 }
    mov64 r7, 7                                     r7 = 7 as i32 as i64 as u64
    ldxb r1, [r1+0x1]                       
    jeq r1, 0, lbb_13797                            if r1 == (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r7, 10                                    r7 = 10 as i32 as i64 as u64
    jeq r9, 2, lbb_13797                            if r9 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_13802                                    if true { pc += 5 }
lbb_13797:
    mov64 r1, r7                                    r1 = r7
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
lbb_13799:
    stxw [r6+0x4], r1                       
    stxw [r6+0x0], r7                       
    exit                                    
lbb_13802:
    mov64 r2, r9                                    r2 = r9
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    add64 r2, -24                                   r2 += -24   ///  r2 = r2.wrapping_add(-24 as i32 as i64 as u64)
    lddw r3, 0x7ffffffffffffff8                     r3 load str located at 9223372036854775800
    jgt r2, r3, lbb_13831                           if r2 > r3 { pc += 22 }
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jeq r9, 3, lbb_13851                            if r9 == (3 as i32 as i64 as u64) { pc += 39 }
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r4, 0x300008000                            r4 load str located at 12884934656
    jeq r1, 0, lbb_13819                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r1                                    r4 = r1
lbb_13819:
    mov64 r1, r4                                    r1 = r4
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jgt r1, r4, lbb_13825                           if r1 > r4 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_13825:
    jne r5, 0, lbb_13827                            if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r1                                    r3 = r1
lbb_13827:
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    lddw r4, 0x300000007                            r4 load str located at 12884901895
    jgt r3, r4, lbb_13834                           if r3 > r4 { pc += 3 }
lbb_13831:
    lddw r3, 0x10002f560 --> b"\x00\x00\x00\x00j\xd0\x02\x00b\x00\x00\x00\x00\x00\x00\x00\xb3\x07\x00\x0…        r3 load str located at 4295161184
    call function_18348                     
lbb_13834:
    mov64 r5, r8                                    r5 = r8
    add64 r5, 24                                    r5 += 24   ///  r5 = r5.wrapping_add(24 as i32 as i64 as u64)
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 3                                     r1 >>= 3   ///  r1 = r1.wrapping_shr(3)
    and64 r3, -8                                    r3 &= -8   ///  r3 = r3.and(-8)
    lddw r4, 0x300000000                            r4 load str located at 12884901888
    stxdw [r4+0x0], r3                      
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r0, r3                                    r0 = r3
lbb_13844:
    stxdw [r0+0x0], r5                      
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, -8                                    r2 += -8   ///  r2 = r2.wrapping_add(-8 as i32 as i64 as u64)
    jeq r2, 0, lbb_13851                            if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_13844                                    if true { pc += -7 }
lbb_13851:
    mov64 r2, r8                                    r2 = r8
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r5, r8                                    r5 = r8
    add64 r5, 16                                    r5 += 16   ///  r5 = r5.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x8], r5                     
    stxdw [r10-0x10], r2                    
    stxdw [r10-0x18], r8                    
    stxdw [r10-0x20], r4                    
    stxdw [r10-0x30], r1                    
    stxw [r10-0x28], r3                     
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    stxw [r10-0x24], r3                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -48                                   r3 += -48   ///  r3 = r3.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -80                                   r4 += -80   ///  r4 = r4.wrapping_add(-80 as i32 as i64 as u64)
    ldxdw r2, [r10-0x60]                    
    call function_10600                     
    ldxw r1, [r10-0x54]                     
    ldxw r7, [r10-0x58]                     
    ja lbb_13799                                    if true { pc += -75 }
lbb_13874:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    lddw r3, 0x10002f4e8 --> b"\x00\x00\x00\x00\xf9\xd2\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00\x92\x00\…        r3 load str located at 4295161064
    call function_20246                     
    mov64 r8, r1                                    r8 = r1
    ldxdw r1, [r5-0xff8]                    
    jeq r1, 0, lbb_14100                            if r1 == (0 as i32 as i64 as u64) { pc += 218 }
    jlt r1, 5, lbb_13924                            if r1 < (5 as i32 as i64 as u64) { pc += 41 }
    ldxdw r5, [r5-0x1000]                   
    ldxw r0, [r5+0x1]                       
    add64 r5, 5                                     r5 += 5   ///  r5 = r5.wrapping_add(5 as i32 as i64 as u64)
    stxdw [r10-0xc8], r5                    
    add64 r1, -5                                    r1 += -5   ///  r1 = r1.wrapping_add(-5 as i32 as i64 as u64)
    stxdw [r10-0xc0], r1                    
    stxdw [r10-0xe0], r0                    
    jeq r0, 0, lbb_13995                            if r0 == (0 as i32 as i64 as u64) { pc += 104 }
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r0, 0x300008000                            r0 load str located at 12884934656
    jeq r1, 0, lbb_13898                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, r1                                    r0 = r1
lbb_13898:
    ldxdw r5, [r10-0xe0]                    
    mov64 r1, r5                                    r1 = r5
    jlt r5, 51, lbb_13902                           if r5 < (51 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 51                                    r1 = 51 as i32 as i64 as u64
lbb_13902:
    mov64 r5, r1                                    r5 = r1
    mul64 r5, 80                                    r5 *= 80   ///  r5 = r5.wrapping_mul(80 as u64)
    mov64 r6, r0                                    r6 = r0
    sub64 r6, r5                                    r6 -= r5   ///  r6 = r6.wrapping_sub(r5)
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jgt r6, r0, lbb_13909                           if r6 > r0 { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_13909:
    stxdw [r10-0xf8], r4                    
    stxdw [r10-0x100], r3                   
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jne r7, 0, lbb_13914                            if r7 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r6                                    r3 = r6
lbb_13914:
    stxdw [r10-0xf0], r8                    
    stxdw [r10-0x108], r2                   
    lddw r2, 0x300000007                            r2 load str located at 12884901895
    jgt r3, r2, lbb_13928                           if r3 > r2 { pc += 9 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, r5                                    r2 = r5
    lddw r3, 0x10002eae0 --> b"\x00\x00\x00\x00(\xd0\x02\x00\x0d\x00\x00\x00\x00\x00\x00\x00\x9f\x01\x00…        r3 load str located at 4295158496
    call function_18348                     
lbb_13924:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    ja lbb_14004                                    if true { pc += 76 }
lbb_13928:
    and64 r3, -8                                    r3 &= -8   ///  r3 = r3.and(-8)
    lddw r2, 0x300000000                            r2 load str located at 12884901888
    stxdw [r2+0x0], r3                      
    stxdw [r10-0xb8], r1                    
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    stxdw [r10-0xe8], r1                    
    mov64 r7, r3                                    r7 = r3
    stxdw [r10-0xb0], r3                    
    stdw [r10-0xa8], 0                      
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ja lbb_13969                                    if true { pc += 27 }
lbb_13942:
    mov64 r7, r2                                    r7 = r2
    mov64 r1, r2                                    r1 = r2
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    stxdw [r1+0x0], r9                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -72                                   r2 += -72   ///  r2 = r2.wrapping_add(-72 as i32 as i64 as u64)
    mov64 r3, 72                                    r3 = 72 as i32 as i64 as u64
    call function_21513                     
    ldxdw r9, [r10-0xd8]                    
    stxdw [r10-0xa8], r9                    
    add64 r8, 80                                    r8 += 80   ///  r8 = r8.wrapping_add(80 as i32 as i64 as u64)
    mov64 r6, r9                                    r6 = r9
    ldxdw r1, [r10-0xe0]                    
    jlt r9, r1, lbb_13969                           if r9 < r1 { pc += 12 }
    ldxdw r0, [r10-0xb0]                    
    ldxdw r1, [r10-0xb8]                    
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    ldxdw r8, [r10-0xf0]                    
    jeq r1, r2, lbb_14004                           if r1 == r2 { pc += 41 }
    ldxdw r1, [r10-0xc0]                    
    ldxdw r2, [r10-0x108]                   
    ldxdw r3, [r10-0x100]                   
    ldxdw r4, [r10-0xf8]                    
    jeq r1, 0, lbb_14016                            if r1 == (0 as i32 as i64 as u64) { pc += 48 }
    ja lbb_13998                                    if true { pc += 29 }
lbb_13969:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -160                                  r1 += -160   ///  r1 = r1.wrapping_add(-160 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -200                                  r2 += -200   ///  r2 = r2.wrapping_add(-200 as i32 as i64 as u64)
    call function_14766                     
    ldxdw r0, [r10-0x98]                    
    ldxdw r1, [r10-0xa0]                    
    jne r1, 0, lbb_14003                            if r1 != (0 as i32 as i64 as u64) { pc += 26 }
    mov64 r9, r0                                    r9 = r0
    mov64 r1, r6                                    r1 = r6
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0xd8], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    ldxdw r2, [r10-0xe8]                    
    mov64 r3, 72                                    r3 = 72 as i32 as i64 as u64
    call function_21513                     
    ldxdw r1, [r10-0xb8]                    
    mov64 r2, r7                                    r2 = r7
    jeq r6, r1, lbb_13990                           if r6 == r1 { pc += 1 }
    ja lbb_13942                                    if true { pc += -48 }
lbb_13990:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -184                                  r1 += -184   ///  r1 = r1.wrapping_add(-184 as i32 as i64 as u64)
    call function_474                       
    ldxdw r2, [r10-0xb0]                    
    ja lbb_13942                                    if true { pc += -53 }
lbb_13995:
    mov64 r0, 8                                     r0 = 8 as i32 as i64 as u64
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_14016                            if r1 == (0 as i32 as i64 as u64) { pc += 18 }
lbb_13998:
    lddw r1, 0x10002d179 --> b"Not all bytes read"        r1 load str located at 4295151993
    mov64 r2, 18                                    r2 = 18 as i32 as i64 as u64
    call function_71                        
    ja lbb_14004                                    if true { pc += 1 }
lbb_14003:
    ldxdw r8, [r10-0xf0]                    
lbb_14004:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r1, r0                                    r1 = r0
    and64 r1, 3                                     r1 &= 3   ///  r1 = r1.and(3)
    jne r1, 1, lbb_14013                            if r1 != (1 as i32 as i64 as u64) { pc += 5 }
    ldxdw r1, [r0+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_14013                            if r2 == (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r0-0x1]                      
    callx r2                                
lbb_14013:
    stxw [r8+0x4], r3                       
    stxw [r8+0x0], r6                       
    exit                                    
lbb_14016:
    mov64 r6, 10                                    r6 = 10 as i32 as i64 as u64
    jeq r4, 0, lbb_14056                            if r4 == (0 as i32 as i64 as u64) { pc += 38 }
    mov64 r6, 7                                     r6 = 7 as i32 as i64 as u64
    ldxdw r1, [r3+0x0]                      
    stxdw [r10-0xe0], r1                    
    ldxb r1, [r1+0x1]                       
    jeq r1, 0, lbb_14056                            if r1 == (0 as i32 as i64 as u64) { pc += 33 }
    mov64 r7, r0                                    r7 = r0
    mov64 r5, r4                                    r5 = r4
    lsh64 r5, 3                                     r5 <<= 3   ///  r5 = r5.wrapping_shl(3)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    add64 r5, -8                                    r5 += -8   ///  r5 = r5.wrapping_add(-8 as i32 as i64 as u64)
    lddw r0, 0x7ffffffffffffff8                     r0 load str located at 9223372036854775800
    jgt r5, r0, lbb_14052                           if r5 > r0 { pc += 21 }
    mov64 r6, 8                                     r6 = 8 as i32 as i64 as u64
    jeq r4, 1, lbb_14072                            if r4 == (1 as i32 as i64 as u64) { pc += 39 }
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r4, 0x300008000                            r4 load str located at 12884934656
    jeq r1, 0, lbb_14040                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r1                                    r4 = r1
lbb_14040:
    mov64 r1, r4                                    r1 = r4
    sub64 r1, r5                                    r1 -= r5   ///  r1 = r1.wrapping_sub(r5)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jgt r1, r4, lbb_14046                           if r1 > r4 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_14046:
    jne r0, 0, lbb_14048                            if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r1                                    r6 = r1
lbb_14048:
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    lddw r4, 0x300000007                            r4 load str located at 12884901895
    jgt r6, r4, lbb_14058                           if r6 > r4 { pc += 6 }
lbb_14052:
    mov64 r2, r5                                    r2 = r5
    lddw r3, 0x10002f560 --> b"\x00\x00\x00\x00j\xd0\x02\x00b\x00\x00\x00\x00\x00\x00\x00\xb3\x07\x00\x0…        r3 load str located at 4295161184
    call function_18348                     
lbb_14056:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ja lbb_14013                                    if true { pc += -45 }
lbb_14058:
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    and64 r6, -8                                    r6 &= -8   ///  r6 = r6.and(-8)
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    stxdw [r1+0x0], r6                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
lbb_14065:
    stxdw [r4+0x0], r3                      
    add64 r4, 8                                     r4 += 8   ///  r4 = r4.wrapping_add(8 as i32 as i64 as u64)
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    add64 r5, -8                                    r5 += -8   ///  r5 = r5.wrapping_add(-8 as i32 as i64 as u64)
    jeq r5, 0, lbb_14072                            if r5 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14065                                    if true { pc += -7 }
lbb_14072:
    stxdw [r10-0xd8], r6                    
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r5, r7                                    r5 = r7
    jne r1, r9, lbb_14013                           if r1 != r9 { pc += -63 }
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0xf0], r8                    
lbb_14078:
    mov64 r6, 26                                    r6 = 26 as i32 as i64 as u64
    add64 r9, -1                                    r9 += -1   ///  r9 = r9.wrapping_add(-1 as i32 as i64 as u64)
    jeq r9, 0, lbb_14013                            if r9 == (0 as i32 as i64 as u64) { pc += -68 }
    mov64 r8, r5                                    r8 = r5
    add64 r8, 80                                    r8 += 80   ///  r8 = r8.wrapping_add(80 as i32 as i64 as u64)
    ldxdw r3, [r10-0xd8]                    
    ldxdw r1, [r3+0x0]                      
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xd8], r3                    
    ldxdw r4, [r1+0x0]                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r6, r2                                    r6 = r2
    ldxdw r3, [r10-0xe0]                    
    call function_7544                      
    mov64 r2, r6                                    r2 = r6
    ldxw r6, [r10-0xd0]                     
    mov64 r5, r8                                    r5 = r8
    ldxdw r8, [r10-0xf0]                    
    jeq r6, 26, lbb_14078                           if r6 == (26 as i32 as i64 as u64) { pc += -20 }
    ldxw r3, [r10-0xcc]                     
    ja lbb_14013                                    if true { pc += -87 }
lbb_14100:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    lddw r3, 0x10002f500 --> b"\x00\x00\x00\x00\xf9\xd2\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00\x9a\x00\…        r3 load str located at 4295161088
    call function_20246                     
    mov64 r7, r1                                    r7 = r1
    ldxdw r6, [r5-0xff8]                    
    jeq r6, 0, lbb_14186                            if r6 == (0 as i32 as i64 as u64) { pc += 78 }
    jlt r6, 9, lbb_14164                            if r6 < (9 as i32 as i64 as u64) { pc += 55 }
    stxdw [r10-0x88], r7                    
    stxdw [r10-0x98], r4                    
    stxdw [r10-0x90], r3                    
    stxdw [r10-0xa8], r2                    
    ldxdw r7, [r5-0x1000]                   
    ldxdw r1, [r7+0x1]                      
    stxdw [r10-0xa0], r1                    
    mov64 r2, r1                                    r2 = r1
    call function_22497                     
    jne r0, 0, lbb_14301                            if r0 != (0 as i32 as i64 as u64) { pc += 182 }
    mov64 r8, r6                                    r8 = r6
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r9, r8                                    r9 = r8
    and64 r9, -8                                    r9 &= -8   ///  r9 = r9.and(-8)
    jeq r9, 8, lbb_14159                            if r9 == (8 as i32 as i64 as u64) { pc += 35 }
    ldxdw r1, [r7+0x9]                      
    stxdw [r10-0xb0], r1                    
    mov64 r2, r1                                    r2 = r1
    call function_22497                     
    jne r0, 0, lbb_14301                            if r0 != (0 as i32 as i64 as u64) { pc += 172 }
    jeq r9, 16, lbb_14159                           if r9 == (16 as i32 as i64 as u64) { pc += 29 }
    ldxdw r1, [r7+0x11]                     
    stxdw [r10-0xb8], r1                    
    mov64 r2, r1                                    r2 = r1
    call function_22497                     
    jne r0, 0, lbb_14301                            if r0 != (0 as i32 as i64 as u64) { pc += 166 }
    jeq r9, 24, lbb_14159                           if r9 == (24 as i32 as i64 as u64) { pc += 23 }
    ldxdw r1, [r7+0x19]                     
    stxdw [r10-0xc0], r1                    
    mov64 r2, r1                                    r2 = r1
    call function_22497                     
    jne r0, 0, lbb_14301                            if r0 != (0 as i32 as i64 as u64) { pc += 160 }
    ldxdw r1, [r10-0x90]                    
    ldxdw r2, [r10-0x98]                    
    jeq r9, 32, lbb_14159                           if r9 == (32 as i32 as i64 as u64) { pc += 15 }
    mov64 r3, r8                                    r3 = r8
    and64 r3, -4                                    r3 &= -4   ///  r3 = r3.and(-4)
    jeq r3, 40, lbb_14159                           if r3 == (40 as i32 as i64 as u64) { pc += 12 }
    mov64 r3, r6                                    r3 = r6
    add64 r3, -45                                   r3 += -45   ///  r3 = r3.wrapping_add(-45 as i32 as i64 as u64)
    jlt r3, 8, lbb_14159                            if r3 < (8 as i32 as i64 as u64) { pc += 9 }
    mov64 r3, r6                                    r3 = r6
    add64 r3, -53                                   r3 += -53   ///  r3 = r3.wrapping_add(-53 as i32 as i64 as u64)
    jlt r3, 8, lbb_14159                            if r3 < (8 as i32 as i64 as u64) { pc += 6 }
    mov64 r3, r6                                    r3 = r6
    add64 r3, -61                                   r3 += -61   ///  r3 = r3.wrapping_add(-61 as i32 as i64 as u64)
    jlt r3, 8, lbb_14159                            if r3 < (8 as i32 as i64 as u64) { pc += 3 }
    add64 r6, -69                                   r6 += -69   ///  r6 = r6.wrapping_add(-69 as i32 as i64 as u64)
    jlt r6, 8, lbb_14159                            if r6 < (8 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14179                                    if true { pc += 20 }
lbb_14159:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    ldxdw r7, [r10-0x88]                    
    ja lbb_14167                                    if true { pc += 3 }
lbb_14164:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
lbb_14167:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r2, r0                                    r2 = r0
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    jne r2, 1, lbb_14176                            if r2 != (1 as i32 as i64 as u64) { pc += 5 }
    ldxdw r1, [r0+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_14176                            if r2 == (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r0-0x1]                      
    callx r2                                
lbb_14176:
    stxw [r7+0x4], r1                       
    stxw [r7+0x0], r6                       
    exit                                    
lbb_14179:
    jeq r8, 76, lbb_14191                           if r8 == (76 as i32 as i64 as u64) { pc += 11 }
    lddw r1, 0x10002d179 --> b"Not all bytes read"        r1 load str located at 4295151993
    mov64 r2, 18                                    r2 = 18 as i32 as i64 as u64
lbb_14183:
    call function_71                        
    ldxdw r7, [r10-0x88]                    
    ja lbb_14167                                    if true { pc += -19 }
lbb_14186:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    lddw r3, 0x10002f518 --> b"\x00\x00\x00\x00\xf9\xd2\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00\xa2\x00\…        r3 load str located at 4295161112
    call function_20246                     
lbb_14191:
    ldxdw r3, [r7+0x21]                     
    ldxw r4, [r7+0x29]                      
    ldxdw r5, [r7+0x2d]                     
    ldxdw r0, [r7+0x35]                     
    ldxdw r6, [r7+0x3d]                     
    ldxdw r7, [r7+0x45]                     
    stxw [r10-0x30], r4                     
    stxdw [r10-0x38], r7                    
    stxdw [r10-0x40], r6                    
    stxdw [r10-0x48], r0                    
    stxdw [r10-0x50], r5                    
    stxdw [r10-0x58], r3                    
    ldxdw r3, [r10-0xc0]                    
    stxdw [r10-0x60], r3                    
    ldxdw r3, [r10-0xb8]                    
    stxdw [r10-0x68], r3                    
    ldxdw r3, [r10-0xb0]                    
    stxdw [r10-0x70], r3                    
    ldxdw r4, [r10-0xa0]                    
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    stxw [r10-0x74], r3                     
    stxw [r10-0x78], r4                     
    mov64 r6, 10                                    r6 = 10 as i32 as i64 as u64
    jeq r2, 0, lbb_14259                            if r2 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r6, 7                                     r6 = 7 as i32 as i64 as u64
    ldxdw r3, [r1+0x0]                      
    ldxb r3, [r3+0x1]                       
    jeq r3, 0, lbb_14259                            if r3 == (0 as i32 as i64 as u64) { pc += 39 }
    mov64 r6, 10                                    r6 = 10 as i32 as i64 as u64
    jeq r2, 1, lbb_14259                            if r2 == (1 as i32 as i64 as u64) { pc += 37 }
    lddw r6, 0x200000000                            r6 load str located at 8589934592
    ldxdw r1, [r1+0x8]                      
    ldxb r1, [r1+0x2]                       
    jeq r1, 0, lbb_14259                            if r1 == (0 as i32 as i64 as u64) { pc += 32 }
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    add64 r2, -16                                   r2 += -16   ///  r2 = r2.wrapping_add(-16 as i32 as i64 as u64)
    lddw r3, 0x7ffffffffffffff8                     r3 load str located at 9223372036854775800
    jgt r2, r3, lbb_14256                           if r2 > r3 { pc += 23 }
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r5, [r10-0x98]                    
    jeq r5, 2, lbb_14280                            if r5 == (2 as i32 as i64 as u64) { pc += 43 }
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r4, 0x300008000                            r4 load str located at 12884934656
    jeq r1, 0, lbb_14244                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r1                                    r4 = r1
lbb_14244:
    mov64 r1, r4                                    r1 = r4
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jgt r1, r4, lbb_14250                           if r1 > r4 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_14250:
    jne r5, 0, lbb_14252                            if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r1                                    r3 = r1
lbb_14252:
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    lddw r4, 0x300000007                            r4 load str located at 12884901895
    jgt r3, r4, lbb_14263                           if r3 > r4 { pc += 7 }
lbb_14256:
    lddw r3, 0x10002f560 --> b"\x00\x00\x00\x00j\xd0\x02\x00b\x00\x00\x00\x00\x00\x00\x00\xb3\x07\x00\x0…        r3 load str located at 4295161184
    call function_18348                     
lbb_14259:
    mov64 r1, r6                                    r1 = r6
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
lbb_14261:
    ldxdw r7, [r10-0x88]                    
    ja lbb_14176                                    if true { pc += -87 }
lbb_14263:
    ldxdw r5, [r10-0x90]                    
    add64 r5, 16                                    r5 += 16   ///  r5 = r5.wrapping_add(16 as i32 as i64 as u64)
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 3                                     r1 >>= 3   ///  r1 = r1.wrapping_shr(3)
    and64 r3, -8                                    r3 &= -8   ///  r3 = r3.and(-8)
    lddw r4, 0x300000000                            r4 load str located at 12884901888
    stxdw [r4+0x0], r3                      
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r0, r3                                    r0 = r3
lbb_14273:
    stxdw [r0+0x0], r5                      
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, -8                                    r2 += -8   ///  r2 = r2.wrapping_add(-8 as i32 as i64 as u64)
    jeq r2, 0, lbb_14280                            if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14273                                    if true { pc += -7 }
lbb_14280:
    ldxdw r2, [r10-0x90]                    
    mov64 r5, r2                                    r5 = r2
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x8], r5                     
    stxdw [r10-0x10], r2                    
    stxdw [r10-0x18], r4                    
    stxdw [r10-0x28], r1                    
    stxw [r10-0x20], r3                     
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    stxw [r10-0x1c], r3                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -40                                   r3 += -40   ///  r3 = r3.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -120                                  r4 += -120   ///  r4 = r4.wrapping_add(-120 as i32 as i64 as u64)
    ldxdw r2, [r10-0xa8]                    
    call function_7682                      
    ldxw r1, [r10-0x7c]                     
    ldxw r6, [r10-0x80]                     
    ja lbb_14261                                    if true { pc += -40 }
lbb_14301:
    lddw r1, 0x10002d0cc --> b"For portability reasons we do not allow to deserialize NaNs."        r1 load str located at 4295151820
    mov64 r2, 60                                    r2 = 60 as i32 as i64 as u64
    ja lbb_14183                                    if true { pc += -122 }

function_14305:
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r5-0xff8]                    
    jeq r1, 1, lbb_14328                            if r1 == (1 as i32 as i64 as u64) { pc += 20 }
    jne r1, 0, lbb_14314                            if r1 != (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    lddw r3, 0x10002f530 --> b"\x00\x00\x00\x00\xf9\xd2\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00\xaa\x00\…        r3 load str located at 4295161136
    call function_20246                     
lbb_14314:
    lddw r1, 0x10002d179 --> b"Not all bytes read"        r1 load str located at 4295151993
    mov64 r2, 18                                    r2 = 18 as i32 as i64 as u64
    call function_71                        
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r2, r0                                    r2 = r0
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    jne r2, 1, lbb_14376                            if r2 != (1 as i32 as i64 as u64) { pc += 54 }
    ldxdw r1, [r0+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_14376                            if r2 == (0 as i32 as i64 as u64) { pc += 51 }
    ldxdw r1, [r0-0x1]                      
    callx r2                                
    ja lbb_14376                                    if true { pc += 48 }
lbb_14328:
    mov64 r7, 10                                    r7 = 10 as i32 as i64 as u64
    jeq r4, 0, lbb_14374                            if r4 == (0 as i32 as i64 as u64) { pc += 44 }
    mov64 r7, 7                                     r7 = 7 as i32 as i64 as u64
    ldxdw r1, [r3+0x0]                      
    ldxb r1, [r1+0x1]                       
    jeq r1, 0, lbb_14374                            if r1 == (0 as i32 as i64 as u64) { pc += 40 }
    mov64 r7, 10                                    r7 = 10 as i32 as i64 as u64
    jeq r4, 1, lbb_14374                            if r4 == (1 as i32 as i64 as u64) { pc += 38 }
    lddw r7, 0x200000000                            r7 load str located at 8589934592
    ldxdw r1, [r3+0x8]                      
    ldxb r1, [r1+0x2]                       
    jeq r1, 0, lbb_14374                            if r1 == (0 as i32 as i64 as u64) { pc += 33 }
    mov64 r5, r4                                    r5 = r4
    lsh64 r5, 3                                     r5 <<= 3   ///  r5 = r5.wrapping_shl(3)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    add64 r5, -16                                   r5 += -16   ///  r5 = r5.wrapping_add(-16 as i32 as i64 as u64)
    lddw r0, 0x7ffffffffffffff8                     r0 load str located at 9223372036854775800
    jgt r5, r0, lbb_14370                           if r5 > r0 { pc += 22 }
    mov64 r0, 8                                     r0 = 8 as i32 as i64 as u64
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jeq r4, 2, lbb_14396                            if r4 == (2 as i32 as i64 as u64) { pc += 45 }
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r4, 0x300008000                            r4 load str located at 12884934656
    jeq r1, 0, lbb_14358                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r1                                    r4 = r1
lbb_14358:
    mov64 r1, r4                                    r1 = r4
    sub64 r1, r5                                    r1 -= r5   ///  r1 = r1.wrapping_sub(r5)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jgt r1, r4, lbb_14364                           if r1 > r4 { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_14364:
    jne r7, 0, lbb_14366                            if r7 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, r1                                    r0 = r1
lbb_14366:
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    lddw r4, 0x300000007                            r4 load str located at 12884901895
    jgt r0, r4, lbb_14379                           if r0 > r4 { pc += 9 }
lbb_14370:
    mov64 r2, r5                                    r2 = r5
    lddw r3, 0x10002f560 --> b"\x00\x00\x00\x00j\xd0\x02\x00b\x00\x00\x00\x00\x00\x00\x00\xb3\x07\x00\x0…        r3 load str located at 4295161184
    call function_18348                     
lbb_14374:
    mov64 r1, r7                                    r1 = r7
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
lbb_14376:
    stxw [r6+0x4], r1                       
    stxw [r6+0x0], r7                       
    exit                                    
lbb_14379:
    mov64 r4, r3                                    r4 = r3
    add64 r4, 16                                    r4 += 16   ///  r4 = r4.wrapping_add(16 as i32 as i64 as u64)
    mov64 r1, r5                                    r1 = r5
    rsh64 r1, 3                                     r1 >>= 3   ///  r1 = r1.wrapping_shr(3)
    and64 r0, -8                                    r0 &= -8   ///  r0 = r0.and(-8)
    lddw r7, 0x300000000                            r7 load str located at 12884901888
    stxdw [r7+0x0], r0                      
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r8, r0                                    r8 = r0
lbb_14389:
    stxdw [r8+0x0], r4                      
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    add64 r4, 8                                     r4 += 8   ///  r4 = r4.wrapping_add(8 as i32 as i64 as u64)
    add64 r5, -8                                    r5 += -8   ///  r5 = r5.wrapping_add(-8 as i32 as i64 as u64)
    jeq r5, 0, lbb_14396                            if r5 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14389                                    if true { pc += -7 }
lbb_14396:
    mov64 r4, r3                                    r4 = r3
    add64 r4, 8                                     r4 += 8   ///  r4 = r4.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x8], r4                     
    stxdw [r10-0x10], r3                    
    stxdw [r10-0x18], r7                    
    stxdw [r10-0x28], r1                    
    stxw [r10-0x20], r0                     
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    stxw [r10-0x1c], r0                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -40                                   r3 += -40   ///  r3 = r3.wrapping_add(-40 as i32 as i64 as u64)
    call function_10798                     
    ldxw r1, [r10-0x2c]                     
    ldxw r7, [r10-0x30]                     
    ja lbb_14376                                    if true { pc += -37 }

function_14413:
    mov64 r7, r5                                    r7 = r5
    stxdw [r10-0xe0], r4                    
    mov64 r9, r3                                    r9 = r3
    stxdw [r10-0xe8], r2                    
    mov64 r6, r1                                    r6 = r1
    lddw r1, 0x10002d96f --> b"Instruction: swap_with_limit_price"        r1 load str located at 4295154031
    mov64 r2, 34                                    r2 = 34 as i32 as i64 as u64
    syscall [invalid]                       
    ldxdw r8, [r7-0xff8]                    
    jeq r8, 0, lbb_14502                            if r8 == (0 as i32 as i64 as u64) { pc += 78 }
    jlt r8, 9, lbb_14445                            if r8 < (9 as i32 as i64 as u64) { pc += 20 }
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    jeq r1, 8, lbb_14445                            if r1 == (8 as i32 as i64 as u64) { pc += 16 }
    jeq r1, 16, lbb_14445                           if r1 == (16 as i32 as i64 as u64) { pc += 15 }
    ldxdw r1, [r7-0x1000]                   
    ldxdw r7, [r1+0x1]                      
    ldxdw r2, [r1+0x9]                      
    stxdw [r10-0xf8], r2                    
    ldxdw r1, [r1+0x11]                     
    stxdw [r10-0xf0], r1                    
    mov64 r2, r1                                    r2 = r1
    call function_22497                     
    jne r0, 0, lbb_14507                            if r0 != (0 as i32 as i64 as u64) { pc += 68 }
    jeq r8, 24, lbb_14460                           if r8 == (24 as i32 as i64 as u64) { pc += 20 }
    lddw r1, 0x10002d179 --> b"Not all bytes read"        r1 load str located at 4295151993
    mov64 r2, 18                                    r2 = 18 as i32 as i64 as u64
lbb_14443:
    call function_71                        
    ja lbb_14448                                    if true { pc += 3 }
lbb_14445:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
lbb_14448:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r1, r0                                    r1 = r0
    and64 r1, 3                                     r1 &= 3   ///  r1 = r1.and(3)
    jne r1, 1, lbb_14457                            if r1 != (1 as i32 as i64 as u64) { pc += 5 }
    ldxdw r1, [r0+0x7]                      
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_14457                            if r2 == (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r0-0x1]                      
    callx r2                                
lbb_14457:
    stxw [r6+0x4], r8                       
    stxw [r6+0x0], r7                       
    exit                                    
lbb_14460:
    stxdw [r10-0x100], r7                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    ldxdw r3, [r10-0xe0]                    
    call function_14928                     
    ldxw r8, [r10-0x5c]                     
    ldxw r7, [r10-0x60]                     
    ldxdw r9, [r10-0x68]                    
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    jeq r9, r1, lbb_14457                           if r9 == r1 { pc += -15 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -192                                  r1 += -192   ///  r1 = r1.wrapping_add(-192 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -88                                   r2 += -88   ///  r2 = r2.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r3, 88                                    r3 = 88 as i32 as i64 as u64
    call function_21513                     
    stxw [r10-0xc4], r8                     
    stxw [r10-0xc8], r7                     
    stxdw [r10-0xd0], r9                    
    ldxdw r1, [r10-0xf8]                    
    stxdw [r10-0x60], r1                    
    ldxdw r1, [r10-0x100]                   
    stxdw [r10-0x68], r1                    
    ldxdw r1, [r10-0xf0]                    
    stxdw [r10-0xfe8], r1                   
    stdw [r10-0xff0], 1                     
    stdw [r10-0xff8], 0                     
    stdw [r10-0x1000], 1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -216                                  r1 += -216   ///  r1 = r1.wrapping_add(-216 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -208                                  r3 += -208   ///  r3 = r3.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -104                                  r4 += -104   ///  r4 = r4.wrapping_add(-104 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0xe8]                    
    call function_7831                      
    ldxw r8, [r10-0xd4]                     
    ldxw r7, [r10-0xd8]                     
    ja lbb_14457                                    if true { pc += -45 }
lbb_14502:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    lddw r3, 0x10002f548 --> b"\x00\x00\x00\x00\xf9\xd2\x02\x00\x1e\x00\x00\x00\x00\x00\x00\x00\xb2\x00\…        r3 load str located at 4295161160
    call function_20246                     
lbb_14507:
    lddw r1, 0x10002d0cc --> b"For portability reasons we do not allow to deserialize NaNs."        r1 load str located at 4295151820
    mov64 r2, 60                                    r2 = 60 as i32 as i64 as u64
    ja lbb_14443                                    if true { pc += -68 }

function_14511:
    mov64 r4, r3                                    r4 = r3
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r2                                    r5 = r2
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    stxdw [r10-0x8], r5                     
    jeq r3, 0, lbb_14519                            if r3 == (0 as i32 as i64 as u64) { pc += 2 }
    jeq r3, 1, lbb_14519                            if r3 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14524                                    if true { pc += 5 }
lbb_14519:
    stdw [r1+0x8], 10                       
lbb_14520:
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    stxdw [r1+0x0], r2                      
lbb_14523:
    exit                                    
lbb_14524:
    ldxdw r4, [r2+0x8]                      
    ldxb r5, [r4+0x2]                       
    jne r5, 0, lbb_14531                            if r5 != (0 as i32 as i64 as u64) { pc += 4 }
lbb_14527:
    lddw r2, 0x200000000                            r2 load str located at 8589934592
    stxdw [r1+0x8], r2                      
    ja lbb_14520                                    if true { pc += -11 }
lbb_14531:
    ldxb r4, [r4+0x1]                       
    jne r4, 0, lbb_14535                            if r4 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_14533:
    stw [r1+0x8], 7                         
    ja lbb_14520                                    if true { pc += -15 }
lbb_14535:
    jeq r3, 2, lbb_14519                            if r3 == (2 as i32 as i64 as u64) { pc += -17 }
    ldxdw r4, [r2+0x10]                     
    ldxb r5, [r4+0x2]                       
    jne r5, 0, lbb_14540                            if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14527                                    if true { pc += -13 }
lbb_14540:
    ldxb r4, [r4+0x1]                       
    jne r4, 0, lbb_14543                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14533                                    if true { pc += -10 }
lbb_14543:
    jeq r3, 3, lbb_14519                            if r3 == (3 as i32 as i64 as u64) { pc += -25 }
    jne r3, 4, lbb_14546                            if r3 != (4 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14519                                    if true { pc += -27 }
lbb_14546:
    ldxdw r4, [r2+0x20]                     
    ldxb r4, [r4+0x2]                       
    jne r4, 0, lbb_14550                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14527                                    if true { pc += -23 }
lbb_14550:
    jeq r3, 7, lbb_14519                            if r3 == (7 as i32 as i64 as u64) { pc += -32 }
    jeq r3, 6, lbb_14519                            if r3 == (6 as i32 as i64 as u64) { pc += -33 }
    jne r3, 5, lbb_14554                            if r3 != (5 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14519                                    if true { pc += -35 }
lbb_14554:
    mov64 r3, r2                                    r3 = r2
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x18], r3                    
    mov64 r3, r2                                    r3 = r2
    add64 r3, 16                                    r3 += 16   ///  r3 = r3.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x20], r3                    
    mov64 r3, r2                                    r3 = r2
    add64 r3, 24                                    r3 += 24   ///  r3 = r3.wrapping_add(24 as i32 as i64 as u64)
    stxdw [r10-0x28], r3                    
    mov64 r7, r2                                    r7 = r2
    add64 r7, 32                                    r7 += 32   ///  r7 = r7.wrapping_add(32 as i32 as i64 as u64)
    mov64 r8, r2                                    r8 = r2
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    add64 r3, 64                                    r3 += 64   ///  r3 = r3.wrapping_add(64 as i32 as i64 as u64)
    stxdw [r10-0x10], r3                    
    mov64 r3, r10                                   r3 = r10
    add64 r3, -16                                   r3 += -16   ///  r3 = r3.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r6, r1                                    r6 = r1
    mov64 r9, r2                                    r9 = r2
    mov64 r2, r3                                    r2 = r3
    call function_2160                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 56                                    r1 += 56   ///  r1 = r1.wrapping_add(56 as i32 as i64 as u64)
    stxdw [r6+0x50], r1                     
    mov64 r1, r9                                    r1 = r9
    add64 r1, 48                                    r1 += 48   ///  r1 = r1.wrapping_add(48 as i32 as i64 as u64)
    stxdw [r6+0x48], r1                     
    stxdw [r6+0x40], r8                     
    stxdw [r6+0x38], r7                     
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x30], r1                     
    ldxdw r1, [r10-0x20]                    
    stxdw [r6+0x28], r1                     
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x20], r1                     
    stxdw [r6+0x18], r9                     
    ja lbb_14523                                    if true { pc += -69 }

function_14592:
    mov64 r4, r3                                    r4 = r3
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r2                                    r5 = r2
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    stxdw [r10-0x8], r5                     
    jeq r3, 0, lbb_14600                            if r3 == (0 as i32 as i64 as u64) { pc += 2 }
    jeq r3, 1, lbb_14600                            if r3 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14605                                    if true { pc += 5 }
lbb_14600:
    stdw [r1+0x8], 10                       
lbb_14601:
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    stxdw [r1+0x0], r2                      
lbb_14604:
    exit                                    
lbb_14605:
    ldxdw r4, [r2+0x8]                      
    ldxb r5, [r4+0x2]                       
    jne r5, 0, lbb_14612                            if r5 != (0 as i32 as i64 as u64) { pc += 4 }
lbb_14608:
    lddw r2, 0x200000000                            r2 load str located at 8589934592
    stxdw [r1+0x8], r2                      
    ja lbb_14601                                    if true { pc += -11 }
lbb_14612:
    ldxb r4, [r4+0x1]                       
    jne r4, 0, lbb_14616                            if r4 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_14614:
    stw [r1+0x8], 7                         
    ja lbb_14601                                    if true { pc += -15 }
lbb_14616:
    jeq r3, 2, lbb_14600                            if r3 == (2 as i32 as i64 as u64) { pc += -17 }
    mov64 r7, r2                                    r7 = r2
    add64 r7, 24                                    r7 += 24   ///  r7 = r7.wrapping_add(24 as i32 as i64 as u64)
    stxdw [r10-0x10], r7                    
    ldxdw r4, [r2+0x10]                     
    ldxb r5, [r4+0x2]                       
    jne r5, 0, lbb_14624                            if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14608                                    if true { pc += -16 }
lbb_14624:
    ldxb r4, [r4+0x1]                       
    jne r4, 0, lbb_14627                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14614                                    if true { pc += -13 }
lbb_14627:
    jeq r3, 3, lbb_14600                            if r3 == (3 as i32 as i64 as u64) { pc += -28 }
    jne r3, 4, lbb_14630                            if r3 != (4 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14600                                    if true { pc += -30 }
lbb_14630:
    jne r3, 5, lbb_14632                            if r3 != (5 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14600                                    if true { pc += -32 }
lbb_14632:
    ldxdw r4, [r2+0x28]                     
    ldxb r4, [r4+0x1]                       
    jeq r4, 0, lbb_14614                            if r4 == (0 as i32 as i64 as u64) { pc += -21 }
    jeq r3, 6, lbb_14600                            if r3 == (6 as i32 as i64 as u64) { pc += -36 }
    jne r3, 7, lbb_14638                            if r3 != (7 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14600                                    if true { pc += -38 }
lbb_14638:
    jne r3, 8, lbb_14640                            if r3 != (8 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14600                                    if true { pc += -40 }
lbb_14640:
    ldxdw r4, [r2+0x40]                     
    ldxb r4, [r4+0x1]                       
    jeq r4, 0, lbb_14614                            if r4 == (0 as i32 as i64 as u64) { pc += -29 }
    jeq r3, 9, lbb_14600                            if r3 == (9 as i32 as i64 as u64) { pc += -44 }
    jeq r3, 10, lbb_14600                           if r3 == (10 as i32 as i64 as u64) { pc += -45 }
    mov64 r3, r2                                    r3 = r2
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x18], r3                    
    mov64 r3, r2                                    r3 = r2
    add64 r3, 16                                    r3 += 16   ///  r3 = r3.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x20], r3                    
    mov64 r3, r2                                    r3 = r2
    add64 r3, 32                                    r3 += 32   ///  r3 = r3.wrapping_add(32 as i32 as i64 as u64)
    stxdw [r10-0x28], r3                    
    mov64 r3, r2                                    r3 = r2
    add64 r3, 40                                    r3 += 40   ///  r3 = r3.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x30], r3                    
    mov64 r3, r2                                    r3 = r2
    add64 r3, 48                                    r3 += 48   ///  r3 = r3.wrapping_add(48 as i32 as i64 as u64)
    stxdw [r10-0x38], r3                    
    mov64 r3, r2                                    r3 = r2
    add64 r3, 56                                    r3 += 56   ///  r3 = r3.wrapping_add(56 as i32 as i64 as u64)
    stxdw [r10-0x40], r3                    
    mov64 r8, r2                                    r8 = r2
    add64 r8, 64                                    r8 += 64   ///  r8 = r8.wrapping_add(64 as i32 as i64 as u64)
    mov64 r9, r2                                    r9 = r2
    add64 r9, 72                                    r9 += 72   ///  r9 = r9.wrapping_add(72 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    add64 r3, 88                                    r3 += 88   ///  r3 = r3.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x10], r3                    
    mov64 r3, r10                                   r3 = r10
    add64 r3, -16                                   r3 += -16   ///  r3 = r3.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r6, r1                                    r6 = r1
    stxdw [r10-0x48], r2                    
    mov64 r2, r3                                    r2 = r3
    call function_2160                      
    ldxdw r2, [r10-0x48]                    
    mov64 r1, r2                                    r1 = r2
    add64 r1, 80                                    r1 += 80   ///  r1 = r1.wrapping_add(80 as i32 as i64 as u64)
    stxdw [r6+0x68], r1                     
    stxdw [r6+0x60], r9                     
    stxdw [r6+0x58], r8                     
    ldxdw r1, [r10-0x40]                    
    stxdw [r6+0x50], r1                     
    ldxdw r1, [r10-0x38]                    
    stxdw [r6+0x48], r1                     
    ldxdw r1, [r10-0x30]                    
    stxdw [r6+0x40], r1                     
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x38], r1                     
    stxdw [r6+0x30], r7                     
    ldxdw r1, [r10-0x20]                    
    stxdw [r6+0x28], r1                     
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x20], r1                     
    stxdw [r6+0x18], r2                     
    ja lbb_14604                                    if true { pc += -93 }

function_14697:
    mov64 r4, r3                                    r4 = r3
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r2                                    r5 = r2
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    stxdw [r10-0x8], r5                     
    jne r3, 0, lbb_14705                            if r3 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_14703:
    stdw [r1+0x8], 10                       
    ja lbb_14717                                    if true { pc += 12 }
lbb_14705:
    ldxdw r4, [r2+0x0]                      
    ldxb r4, [r4+0x1]                       
    jeq r4, 0, lbb_14716                            if r4 == (0 as i32 as i64 as u64) { pc += 8 }
    jeq r3, 1, lbb_14703                            if r3 == (1 as i32 as i64 as u64) { pc += -6 }
    ldxdw r4, [r2+0x8]                      
    ldxb r4, [r4+0x2]                       
    jne r4, 0, lbb_14721                            if r4 != (0 as i32 as i64 as u64) { pc += 9 }
lbb_14712:
    lddw r2, 0x200000000                            r2 load str located at 8589934592
    stxdw [r1+0x8], r2                      
    ja lbb_14717                                    if true { pc += 1 }
lbb_14716:
    stw [r1+0x8], 7                         
lbb_14717:
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    stxdw [r1+0x0], r2                      
lbb_14720:
    exit                                    
lbb_14721:
    jne r3, 2, lbb_14723                            if r3 != (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14703                                    if true { pc += -20 }
lbb_14723:
    ldxdw r4, [r2+0x10]                     
    ldxb r4, [r4+0x2]                       
    jne r4, 0, lbb_14727                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14712                                    if true { pc += -15 }
lbb_14727:
    jeq r3, 3, lbb_14703                            if r3 == (3 as i32 as i64 as u64) { pc += -25 }
    jeq r3, 4, lbb_14703                            if r3 == (4 as i32 as i64 as u64) { pc += -26 }
    ldxdw r4, [r2+0x20]                     
    ldxb r4, [r4+0x2]                       
    jne r4, 0, lbb_14733                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14712                                    if true { pc += -21 }
lbb_14733:
    jne r3, 5, lbb_14735                            if r3 != (5 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14703                                    if true { pc += -32 }
lbb_14735:
    mov64 r3, r2                                    r3 = r2
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x18], r3                    
    mov64 r3, r2                                    r3 = r2
    add64 r3, 16                                    r3 += 16   ///  r3 = r3.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x20], r3                    
    mov64 r9, r2                                    r9 = r2
    add64 r9, 24                                    r9 += 24   ///  r9 = r9.wrapping_add(24 as i32 as i64 as u64)
    mov64 r7, r2                                    r7 = r2
    add64 r7, 32                                    r7 += 32   ///  r7 = r7.wrapping_add(32 as i32 as i64 as u64)
    mov64 r8, r2                                    r8 = r2
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    add64 r3, 48                                    r3 += 48   ///  r3 = r3.wrapping_add(48 as i32 as i64 as u64)
    stxdw [r10-0x10], r3                    
    mov64 r3, r10                                   r3 = r10
    add64 r3, -16                                   r3 += -16   ///  r3 = r3.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r6, r1                                    r6 = r1
    stxdw [r10-0x28], r2                    
    mov64 r2, r3                                    r2 = r3
    call function_2160                      
    stxdw [r6+0x40], r8                     
    stxdw [r6+0x38], r7                     
    stxdw [r6+0x30], r9                     
    ldxdw r1, [r10-0x20]                    
    stxdw [r6+0x28], r1                     
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x20], r1                     
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x18], r1                     
    ja lbb_14720                                    if true { pc += -46 }

function_14766:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r9, [r7+0x0]                      
    ldxdw r8, [r7+0x8]                      
    jlt r8, 8, lbb_14845                            if r8 < (8 as i32 as i64 as u64) { pc += 74 }
    stxdw [r10-0x48], r6                    
    ldxdw r2, [r9+0x0]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r7+0x0], r1                      
    mov64 r6, r8                                    r6 = r8
    add64 r6, -8                                    r6 += -8   ///  r6 = r6.wrapping_add(-8 as i32 as i64 as u64)
    stxdw [r7+0x8], r6                      
    mov64 r1, r2                                    r1 = r2
    stxdw [r10-0x50], r2                    
    call function_22497                     
    jne r0, 0, lbb_14923                            if r0 != (0 as i32 as i64 as u64) { pc += 140 }
    jlt r6, 8, lbb_14852                            if r6 < (8 as i32 as i64 as u64) { pc += 68 }
    ldxdw r2, [r9+0x8]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 16                                    r1 += 16   ///  r1 = r1.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r7+0x0], r1                      
    mov64 r6, r8                                    r6 = r8
    add64 r6, -16                                   r6 += -16   ///  r6 = r6.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r7+0x8], r6                      
    mov64 r1, r2                                    r1 = r2
    stxdw [r10-0x58], r2                    
    call function_22497                     
    jne r0, 0, lbb_14923                            if r0 != (0 as i32 as i64 as u64) { pc += 128 }
    jlt r6, 8, lbb_14852                            if r6 < (8 as i32 as i64 as u64) { pc += 56 }
    ldxdw r2, [r9+0x10]                     
    mov64 r1, r9                                    r1 = r9
    add64 r1, 24                                    r1 += 24   ///  r1 = r1.wrapping_add(24 as i32 as i64 as u64)
    stxdw [r7+0x0], r1                      
    mov64 r6, r8                                    r6 = r8
    add64 r6, -24                                   r6 += -24   ///  r6 = r6.wrapping_add(-24 as i32 as i64 as u64)
    stxdw [r7+0x8], r6                      
    mov64 r1, r2                                    r1 = r2
    stxdw [r10-0x60], r2                    
    call function_22497                     
    jne r0, 0, lbb_14923                            if r0 != (0 as i32 as i64 as u64) { pc += 116 }
    jlt r6, 8, lbb_14852                            if r6 < (8 as i32 as i64 as u64) { pc += 44 }
    mov64 r2, r8                                    r2 = r8
    add64 r2, -32                                   r2 += -32   ///  r2 = r2.wrapping_add(-32 as i32 as i64 as u64)
    ldxdw r1, [r9+0x18]                     
    stxdw [r10-0x68], r2                    
    stxdw [r7+0x8], r2                      
    mov64 r2, r9                                    r2 = r9
    add64 r2, 32                                    r2 += 32   ///  r2 = r2.wrapping_add(32 as i32 as i64 as u64)
    stxdw [r10-0x70], r2                    
    stxdw [r7+0x0], r2                      
    stxdw [r10-0x78], r1                    
    mov64 r2, r1                                    r2 = r1
    call function_22497                     
    jne r0, 0, lbb_14923                            if r0 != (0 as i32 as i64 as u64) { pc += 102 }
    ldxdw r6, [r10-0x48]                    
    ldxdw r1, [r10-0x68]                    
    jlt r1, 8, lbb_14863                            if r1 < (8 as i32 as i64 as u64) { pc += 39 }
    mov64 r1, r8                                    r1 = r8
    and64 r1, -4                                    r1 &= -4   ///  r1 = r1.and(-4)
    jeq r1, 40, lbb_14863                           if r1 == (40 as i32 as i64 as u64) { pc += 36 }
    ldxdw r1, [r10-0x70]                    
    ldxdw r1, [r1+0x0]                      
    stxdw [r10-0x68], r1                    
    add64 r8, -44                                   r8 += -44   ///  r8 = r8.wrapping_add(-44 as i32 as i64 as u64)
    ldxw r6, [r9+0x28]                      
    stxdw [r7+0x8], r8                      
    add64 r9, 44                                    r9 += 44   ///  r9 = r9.wrapping_add(44 as i32 as i64 as u64)
    stxdw [r7+0x0], r9                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_197                       
    ldxdw r8, [r10-0x8]                     
    ldxdw r1, [r10-0x10]                    
    jeq r1, 0, lbb_14870                            if r1 == (0 as i32 as i64 as u64) { pc += 28 }
    ldxdw r6, [r10-0x48]                    
    stxdw [r6+0x8], r8                      
    ja lbb_14860                                    if true { pc += 15 }
lbb_14845:
    add64 r9, r8                                    r9 += r8   ///  r9 = r9.wrapping_add(r8)
    stxdw [r7+0x0], r9                      
    stdw [r7+0x8], 0                        
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    ja lbb_14859                                    if true { pc += 7 }
lbb_14852:
    add64 r9, r8                                    r9 += r8   ///  r9 = r9.wrapping_add(r8)
    stxdw [r7+0x0], r9                      
    stdw [r7+0x8], 0                        
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
lbb_14858:
    ldxdw r6, [r10-0x48]                    
lbb_14859:
    stxdw [r6+0x8], r0                      
lbb_14860:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_14861:
    stxdw [r6+0x0], r1                      
    exit                                    
lbb_14863:
    add64 r9, r8                                    r9 += r8   ///  r9 = r9.wrapping_add(r8)
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    stxdw [r7+0x0], r9                      
    stdw [r7+0x8], 0                        
    ja lbb_14859                                    if true { pc += -11 }
lbb_14870:
    stxdw [r10-0x70], r6                    
    ldxdw r1, [r10-0x68]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_197                       
    ldxdw r9, [r10-0x18]                    
    ldxdw r1, [r10-0x20]                    
    jeq r1, 0, lbb_14882                            if r1 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r6, [r10-0x48]                    
    stxdw [r6+0x8], r9                      
    ja lbb_14860                                    if true { pc += -22 }
lbb_14882:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_197                       
    ldxdw r2, [r10-0x28]                    
    ldxdw r1, [r10-0x30]                    
    jeq r1, 0, lbb_14892                            if r1 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r6, [r10-0x48]                    
    stxdw [r6+0x8], r2                      
    ja lbb_14860                                    if true { pc += -32 }
lbb_14892:
    mov64 r6, r2                                    r6 = r2
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_197                       
    ldxdw r1, [r10-0x38]                    
    ldxdw r2, [r10-0x40]                    
    jeq r2, 0, lbb_14903                            if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r6, [r10-0x48]                    
    stxdw [r6+0x8], r1                      
    ja lbb_14860                                    if true { pc += -43 }
lbb_14903:
    ldxdw r2, [r10-0x48]                    
    ldxdw r3, [r10-0x70]                    
    stxw [r2+0x50], r3                      
    stxdw [r2+0x48], r1                     
    stxdw [r2+0x40], r6                     
    stxdw [r2+0x38], r9                     
    stxdw [r2+0x30], r8                     
    mov64 r6, r2                                    r6 = r2
    ldxdw r1, [r10-0x68]                    
    stxdw [r6+0x28], r1                     
    ldxdw r1, [r10-0x78]                    
    stxdw [r6+0x20], r1                     
    ldxdw r1, [r10-0x60]                    
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x58]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x50]                    
    stxdw [r6+0x8], r1                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_14861                                    if true { pc += -62 }
lbb_14923:
    lddw r1, 0x10002d0cc --> b"For portability reasons we do not allow to deserialize NaNs."        r1 load str located at 4295151820
    mov64 r2, 60                                    r2 = 60 as i32 as i64 as u64
    call function_71                        
    ja lbb_14858                                    if true { pc += -70 }

function_14928:
    mov64 r4, r3                                    r4 = r3
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r2                                    r5 = r2
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    stxdw [r10-0x20], r5                    
    jne r3, 0, lbb_14935                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14982                                    if true { pc += 47 }
lbb_14935:
    ldxdw r4, [r2+0x0]                      
    ldxb r4, [r4+0x2]                       
    jne r4, 0, lbb_14942                            if r4 != (0 as i32 as i64 as u64) { pc += 4 }
lbb_14938:
    lddw r2, 0x200000000                            r2 load str located at 8589934592
    stxdw [r1+0x8], r2                      
    ja lbb_14983                                    if true { pc += 41 }
lbb_14942:
    jne r3, 1, lbb_14944                            if r3 != (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14982                                    if true { pc += 38 }
lbb_14944:
    ldxdw r4, [r2+0x8]                      
    ldxb r4, [r4+0x2]                       
    jne r4, 0, lbb_14948                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14938                                    if true { pc += -10 }
lbb_14948:
    jne r3, 2, lbb_14950                            if r3 != (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14982                                    if true { pc += 32 }
lbb_14950:
    mov64 r6, r2                                    r6 = r2
    add64 r6, 24                                    r6 += 24   ///  r6 = r6.wrapping_add(24 as i32 as i64 as u64)
    stxdw [r10-0x28], r6                    
    ldxdw r4, [r2+0x10]                     
    ldxb r4, [r4+0x2]                       
    jne r4, 0, lbb_14957                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14938                                    if true { pc += -19 }
lbb_14957:
    jne r3, 3, lbb_14959                            if r3 != (3 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14982                                    if true { pc += 23 }
lbb_14959:
    ldxdw r4, [r2+0x18]                     
    ldxb r4, [r4+0x2]                       
    jne r4, 0, lbb_14963                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14938                                    if true { pc += -25 }
lbb_14963:
    jne r3, 4, lbb_14965                            if r3 != (4 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14982                                    if true { pc += 17 }
lbb_14965:
    ldxdw r4, [r2+0x20]                     
    ldxb r4, [r4+0x2]                       
    jne r4, 0, lbb_14969                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14938                                    if true { pc += -31 }
lbb_14969:
    jne r3, 5, lbb_14971                            if r3 != (5 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14982                                    if true { pc += 11 }
lbb_14971:
    ldxdw r4, [r2+0x28]                     
    ldxb r4, [r4+0x2]                       
    jne r4, 0, lbb_14975                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14938                                    if true { pc += -37 }
lbb_14975:
    jne r3, 6, lbb_14977                            if r3 != (6 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14982                                    if true { pc += 5 }
lbb_14977:
    ldxdw r4, [r2+0x30]                     
    ldxb r4, [r4+0x2]                       
    jne r4, 0, lbb_14981                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14938                                    if true { pc += -43 }
lbb_14981:
    jne r3, 7, lbb_14987                            if r3 != (7 as i32 as i64 as u64) { pc += 5 }
lbb_14982:
    stdw [r1+0x8], 10                       
lbb_14983:
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    stxdw [r1+0x0], r2                      
lbb_14986:
    exit                                    
lbb_14987:
    ldxdw r4, [r2+0x38]                     
    ldxb r4, [r4+0x1]                       
    jeq r4, 0, lbb_14996                            if r4 == (0 as i32 as i64 as u64) { pc += 6 }
    jeq r3, 8, lbb_14982                            if r3 == (8 as i32 as i64 as u64) { pc += -9 }
    stxdw [r10-0x30], r1                    
    jeq r3, 9, lbb_14994                            if r3 == (9 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14998                                    if true { pc += 4 }
lbb_14994:
    ldxdw r1, [r10-0x30]                    
    ja lbb_14982                                    if true { pc += -14 }
lbb_14996:
    stw [r1+0x8], 7                         
    ja lbb_14983                                    if true { pc += -15 }
lbb_14998:
    mov64 r7, r2                                    r7 = r2
    mov64 r1, r7                                    r1 = r7
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x38], r1                    
    mov64 r1, r7                                    r1 = r7
    add64 r1, 16                                    r1 += 16   ///  r1 = r1.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r7                                    r1 = r7
    add64 r1, 32                                    r1 += 32   ///  r1 = r1.wrapping_add(32 as i32 as i64 as u64)
    stxdw [r10-0x48], r1                    
    mov64 r1, r7                                    r1 = r7
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x50], r1                    
    mov64 r1, r7                                    r1 = r7
    add64 r1, 48                                    r1 += 48   ///  r1 = r1.wrapping_add(48 as i32 as i64 as u64)
    stxdw [r10-0x58], r1                    
    mov64 r8, r7                                    r8 = r7
    add64 r8, 56                                    r8 += 56   ///  r8 = r8.wrapping_add(56 as i32 as i64 as u64)
    mov64 r9, r7                                    r9 = r7
    add64 r9, 64                                    r9 += 64   ///  r9 = r9.wrapping_add(64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    add64 r1, 80                                    r1 += 80   ///  r1 = r1.wrapping_add(80 as i32 as i64 as u64)
    stxdw [r10-0x28], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -40                                   r2 += -40   ///  r2 = r2.wrapping_add(-40 as i32 as i64 as u64)
    call function_2160                      
    ldxdw r1, [r10-0x8]                     
    ldxdw r2, [r10-0x30]                    
    stxdw [r2+0x10], r1                     
    ldxdw r1, [r10-0x10]                    
    stxdw [r2+0x8], r1                      
    ldxdw r1, [r10-0x18]                    
    stxdw [r2+0x0], r1                      
    mov64 r1, r7                                    r1 = r7
    add64 r1, 72                                    r1 += 72   ///  r1 = r1.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r2+0x60], r1                     
    stxdw [r2+0x58], r9                     
    stxdw [r2+0x50], r8                     
    ldxdw r1, [r10-0x58]                    
    stxdw [r2+0x48], r1                     
    ldxdw r1, [r10-0x50]                    
    stxdw [r2+0x40], r1                     
    ldxdw r1, [r10-0x48]                    
    stxdw [r2+0x38], r1                     
    stxdw [r2+0x30], r6                     
    ldxdw r1, [r10-0x40]                    
    stxdw [r2+0x28], r1                     
    ldxdw r1, [r10-0x38]                    
    stxdw [r2+0x20], r1                     
    stxdw [r2+0x18], r7                     
    ja lbb_14986                                    if true { pc += -65 }

function_15051:
    mov64 r4, r3                                    r4 = r3
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r2                                    r5 = r2
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    stxdw [r10-0x8], r5                     
    jne r3, 0, lbb_15058                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15078                                    if true { pc += 20 }
lbb_15058:
    ldxdw r4, [r2+0x0]                      
    ldxb r4, [r4+0x2]                       
    jne r4, 0, lbb_15065                            if r4 != (0 as i32 as i64 as u64) { pc += 4 }
lbb_15061:
    lddw r2, 0x200000000                            r2 load str located at 8589934592
    stxdw [r1+0x8], r2                      
    ja lbb_15079                                    if true { pc += 14 }
lbb_15065:
    jne r3, 1, lbb_15067                            if r3 != (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15078                                    if true { pc += 11 }
lbb_15067:
    ldxdw r4, [r2+0x8]                      
    ldxb r4, [r4+0x1]                       
    jeq r4, 0, lbb_15075                            if r4 == (0 as i32 as i64 as u64) { pc += 5 }
    jeq r3, 2, lbb_15078                            if r3 == (2 as i32 as i64 as u64) { pc += 7 }
    ldxdw r4, [r2+0x10]                     
    ldxb r4, [r4+0x2]                       
    jne r4, 0, lbb_15077                            if r4 != (0 as i32 as i64 as u64) { pc += 3 }
    ja lbb_15061                                    if true { pc += -14 }
lbb_15075:
    stw [r1+0x8], 7                         
    ja lbb_15079                                    if true { pc += 2 }
lbb_15077:
    jne r3, 3, lbb_15083                            if r3 != (3 as i32 as i64 as u64) { pc += 5 }
lbb_15078:
    stdw [r1+0x8], 10                       
lbb_15079:
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    stxdw [r1+0x0], r2                      
lbb_15082:
    exit                                    
lbb_15083:
    ldxdw r4, [r2+0x18]                     
    ldxb r4, [r4+0x2]                       
    jne r4, 0, lbb_15087                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15061                                    if true { pc += -26 }
lbb_15087:
    jeq r3, 4, lbb_15078                            if r3 == (4 as i32 as i64 as u64) { pc += -10 }
    mov64 r3, r2                                    r3 = r2
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x18], r3                    
    mov64 r8, r2                                    r8 = r2
    add64 r8, 16                                    r8 += 16   ///  r8 = r8.wrapping_add(16 as i32 as i64 as u64)
    mov64 r9, r2                                    r9 = r2
    add64 r9, 24                                    r9 += 24   ///  r9 = r9.wrapping_add(24 as i32 as i64 as u64)
    mov64 r7, r2                                    r7 = r2
    add64 r7, 32                                    r7 += 32   ///  r7 = r7.wrapping_add(32 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    add64 r3, 40                                    r3 += 40   ///  r3 = r3.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x10], r3                    
    mov64 r3, r10                                   r3 = r10
    add64 r3, -16                                   r3 += -16   ///  r3 = r3.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r6, r1                                    r6 = r1
    stxdw [r10-0x20], r2                    
    mov64 r2, r3                                    r2 = r3
    call function_2160                      
    stxdw [r6+0x38], r7                     
    stxdw [r6+0x30], r9                     
    stxdw [r6+0x28], r8                     
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x20], r1                     
    ldxdw r1, [r10-0x20]                    
    stxdw [r6+0x18], r1                     
    ja lbb_15082                                    if true { pc += -32 }

function_15114:
    mov64 r4, r3                                    r4 = r3
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r2                                    r5 = r2
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    stxdw [r10-0x8], r5                     
    jne r3, 0, lbb_15122                            if r3 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_15120:
    stdw [r1+0x8], 10                       
    ja lbb_15128                                    if true { pc += 6 }
lbb_15122:
    ldxdw r4, [r2+0x0]                      
    ldxb r4, [r4+0x2]                       
    jne r4, 0, lbb_15132                            if r4 != (0 as i32 as i64 as u64) { pc += 7 }
lbb_15125:
    lddw r2, 0x200000000                            r2 load str located at 8589934592
    stxdw [r1+0x8], r2                      
lbb_15128:
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    stxdw [r1+0x0], r2                      
lbb_15131:
    exit                                    
lbb_15132:
    jne r3, 1, lbb_15134                            if r3 != (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15120                                    if true { pc += -14 }
lbb_15134:
    ldxdw r4, [r2+0x8]                      
    ldxb r4, [r4+0x2]                       
    jne r4, 0, lbb_15138                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15125                                    if true { pc += -13 }
lbb_15138:
    jne r3, 2, lbb_15140                            if r3 != (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15120                                    if true { pc += -20 }
lbb_15140:
    ldxdw r4, [r2+0x10]                     
    ldxb r4, [r4+0x2]                       
    jne r4, 0, lbb_15144                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15125                                    if true { pc += -19 }
lbb_15144:
    jne r3, 3, lbb_15146                            if r3 != (3 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15120                                    if true { pc += -26 }
lbb_15146:
    ldxdw r4, [r2+0x18]                     
    ldxb r4, [r4+0x1]                       
    jeq r4, 0, lbb_15176                            if r4 == (0 as i32 as i64 as u64) { pc += 27 }
    jeq r3, 4, lbb_15120                            if r3 == (4 as i32 as i64 as u64) { pc += -30 }
    mov64 r3, r2                                    r3 = r2
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x18], r3                    
    mov64 r8, r2                                    r8 = r2
    add64 r8, 16                                    r8 += 16   ///  r8 = r8.wrapping_add(16 as i32 as i64 as u64)
    mov64 r9, r2                                    r9 = r2
    add64 r9, 24                                    r9 += 24   ///  r9 = r9.wrapping_add(24 as i32 as i64 as u64)
    mov64 r7, r2                                    r7 = r2
    add64 r7, 32                                    r7 += 32   ///  r7 = r7.wrapping_add(32 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    add64 r3, 40                                    r3 += 40   ///  r3 = r3.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x10], r3                    
    mov64 r3, r10                                   r3 = r10
    add64 r3, -16                                   r3 += -16   ///  r3 = r3.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r6, r1                                    r6 = r1
    stxdw [r10-0x20], r2                    
    mov64 r2, r3                                    r2 = r3
    call function_2160                      
    stxdw [r6+0x38], r7                     
    stxdw [r6+0x30], r9                     
    stxdw [r6+0x28], r8                     
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x20], r1                     
    ldxdw r1, [r10-0x20]                    
    stxdw [r6+0x18], r1                     
    ja lbb_15131                                    if true { pc += -45 }
lbb_15176:
    stw [r1+0x8], 7                         
    ja lbb_15128                                    if true { pc += -50 }

function_15178:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    call function_904                       
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x18]                    
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    jeq r2, r3, lbb_15203                           if r2 == r3 { pc += 15 }
    ldxdw r8, [r7+0x0]                      
    ldxdw r3, [r7+0x8]                      
    jlt r3, 8, lbb_15192                            if r3 < (8 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15206                                    if true { pc += 14 }
lbb_15192:
    add64 r8, r3                                    r8 += r3   ///  r8 = r8.wrapping_add(r3)
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    stxdw [r7+0x0], r8                      
    stdw [r7+0x8], 0                        
    stxdw [r6+0x8], r0                      
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    stxdw [r6+0x0], r1                      
    ja lbb_15205                                    if true { pc += 2 }
lbb_15203:
    stxdw [r6+0x0], r3                      
    stxdw [r6+0x8], r1                      
lbb_15205:
    exit                                    
lbb_15206:
    mov64 r4, r3                                    r4 = r3
    and64 r4, -2                                    r4 &= -2   ///  r4 = r4.and(-2)
    jeq r4, 8, lbb_15192                            if r4 == (8 as i32 as i64 as u64) { pc += -17 }
    jeq r4, 10, lbb_15192                           if r4 == (10 as i32 as i64 as u64) { pc += -18 }
    ldxdw r4, [r10-0x8]                     
    ldxdw r5, [r8+0x0]                      
    ldxh r0, [r8+0x8]                       
    add64 r3, -12                                   r3 += -12   ///  r3 = r3.wrapping_add(-12 as i32 as i64 as u64)
    ldxh r9, [r8+0xa]                       
    stxdw [r7+0x8], r3                      
    add64 r8, 12                                    r8 += 12   ///  r8 = r8.wrapping_add(12 as i32 as i64 as u64)
    stxdw [r7+0x0], r8                      
    stxh [r6+0x22], r9                      
    stxh [r6+0x20], r0                      
    stxdw [r6+0x18], r5                     
    stxdw [r6+0x10], r4                     
    stxdw [r6+0x8], r1                      
    stxdw [r6+0x0], r2                      
    ja lbb_15205                                    if true { pc += -20 }

function_15225:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    call function_904                       
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x18]                    
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    jeq r2, r3, lbb_15250                           if r2 == r3 { pc += 15 }
    ldxdw r8, [r7+0x0]                      
    ldxdw r3, [r7+0x8]                      
    jlt r3, 8, lbb_15239                            if r3 < (8 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15253                                    if true { pc += 14 }
lbb_15239:
    add64 r8, r3                                    r8 += r3   ///  r8 = r8.wrapping_add(r3)
lbb_15240:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    stxdw [r7+0x0], r8                      
    stdw [r7+0x8], 0                        
    stxdw [r6+0x8], r0                      
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    stxdw [r6+0x0], r1                      
    ja lbb_15252                                    if true { pc += 2 }
lbb_15250:
    stxdw [r6+0x0], r3                      
    stxdw [r6+0x8], r1                      
lbb_15252:
    exit                                    
lbb_15253:
    mov64 r4, r3                                    r4 = r3
    and64 r4, -2                                    r4 &= -2   ///  r4 = r4.and(-2)
    jeq r4, 8, lbb_15239                            if r4 == (8 as i32 as i64 as u64) { pc += -17 }
    jeq r4, 10, lbb_15239                           if r4 == (10 as i32 as i64 as u64) { pc += -18 }
    jeq r3, 12, lbb_15259                           if r3 == (12 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15261                                    if true { pc += 2 }
lbb_15259:
    add64 r8, 12                                    r8 += 12   ///  r8 = r8.wrapping_add(12 as i32 as i64 as u64)
    ja lbb_15240                                    if true { pc += -21 }
lbb_15261:
    ldxdw r4, [r10-0x8]                     
    stxdw [r10-0x20], r4                    
    ldxdw r5, [r8+0x0]                      
    ldxh r0, [r8+0x8]                       
    ldxh r9, [r8+0xa]                       
    add64 r3, -13                                   r3 += -13   ///  r3 = r3.wrapping_add(-13 as i32 as i64 as u64)
    ldxb r4, [r8+0xc]                       
    stxdw [r7+0x8], r3                      
    add64 r8, 13                                    r8 += 13   ///  r8 = r8.wrapping_add(13 as i32 as i64 as u64)
    stxdw [r7+0x0], r8                      
    stxb [r6+0x24], r4                      
    stxh [r6+0x22], r9                      
    stxh [r6+0x20], r0                      
    stxdw [r6+0x18], r5                     
    ldxdw r3, [r10-0x20]                    
    stxdw [r6+0x10], r3                     
    stxdw [r6+0x8], r1                      
    stxdw [r6+0x0], r2                      
    ja lbb_15252                                    if true { pc += -28 }

function_15280:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    call function_540                       
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x18]                    
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    jeq r2, r3, lbb_15305                           if r2 == r3 { pc += 15 }
    ldxdw r8, [r7+0x0]                      
    ldxdw r3, [r7+0x8]                      
    jlt r3, 8, lbb_15294                            if r3 < (8 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15308                                    if true { pc += 14 }
lbb_15294:
    add64 r8, r3                                    r8 += r3   ///  r8 = r8.wrapping_add(r3)
lbb_15295:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    stxdw [r7+0x0], r8                      
    stdw [r7+0x8], 0                        
    stxdw [r6+0x8], r0                      
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    stxdw [r6+0x0], r1                      
    ja lbb_15307                                    if true { pc += 2 }
lbb_15305:
    stxdw [r6+0x0], r3                      
    stxdw [r6+0x8], r1                      
lbb_15307:
    exit                                    
lbb_15308:
    mov64 r4, r3                                    r4 = r3
    and64 r4, -8                                    r4 &= -8   ///  r4 = r4.and(-8)
    jeq r4, 8, lbb_15294                            if r4 == (8 as i32 as i64 as u64) { pc += -17 }
    mov64 r4, r3                                    r4 = r3
    and64 r4, -2                                    r4 &= -2   ///  r4 = r4.and(-2)
    jeq r4, 16, lbb_15294                           if r4 == (16 as i32 as i64 as u64) { pc += -20 }
    jeq r3, 18, lbb_15316                           if r3 == (18 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15318                                    if true { pc += 2 }
lbb_15316:
    add64 r8, 18                                    r8 += 18   ///  r8 = r8.wrapping_add(18 as i32 as i64 as u64)
    ja lbb_15295                                    if true { pc += -23 }
lbb_15318:
    ldxdw r4, [r10-0x8]                     
    stxdw [r10-0x20], r4                    
    ldxdw r5, [r8+0x0]                      
    ldxdw r0, [r8+0x8]                      
    ldxh r9, [r8+0x10]                      
    add64 r3, -19                                   r3 += -19   ///  r3 = r3.wrapping_add(-19 as i32 as i64 as u64)
    ldxb r4, [r8+0x12]                      
    stxdw [r7+0x8], r3                      
    add64 r8, 19                                    r8 += 19   ///  r8 = r8.wrapping_add(19 as i32 as i64 as u64)
    stxdw [r7+0x0], r8                      
    stxb [r6+0x2a], r4                      
    stxh [r6+0x28], r9                      
    stxdw [r6+0x20], r0                     
    stxdw [r6+0x18], r5                     
    ldxdw r3, [r10-0x20]                    
    stxdw [r6+0x10], r3                     
    stxdw [r6+0x8], r1                      
    stxdw [r6+0x0], r2                      
    ja lbb_15307                                    if true { pc += -30 }

function_15337:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r7+0x8]                      
    jeq r1, 0, lbb_15360                            if r1 == (0 as i32 as i64 as u64) { pc += 19 }
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    ldxdw r2, [r7+0x0]                      
    ldxb r9, [r2+0x0]                       
    stxdw [r7+0x8], r1                      
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r7+0x0], r2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_540                       
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x18]                    
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    jeq r2, r3, lbb_15357                           if r2 == r3 { pc += 1 }
    ja lbb_15364                                    if true { pc += 7 }
lbb_15357:
    stxdw [r6+0x0], r3                      
    stxdw [r6+0x8], r1                      
    ja lbb_15378                                    if true { pc += 18 }
lbb_15360:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    ja lbb_15374                                    if true { pc += 10 }
lbb_15364:
    ldxdw r8, [r7+0x0]                      
    ldxdw r3, [r7+0x8]                      
    jlt r3, 8, lbb_15368                            if r3 < (8 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15379                                    if true { pc += 11 }
lbb_15368:
    add64 r8, r3                                    r8 += r3   ///  r8 = r8.wrapping_add(r3)
lbb_15369:
    lddw r1, 0x10002ea08 --> b"\x00\x00\x00\x00M\xd0\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295158280
    call function_17790                     
    stxdw [r7+0x0], r8                      
    stdw [r7+0x8], 0                        
lbb_15374:
    stxdw [r6+0x8], r0                      
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    stxdw [r6+0x0], r1                      
lbb_15378:
    exit                                    
lbb_15379:
    mov64 r4, r3                                    r4 = r3
    and64 r4, -8                                    r4 &= -8   ///  r4 = r4.and(-8)
    jeq r4, 8, lbb_15368                            if r4 == (8 as i32 as i64 as u64) { pc += -14 }
    mov64 r4, r3                                    r4 = r3
    and64 r4, -2                                    r4 &= -2   ///  r4 = r4.and(-2)
    jeq r4, 16, lbb_15368                           if r4 == (16 as i32 as i64 as u64) { pc += -17 }
    jeq r3, 18, lbb_15387                           if r3 == (18 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15389                                    if true { pc += 2 }
lbb_15387:
    add64 r8, 18                                    r8 += 18   ///  r8 = r8.wrapping_add(18 as i32 as i64 as u64)
    ja lbb_15369                                    if true { pc += -20 }
lbb_15389:
    ldxdw r4, [r10-0x8]                     
    stxdw [r10-0x20], r4                    
    ldxdw r4, [r8+0x0]                      
    stxdw [r10-0x28], r4                    
    ldxdw r0, [r8+0x8]                      
    ldxh r5, [r8+0x10]                      
    add64 r3, -19                                   r3 += -19   ///  r3 = r3.wrapping_add(-19 as i32 as i64 as u64)
    ldxb r4, [r8+0x12]                      
    stxdw [r7+0x8], r3                      
    add64 r8, 19                                    r8 += 19   ///  r8 = r8.wrapping_add(19 as i32 as i64 as u64)
    stxdw [r7+0x0], r8                      
    stxb [r6+0x2b], r4                      
    stxb [r6+0x2a], r9                      
    stxh [r6+0x28], r5                      
    stxdw [r6+0x20], r0                     
    ldxdw r3, [r10-0x28]                    
    stxdw [r6+0x18], r3                     
    ldxdw r3, [r10-0x20]                    
    stxdw [r6+0x10], r3                     
    stxdw [r6+0x8], r1                      
    stxdw [r6+0x0], r2                      
    ja lbb_15378                                    if true { pc += -33 }

function_15411:
    call function_18166                     
    exit                                    

function_15413:
    stxdw [r10-0xf0], r4                    
    stxdw [r10-0xf8], r3                    
    stxdw [r10-0xe8], r1                    
    ldxdw r7, [r2+0x8]                      
    ldxdw r3, [r7+0x0]                      
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r1, [r2+0x0]                      
    ldxdw r4, [r1+0x0]                      
    stxdw [r10-0xd0], r3                    
    add64 r4, 8                                     r4 += 8   ///  r4 = r4.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xe0], r4                    
    sth [r10-0xc8], 0                       
    sth [r10-0xd8], 1                       
    stb [r10-0xb9], 18                      
    ldxdw r2, [r2+0x10]                     
    ldxb r3, [r2+0x0]                       
    stxb [r10-0xb8], r3                     
    ldxb r3, [r2+0x1]                       
    stxb [r10-0xb7], r3                     
    ldxb r3, [r2+0x2]                       
    stxb [r10-0xb6], r3                     
    ldxb r3, [r2+0x3]                       
    stxb [r10-0xb5], r3                     
    ldxb r3, [r2+0x4]                       
    stxb [r10-0xb4], r3                     
    ldxb r3, [r2+0x5]                       
    stxb [r10-0xb3], r3                     
    ldxb r3, [r2+0x6]                       
    stxb [r10-0xb2], r3                     
    ldxb r3, [r2+0x7]                       
    stxb [r10-0xb1], r3                     
    ldxb r3, [r2+0x8]                       
    stxb [r10-0xb0], r3                     
    ldxb r3, [r2+0x9]                       
    stxb [r10-0xaf], r3                     
    ldxb r3, [r2+0xa]                       
    stxb [r10-0xae], r3                     
    ldxb r3, [r2+0xb]                       
    stxb [r10-0xad], r3                     
    ldxb r3, [r2+0xc]                       
    stxb [r10-0xac], r3                     
    ldxb r3, [r2+0xd]                       
    stxb [r10-0xab], r3                     
    ldxb r3, [r2+0xe]                       
    stxb [r10-0xaa], r3                     
    ldxb r3, [r2+0xf]                       
    stxb [r10-0xa9], r3                     
    ldxb r3, [r2+0x10]                      
    stxb [r10-0xa8], r3                     
    ldxb r3, [r2+0x11]                      
    stxb [r10-0xa7], r3                     
    ldxb r3, [r2+0x12]                      
    stxb [r10-0xa6], r3                     
    ldxb r3, [r2+0x13]                      
    stxb [r10-0xa5], r3                     
    ldxb r3, [r2+0x14]                      
    stxb [r10-0xa4], r3                     
    ldxb r3, [r2+0x15]                      
    stxb [r10-0xa3], r3                     
    ldxb r3, [r2+0x16]                      
    stxb [r10-0xa2], r3                     
    ldxb r3, [r2+0x17]                      
    stxb [r10-0xa1], r3                     
    ldxb r3, [r2+0x18]                      
    stxb [r10-0xa0], r3                     
    ldxb r3, [r2+0x19]                      
    stxb [r10-0x9f], r3                     
    ldxb r3, [r2+0x1a]                      
    stxb [r10-0x9e], r3                     
    ldxb r3, [r2+0x1b]                      
    stxb [r10-0x9d], r3                     
    ldxb r3, [r2+0x1c]                      
    stxb [r10-0x9c], r3                     
    ldxb r3, [r2+0x1d]                      
    stxb [r10-0x9b], r3                     
    ldxb r3, [r2+0x1e]                      
    stxb [r10-0x9a], r3                     
    ldxb r2, [r2+0x1f]                      
    stxb [r10-0x99], r2                     
    ldxdw r6, [r1+0x0]                      
    ldxdw r2, [r10-0xe0]                    
    mov64 r9, r6                                    r9 = r6
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_15606                            if r0 != (0 as i32 as i64 as u64) { pc += 103 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxb r2, [r10-0xd8]                     
    jne r2, 0, lbb_15507                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 119                                   r1 = 119 as i32 as i64 as u64
lbb_15507:
    ldxb r2, [r6+0x0]                       
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r8, 11                                    r8 = 11 as i32 as i64 as u64
    jne r1, 255, lbb_15606                          if r1 != (255 as i32 as i64 as u64) { pc += 94 }
    ldxb r1, [r6+0x1]                       
    ldxb r2, [r6+0x2]                       
    ldxb r4, [r6+0x3]                       
    ldxdw r3, [r6+0x50]                     
    mov64 r5, r6                                    r5 = r6
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r5                    
    mov64 r5, r6                                    r5 = r6
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r5                    
    stxdw [r10-0x88], r3                    
    add64 r6, 72                                    r6 += 72   ///  r6 = r6.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r6                    
    stxdw [r10-0x98], r9                    
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_15530                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_15530:
    stxb [r10-0x66], r3                     
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_15534                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_15534:
    stxb [r10-0x67], r3                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_15538                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_15538:
    stxb [r10-0x68], r2                     
    stdw [r10-0x70], 0                      
    ldxdw r7, [r7+0x0]                      
    ldxdw r2, [r10-0xd0]                    
    mov64 r9, r7                                    r9 = r7
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_21522                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_15606                            if r0 != (0 as i32 as i64 as u64) { pc += 56 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxb r2, [r10-0xc8]                     
    jne r2, 0, lbb_15554                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 119                                   r1 = 119 as i32 as i64 as u64
lbb_15554:
    ldxb r2, [r7+0x0]                       
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r8, 11                                    r8 = 11 as i32 as i64 as u64
    jne r1, 255, lbb_15606                          if r1 != (255 as i32 as i64 as u64) { pc += 47 }
    ldxb r3, [r7+0x1]                       
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_15564                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_15564:
    ldxb r4, [r7+0x2]                       
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_15568                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_15568:
    ldxb r4, [r7+0x3]                       
    jne r4, 0, lbb_15571                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_15571:
    ldxdw r4, [r7+0x50]                     
    mov64 r5, r7                                    r5 = r7
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r5                    
    mov64 r5, r7                                    r5 = r7
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r5                    
    stxdw [r10-0x50], r4                    
    add64 r7, 72                                    r7 += 72   ///  r7 = r7.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r7                    
    stxdw [r10-0x60], r9                    
    stxb [r10-0x2e], r2                     
    stxb [r10-0x2f], r3                     
    stxb [r10-0x30], r1                     
    stdw [r10-0x38], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -185                                  r1 += -185   ///  r1 = r1.wrapping_add(-185 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -224                                  r1 += -224   ///  r1 = r1.wrapping_add(-224 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x10002cf08 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r1 load str located at 4295151368
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 33                      
    stdw [r10-0x18], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -152                                  r2 += -152   ///  r2 = r2.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    ldxdw r4, [r10-0xf8]                    
    ldxdw r5, [r10-0xf0]                    
    syscall [invalid]                       
    mov64 r8, 26                                    r8 = 26 as i32 as i64 as u64
lbb_15606:
    ldxdw r1, [r10-0xe8]                    
    stxw [r1+0x0], r8                       
    exit                                    

function_15609:
    stxdw [r10-0x120], r3                   
    ldxdw r3, [r2+0x0]                      
    ldxdw r9, [r3+0x0]                      
    ldxdw r3, [r2+0x8]                      
    ldxdw r6, [r3+0x0]                      
    ldxdw r3, [r2+0x10]                     
    ldxdw r3, [r3+0x0]                      
    stxdw [r10-0x118], r3                   
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xf0], r3                    
    mov64 r7, r6                                    r7 = r6
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x100], r7                   
    mov64 r0, r9                                    r0 = r9
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x110], r0                   
    sth [r10-0xe8], 256                     
    sth [r10-0xf8], 1                       
    sth [r10-0x108], 1                      
    stb [r10-0xd9], 3                       
    ldxdw r2, [r2+0x18]                     
    stxdw [r10-0xd8], r2                    
    ldxb r2, [r9+0x0]                       
    jne r2, 255, lbb_15750                          if r2 != (255 as i32 as i64 as u64) { pc += 117 }
    ldxb r2, [r9+0x1]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_15637                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_15637:
    stxdw [r10-0x130], r5                   
    ldxb r2, [r9+0x2]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_15642                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_15642:
    stxdw [r10-0x128], r1                   
    stxdw [r10-0x138], r4                   
    ldxb r2, [r9+0x3]                       
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_15648                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_15648:
    ldxdw r2, [r9+0x50]                     
    mov64 r8, r9                                    r8 = r9
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xb0], r8                    
    mov64 r8, r9                                    r8 = r9
    add64 r8, 88                                    r8 += 88   ///  r8 = r8.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xb8], r8                    
    stxdw [r10-0xc0], r2                    
    add64 r9, 72                                    r9 += 72   ///  r9 = r9.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0xc8], r9                    
    stxdw [r10-0xd0], r0                    
    stxb [r10-0x9e], r1                     
    stxb [r10-0x9f], r5                     
    ldxdw r1, [r10-0x130]                   
    stxb [r10-0xa0], r1                     
    stdw [r10-0xa8], 0                      
    ldxb r2, [r6+0x0]                       
    ldxdw r1, [r10-0x128]                   
    jne r2, 255, lbb_15750                          if r2 != (255 as i32 as i64 as u64) { pc += 83 }
    ldxb r1, [r6+0x1]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_15672                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_15672:
    ldxb r1, [r6+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r0, [r10-0x118]                   
    jne r1, 0, lbb_15677                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_15677:
    ldxb r1, [r6+0x3]                       
    jne r1, 0, lbb_15680                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_15680:
    ldxdw r1, [r6+0x50]                     
    mov64 r8, r6                                    r8 = r6
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r8                    
    mov64 r8, r6                                    r8 = r6
    add64 r8, 88                                    r8 += 88   ///  r8 = r8.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r8                    
    stxdw [r10-0x88], r1                    
    add64 r6, 72                                    r6 += 72   ///  r6 = r6.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r6                    
    stxdw [r10-0x98], r7                    
    stxb [r10-0x66], r5                     
    stxb [r10-0x67], r4                     
    stxb [r10-0x68], r2                     
    stdw [r10-0x70], 0                      
    ldxb r2, [r0+0x0]                       
    and64 r2, 136                                   r2 &= 136   ///  r2 = r2.and(136)
    ldxdw r5, [r10-0x138]                   
    ldxdw r1, [r10-0x128]                   
    jne r2, 136, lbb_15750                          if r2 != (136 as i32 as i64 as u64) { pc += 50 }
    ldxdw r0, [r10-0x118]                   
    ldxb r1, [r0+0x1]                       
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_15706                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_15706:
    ldxb r1, [r0+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_15710                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_15710:
    ldxdw r6, [r10-0x128]                   
    ldxb r1, [r0+0x3]                       
    jne r1, 0, lbb_15714                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_15714:
    ldxdw r1, [r0+0x50]                     
    mov64 r7, r0                                    r7 = r0
    add64 r7, 40                                    r7 += 40   ///  r7 = r7.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r7                    
    mov64 r7, r0                                    r7 = r0
    add64 r7, 88                                    r7 += 88   ///  r7 = r7.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r7                    
    stxdw [r10-0x50], r1                    
    add64 r0, 72                                    r0 += 72   ///  r0 = r0.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r0                    
    stxdw [r10-0x60], r3                    
    stxb [r10-0x2e], r8                     
    stxb [r10-0x2f], r4                     
    stxb [r10-0x30], r2                     
    stdw [r10-0x38], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -217                                  r1 += -217   ///  r1 = r1.wrapping_add(-217 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -272                                  r1 += -272   ///  r1 = r1.wrapping_add(-272 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x10002cf08 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r1 load str located at 4295151368
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 9                       
    stdw [r10-0x18], 3                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -208                                  r2 += -208   ///  r2 = r2.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r3, 3                                     r3 = 3 as i32 as i64 as u64
    ldxdw r4, [r10-0x120]                   
    syscall [invalid]                       
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    mov64 r1, r6                                    r1 = r6
    ja lbb_15751                                    if true { pc += 1 }
lbb_15750:
    mov64 r2, 11                                    r2 = 11 as i32 as i64 as u64
lbb_15751:
    stxw [r1+0x0], r2                       
    exit                                    

function_15753:
    stxdw [r10-0x38], r3                    
    ldxdw r6, [r2+0x18]                     
    ldxb r3, [r6+0x109]                     
    jne r3, 0, lbb_15761                            if r3 != (0 as i32 as i64 as u64) { pc += 4 }
    stw [r1+0x10], 0                        
    stdw [r1+0x8], 0                        
    stdw [r1+0x0], 0                        
    ja lbb_15958                                    if true { pc += 197 }
lbb_15761:
    stxdw [r10-0x40], r4                    
    ldxdw r4, [r5-0xfc0]                    
    ldxdw r9, [r5-0xfc8]                    
    ldxb r0, [r6+0x10a]                     
    ldxdw r3, [r10-0x38]                    
    jne r3, 0, lbb_15768                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15785                                    if true { pc += 17 }
lbb_15768:
    mov64 r7, 40                                    r7 = 40 as i32 as i64 as u64
    ldxdw r8, [r2+0x20]                     
    ldxdw r3, [r2+0x8]                      
    stxdw [r10-0x58], r3                    
    ldxdw r3, [r6+0xa0]                     
    stxdw [r10-0x50], r3                    
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    mov64 r3, r4                                    r3 = r4
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    stxdw [r10-0x48], r9                    
    jeq r0, 0, lbb_15796                            if r0 == (0 as i32 as i64 as u64) { pc += 17 }
lbb_15779:
    mov64 r4, r2                                    r4 = r2
    add64 r4, r7                                    r4 += r7   ///  r4 = r4.wrapping_add(r7)
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r4, [r4+0x0]                      
    ldxdw r4, [r4+0x8]                      
    ja lbb_15796                                    if true { pc += 11 }
lbb_15785:
    stxdw [r10-0x48], r4                    
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r7, 32                                    r7 = 32 as i32 as i64 as u64
    ldxdw r8, [r2+0x28]                     
    ldxdw r3, [r2+0x10]                     
    stxdw [r10-0x58], r3                    
    ldxdw r3, [r6+0xa8]                     
    stxdw [r10-0x50], r3                    
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    mov64 r3, r9                                    r3 = r9
    jne r0, 0, lbb_15779                            if r0 != (0 as i32 as i64 as u64) { pc += -17 }
lbb_15796:
    mov64 r7, r8                                    r7 = r8
    mov64 r9, r3                                    r9 = r3
    sub64 r9, r4                                    r9 -= r4   ///  r9 = r9.wrapping_sub(r4)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jgt r9, r3, lbb_15803                           if r9 > r3 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_15803:
    ldxdw r3, [r5-0x1000]                   
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r4, 0, lbb_15807                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, r9                                    r8 = r9
lbb_15807:
    mov64 r9, r8                                    r9 = r8
    jlt r9, r3, lbb_15810                           if r9 < r3 { pc += 1 }
    mov64 r9, r3                                    r9 = r3
lbb_15810:
    ldxdw r4, [r7+0x0]                      
    mov64 r3, r4                                    r3 = r4
    ldxdw r8, [r10-0x48]                    
    sub64 r3, r8                                    r3 -= r8   ///  r3 = r3.wrapping_sub(r8)
    jgt r3, r4, lbb_15816                           if r3 > r4 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_15816:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jne r0, 0, lbb_15820                            if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, r3                                    r8 = r3
lbb_15820:
    ldxdw r3, [r10-0x40]                    
    jlt r8, r3, lbb_15823                           if r8 < r3 { pc += 1 }
    mov64 r8, r3                                    r8 = r3
lbb_15823:
    stxdw [r10-0x40], r9                    
    ldxdw r3, [r5-0xfb8]                    
    stxdw [r10-0x70], r3                    
    ldxdw r3, [r5-0xfe0]                    
    stxdw [r10-0x68], r3                    
    ldxdw r3, [r5-0xfe8]                    
    stxdw [r10-0x60], r3                    
    ldxdw r3, [r5-0xff0]                    
    stxdw [r10-0x48], r3                    
    ldxdw r0, [r5-0xff8]                    
    ldxb r4, [r6+0x108]                     
    jeq r4, 2, lbb_15902                            if r4 == (2 as i32 as i64 as u64) { pc += 67 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jne r4, 1, lbb_15955                            if r4 != (1 as i32 as i64 as u64) { pc += 117 }
    stxdw [r10-0x80], r0                    
    stxdw [r10-0x78], r1                    
    ldxdw r1, [r5-0xfd8]                    
    stxdw [r10-0x88], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r7, r2                                    r7 = r2
    call function_15963                     
    ldxw r1, [r10-0x20]                     
    ldxdw r2, [r10-0x38]                    
    jne r2, 0, lbb_15850                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    ldxw r1, [r10-0x1c]                     
lbb_15850:
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r9, r1                                    r9 = r1
    mul64 r9, 1000                                  r9 *= 1000   ///  r9 = r9.wrapping_mul(1000 as u64)
    mov64 r2, r9                                    r2 = r9
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    jne r2, r9, lbb_15858                           if r2 != r9 { pc += 0 }
lbb_15858:
    jne r2, r9, lbb_15959                           if r2 != r9 { pc += 100 }
lbb_15859:
    ldxdw r2, [r7+0x0]                      
    stxdw [r10-0x38], r2                    
    ldxdw r3, [r6+0xc8]                     
    ldxdw r4, [r6+0xd0]                     
    ldxdw r5, [r6+0xb0]                     
    ldxdw r0, [r6+0xd8]                     
    ldxdw r7, [r6+0xe0]                     
    ldxdw r6, [r6+0xe8]                     
    ldxdw r2, [r10-0x70]                    
    stxdw [r10-0xf80], r2                   
    stxdw [r10-0xf88], r6                   
    stxdw [r10-0xf90], r7                   
    stxdw [r10-0xf98], r0                   
    stxdw [r10-0xfa0], r5                   
    ldxdw r2, [r10-0x88]                    
    stxdw [r10-0xfa8], r2                   
    stxdw [r10-0xfb0], r4                   
    stxdw [r10-0xfb8], r3                   
    ldxdw r3, [r10-0x68]                    
    stxdw [r10-0xfc0], r3                   
    stxdw [r10-0xfc8], r1                   
    ldxdw r1, [r10-0x38]                    
    stxdw [r10-0xfd8], r1                   
    ldxdw r1, [r10-0x60]                    
    stxdw [r10-0xfe0], r1                   
    ldxdw r1, [r10-0x58]                    
    stxdw [r10-0xff0], r1                   
    ldxdw r1, [r10-0x48]                    
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0x80]                    
    stxdw [r10-0x1000], r1                  
    stdw [r10-0xfd0], 10                    
    stdw [r10-0xfe8], 10                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r8                                    r2 = r8
    ldxdw r3, [r10-0x40]                    
    ldxdw r4, [r10-0x50]                    
    call function_16117                     
    ldxdw r7, [r10-0x28]                    
    ldxdw r3, [r10-0x30]                    
    ja lbb_15954                                    if true { pc += 52 }
lbb_15902:
    stxdw [r10-0x80], r0                    
    stxdw [r10-0x78], r1                    
    ldxdw r1, [r5-0xfd0]                    
    stxdw [r10-0x88], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -8                                    r1 += -8   ///  r1 = r1.wrapping_add(-8 as i32 as i64 as u64)
    mov64 r7, r2                                    r7 = r2
    call function_16676                     
    ldxw r9, [r10-0x8]                      
    ldxdw r1, [r10-0x38]                    
    jne r1, 0, lbb_15914                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ldxw r9, [r10-0x4]                      
lbb_15914:
    ldxdw r1, [r7+0x0]                      
    ldxdw r2, [r6+0xc8]                     
    ldxdw r3, [r6+0xd0]                     
    ldxdw r4, [r6+0xc0]                     
    ldxdw r5, [r6+0xd8]                     
    ldxdw r0, [r6+0xe0]                     
    ldxdw r6, [r6+0xe8]                     
    ldxdw r7, [r10-0x70]                    
    stxdw [r10-0xf80], r7                   
    stxdw [r10-0xf88], r6                   
    stxdw [r10-0xf90], r0                   
    stxdw [r10-0xf98], r5                   
    stxdw [r10-0xfa0], r4                   
    ldxdw r4, [r10-0x88]                    
    stxdw [r10-0xfa8], r4                   
    stxdw [r10-0xfb0], r3                   
    stxdw [r10-0xfb8], r2                   
    ldxdw r2, [r10-0x68]                    
    stxdw [r10-0xfc0], r2                   
    stxdw [r10-0xfd8], r1                   
    ldxdw r1, [r10-0x60]                    
    stxdw [r10-0xfe0], r1                   
    ldxdw r1, [r10-0x58]                    
    stxdw [r10-0xff0], r1                   
    ldxdw r1, [r10-0x48]                    
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0x80]                    
    stxdw [r10-0x1000], r1                  
    stxdw [r10-0xfc8], r9                   
    stdw [r10-0xfd0], 10                    
    stdw [r10-0xfe8], 10                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r8                                    r2 = r8
    ldxdw r3, [r10-0x40]                    
    ldxdw r4, [r10-0x50]                    
    call function_16849                     
    ldxdw r7, [r10-0x10]                    
    ldxdw r3, [r10-0x18]                    
lbb_15954:
    ldxdw r1, [r10-0x78]                    
lbb_15955:
    stxw [r1+0x10], r9                      
    stxdw [r1+0x8], r7                      
    stxdw [r1+0x0], r3                      
lbb_15958:
    exit                                    
lbb_15959:
    mov64 r9, r1                                    r9 = r1
    arsh64 r9, 31                                   r9 >>= 31 (signed)   ///  r9 = (r9 as i64).wrapping_shr(31)
    xor64 r9, 2147483647                            r9 ^= 2147483647   ///  r9 = r9.xor(2147483647)
    ja lbb_15859                                    if true { pc += -104 }

function_15963:
    ldxdw r3, [r2+0x28]                     
    ldxw r4, [r3+0x10]                      
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    ldxdw r3, [r2+0x20]                     
    ldxw r3, [r3+0x10]                      
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    sub64 r3, r4                                    r3 -= r4   ///  r3 = r3.wrapping_sub(r4)
    jslt r3, 2147483647, lbb_15974                  if (r3 as i64) < (2147483647 as i32 as i64) { pc += 1 }
    mov64 r3, 2147483647                            r3 = 2147483647 as i32 as i64 as u64
lbb_15974:
    jsgt r3, -2147483648, lbb_15976                 if (r3 as i64) > (-2147483648 as i32 as i64) { pc += 1 }
    mov64 r3, -2147483648                           r3 = -2147483648 as i32 as i64 as u64
lbb_15976:
    ldxdw r0, [r2+0x18]                     
    ldxw r2, [r0+0xf4]                      
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    jslt r3, 2147483647, lbb_15983                  if (r3 as i64) < (2147483647 as i32 as i64) { pc += 1 }
    mov64 r3, 2147483647                            r3 = 2147483647 as i32 as i64 as u64
lbb_15983:
    jsgt r3, -2147483648, lbb_15985                 if (r3 as i64) > (-2147483648 as i32 as i64) { pc += 1 }
    mov64 r3, -2147483648                           r3 = -2147483648 as i32 as i64 as u64
lbb_15985:
    ldxw r4, [r0+0xf0]                      
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    jeq r4, -2147483648, lbb_16033                  if r4 == (-2147483648 as i32 as i64 as u64) { pc += 44 }
    ldxw r5, [r0+0xf8]                      
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    arsh64 r5, 32                                   r5 >>= 32 (signed)   ///  r5 = (r5 as i64).wrapping_shr(32)
    mov64 r2, r4                                    r2 = r4
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    jsge r5, r2, lbb_16003                          if (r5 as i64) >= (r2 as i64) { pc += 6 }
lbb_15997:
    lddw r1, 0x10002d9a9 --> b"assertion failed: min <= max"        r1 load str located at 4295154089
    mov64 r2, 28                                    r2 = 28 as i32 as i64 as u64
    lddw r3, 0x10002f588 --> b"\x00\x00\x00\x00\xc5\xd9\x02\x00Q\x00\x00\x00\x00\x00\x00\x00\x07\x04\x00…        r3 load str located at 4295161224
    call function_18704                     
lbb_16003:
    mov64 r6, r3                                    r6 = r3
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    lddw r7, 0x80000000                             r7 load str located at 2147483648
    jeq r6, r7, lbb_16036                           if r6 == r7 { pc += 27 }
    ldxw r0, [r0+0xfc]                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    jsge r0, r2, lbb_16014                          if (r0 as i64) >= (r2 as i64) { pc += 1 }
    ja lbb_15997                                    if true { pc += -17 }
lbb_16014:
    mov64 r6, r3                                    r6 = r3
    neg64 r6                                        r6 = -r6   ///  r6 = (r6 as i64).wrapping_neg() as u64
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    arsh64 r6, 32                                   r6 >>= 32 (signed)   ///  r6 = (r6 as i64).wrapping_shr(32)
    jslt r6, r0, lbb_16020                          if (r6 as i64) < (r0 as i64) { pc += 1 }
    mov64 r6, r0                                    r6 = r0
lbb_16020:
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    mov64 r0, r2                                    r0 = r2
    jsgt r3, r4, lbb_16025                          if (r3 as i64) > (r4 as i64) { pc += 1 }
    mov64 r0, r6                                    r0 = r6
lbb_16025:
    mov64 r4, r3                                    r4 = r3
    jslt r3, r5, lbb_16028                          if (r3 as i64) < (r5 as i64) { pc += 1 }
    mov64 r4, r5                                    r4 = r5
lbb_16028:
    jslt r3, r2, lbb_16030                          if (r3 as i64) < (r2 as i64) { pc += 1 }
    mov64 r2, r4                                    r2 = r4
lbb_16030:
    stxw [r1+0x0], r2                       
    stxw [r1+0x4], r0                       
    exit                                    
lbb_16033:
    lddw r1, 0x10002f5a0 --> b"\x00\x00\x00\x00\x16\xda\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00&\x00\x00…        r1 load str located at 4295161248
    call function_20979                     
lbb_16036:
    lddw r1, 0x10002f5b8 --> b"\x00\x00\x00\x00\x16\xda\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00(\x00\x00…        r1 load str located at 4295161272
    call function_20979                     

function_16039:
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    sub64 r3, r4                                    r3 -= r4   ///  r3 = r3.wrapping_sub(r4)
    jslt r3, 2147483647, lbb_16046                  if (r3 as i64) < (2147483647 as i32 as i64) { pc += 1 }
    mov64 r3, 2147483647                            r3 = 2147483647 as i32 as i64 as u64
lbb_16046:
    jsgt r3, -2147483648, lbb_16048                 if (r3 as i64) > (-2147483648 as i32 as i64) { pc += 1 }
    mov64 r3, -2147483648                           r3 = -2147483648 as i32 as i64 as u64
lbb_16048:
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    jslt r3, 2147483647, lbb_16053                  if (r3 as i64) < (2147483647 as i32 as i64) { pc += 1 }
    mov64 r3, 2147483647                            r3 = 2147483647 as i32 as i64 as u64
lbb_16053:
    jsgt r3, -2147483648, lbb_16055                 if (r3 as i64) > (-2147483648 as i32 as i64) { pc += 1 }
    mov64 r3, -2147483648                           r3 = -2147483648 as i32 as i64 as u64
lbb_16055:
    ldxdw r0, [r5-0x1000]                   
    mov64 r2, r0                                    r2 = r0
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    lddw r4, 0x80000000                             r4 load str located at 2147483648
    jeq r2, r4, lbb_16111                           if r2 == r4 { pc += 49 }
    ldxdw r4, [r5-0xff8]                    
    mov64 r2, r0                                    r2 = r0
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r6, r4                                    r6 = r4
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    arsh64 r6, 32                                   r6 >>= 32 (signed)   ///  r6 = (r6 as i64).wrapping_shr(32)
    jsle r2, r6, lbb_16077                          if (r2 as i64) <= (r6 as i64) { pc += 6 }
lbb_16071:
    lddw r1, 0x10002d9a9 --> b"assertion failed: min <= max"        r1 load str located at 4295154089
    mov64 r2, 28                                    r2 = 28 as i32 as i64 as u64
    lddw r3, 0x10002f588 --> b"\x00\x00\x00\x00\xc5\xd9\x02\x00Q\x00\x00\x00\x00\x00\x00\x00\x07\x04\x00…        r3 load str located at 4295161224
    call function_18704                     
lbb_16077:
    mov64 r6, r3                                    r6 = r3
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    lddw r7, 0x80000000                             r7 load str located at 2147483648
    jeq r6, r7, lbb_16114                           if r6 == r7 { pc += 31 }
    ldxdw r5, [r5-0xff0]                    
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    arsh64 r5, 32                                   r5 >>= 32 (signed)   ///  r5 = (r5 as i64).wrapping_shr(32)
    jsle r2, r5, lbb_16088                          if (r2 as i64) <= (r5 as i64) { pc += 1 }
    ja lbb_16071                                    if true { pc += -17 }
lbb_16088:
    mov64 r6, r3                                    r6 = r3
    neg64 r6                                        r6 = -r6   ///  r6 = (r6 as i64).wrapping_neg() as u64
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    arsh64 r6, 32                                   r6 >>= 32 (signed)   ///  r6 = (r6 as i64).wrapping_shr(32)
    jslt r6, r5, lbb_16094                          if (r6 as i64) < (r5 as i64) { pc += 1 }
    mov64 r6, r5                                    r6 = r5
lbb_16094:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    mov64 r5, r2                                    r5 = r2
    jsgt r3, r0, lbb_16101                          if (r3 as i64) > (r0 as i64) { pc += 1 }
    mov64 r5, r6                                    r5 = r6
lbb_16101:
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    mov64 r0, r3                                    r0 = r3
    jslt r3, r4, lbb_16106                          if (r3 as i64) < (r4 as i64) { pc += 1 }
    mov64 r0, r4                                    r0 = r4
lbb_16106:
    jslt r3, r2, lbb_16108                          if (r3 as i64) < (r2 as i64) { pc += 1 }
    mov64 r2, r0                                    r2 = r0
lbb_16108:
    stxw [r1+0x0], r2                       
    stxw [r1+0x4], r5                       
    exit                                    
lbb_16111:
    lddw r1, 0x10002f5a0 --> b"\x00\x00\x00\x00\x16\xda\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00&\x00\x00…        r1 load str located at 4295161248
    call function_20979                     
lbb_16114:
    lddw r1, 0x10002f5b8 --> b"\x00\x00\x00\x00\x16\xda\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00(\x00\x00…        r1 load str located at 4295161272
    call function_20979                     

function_16117:
    stxdw [r10-0xe0], r4                    
    stxdw [r10-0xb8], r3                    
    ldxdw r3, [r5-0xf98]                    
    ldxdw r0, [r5-0xfa0]                    
    mov64 r4, r0                                    r4 = r0
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    stxdw [r10-0x108], r3                   
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    stxdw [r10-0xd0], r0                    
    jlt r4, r0, lbb_16129                           if r4 < r0 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_16129:
    mov64 r0, -1                                    r0 = -1 as i32 as i64 as u64
    jne r3, 0, lbb_16132                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, r4                                    r0 = r4
lbb_16132:
    ldxdw r3, [r5-0xfa8]                    
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    stxdw [r10-0xd8], r3                    
    jle r0, r3, lbb_16661                           if r0 <= r3 { pc += 525 }
    ldxdw r6, [r5-0xfc8]                    
    mov64 r3, r6                                    r3 = r6
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    mul64 r3, 1000                                  r3 *= 1000   ///  r3 = r3.wrapping_mul(1000 as u64)
    mov64 r4, r3                                    r4 = r3
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    mov64 r7, r3                                    r7 = r3
    jne r4, r3, lbb_16146                           if r4 != r3 { pc += 0 }
lbb_16146:
    ldxdw r3, [r5-0xfd0]                    
    ldxdw r0, [r5-0xfe8]                    
    jne r4, r7, lbb_16665                           if r4 != r7 { pc += 516 }
lbb_16149:
    jlt r0, r3, lbb_16151                           if r0 < r3 { pc += 1 }
    mov64 r0, r3                                    r0 = r3
lbb_16151:
    stxdw [r10-0x138], r1                   
    stxdw [r10-0xa0], r0                    
    jlt r0, 10, lbb_16156                           if r0 < (10 as i32 as i64 as u64) { pc += 2 }
    mov64 r1, 10                                    r1 = 10 as i32 as i64 as u64
    stxdw [r10-0xa0], r1                    
lbb_16156:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x108], r1                   
    stxdw [r10-0x120], r2                   
    jeq r0, 0, lbb_16651                            if r0 == (0 as i32 as i64 as u64) { pc += 491 }
    ldxdw r2, [r5-0xf80]                    
    ldxdw r1, [r5-0xf88]                    
    stxdw [r10-0x128], r1                   
    ldxdw r1, [r5-0xf90]                    
    stxdw [r10-0x130], r1                   
    ldxdw r1, [r5-0xfb0]                    
    stxdw [r10-0xf0], r1                    
    ldxdw r1, [r5-0xfb8]                    
    stxdw [r10-0xf8], r1                    
    ldxdw r1, [r5-0xfc0]                    
    stxdw [r10-0x100], r1                   
    ldxdw r3, [r5-0xfd8]                    
    ldxdw r1, [r5-0xfe0]                    
    stxdw [r10-0xa8], r1                    
    ldxdw r4, [r5-0xff0]                    
    ldxdw r1, [r5-0xff8]                    
    stxdw [r10-0x98], r1                    
    ldxdw r1, [r5-0x1000]                   
    stxdw [r10-0xb0], r1                    
    ldxdw r5, [r10-0xd8]                    
    ldxdw r1, [r10-0xd0]                    
    sub64 r5, r1                                    r5 -= r1   ///  r5 = r5.wrapping_sub(r1)
    stxdw [r10-0xc8], r5                    
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x108], r1                   
    add64 r2, 16                                    r2 += 16   ///  r2 = r2.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x140], r2                   
    add64 r3, 16                                    r3 += 16   ///  r3 = r3.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x148], r3                   
    add64 r4, 16                                    r4 += 16   ///  r4 = r4.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x150], r4                   
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    arsh64 r7, 32                                   r7 >>= 32 (signed)   ///  r7 = (r7 as i64).wrapping_shr(32)
    stxdw [r10-0xe8], r7                    
    ldxdw r1, [r10-0x120]                   
    stxdw [r10-0x88], r1                    
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_16197:
    ldxdw r3, [r10-0x120]                   
    mov64 r4, r3                                    r4 = r3
    ldxdw r2, [r10-0x88]                    
    sub64 r4, r2                                    r4 -= r2   ///  r4 = r4.wrapping_sub(r2)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jgt r4, r3, lbb_16204                           if r4 > r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_16204:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jne r2, 0, lbb_16207                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r4                                    r1 = r4
lbb_16207:
    stxdw [r10-0x118], r1                   
    call function_22513                     
    mov64 r6, r0                                    r6 = r0
    ldxdw r1, [r10-0x108]                   
    call function_22513                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x98]                    
    call function_22254                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r0                                    r2 = r0
    call function_22891                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0xbff0000000000000                     r2 load str located at -4616189618054758400
    call function_22576                     
    stxdw [r10-0x110], r0                   
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x88]                    
    jeq r1, 0, lbb_16651                            if r1 == (0 as i32 as i64 as u64) { pc += 424 }
    mov64 r1, r8                                    r1 = r8
    mul64 r1, 40                                    r1 *= 40   ///  r1 = r1.wrapping_mul(40 as u64)
    ldxdw r2, [r10-0x140]                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    stxdw [r10-0x70], r2                    
    mov64 r1, r8                                    r1 = r8
    mul64 r1, 56                                    r1 *= 56   ///  r1 = r1.wrapping_mul(56 as u64)
    ldxdw r2, [r10-0x148]                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    stxdw [r10-0x68], r2                    
    mov64 r1, r8                                    r1 = r8
    lsh64 r1, 6                                     r1 <<= 6   ///  r1 = r1.wrapping_shl(6)
    ldxdw r4, [r10-0x150]                   
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    ja lbb_16266                                    if true { pc += 24 }
lbb_16242:
    mov64 r1, r6                                    r1 = r6
    call function_22513                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r0                                    r2 = r0
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    ldxdw r2, [r10-0x98]                    
    call function_22252                     
    jslt r0, 0, lbb_16252                           if (r0 as i64) < (0 as i32 as i64) { pc += 1 }
    ja lbb_16622                                    if true { pc += 370 }
lbb_16252:
    ldxdw r8, [r10-0x78]                    
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
    ldxdw r1, [r10-0x70]                    
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x70], r1                    
    ldxdw r1, [r10-0x68]                    
    add64 r1, 56                                    r1 += 56   ///  r1 = r1.wrapping_add(56 as i32 as i64 as u64)
    stxdw [r10-0x68], r1                    
    ldxdw r4, [r10-0x80]                    
    add64 r4, 64                                    r4 += 64   ///  r4 = r4.wrapping_add(64 as i32 as i64 as u64)
    ldxdw r2, [r10-0x88]                    
    ldxdw r1, [r10-0xa0]                    
    jlt r8, r1, lbb_16266                           if r8 < r1 { pc += 1 }
    ja lbb_16651                                    if true { pc += 385 }
lbb_16266:
    stxdw [r10-0x78], r8                    
    ldxdw r9, [r4-0x8]                      
    ldxdw r1, [r10-0xa8]                    
    jne r1, 0, lbb_16280                            if r1 != (0 as i32 as i64 as u64) { pc += 10 }
    ldxdw r2, [r4+0x0]                      
    mov64 r1, r9                                    r1 = r9
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jgt r1, r9, lbb_16277                           if r1 > r9 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_16277:
    jne r3, 0, lbb_16279                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, r1                                    r2 = r1
lbb_16279:
    mov64 r9, r2                                    r9 = r2
lbb_16280:
    stxdw [r10-0x80], r4                    
    jeq r9, 0, lbb_16252                            if r9 == (0 as i32 as i64 as u64) { pc += -30 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0xd0]                    
    ldxdw r2, [r10-0xd8]                    
    jle r2, r1, lbb_16330                           if r2 <= r1 { pc += 44 }
    ldxdw r1, [r10-0x68]                    
    ldxw r1, [r1-0x8]                       
    lddw r2, 0xffffffff                             r2 load str located at 4294967295
    jeq r1, r2, lbb_16330                           if r1 == r2 { pc += 39 }
    ldxdw r2, [r10-0x68]                    
    ldxw r8, [r2-0x4]                       
    mov64 r2, r8                                    r2 = r8
    mov64 r7, r1                                    r7 = r1
    jgt r1, r2, lbb_16297                           if r1 > r2 { pc += 1 }
    mov64 r7, r2                                    r7 = r2
lbb_16297:
    ldxdw r2, [r10-0xc8]                    
    jgt r2, r1, lbb_16536                           if r2 > r1 { pc += 237 }
lbb_16299:
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0xffffffff                             r2 load str located at 4294967295
    jeq r1, r2, lbb_16330                           if r1 == r2 { pc += 27 }
    ldxdw r1, [r10-0xc8]                    
    jgt r1, r7, lbb_16306                           if r1 > r7 { pc += 1 }
    ja lbb_16330                                    if true { pc += 24 }
lbb_16306:
    ldxdw r2, [r10-0xc8]                    
    sub64 r2, r7                                    r2 -= r7   ///  r2 = r2.wrapping_sub(r7)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0x128]                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x18]                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_16319                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_16319:
    mov64 r2, -1                                    r2 = -1 as i32 as i64 as u64
    mov64 r4, -1                                    r4 = -1 as i32 as i64 as u64
    jne r3, 0, lbb_16323                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    ldxdw r4, [r10-0x20]                    
lbb_16323:
    mov64 r3, r6                                    r3 = r6
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    jlt r3, r6, lbb_16327                           if r3 < r6 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_16327:
    jne r1, 0, lbb_16329                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, r3                                    r2 = r3
lbb_16329:
    mov64 r6, r2                                    r6 = r2
lbb_16330:
    mov64 r1, r6                                    r1 = r6
    add64 r1, 1000                                  r1 += 1000   ///  r1 = r1.wrapping_add(1000 as i32 as i64 as u64)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r6, lbb_16336                           if r1 < r6 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_16336:
    mov64 r7, -1                                    r7 = -1 as i32 as i64 as u64
    mov64 r6, -1                                    r6 = -1 as i32 as i64 as u64
    jne r2, 0, lbb_16340                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r1                                    r6 = r1
lbb_16340:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, 1000                                  r4 = 1000 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    ldxdw r2, [r10-0x58]                    
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_16351                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_16351:
    mov64 r2, -1                                    r2 = -1 as i32 as i64 as u64
    jne r1, 0, lbb_16354                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ldxdw r2, [r10-0x60]                    
lbb_16354:
    ldxdw r1, [r10-0x70]                    
    stxdw [r1+0x8], r2                      
    ldxdw r1, [r10-0x68]                    
    ldxw r2, [r1+0x0]                       
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0xf8]                    
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    ldxdw r1, [r10-0x48]                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_16368                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_16368:
    lddw r1, 0x7fffffffffffffff                     r1 load str located at 9223372036854775807
    jne r2, 0, lbb_16373                            if r2 != (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r10-0x50]                    
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
lbb_16373:
    mov64 r3, r1                                    r3 = r1
    ldxdw r2, [r10-0xf0]                    
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r3, r1, lbb_16379                           if r3 < r1 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_16379:
    mov64 r2, -1                                    r2 = -1 as i32 as i64 as u64
    jne r4, 0, lbb_16382                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, r3                                    r2 = r3
lbb_16382:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    ldxdw r2, [r10-0x38]                    
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_16392                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_16392:
    mov64 r2, -1                                    r2 = -1 as i32 as i64 as u64
    jne r1, 0, lbb_16395                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ldxdw r2, [r10-0x40]                    
lbb_16395:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0x100]                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    ldxdw r1, [r10-0x28]                    
    jne r1, 0, lbb_16404                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_16404:
    jne r8, 0, lbb_16406                            if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    ldxdw r7, [r10-0x30]                    
lbb_16406:
    mov64 r2, 2147483647                            r2 = 2147483647 as i32 as i64 as u64
    lddw r1, 0x7a11fffffffff                        r1 load str located at 2147483647999999
    jgt r7, r1, lbb_16412                           if r7 > r1 { pc += 2 }
    div64 r7, 1000000                               r7 /= 1000000   ///  r7 = r7 / (1000000 as u64)
    mov64 r2, r7                                    r2 = r7
lbb_16412:
    ldxdw r1, [r10-0x70]                    
    stxw [r1+0x0], r2                       
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    ldxdw r1, [r10-0xe8]                    
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    jslt r1, 2147483647, lbb_16420                  if (r1 as i64) < (2147483647 as i32 as i64) { pc += 1 }
    mov64 r1, 2147483647                            r1 = 2147483647 as i32 as i64 as u64
lbb_16420:
    jsgt r1, -2147483648, lbb_16422                 if (r1 as i64) > (-2147483648 as i32 as i64) { pc += 1 }
    mov64 r1, -2147483648                           r1 = -2147483648 as i32 as i64 as u64
lbb_16422:
    call function_22931                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x3e7ad7f29abcaf48                     r2 load str located at 4502148214488346440
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x3ff0000000000000                     r2 load str located at 4607182418800017408
    call function_22576                     
    mov64 r1, r0                                    r1 = r0
    ldxdw r2, [r10-0xe0]                    
    call function_22254                     
    mov64 r7, r0                                    r7 = r0
    ldxdw r1, [r10-0xb0]                    
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    stxdw [r10-0x90], r7                    
    jeq r1, 0, lbb_16476                            if r1 == (0 as i32 as i64 as u64) { pc += 37 }
    mov64 r1, r7                                    r1 = r7
    ldxdw r2, [r10-0x98]                    
    call function_22252                     
    jslt r0, 0, lbb_16444                           if (r0 as i64) < (0 as i32 as i64) { pc += 1 }
    ja lbb_16476                                    if true { pc += 32 }
lbb_16444:
    ldxdw r1, [r10-0x98]                    
    mov64 r2, r7                                    r2 = r7
    call function_22891                     
    ldxdw r1, [r10-0x110]                   
    mov64 r2, r0                                    r2 = r0
    call function_21530                     
    mov64 r8, r0                                    r8 = r0
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r8                                    r2 = r8
    call function_22497                     
    jeq r0, 0, lbb_16456                            if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_16490                                    if true { pc += 34 }
lbb_16456:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r1, r8                                    r1 = r8
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_22511                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r8                                    r1 = r8
    call function_22905                     
    jslt r7, 0, lbb_16465                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r6, r0                                    r6 = r0
lbb_16465:
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_22929                     
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
    jsgt r0, 0, lbb_16472                           if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r1, r6                                    r1 = r6
lbb_16472:
    ldxdw r7, [r10-0x90]                    
    jlt r9, r1, lbb_16475                           if r9 < r1 { pc += 1 }
    mov64 r9, r1                                    r9 = r1
lbb_16475:
    jeq r1, 0, lbb_16252                            if r1 == (0 as i32 as i64 as u64) { pc += -224 }
lbb_16476:
    ldxdw r1, [r10-0x88]                    
    mov64 r6, r1                                    r6 = r1
    jlt r1, r9, lbb_16480                           if r1 < r9 { pc += 1 }
    mov64 r6, r9                                    r6 = r9
lbb_16480:
    mov64 r1, r6                                    r1 = r6
    call function_22513                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r9, r0                                    r9 = r0
    mov64 r1, r9                                    r1 = r9
    mov64 r2, r9                                    r2 = r9
    call function_22497                     
    jeq r0, 0, lbb_16496                            if r0 == (0 as i32 as i64 as u64) { pc += 6 }
lbb_16490:
    lddw r1, 0x10002cec8 --> b"assertion failed: !self.is_nan()"        r1 load str located at 4295151304
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    lddw r3, 0x10002f660 --> b"\x00\x00\x00\x00P\xda\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00"\x00\x00\x0…        r3 load str located at 4295161440
    call function_18704                     
lbb_16496:
    stxdw [r10-0xc0], r6                    
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, r9                                    r1 = r9
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_22511                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r9                                    r1 = r9
    call function_22905                     
    jslt r7, 0, lbb_16506                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r8, r0                                    r8 = r0
lbb_16506:
    mov64 r1, r9                                    r1 = r9
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_22929                     
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
    jsgt r0, 0, lbb_16513                           if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r1, r8                                    r1 = r8
lbb_16513:
    ldxdw r2, [r10-0xb8]                    
    mov64 r9, r2                                    r9 = r2
    ldxdw r6, [r10-0x90]                    
    jlt r2, r1, lbb_16518                           if r2 < r1 { pc += 1 }
    mov64 r9, r1                                    r9 = r1
lbb_16518:
    jeq r9, 0, lbb_16252                            if r9 == (0 as i32 as i64 as u64) { pc += -267 }
    ldxdw r2, [r10-0xb0]                    
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jeq r2, 0, lbb_16622                            if r2 == (0 as i32 as i64 as u64) { pc += 100 }
    ldxdw r2, [r10-0xb8]                    
    jgt r1, r2, lbb_16525                           if r1 > r2 { pc += 1 }
    ja lbb_16600                                    if true { pc += 75 }
lbb_16525:
    mov64 r1, r9                                    r1 = r9
    call function_22513                     
    mov64 r1, r0                                    r1 = r0
    mov64 r2, r6                                    r2 = r6
    call function_21530                     
    mov64 r8, r0                                    r8 = r0
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r8                                    r2 = r8
    call function_22497                     
    jeq r0, 0, lbb_16555                            if r0 == (0 as i32 as i64 as u64) { pc += 20 }
    ja lbb_16490                                    if true { pc += -46 }
lbb_16536:
    ldxdw r3, [r10-0xc8]                    
    mov64 r2, r3                                    r2 = r3
    jlt r3, r7, lbb_16540                           if r3 < r7 { pc += 1 }
    mov64 r2, r7                                    r2 = r7
lbb_16540:
    sub64 r2, r1                                    r2 -= r1   ///  r2 = r2.wrapping_sub(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0x130]                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x8]                     
    jne r2, 0, lbb_16551                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_16551:
    mov64 r6, -1                                    r6 = -1 as i32 as i64 as u64
    jne r1, 0, lbb_16299                            if r1 != (0 as i32 as i64 as u64) { pc += -254 }
    ldxdw r6, [r10-0x10]                    
    ja lbb_16299                                    if true { pc += -256 }
lbb_16555:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r1, r8                                    r1 = r8
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_22511                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r8                                    r1 = r8
    call function_22905                     
    jslt r7, 0, lbb_16564                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r6, r0                                    r6 = r0
lbb_16564:
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_22929                     
    mov64 r7, -1                                    r7 = -1 as i32 as i64 as u64
    jsgt r0, 0, lbb_16571                           if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r7, r6                                    r7 = r6
lbb_16571:
    mov64 r1, r7                                    r1 = r7
    call function_22513                     
    ldxdw r1, [r10-0x90]                    
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r6, r0                                    r6 = r0
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r6                                    r2 = r6
    call function_22497                     
    jeq r0, 0, lbb_16582                            if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_16490                                    if true { pc += -92 }
lbb_16582:
    stxdw [r10-0xc0], r7                    
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_22511                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r6                                    r1 = r6
    call function_22905                     
    jslt r7, 0, lbb_16592                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r8, r0                                    r8 = r0
lbb_16592:
    mov64 r1, r6                                    r1 = r6
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_22929                     
    mov64 r9, -1                                    r9 = -1 as i32 as i64 as u64
    jsgt r0, 0, lbb_16599                           if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r9, r8                                    r9 = r8
lbb_16599:
    jeq r9, 0, lbb_16252                            if r9 == (0 as i32 as i64 as u64) { pc += -348 }
lbb_16600:
    ldxdw r1, [r10-0x108]                   
    mov64 r2, r1                                    r2 = r1
    add64 r2, r9                                    r2 += r9   ///  r2 = r2.wrapping_add(r9)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r2, r1, lbb_16607                           if r2 < r1 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_16607:
    mov64 r6, -1                                    r6 = -1 as i32 as i64 as u64
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
    jne r3, 0, lbb_16611                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r2                                    r1 = r2
lbb_16611:
    call function_22513                     
    mov64 r7, r0                                    r7 = r0
    ldxdw r2, [r10-0x118]                   
    mov64 r1, r2                                    r1 = r2
    ldxdw r3, [r10-0xc0]                    
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    jlt r1, r2, lbb_16619                           if r1 < r2 { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_16619:
    jne r8, 0, lbb_16242                            if r8 != (0 as i32 as i64 as u64) { pc += -378 }
    mov64 r6, r1                                    r6 = r1
    ja lbb_16242                                    if true { pc += -380 }
lbb_16622:
    ldxdw r1, [r10-0x88]                    
    ldxdw r2, [r10-0xc0]                    
    jlt r1, r2, lbb_16673                           if r1 < r2 { pc += 48 }
    ldxdw r1, [r10-0xb8]                    
    jlt r1, r9, lbb_16670                           if r1 < r9 { pc += 43 }
    ldxdw r3, [r10-0x108]                   
    mov64 r1, r3                                    r1 = r3
    add64 r1, r9                                    r1 += r9   ///  r1 = r1.wrapping_add(r9)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r3, lbb_16633                           if r1 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_16633:
    mov64 r3, -1                                    r3 = -1 as i32 as i64 as u64
    stxdw [r10-0x108], r3                   
    jne r2, 0, lbb_16637                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    stxdw [r10-0x108], r1                   
lbb_16637:
    ldxdw r8, [r10-0x78]                    
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
    ldxdw r2, [r10-0x88]                    
    ldxdw r3, [r10-0xc0]                    
    sub64 r2, r3                                    r2 -= r3   ///  r2 = r2.wrapping_sub(r3)
    ldxdw r1, [r10-0x70]                    
    stxdw [r1-0x10], r3                     
    stxdw [r1-0x8], r9                      
    ldxdw r1, [r10-0xb8]                    
    sub64 r1, r9                                    r1 -= r9   ///  r1 = r1.wrapping_sub(r9)
    stxdw [r10-0xb8], r1                    
    stxdw [r10-0x88], r2                    
    ldxdw r1, [r10-0xa0]                    
    jlt r8, r1, lbb_16197                           if r8 < r1 { pc += -454 }
lbb_16651:
    ldxdw r1, [r10-0x120]                   
    mov64 r3, r1                                    r3 = r1
    sub64 r3, r2                                    r3 -= r2   ///  r3 = r3.wrapping_sub(r2)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jgt r3, r1, lbb_16658                           if r3 > r1 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_16658:
    ldxdw r1, [r10-0x138]                   
    jne r2, 0, lbb_16661                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r3                                    r4 = r3
lbb_16661:
    ldxdw r2, [r10-0x108]                   
    stxdw [r1+0x8], r2                      
    stxdw [r1+0x0], r4                      
    exit                                    
lbb_16665:
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    arsh64 r6, 63                                   r6 >>= 63 (signed)   ///  r6 = (r6 as i64).wrapping_shr(63)
    xor64 r6, 2147483647                            r6 ^= 2147483647   ///  r6 = r6.xor(2147483647)
    mov64 r7, r6                                    r7 = r6
    ja lbb_16149                                    if true { pc += -521 }
lbb_16670:
    lddw r1, 0x10002f5e8 --> b"\x00\x00\x00\x00\x16\xda\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xc8\x00\…        r1 load str located at 4295161320
    call function_20957                     
lbb_16673:
    lddw r1, 0x10002f5d0 --> b"\x00\x00\x00\x00\x16\xda\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xc6\x00\…        r1 load str located at 4295161296
    call function_20957                     

function_16676:
    ldxdw r4, [r2+0x18]                     
    ldxw r0, [r4+0xf0]                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    mov64 r3, r0                                    r3 = r0
    mul64 r3, 1000                                  r3 *= 1000   ///  r3 = r3.wrapping_mul(1000 as u64)
    mov64 r6, r3                                    r6 = r3
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    arsh64 r6, 32                                   r6 >>= 32 (signed)   ///  r6 = (r6 as i64).wrapping_shr(32)
    jne r6, r3, lbb_16686                           if r6 != r3 { pc += 0 }
lbb_16686:
    ldxdw r5, [r2+0x28]                     
    ldxw r5, [r5+0x14]                      
    ldxdw r2, [r2+0x20]                     
    ldxw r2, [r2+0x14]                      
    jne r6, r3, lbb_16760                           if r6 != r3 { pc += 69 }
lbb_16691:
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    arsh64 r5, 32                                   r5 >>= 32 (signed)   ///  r5 = (r5 as i64).wrapping_shr(32)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    sub64 r2, r5                                    r2 -= r5   ///  r2 = r2.wrapping_sub(r5)
    jslt r2, 2147483647, lbb_16698                  if (r2 as i64) < (2147483647 as i32 as i64) { pc += 1 }
    mov64 r2, 2147483647                            r2 = 2147483647 as i32 as i64 as u64
lbb_16698:
    ldxw r5, [r4+0xf4]                      
    jsgt r2, -2147483648, lbb_16701                 if (r2 as i64) > (-2147483648 as i32 as i64) { pc += 1 }
    mov64 r2, -2147483648                           r2 = -2147483648 as i32 as i64 as u64
lbb_16701:
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    arsh64 r5, 32                                   r5 >>= 32 (signed)   ///  r5 = (r5 as i64).wrapping_shr(32)
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    jslt r2, 2147483647, lbb_16706                  if (r2 as i64) < (2147483647 as i32 as i64) { pc += 1 }
    mov64 r2, 2147483647                            r2 = 2147483647 as i32 as i64 as u64
lbb_16706:
    jsgt r2, -2147483648, lbb_16708                 if (r2 as i64) > (-2147483648 as i32 as i64) { pc += 1 }
    mov64 r2, -2147483648                           r2 = -2147483648 as i32 as i64 as u64
lbb_16708:
    mov64 r5, r3                                    r5 = r3
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    lddw r0, 0x80000000                             r0 load str located at 2147483648
    jeq r5, r0, lbb_16765                           if r5 == r0 { pc += 51 }
    ldxw r0, [r4+0x100]                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    mov64 r5, r3                                    r5 = r3
    neg64 r5                                        r5 = -r5   ///  r5 = (r5 as i64).wrapping_neg() as u64
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    arsh64 r5, 32                                   r5 >>= 32 (signed)   ///  r5 = (r5 as i64).wrapping_shr(32)
    jsge r0, r5, lbb_16728                          if (r0 as i64) >= (r5 as i64) { pc += 6 }
lbb_16722:
    lddw r1, 0x10002d9a9 --> b"assertion failed: min <= max"        r1 load str located at 4295154089
    mov64 r2, 28                                    r2 = 28 as i32 as i64 as u64
    lddw r3, 0x10002f588 --> b"\x00\x00\x00\x00\xc5\xd9\x02\x00Q\x00\x00\x00\x00\x00\x00\x00\x07\x04\x00…        r3 load str located at 4295161224
    call function_18704                     
lbb_16728:
    mov64 r6, r2                                    r6 = r2
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    lddw r7, 0x80000000                             r7 load str located at 2147483648
    jeq r6, r7, lbb_16768                           if r6 == r7 { pc += 34 }
    ldxw r6, [r4+0x104]                     
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    arsh64 r6, 32                                   r6 >>= 32 (signed)   ///  r6 = (r6 as i64).wrapping_shr(32)
    jsge r6, r5, lbb_16739                          if (r6 as i64) >= (r5 as i64) { pc += 1 }
    ja lbb_16722                                    if true { pc += -17 }
lbb_16739:
    mov64 r4, r2                                    r4 = r2
    neg64 r4                                        r4 = -r4   ///  r4 = (r4 as i64).wrapping_neg() as u64
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    jslt r4, r6, lbb_16745                          if (r4 as i64) < (r6 as i64) { pc += 1 }
    mov64 r4, r6                                    r4 = r6
lbb_16745:
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r6, r5                                    r6 = r5
    jsgt r2, r3, lbb_16752                          if (r2 as i64) > (r3 as i64) { pc += 1 }
    mov64 r6, r4                                    r6 = r4
lbb_16752:
    mov64 r3, r2                                    r3 = r2
    jslt r2, r0, lbb_16755                          if (r2 as i64) < (r0 as i64) { pc += 1 }
    mov64 r3, r0                                    r3 = r0
lbb_16755:
    jslt r2, r5, lbb_16757                          if (r2 as i64) < (r5 as i64) { pc += 1 }
    mov64 r5, r3                                    r5 = r3
lbb_16757:
    stxw [r1+0x0], r5                       
    stxw [r1+0x4], r6                       
    exit                                    
lbb_16760:
    lddw r3, 0x80000000                             r3 load str located at 2147483648
    jslt r0, 0, lbb_16691                           if (r0 as i64) < (0 as i32 as i64) { pc += -72 }
    mov64 r3, 2147483647                            r3 = 2147483647 as i32 as i64 as u64
    ja lbb_16691                                    if true { pc += -74 }
lbb_16765:
    lddw r1, 0x10002f600 --> b"\x00\x00\x00\x003\xda\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00'\x00\x00\x0…        r1 load str located at 4295161344
    call function_20979                     
lbb_16768:
    lddw r1, 0x10002f618 --> b"\x00\x00\x00\x003\xda\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00*\x00\x00\x0…        r1 load str located at 4295161368
    call function_20979                     

function_16771:
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    sub64 r3, r4                                    r3 -= r4   ///  r3 = r3.wrapping_sub(r4)
    jslt r3, 2147483647, lbb_16778                  if (r3 as i64) < (2147483647 as i32 as i64) { pc += 1 }
    mov64 r3, 2147483647                            r3 = 2147483647 as i32 as i64 as u64
lbb_16778:
    jsgt r3, -2147483648, lbb_16780                 if (r3 as i64) > (-2147483648 as i32 as i64) { pc += 1 }
    mov64 r3, -2147483648                           r3 = -2147483648 as i32 as i64 as u64
lbb_16780:
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    jslt r3, 2147483647, lbb_16785                  if (r3 as i64) < (2147483647 as i32 as i64) { pc += 1 }
    mov64 r3, 2147483647                            r3 = 2147483647 as i32 as i64 as u64
lbb_16785:
    jsgt r3, -2147483648, lbb_16787                 if (r3 as i64) > (-2147483648 as i32 as i64) { pc += 1 }
    mov64 r3, -2147483648                           r3 = -2147483648 as i32 as i64 as u64
lbb_16787:
    ldxdw r0, [r5-0x1000]                   
    mov64 r2, r0                                    r2 = r0
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    lddw r4, 0x80000000                             r4 load str located at 2147483648
    jeq r2, r4, lbb_16843                           if r2 == r4 { pc += 49 }
    ldxdw r4, [r5-0xff8]                    
    mov64 r2, r0                                    r2 = r0
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r6, r4                                    r6 = r4
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    arsh64 r6, 32                                   r6 >>= 32 (signed)   ///  r6 = (r6 as i64).wrapping_shr(32)
    jsle r2, r6, lbb_16809                          if (r2 as i64) <= (r6 as i64) { pc += 6 }
lbb_16803:
    lddw r1, 0x10002d9a9 --> b"assertion failed: min <= max"        r1 load str located at 4295154089
    mov64 r2, 28                                    r2 = 28 as i32 as i64 as u64
    lddw r3, 0x10002f588 --> b"\x00\x00\x00\x00\xc5\xd9\x02\x00Q\x00\x00\x00\x00\x00\x00\x00\x07\x04\x00…        r3 load str located at 4295161224
    call function_18704                     
lbb_16809:
    mov64 r6, r3                                    r6 = r3
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    lddw r7, 0x80000000                             r7 load str located at 2147483648
    jeq r6, r7, lbb_16846                           if r6 == r7 { pc += 31 }
    ldxdw r5, [r5-0xff0]                    
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    arsh64 r5, 32                                   r5 >>= 32 (signed)   ///  r5 = (r5 as i64).wrapping_shr(32)
    jsle r2, r5, lbb_16820                          if (r2 as i64) <= (r5 as i64) { pc += 1 }
    ja lbb_16803                                    if true { pc += -17 }
lbb_16820:
    mov64 r6, r3                                    r6 = r3
    neg64 r6                                        r6 = -r6   ///  r6 = (r6 as i64).wrapping_neg() as u64
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    arsh64 r6, 32                                   r6 >>= 32 (signed)   ///  r6 = (r6 as i64).wrapping_shr(32)
    jslt r6, r5, lbb_16826                          if (r6 as i64) < (r5 as i64) { pc += 1 }
    mov64 r6, r5                                    r6 = r5
lbb_16826:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    mov64 r5, r2                                    r5 = r2
    jsgt r3, r0, lbb_16833                          if (r3 as i64) > (r0 as i64) { pc += 1 }
    mov64 r5, r6                                    r5 = r6
lbb_16833:
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    mov64 r0, r3                                    r0 = r3
    jslt r3, r4, lbb_16838                          if (r3 as i64) < (r4 as i64) { pc += 1 }
    mov64 r0, r4                                    r0 = r4
lbb_16838:
    jslt r3, r2, lbb_16840                          if (r3 as i64) < (r2 as i64) { pc += 1 }
    mov64 r2, r0                                    r2 = r0
lbb_16840:
    stxw [r1+0x0], r2                       
    stxw [r1+0x4], r5                       
    exit                                    
lbb_16843:
    lddw r1, 0x10002f600 --> b"\x00\x00\x00\x003\xda\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00'\x00\x00\x0…        r1 load str located at 4295161344
    call function_20979                     
lbb_16846:
    lddw r1, 0x10002f618 --> b"\x00\x00\x00\x003\xda\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00*\x00\x00\x0…        r1 load str located at 4295161368
    call function_20979                     

function_16849:
    stxdw [r10-0xd0], r4                    
    stxdw [r10-0xa8], r3                    
    ldxdw r3, [r5-0xf98]                    
    ldxdw r0, [r5-0xfa0]                    
    mov64 r4, r0                                    r4 = r0
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    stxdw [r10-0xf8], r3                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    stxdw [r10-0xc0], r0                    
    jlt r4, r0, lbb_16861                           if r4 < r0 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_16861:
    mov64 r0, -1                                    r0 = -1 as i32 as i64 as u64
    jne r3, 0, lbb_16864                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, r4                                    r0 = r4
lbb_16864:
    ldxdw r3, [r5-0xfa8]                    
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    stxdw [r10-0xc8], r3                    
    jle r0, r3, lbb_17373                           if r0 <= r3 { pc += 505 }
    ldxdw r3, [r5-0xfd0]                    
    ldxdw r4, [r5-0xfe8]                    
    jlt r4, r3, lbb_16872                           if r4 < r3 { pc += 1 }
    mov64 r4, r3                                    r4 = r3
lbb_16872:
    stxdw [r10-0x128], r1                   
    stxdw [r10-0x90], r4                    
    jlt r4, 10, lbb_16877                           if r4 < (10 as i32 as i64 as u64) { pc += 2 }
    mov64 r1, 10                                    r1 = 10 as i32 as i64 as u64
    stxdw [r10-0x90], r1                    
lbb_16877:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xf8], r1                    
    stxdw [r10-0x110], r2                   
    mov64 r1, r2                                    r1 = r2
    jeq r4, 0, lbb_17363                            if r4 == (0 as i32 as i64 as u64) { pc += 481 }
    ldxdw r2, [r5-0xf80]                    
    ldxdw r1, [r5-0xf88]                    
    stxdw [r10-0x118], r1                   
    ldxdw r1, [r5-0xf90]                    
    stxdw [r10-0x120], r1                   
    ldxdw r1, [r5-0xfb0]                    
    stxdw [r10-0xd8], r1                    
    ldxdw r1, [r5-0xfb8]                    
    stxdw [r10-0xe0], r1                    
    ldxdw r1, [r5-0xfc0]                    
    stxdw [r10-0xe8], r1                    
    ldxdw r3, [r5-0xfc8]                    
    ldxdw r4, [r5-0xfd8]                    
    ldxdw r1, [r5-0xfe0]                    
    stxdw [r10-0x98], r1                    
    ldxdw r0, [r5-0xff0]                    
    ldxdw r1, [r5-0xff8]                    
    stxdw [r10-0x88], r1                    
    ldxdw r1, [r5-0x1000]                   
    stxdw [r10-0xa0], r1                    
    ldxdw r5, [r10-0xc8]                    
    ldxdw r1, [r10-0xc0]                    
    sub64 r5, r1                                    r5 -= r1   ///  r5 = r5.wrapping_sub(r1)
    stxdw [r10-0xb8], r5                    
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xf8], r1                    
    add64 r2, 16                                    r2 += 16   ///  r2 = r2.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x130], r2                   
    add64 r4, 16                                    r4 += 16   ///  r4 = r4.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x138], r4                   
    add64 r0, 16                                    r0 += 16   ///  r0 = r0.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x140], r0                   
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    arsh64 r3, 32                                   r3 >>= 32 (signed)   ///  r3 = (r3 as i64).wrapping_shr(32)
    stxdw [r10-0xf0], r3                    
    ldxdw r1, [r10-0x110]                   
    stxdw [r10-0x78], r1                    
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_16920:
    ldxdw r3, [r10-0x110]                   
    mov64 r4, r3                                    r4 = r3
    ldxdw r2, [r10-0x78]                    
    sub64 r4, r2                                    r4 -= r2   ///  r4 = r4.wrapping_sub(r2)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jgt r4, r3, lbb_16927                           if r4 > r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_16927:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jne r2, 0, lbb_16930                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r4                                    r1 = r4
lbb_16930:
    stxdw [r10-0x108], r1                   
    call function_22513                     
    mov64 r6, r0                                    r6 = r0
    ldxdw r1, [r10-0xf8]                    
    call function_22513                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x88]                    
    call function_22254                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r0                                    r2 = r0
    call function_22891                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0xbff0000000000000                     r2 load str located at -4616189618054758400
    call function_22576                     
    stxdw [r10-0x100], r0                   
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0x78]                    
    jeq r2, 0, lbb_17363                            if r2 == (0 as i32 as i64 as u64) { pc += 413 }
    mov64 r1, r8                                    r1 = r8
    mul64 r1, 40                                    r1 *= 40   ///  r1 = r1.wrapping_mul(40 as u64)
    ldxdw r2, [r10-0x130]                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    stxdw [r10-0x60], r2                    
    mov64 r1, r8                                    r1 = r8
    mul64 r1, 56                                    r1 *= 56   ///  r1 = r1.wrapping_mul(56 as u64)
    ldxdw r2, [r10-0x138]                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    stxdw [r10-0x58], r2                    
    mov64 r1, r8                                    r1 = r8
    lsh64 r1, 6                                     r1 <<= 6   ///  r1 = r1.wrapping_shl(6)
    ldxdw r4, [r10-0x140]                   
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    ja lbb_16989                                    if true { pc += 24 }
lbb_16965:
    mov64 r1, r6                                    r1 = r6
    call function_22513                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r0                                    r2 = r0
    call function_21530                     
    mov64 r1, r0                                    r1 = r0
    ldxdw r2, [r10-0x88]                    
    call function_22252                     
    jslt r0, 0, lbb_16975                           if (r0 as i64) < (0 as i32 as i64) { pc += 1 }
    ja lbb_17333                                    if true { pc += 358 }
lbb_16975:
    ldxdw r8, [r10-0x68]                    
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
    ldxdw r1, [r10-0x60]                    
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x60], r1                    
    ldxdw r1, [r10-0x58]                    
    add64 r1, 56                                    r1 += 56   ///  r1 = r1.wrapping_add(56 as i32 as i64 as u64)
    stxdw [r10-0x58], r1                    
    ldxdw r4, [r10-0x70]                    
    add64 r4, 64                                    r4 += 64   ///  r4 = r4.wrapping_add(64 as i32 as i64 as u64)
    ldxdw r1, [r10-0x78]                    
    ldxdw r2, [r10-0x90]                    
    jlt r8, r2, lbb_16989                           if r8 < r2 { pc += 1 }
    ja lbb_17363                                    if true { pc += 374 }
lbb_16989:
    stxdw [r10-0x68], r8                    
    ldxdw r9, [r4-0x8]                      
    ldxdw r1, [r10-0x98]                    
    jne r1, 0, lbb_17003                            if r1 != (0 as i32 as i64 as u64) { pc += 10 }
    ldxdw r2, [r4+0x0]                      
    mov64 r1, r9                                    r1 = r9
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jgt r1, r9, lbb_17000                           if r1 > r9 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_17000:
    jne r3, 0, lbb_17002                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, r1                                    r2 = r1
lbb_17002:
    mov64 r9, r2                                    r9 = r2
lbb_17003:
    stxdw [r10-0x70], r4                    
    jeq r9, 0, lbb_16975                            if r9 == (0 as i32 as i64 as u64) { pc += -30 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0xc0]                    
    ldxdw r2, [r10-0xc8]                    
    jle r2, r1, lbb_17053                           if r2 <= r1 { pc += 44 }
    ldxdw r1, [r10-0x58]                    
    ldxw r1, [r1-0x8]                       
    lddw r2, 0xffffffff                             r2 load str located at 4294967295
    jeq r1, r2, lbb_17053                           if r1 == r2 { pc += 39 }
    ldxdw r2, [r10-0x58]                    
    ldxw r8, [r2-0x4]                       
    mov64 r2, r8                                    r2 = r8
    mov64 r7, r1                                    r7 = r1
    jgt r1, r2, lbb_17020                           if r1 > r2 { pc += 1 }
    mov64 r7, r2                                    r7 = r2
lbb_17020:
    ldxdw r2, [r10-0xb8]                    
    jgt r2, r1, lbb_17247                           if r2 > r1 { pc += 225 }
lbb_17022:
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0xffffffff                             r2 load str located at 4294967295
    jeq r1, r2, lbb_17053                           if r1 == r2 { pc += 27 }
    ldxdw r1, [r10-0xb8]                    
    jgt r1, r7, lbb_17029                           if r1 > r7 { pc += 1 }
    ja lbb_17053                                    if true { pc += 24 }
lbb_17029:
    ldxdw r2, [r10-0xb8]                    
    sub64 r2, r7                                    r2 -= r7   ///  r2 = r2.wrapping_sub(r7)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0x118]                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x18]                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_17042                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_17042:
    mov64 r2, -1                                    r2 = -1 as i32 as i64 as u64
    mov64 r4, -1                                    r4 = -1 as i32 as i64 as u64
    jne r3, 0, lbb_17046                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    ldxdw r4, [r10-0x20]                    
lbb_17046:
    mov64 r3, r6                                    r3 = r6
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    jlt r3, r6, lbb_17050                           if r3 < r6 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_17050:
    jne r1, 0, lbb_17052                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, r3                                    r2 = r3
lbb_17052:
    mov64 r6, r2                                    r6 = r2
lbb_17053:
    mov64 r1, r6                                    r1 = r6
    add64 r1, 1000000                               r1 += 1000000   ///  r1 = r1.wrapping_add(1000000 as i32 as i64 as u64)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r6, lbb_17059                           if r1 < r6 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_17059:
    mov64 r7, -1                                    r7 = -1 as i32 as i64 as u64
    mov64 r6, -1                                    r6 = -1 as i32 as i64 as u64
    jne r2, 0, lbb_17063                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r1                                    r6 = r1
lbb_17063:
    ldxdw r1, [r10-0x60]                    
    stxdw [r1+0x8], r6                      
    ldxdw r1, [r10-0x58]                    
    ldxw r2, [r1+0x0]                       
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0xe0]                    
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    ldxdw r1, [r10-0x48]                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_17077                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_17077:
    lddw r1, 0x20c49ba5e353f7                       r1 load str located at 9223372036854775
    jne r2, 0, lbb_17082                            if r2 != (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r10-0x50]                    
    div64 r1, 2000                                  r1 /= 2000   ///  r1 = r1 / (2000 as u64)
lbb_17082:
    mov64 r3, r1                                    r3 = r1
    ldxdw r2, [r10-0xd8]                    
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r3, r1, lbb_17088                           if r3 < r1 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_17088:
    mov64 r2, -1                                    r2 = -1 as i32 as i64 as u64
    jne r4, 0, lbb_17091                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, r3                                    r2 = r3
lbb_17091:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    ldxdw r2, [r10-0x38]                    
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_17101                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_17101:
    lddw r2, 0x10c6f7a0b5ed                         r2 load str located at 18446744073709
    jne r1, 0, lbb_17106                            if r1 != (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r2, [r10-0x40]                    
    div64 r2, 1000000                               r2 /= 1000000   ///  r2 = r2 / (1000000 as u64)
lbb_17106:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0xe8]                    
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    ldxdw r1, [r10-0x28]                    
    jne r1, 0, lbb_17115                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_17115:
    jne r8, 0, lbb_17117                            if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    ldxdw r7, [r10-0x30]                    
lbb_17117:
    mov64 r2, 2147483647                            r2 = 2147483647 as i32 as i64 as u64
    lddw r1, 0x1f3ffffffff                          r1 load str located at 2147483647999
    jgt r7, r1, lbb_17123                           if r7 > r1 { pc += 2 }
    div64 r7, 1000                                  r7 /= 1000   ///  r7 = r7 / (1000 as u64)
    mov64 r2, r7                                    r2 = r7
lbb_17123:
    ldxdw r1, [r10-0x60]                    
    stxw [r1+0x0], r2                       
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    ldxdw r1, [r10-0xf0]                    
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    jslt r1, 2147483647, lbb_17131                  if (r1 as i64) < (2147483647 as i32 as i64) { pc += 1 }
    mov64 r1, 2147483647                            r1 = 2147483647 as i32 as i64 as u64
lbb_17131:
    jsgt r1, -2147483648, lbb_17133                 if (r1 as i64) > (-2147483648 as i32 as i64) { pc += 1 }
    mov64 r1, -2147483648                           r1 = -2147483648 as i32 as i64 as u64
lbb_17133:
    call function_22931                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x3e7ad7f29abcaf48                     r2 load str located at 4502148214488346440
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x3ff0000000000000                     r2 load str located at 4607182418800017408
    call function_22576                     
    mov64 r1, r0                                    r1 = r0
    ldxdw r2, [r10-0xd0]                    
    call function_22254                     
    mov64 r7, r0                                    r7 = r0
    ldxdw r1, [r10-0xa0]                    
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    stxdw [r10-0x80], r7                    
    jeq r1, 0, lbb_17187                            if r1 == (0 as i32 as i64 as u64) { pc += 37 }
    mov64 r1, r7                                    r1 = r7
    ldxdw r2, [r10-0x88]                    
    call function_22252                     
    jslt r0, 0, lbb_17155                           if (r0 as i64) < (0 as i32 as i64) { pc += 1 }
    ja lbb_17187                                    if true { pc += 32 }
lbb_17155:
    ldxdw r1, [r10-0x88]                    
    mov64 r2, r7                                    r2 = r7
    call function_22891                     
    ldxdw r1, [r10-0x100]                   
    mov64 r2, r0                                    r2 = r0
    call function_21530                     
    mov64 r8, r0                                    r8 = r0
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r8                                    r2 = r8
    call function_22497                     
    jeq r0, 0, lbb_17167                            if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17201                                    if true { pc += 34 }
lbb_17167:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r1, r8                                    r1 = r8
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_22511                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r8                                    r1 = r8
    call function_22905                     
    jslt r7, 0, lbb_17176                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r6, r0                                    r6 = r0
lbb_17176:
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_22929                     
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
    jsgt r0, 0, lbb_17183                           if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r1, r6                                    r1 = r6
lbb_17183:
    ldxdw r7, [r10-0x80]                    
    jlt r9, r1, lbb_17186                           if r9 < r1 { pc += 1 }
    mov64 r9, r1                                    r9 = r1
lbb_17186:
    jeq r1, 0, lbb_16975                            if r1 == (0 as i32 as i64 as u64) { pc += -212 }
lbb_17187:
    ldxdw r1, [r10-0x78]                    
    mov64 r6, r1                                    r6 = r1
    jlt r1, r9, lbb_17191                           if r1 < r9 { pc += 1 }
    mov64 r6, r9                                    r6 = r9
lbb_17191:
    mov64 r1, r6                                    r1 = r6
    call function_22513                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r9, r0                                    r9 = r0
    mov64 r1, r9                                    r1 = r9
    mov64 r2, r9                                    r2 = r9
    call function_22497                     
    jeq r0, 0, lbb_17207                            if r0 == (0 as i32 as i64 as u64) { pc += 6 }
lbb_17201:
    lddw r1, 0x10002cec8 --> b"assertion failed: !self.is_nan()"        r1 load str located at 4295151304
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    lddw r3, 0x10002f660 --> b"\x00\x00\x00\x00P\xda\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00"\x00\x00\x0…        r3 load str located at 4295161440
    call function_18704                     
lbb_17207:
    stxdw [r10-0xb0], r6                    
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, r9                                    r1 = r9
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_22511                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r9                                    r1 = r9
    call function_22905                     
    jslt r7, 0, lbb_17217                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r8, r0                                    r8 = r0
lbb_17217:
    mov64 r1, r9                                    r1 = r9
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_22929                     
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
    jsgt r0, 0, lbb_17224                           if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r1, r8                                    r1 = r8
lbb_17224:
    ldxdw r2, [r10-0xa8]                    
    mov64 r9, r2                                    r9 = r2
    ldxdw r6, [r10-0x80]                    
    jlt r2, r1, lbb_17229                           if r2 < r1 { pc += 1 }
    mov64 r9, r1                                    r9 = r1
lbb_17229:
    jeq r9, 0, lbb_16975                            if r9 == (0 as i32 as i64 as u64) { pc += -255 }
    ldxdw r2, [r10-0xa0]                    
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jeq r2, 0, lbb_17333                            if r2 == (0 as i32 as i64 as u64) { pc += 100 }
    ldxdw r2, [r10-0xa8]                    
    jgt r1, r2, lbb_17236                           if r1 > r2 { pc += 1 }
    ja lbb_17311                                    if true { pc += 75 }
lbb_17236:
    mov64 r1, r9                                    r1 = r9
    call function_22513                     
    mov64 r1, r0                                    r1 = r0
    mov64 r2, r6                                    r2 = r6
    call function_21530                     
    mov64 r8, r0                                    r8 = r0
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r8                                    r2 = r8
    call function_22497                     
    jeq r0, 0, lbb_17266                            if r0 == (0 as i32 as i64 as u64) { pc += 20 }
    ja lbb_17201                                    if true { pc += -46 }
lbb_17247:
    ldxdw r3, [r10-0xb8]                    
    mov64 r2, r3                                    r2 = r3
    jlt r3, r7, lbb_17251                           if r3 < r7 { pc += 1 }
    mov64 r2, r7                                    r2 = r7
lbb_17251:
    sub64 r2, r1                                    r2 -= r1   ///  r2 = r2.wrapping_sub(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0x120]                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x8]                     
    jne r2, 0, lbb_17262                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_17262:
    mov64 r6, -1                                    r6 = -1 as i32 as i64 as u64
    jne r1, 0, lbb_17022                            if r1 != (0 as i32 as i64 as u64) { pc += -242 }
    ldxdw r6, [r10-0x10]                    
    ja lbb_17022                                    if true { pc += -244 }
lbb_17266:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r1, r8                                    r1 = r8
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_22511                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r8                                    r1 = r8
    call function_22905                     
    jslt r7, 0, lbb_17275                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r6, r0                                    r6 = r0
lbb_17275:
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_22929                     
    mov64 r7, -1                                    r7 = -1 as i32 as i64 as u64
    jsgt r0, 0, lbb_17282                           if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r7, r6                                    r7 = r6
lbb_17282:
    mov64 r1, r7                                    r1 = r7
    call function_22513                     
    ldxdw r1, [r10-0x80]                    
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r6, r0                                    r6 = r0
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r6                                    r2 = r6
    call function_22497                     
    jeq r0, 0, lbb_17293                            if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17201                                    if true { pc += -92 }
lbb_17293:
    stxdw [r10-0xb0], r7                    
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_22511                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r6                                    r1 = r6
    call function_22905                     
    jslt r7, 0, lbb_17303                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r8, r0                                    r8 = r0
lbb_17303:
    mov64 r1, r6                                    r1 = r6
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_22929                     
    mov64 r9, -1                                    r9 = -1 as i32 as i64 as u64
    jsgt r0, 0, lbb_17310                           if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r9, r8                                    r9 = r8
lbb_17310:
    jeq r9, 0, lbb_16975                            if r9 == (0 as i32 as i64 as u64) { pc += -336 }
lbb_17311:
    ldxdw r1, [r10-0xf8]                    
    mov64 r2, r1                                    r2 = r1
    add64 r2, r9                                    r2 += r9   ///  r2 = r2.wrapping_add(r9)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r2, r1, lbb_17318                           if r2 < r1 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_17318:
    mov64 r6, -1                                    r6 = -1 as i32 as i64 as u64
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
    jne r3, 0, lbb_17322                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r2                                    r1 = r2
lbb_17322:
    call function_22513                     
    mov64 r7, r0                                    r7 = r0
    ldxdw r2, [r10-0x108]                   
    mov64 r1, r2                                    r1 = r2
    ldxdw r3, [r10-0xb0]                    
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    jlt r1, r2, lbb_17330                           if r1 < r2 { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_17330:
    jne r8, 0, lbb_16965                            if r8 != (0 as i32 as i64 as u64) { pc += -366 }
    mov64 r6, r1                                    r6 = r1
    ja lbb_16965                                    if true { pc += -368 }
lbb_17333:
    ldxdw r1, [r10-0x78]                    
    ldxdw r2, [r10-0xb0]                    
    jlt r1, r2, lbb_17380                           if r1 < r2 { pc += 44 }
    ldxdw r1, [r10-0xa8]                    
    jlt r1, r9, lbb_17377                           if r1 < r9 { pc += 39 }
    ldxdw r3, [r10-0xf8]                    
    mov64 r1, r3                                    r1 = r3
    add64 r1, r9                                    r1 += r9   ///  r1 = r1.wrapping_add(r9)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r3, lbb_17344                           if r1 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_17344:
    mov64 r3, -1                                    r3 = -1 as i32 as i64 as u64
    stxdw [r10-0xf8], r3                    
    jne r2, 0, lbb_17348                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    stxdw [r10-0xf8], r1                    
lbb_17348:
    ldxdw r8, [r10-0x68]                    
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
    ldxdw r2, [r10-0x78]                    
    ldxdw r3, [r10-0xb0]                    
    sub64 r2, r3                                    r2 -= r3   ///  r2 = r2.wrapping_sub(r3)
    ldxdw r1, [r10-0x60]                    
    stxdw [r1-0x10], r3                     
    stxdw [r1-0x8], r9                      
    ldxdw r1, [r10-0xa8]                    
    sub64 r1, r9                                    r1 -= r9   ///  r1 = r1.wrapping_sub(r9)
    stxdw [r10-0xa8], r1                    
    stxdw [r10-0x78], r2                    
    mov64 r1, r2                                    r1 = r2
    ldxdw r2, [r10-0x90]                    
    jlt r8, r2, lbb_16920                           if r8 < r2 { pc += -443 }
lbb_17363:
    ldxdw r5, [r10-0x110]                   
    mov64 r2, r5                                    r2 = r5
    sub64 r2, r1                                    r2 -= r1   ///  r2 = r2.wrapping_sub(r1)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jgt r2, r5, lbb_17370                           if r2 > r5 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_17370:
    ldxdw r1, [r10-0x128]                   
    jne r3, 0, lbb_17373                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r2                                    r4 = r2
lbb_17373:
    ldxdw r2, [r10-0xf8]                    
    stxdw [r1+0x8], r2                      
    stxdw [r1+0x0], r4                      
    exit                                    
lbb_17377:
    lddw r1, 0x10002f648 --> b"\x00\x00\x00\x003\xda\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xcd\x00\x00…        r1 load str located at 4295161416
    call function_20957                     
lbb_17380:
    lddw r1, 0x10002f630 --> b"\x00\x00\x00\x003\xda\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\xcb\x00\x00…        r1 load str located at 4295161392
    call function_20957                     

function_17383:
    mov64 r6, r1                                    r6 = r1
    mov64 r2, r6                                    r2 = r6
    call function_22497                     
    jne r0, 0, lbb_17405                            if r0 != (0 as i32 as i64 as u64) { pc += 18 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_22511                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r6                                    r1 = r6
    call function_22905                     
    jslt r7, 0, lbb_17396                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r8, r0                                    r8 = r0
lbb_17396:
    mov64 r1, r6                                    r1 = r6
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_22929                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, -1                                    r0 = -1 as i32 as i64 as u64
    jsgt r1, 0, lbb_17404                           if (r1 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r0, r8                                    r0 = r8
lbb_17404:
    exit                                    
lbb_17405:
    lddw r1, 0x10002cec8 --> b"assertion failed: !self.is_nan()"        r1 load str located at 4295151304
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    lddw r3, 0x10002f660 --> b"\x00\x00\x00\x00P\xda\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00"\x00\x00\x0…        r3 load str located at 4295161440
    call function_18704                     

function_17411:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r7                                    r1 = r7
    call function_22497                     
    jne r0, 0, lbb_17442                            if r0 != (0 as i32 as i64 as u64) { pc += 26 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_21869                     
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_22511                     
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jslt r0, 0, lbb_17427                           if (r0 as i64) < (0 as i32 as i64) { pc += 1 }
    ldxdw r8, [r10-0x8]                     
lbb_17427:
    jslt r0, 0, lbb_17429                           if (r0 as i64) < (0 as i32 as i64) { pc += 1 }
    ldxdw r9, [r10-0x10]                    
lbb_17429:
    mov64 r1, r7                                    r1 = r7
    lddw r2, 0x47efffffffffffff                     r2 load str located at 5183643171103440895
    call function_22929                     
    mov64 r2, -1                                    r2 = -1 as i32 as i64 as u64
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
    jsgt r0, 0, lbb_17437                           if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r1, r9                                    r1 = r9
lbb_17437:
    jsgt r0, 0, lbb_17439                           if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r2, r8                                    r2 = r8
lbb_17439:
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    
lbb_17442:
    lddw r1, 0x10002cec8 --> b"assertion failed: !self.is_nan()"        r1 load str located at 4295151304
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    lddw r3, 0x10002f678 --> b"\x00\x00\x00\x00P\xda\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00)\x00\x00\x0…        r3 load str located at 4295161464
    call function_18704                     

function_17448:
    mov64 r6, r2                                    r6 = r2
    call function_22513                     
    mov64 r1, r0                                    r1 = r0
    mov64 r2, r6                                    r2 = r6
    call function_22254                     
    mov64 r6, r0                                    r6 = r0
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r6                                    r2 = r6
    call function_22497                     
    jeq r0, 0, lbb_17464                            if r0 == (0 as i32 as i64 as u64) { pc += 6 }
    lddw r1, 0x10002cec8 --> b"assertion failed: !self.is_nan()"        r1 load str located at 4295151304
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    lddw r3, 0x10002f660 --> b"\x00\x00\x00\x00P\xda\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00"\x00\x00\x0…        r3 load str located at 4295161440
    call function_18704                     
lbb_17464:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_22511                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r6                                    r1 = r6
    call function_22905                     
    jslt r7, 0, lbb_17473                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r8, r0                                    r8 = r0
lbb_17473:
    mov64 r1, r6                                    r1 = r6
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_22929                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, -1                                    r0 = -1 as i32 as i64 as u64
    jsgt r1, 0, lbb_17481                           if (r1 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r0, r8                                    r0 = r8
lbb_17481:
    exit                                    

function_17482:
    mov64 r6, r1                                    r6 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r1, r2                                    r1 = r2
    call function_22931                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x3f1a36e2eb1c432d                     r2 load str located at 4547007122018943789
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x3ff0000000000000                     r2 load str located at 4607182418800017408
    call function_22576                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r6                                    r1 = r6
    call function_22513                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r6, r0                                    r6 = r0
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r6                                    r2 = r6
    call function_22497                     
    jeq r0, 0, lbb_17512                            if r0 == (0 as i32 as i64 as u64) { pc += 6 }
    lddw r1, 0x10002cec8 --> b"assertion failed: !self.is_nan()"        r1 load str located at 4295151304
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    lddw r3, 0x10002f660 --> b"\x00\x00\x00\x00P\xda\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00"\x00\x00\x0…        r3 load str located at 4295161440
    call function_18704                     
lbb_17512:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_22511                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r6                                    r1 = r6
    call function_22905                     
    jslt r7, 0, lbb_17521                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r8, r0                                    r8 = r0
lbb_17521:
    mov64 r1, r6                                    r1 = r6
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_22929                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, -1                                    r0 = -1 as i32 as i64 as u64
    jsgt r1, 0, lbb_17529                           if (r1 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r0, r8                                    r0 = r8
lbb_17529:
    exit                                    

function_17530:
    mov64 r6, r1                                    r6 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r1, r2                                    r1 = r2
    call function_22931                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x3e7ad7f29abcaf48                     r2 load str located at 4502148214488346440
    call function_22254                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x3ff0000000000000                     r2 load str located at 4607182418800017408
    call function_22576                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r6                                    r1 = r6
    call function_22513                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r0                                    r2 = r0
    call function_22254                     
    mov64 r6, r0                                    r6 = r0
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r6                                    r2 = r6
    call function_22497                     
    jeq r0, 0, lbb_17560                            if r0 == (0 as i32 as i64 as u64) { pc += 6 }
    lddw r1, 0x10002cec8 --> b"assertion failed: !self.is_nan()"        r1 load str located at 4295151304
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    lddw r3, 0x10002f660 --> b"\x00\x00\x00\x00P\xda\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00"\x00\x00\x0…        r3 load str located at 4295161440
    call function_18704                     
lbb_17560:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_22511                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r6                                    r1 = r6
    call function_22905                     
    jslt r7, 0, lbb_17569                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r8, r0                                    r8 = r0
lbb_17569:
    mov64 r1, r6                                    r1 = r6
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_22929                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, -1                                    r0 = -1 as i32 as i64 as u64
    jsgt r1, 0, lbb_17577                           if (r1 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r0, r8                                    r0 = r8
lbb_17577:
    exit                                    

function_17578:
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jsgt r1, 20, lbb_17590                          if (r1 as i64) > (20 as i32 as i64) { pc += 9 }
    jsgt r1, 9, lbb_17598                           if (r1 as i64) > (9 as i32 as i64) { pc += 16 }
    jsgt r1, 4, lbb_17626                           if (r1 as i64) > (4 as i32 as i64) { pc += 43 }
    jsgt r1, 1, lbb_17638                           if (r1 as i64) > (1 as i32 as i64) { pc += 54 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_17737                            if r1 == (0 as i32 as i64 as u64) { pc += 151 }
    jeq r1, 1, lbb_17588                            if r1 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17736                                    if true { pc += 148 }
lbb_17588:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 147 }
lbb_17590:
    jsgt r1, 30, lbb_17605                          if (r1 as i64) > (30 as i32 as i64) { pc += 14 }
    jsgt r1, 25, lbb_17632                          if (r1 as i64) > (25 as i32 as i64) { pc += 40 }
    jsgt r1, 22, lbb_17644                          if (r1 as i64) > (22 as i32 as i64) { pc += 51 }
    jeq r1, 21, lbb_17714                           if r1 == (21 as i32 as i64 as u64) { pc += 120 }
    jeq r1, 22, lbb_17596                           if r1 == (22 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17736                                    if true { pc += 140 }
lbb_17596:
    mov64 r0, 22                                    r0 = 22 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 139 }
lbb_17598:
    jsgt r1, 14, lbb_17612                          if (r1 as i64) > (14 as i32 as i64) { pc += 13 }
    jsgt r1, 11, lbb_17662                          if (r1 as i64) > (11 as i32 as i64) { pc += 62 }
    jeq r1, 10, lbb_17732                           if r1 == (10 as i32 as i64 as u64) { pc += 131 }
    jeq r1, 11, lbb_17603                           if r1 == (11 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17736                                    if true { pc += 133 }
lbb_17603:
    mov64 r0, 11                                    r0 = 11 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 132 }
lbb_17605:
    jsgt r1, 35, lbb_17619                          if (r1 as i64) > (35 as i32 as i64) { pc += 13 }
    jsgt r1, 32, lbb_17668                          if (r1 as i64) > (32 as i32 as i64) { pc += 61 }
    jeq r1, 31, lbb_17734                           if r1 == (31 as i32 as i64 as u64) { pc += 126 }
    jeq r1, 32, lbb_17610                           if r1 == (32 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17736                                    if true { pc += 126 }
lbb_17610:
    mov64 r0, 32                                    r0 = 32 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 125 }
lbb_17612:
    jsgt r1, 17, lbb_17674                          if (r1 as i64) > (17 as i32 as i64) { pc += 61 }
    jeq r1, 15, lbb_17706                           if r1 == (15 as i32 as i64 as u64) { pc += 92 }
    jeq r1, 16, lbb_17724                           if r1 == (16 as i32 as i64 as u64) { pc += 109 }
    jeq r1, 17, lbb_17617                           if r1 == (17 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17736                                    if true { pc += 119 }
lbb_17617:
    mov64 r0, 17                                    r0 = 17 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 118 }
lbb_17619:
    jsgt r1, 38, lbb_17680                          if (r1 as i64) > (38 as i32 as i64) { pc += 60 }
    jeq r1, 36, lbb_17708                           if r1 == (36 as i32 as i64 as u64) { pc += 87 }
    jeq r1, 37, lbb_17726                           if r1 == (37 as i32 as i64 as u64) { pc += 104 }
    jeq r1, 38, lbb_17624                           if r1 == (38 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17736                                    if true { pc += 112 }
lbb_17624:
    mov64 r0, 38                                    r0 = 38 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 111 }
lbb_17626:
    jsgt r1, 6, lbb_17650                           if (r1 as i64) > (6 as i32 as i64) { pc += 23 }
    jeq r1, 5, lbb_17716                            if r1 == (5 as i32 as i64 as u64) { pc += 88 }
    jeq r1, 6, lbb_17630                            if r1 == (6 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17736                                    if true { pc += 106 }
lbb_17630:
    mov64 r0, 6                                     r0 = 6 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 105 }
lbb_17632:
    jsgt r1, 27, lbb_17656                          if (r1 as i64) > (27 as i32 as i64) { pc += 23 }
    jeq r1, 26, lbb_17718                           if r1 == (26 as i32 as i64 as u64) { pc += 84 }
    jeq r1, 27, lbb_17636                           if r1 == (27 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17736                                    if true { pc += 100 }
lbb_17636:
    mov64 r0, 27                                    r0 = 27 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 99 }
lbb_17638:
    jeq r1, 2, lbb_17686                            if r1 == (2 as i32 as i64 as u64) { pc += 47 }
    jeq r1, 3, lbb_17698                            if r1 == (3 as i32 as i64 as u64) { pc += 58 }
    jeq r1, 4, lbb_17642                            if r1 == (4 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17736                                    if true { pc += 94 }
lbb_17642:
    mov64 r0, 4                                     r0 = 4 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 93 }
lbb_17644:
    jeq r1, 23, lbb_17688                           if r1 == (23 as i32 as i64 as u64) { pc += 43 }
    jeq r1, 24, lbb_17700                           if r1 == (24 as i32 as i64 as u64) { pc += 54 }
    jeq r1, 25, lbb_17648                           if r1 == (25 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17736                                    if true { pc += 88 }
lbb_17648:
    mov64 r0, 25                                    r0 = 25 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 87 }
lbb_17650:
    jeq r1, 7, lbb_17690                            if r1 == (7 as i32 as i64 as u64) { pc += 39 }
    jeq r1, 8, lbb_17702                            if r1 == (8 as i32 as i64 as u64) { pc += 50 }
    jeq r1, 9, lbb_17654                            if r1 == (9 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17736                                    if true { pc += 82 }
lbb_17654:
    mov64 r0, 9                                     r0 = 9 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 81 }
lbb_17656:
    jeq r1, 28, lbb_17692                           if r1 == (28 as i32 as i64 as u64) { pc += 35 }
    jeq r1, 29, lbb_17704                           if r1 == (29 as i32 as i64 as u64) { pc += 46 }
    jeq r1, 30, lbb_17660                           if r1 == (30 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17736                                    if true { pc += 76 }
lbb_17660:
    mov64 r0, 30                                    r0 = 30 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 75 }
lbb_17662:
    jeq r1, 12, lbb_17694                           if r1 == (12 as i32 as i64 as u64) { pc += 31 }
    jeq r1, 13, lbb_17720                           if r1 == (13 as i32 as i64 as u64) { pc += 56 }
    jeq r1, 14, lbb_17666                           if r1 == (14 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17736                                    if true { pc += 70 }
lbb_17666:
    mov64 r0, 14                                    r0 = 14 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 69 }
lbb_17668:
    jeq r1, 33, lbb_17696                           if r1 == (33 as i32 as i64 as u64) { pc += 27 }
    jeq r1, 34, lbb_17722                           if r1 == (34 as i32 as i64 as u64) { pc += 52 }
    jeq r1, 35, lbb_17672                           if r1 == (35 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17736                                    if true { pc += 64 }
lbb_17672:
    mov64 r0, 35                                    r0 = 35 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 63 }
lbb_17674:
    jeq r1, 18, lbb_17710                           if r1 == (18 as i32 as i64 as u64) { pc += 35 }
    jeq r1, 19, lbb_17728                           if r1 == (19 as i32 as i64 as u64) { pc += 52 }
    jeq r1, 20, lbb_17678                           if r1 == (20 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17736                                    if true { pc += 58 }
lbb_17678:
    mov64 r0, 20                                    r0 = 20 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 57 }
lbb_17680:
    jeq r1, 39, lbb_17712                           if r1 == (39 as i32 as i64 as u64) { pc += 31 }
    jeq r1, 40, lbb_17730                           if r1 == (40 as i32 as i64 as u64) { pc += 48 }
    jeq r1, 41, lbb_17684                           if r1 == (41 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17736                                    if true { pc += 52 }
lbb_17684:
    mov64 r0, 41                                    r0 = 41 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 51 }
lbb_17686:
    mov64 r0, 2                                     r0 = 2 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 49 }
lbb_17688:
    mov64 r0, 23                                    r0 = 23 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 47 }
lbb_17690:
    mov64 r0, 7                                     r0 = 7 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 45 }
lbb_17692:
    mov64 r0, 28                                    r0 = 28 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 43 }
lbb_17694:
    mov64 r0, 12                                    r0 = 12 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 41 }
lbb_17696:
    mov64 r0, 33                                    r0 = 33 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 39 }
lbb_17698:
    mov64 r0, 3                                     r0 = 3 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 37 }
lbb_17700:
    mov64 r0, 24                                    r0 = 24 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 35 }
lbb_17702:
    mov64 r0, 8                                     r0 = 8 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 33 }
lbb_17704:
    mov64 r0, 29                                    r0 = 29 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 31 }
lbb_17706:
    mov64 r0, 15                                    r0 = 15 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 29 }
lbb_17708:
    mov64 r0, 36                                    r0 = 36 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 27 }
lbb_17710:
    mov64 r0, 18                                    r0 = 18 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 25 }
lbb_17712:
    mov64 r0, 39                                    r0 = 39 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 23 }
lbb_17714:
    mov64 r0, 21                                    r0 = 21 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 21 }
lbb_17716:
    mov64 r0, 5                                     r0 = 5 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 19 }
lbb_17718:
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 17 }
lbb_17720:
    mov64 r0, 13                                    r0 = 13 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 15 }
lbb_17722:
    mov64 r0, 34                                    r0 = 34 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 13 }
lbb_17724:
    mov64 r0, 16                                    r0 = 16 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 11 }
lbb_17726:
    mov64 r0, 37                                    r0 = 37 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 9 }
lbb_17728:
    mov64 r0, 19                                    r0 = 19 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 7 }
lbb_17730:
    mov64 r0, 40                                    r0 = 40 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 5 }
lbb_17732:
    mov64 r0, 10                                    r0 = 10 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 3 }
lbb_17734:
    mov64 r0, 31                                    r0 = 31 as i32 as i64 as u64
    ja lbb_17737                                    if true { pc += 1 }
lbb_17736:
    mov64 r0, 42                                    r0 = 42 as i32 as i64 as u64
lbb_17737:
    exit                                    

function_17738:
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_11991                     
    mov64 r6, r0                                    r6 = r0
    jne r6, 0, lbb_17748                            if r6 != (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    lddw r3, 0x10002f690 --> b"\x00\x00\x00\x00h\xda\x02\x00T\x00\x00\x00\x00\x00\x00\x00\x9f\x00\x00\x0…        r3 load str located at 4295161488
    call function_18348                     
lbb_17748:
    lddw r1, 0x706e6920666f2068                     r1 load str located at 8101528367564529768
    stxdw [r6+0x10], r1                     
    lddw r1, 0x74676e656c206465                     r1 load str located at 8387794212885652581
    stxdw [r6+0x8], r1                      
    lddw r1, 0x7463657078656e55                     r1 load str located at 8386658464824651349
    stxdw [r6+0x0], r1                      
    sth [r6+0x18], 29813                    
    mov64 r1, 24                                    r1 = 24 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_11991                     
    jne r0, 0, lbb_17765                            if r0 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    call function_18352                     
lbb_17765:
    stxdw [r0+0x8], r6                      
    stdw [r0+0x10], 26                      
    stdw [r0+0x0], 26                       
    mov64 r1, 21                                    r1 = 21 as i32 as i64 as u64
    mov64 r2, r0                                    r2 = r0
    lddw r3, 0x10002f6c8 --> b"\x00\x00\x00\x00\x90,\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00…        r3 load str located at 4295161544
    call function_18139                     
    exit                                    

function_17774:
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_17779                            if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r1, [r1+0x8]                      
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_12017                     
lbb_17779:
    exit                                    

function_17780:
    stdw [r1+0x0], 0                        
    exit                                    

function_17782:
    exit                                    

function_17783:
    lddw r2, 0xcd7cc2605a5a778b                     r2 load str located at -3639820679733676149
    stxdw [r1+0x8], r2                      
    lddw r2, 0x38559c1419c640e8                     r2 load str located at 4059322249290072296
    stxdw [r1+0x0], r2                      
    exit                                    

function_17790:
    mov64 r6, r1                                    r6 = r1
    mov64 r7, r6                                    r7 = r6
    and64 r7, 3                                     r7 &= 3   ///  r7 = r7.and(3)
    jsgt r7, 1, lbb_17797                           if (r7 as i64) > (1 as i32 as i64) { pc += 3 }
    jeq r7, 0, lbb_17805                            if r7 == (0 as i32 as i64 as u64) { pc += 10 }
    ldxb r1, [r6+0xf]                       
    ja lbb_17806                                    if true { pc += 9 }
lbb_17797:
    mov64 r0, r6                                    r0 = r6
    jeq r7, 2, lbb_17832                            if r7 == (2 as i32 as i64 as u64) { pc += 33 }
    mov64 r1, r6                                    r1 = r6
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    call function_17578                     
    mov64 r1, r0                                    r1 = r0
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    ja lbb_17806                                    if true { pc += 1 }
lbb_17805:
    ldxb r1, [r6+0x10]                      
lbb_17806:
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r0, r6                                    r0 = r6
    jne r1, 37, lbb_17832                           if r1 != (37 as i32 as i64 as u64) { pc += 23 }
    call function_17738                     
    mov64 r1, r7                                    r1 = r7
    add64 r1, -2                                    r1 += -2   ///  r1 = r1.wrapping_add(-2 as i32 as i64 as u64)
    jlt r1, 2, lbb_17832                            if r1 < (2 as i32 as i64 as u64) { pc += 19 }
    jeq r7, 0, lbb_17832                            if r7 == (0 as i32 as i64 as u64) { pc += 18 }
    mov64 r8, r0                                    r8 = r0
    ldxdw r7, [r6-0x1]                      
    ldxdw r9, [r6+0x7]                      
    ldxdw r2, [r9+0x0]                      
    jeq r2, 0, lbb_17821                            if r2 == (0 as i32 as i64 as u64) { pc += 2 }
    mov64 r1, r7                                    r1 = r7
    callx r2                                
lbb_17821:
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    ldxdw r2, [r9+0x8]                      
    jeq r2, 0, lbb_17827                            if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r3, [r9+0x10]                     
    mov64 r1, r7                                    r1 = r7
    call function_12017                     
lbb_17827:
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_12017                     
    mov64 r0, r8                                    r0 = r8
lbb_17832:
    exit                                    

function_17833:
    ldxdw r1, [r1+0x0]                      
    ldxw r3, [r2+0x34]                      
    mov64 r4, r3                                    r4 = r3
    and64 r4, 16                                    r4 &= 16   ///  r4 = r4.and(16)
    jne r4, 0, lbb_17843                            if r4 != (0 as i32 as i64 as u64) { pc += 5 }
    and64 r3, 32                                    r3 &= 32   ///  r3 = r3.and(32)
    jeq r3, 0, lbb_17841                            if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17845                                    if true { pc += 4 }
lbb_17841:
    call function_21137                     
    ja lbb_17846                                    if true { pc += 3 }
lbb_17843:
    call function_21079                     
    ja lbb_17846                                    if true { pc += 1 }
lbb_17845:
    call function_21083                     
lbb_17846:
    exit                                    

function_17847:
    ldxdw r3, [r1+0x0]                      
    ldxw r1, [r3+0x0]                       
    jsgt r1, 12, lbb_17859                          if (r1 as i64) > (12 as i32 as i64) { pc += 9 }
    jsgt r1, 5, lbb_17868                           if (r1 as i64) > (5 as i32 as i64) { pc += 17 }
    jsgt r1, 2, lbb_17898                           if (r1 as i64) > (2 as i32 as i64) { pc += 46 }
    jeq r1, 0, lbb_17934                            if r1 == (0 as i32 as i64 as u64) { pc += 81 }
    jeq r1, 1, lbb_17990                            if r1 == (1 as i32 as i64 as u64) { pc += 136 }
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dad1 --> b"InvalidInstructionData"        r2 load str located at 4295154385
    mov64 r3, 22                                    r3 = 22 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 149 }
lbb_17859:
    jsgt r1, 18, lbb_17876                          if (r1 as i64) > (18 as i32 as i64) { pc += 16 }
    jsgt r1, 15, lbb_17905                          if (r1 as i64) > (15 as i32 as i64) { pc += 44 }
    jeq r1, 13, lbb_17946                           if r1 == (13 as i32 as i64 as u64) { pc += 84 }
    jeq r1, 14, lbb_17995                           if r1 == (14 as i32 as i64 as u64) { pc += 132 }
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dbc8 --> b"AccountNotRentExempt"        r2 load str located at 4295154632
    mov64 r3, 20                                    r3 = 20 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 140 }
lbb_17868:
    jsgt r1, 8, lbb_17884                           if (r1 as i64) > (8 as i32 as i64) { pc += 15 }
    jeq r1, 6, lbb_17924                            if r1 == (6 as i32 as i64 as u64) { pc += 54 }
    jeq r1, 7, lbb_17960                            if r1 == (7 as i32 as i64 as u64) { pc += 89 }
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002db47 --> b"AccountAlreadyInitialized"        r2 load str located at 4295154503
    mov64 r3, 25                                    r3 = 25 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 132 }
lbb_17876:
    jsgt r1, 21, lbb_17891                          if (r1 as i64) > (21 as i32 as i64) { pc += 14 }
    jeq r1, 19, lbb_17929                           if r1 == (19 as i32 as i64 as u64) { pc += 51 }
    jeq r1, 20, lbb_17965                           if r1 == (20 as i32 as i64 as u64) { pc += 86 }
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dc4a --> b"BuiltinProgramsMustConsumeComputeUnits"        r2 load str located at 4295154762
    mov64 r3, 38                                    r3 = 38 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 124 }
lbb_17884:
    jsgt r1, 10, lbb_17912                          if (r1 as i64) > (10 as i32 as i64) { pc += 27 }
    jeq r1, 9, lbb_17970                            if r1 == (9 as i32 as i64 as u64) { pc += 84 }
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002db74 --> b"NotEnoughAccountKeys"        r2 load str located at 4295154548
    mov64 r3, 20                                    r3 = 20 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 117 }
lbb_17891:
    jsgt r1, 23, lbb_17918                          if (r1 as i64) > (23 as i32 as i64) { pc += 26 }
    jeq r1, 22, lbb_17975                           if r1 == (22 as i32 as i64 as u64) { pc += 82 }
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dc83 --> b"ArithmeticOverflow"        r2 load str located at 4295154819
    mov64 r3, 18                                    r3 = 18 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 110 }
lbb_17898:
    jeq r1, 3, lbb_17950                            if r1 == (3 as i32 as i64 as u64) { pc += 51 }
    jeq r1, 4, lbb_17999                            if r1 == (4 as i32 as i64 as u64) { pc += 99 }
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002db0c --> b"InsufficientFunds"        r2 load str located at 4295154444
    mov64 r3, 17                                    r3 = 17 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 103 }
lbb_17905:
    jeq r1, 16, lbb_17955                           if r1 == (16 as i32 as i64 as u64) { pc += 49 }
    jeq r1, 17, lbb_18004                           if r1 == (17 as i32 as i64 as u64) { pc += 97 }
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dbf9 --> b"MaxAccountsDataAllocationsExceeded"        r2 load str located at 4295154681
    mov64 r3, 34                                    r3 = 34 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 96 }
lbb_17912:
    jeq r1, 11, lbb_17980                           if r1 == (11 as i32 as i64 as u64) { pc += 67 }
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002db9b --> b"MaxSeedLengthExceeded"        r2 load str located at 4295154587
    mov64 r3, 21                                    r3 = 21 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 90 }
lbb_17918:
    jeq r1, 24, lbb_17985                           if r1 == (24 as i32 as i64 as u64) { pc += 66 }
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dc9e --> b"IncorrectAuthority"        r2 load str located at 4295154846
    mov64 r3, 18                                    r3 = 18 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 84 }
lbb_17924:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002db1d --> b"IncorrectProgramId"        r2 load str located at 4295154461
    mov64 r3, 18                                    r3 = 18 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 79 }
lbb_17929:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dc1b --> b"InvalidRealloc"        r2 load str located at 4295154715
    mov64 r3, 14                                    r3 = 14 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 74 }
lbb_17934:
    add64 r3, 4                                     r3 += 4   ///  r3 = r3.wrapping_add(4 as i32 as i64 as u64)
    stxdw [r10-0x8], r3                     
    mov64 r4, r10                                   r4 = r10
    add64 r4, -8                                    r4 += -8   ///  r4 = r4.wrapping_add(-8 as i32 as i64 as u64)
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dabc --> b"Custom"              r2 load str located at 4295154364
    mov64 r3, 6                                     r3 = 6 as i32 as i64 as u64
    lddw r5, 0x10002f720 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r5 load str located at 4295161632
    call function_19760                     
    ja lbb_18009                                    if true { pc += 63 }
lbb_17946:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dbb0 --> b"InvalidSeedsBorshIoErrorAccountNotRentExemptUnsupp"        r2 load str located at 4295154608
    ja lbb_18007                                    if true { pc += 57 }
lbb_17950:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dae7 --> b"InvalidAccountData"        r2 load str located at 4295154407
    mov64 r3, 18                                    r3 = 18 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 53 }
lbb_17955:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dbdc --> b"UnsupportedSysvar"        r2 load str located at 4295154652
    mov64 r3, 17                                    r3 = 17 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 48 }
lbb_17960:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002db2f --> b"MissingRequiredSignature"        r2 load str located at 4295154479
    mov64 r3, 24                                    r3 = 24 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 43 }
lbb_17965:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dc29 --> b"MaxInstructionTraceLengthExceeded"        r2 load str located at 4295154729
    mov64 r3, 33                                    r3 = 33 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 38 }
lbb_17970:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002db60 --> b"UninitializedAccount"        r2 load str located at 4295154528
    mov64 r3, 20                                    r3 = 20 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 33 }
lbb_17975:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dc70 --> b"InvalidAccountOwner"        r2 load str located at 4295154800
    mov64 r3, 19                                    r3 = 19 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 28 }
lbb_17980:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002db88 --> b"AccountBorrowFailed"        r2 load str located at 4295154568
    mov64 r3, 19                                    r3 = 19 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 23 }
lbb_17985:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dc95 --> b"Immutable"           r2 load str located at 4295154837
    mov64 r3, 9                                     r3 = 9 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 18 }
lbb_17990:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dac2 --> b"InvalidArgument"        r2 load str located at 4295154370
    mov64 r3, 15                                    r3 = 15 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 13 }
lbb_17995:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dbbc --> b"BorshIoErrorAccountNotRentExemptUnsupportedSysvarI"        r2 load str located at 4295154620
    ja lbb_18007                                    if true { pc += 8 }
lbb_17999:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002daf9 --> b"AccountDataTooSmall"        r2 load str located at 4295154425
    mov64 r3, 19                                    r3 = 19 as i32 as i64 as u64
    ja lbb_18008                                    if true { pc += 4 }
lbb_18004:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dbed --> b"IllegalOwner"        r2 load str located at 4295154669
lbb_18007:
    mov64 r3, 12                                    r3 = 12 as i32 as i64 as u64
lbb_18008:
    call function_19754                     
lbb_18009:
    exit                                    

function_18010:
    ldxdw r2, [r2+0x0]                      
    ldxb r3, [r2+0x0]                       
    mov64 r4, r3                                    r4 = r3
    and64 r4, 8                                     r4 &= 8   ///  r4 = r4.and(8)
    jeq r4, 0, lbb_18018                            if r4 == (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r4, r3                                    r4 = r3
    and64 r4, 7                                     r4 &= 7   ///  r4 = r4.and(7)
    jne r4, 0, lbb_18021                            if r4 != (0 as i32 as i64 as u64) { pc += 3 }
lbb_18018:
    stw [r1+0x8], 11                        
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ja lbb_18028                                    if true { pc += 7 }
lbb_18021:
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    stxb [r2+0x0], r3                       
    ldxdw r3, [r2+0x50]                     
    stxdw [r1+0x8], r3                      
    stxdw [r1+0x10], r2                     
    stb [r1+0x18], 0                        
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
lbb_18028:
    stxdw [r1+0x0], r2                      
    exit                                    

function_18030:
    syscall [invalid]                       
    exit                                    

function_18032:
    stxdw [r10-0x48], r1                    
    lddw r1, 0x10002d2c8 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00012345678…        r1 load str located at 4295152328
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100022ed8 --> b"y\x13\x00\x00\x00\x00\x00\x00a1\x00\x00\x00\x00\x00\x00e\x01\x09\x00\x0c\…        r1 load str located at 4295110360
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    ldxdw r4, [r2+0x28]                     
    ldxdw r1, [r2+0x20]                     
    mov64 r3, r10                                   r3 = r10
    add64 r3, -64                                   r3 += -64   ///  r3 = r3.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r4                                    r2 = r4
    call function_19230                     
    exit                                    

function_18055:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    syscall [invalid]                       
    jne r0, 0, lbb_18072                            if r0 != (0 as i32 as i64 as u64) { pc += 12 }
    ldxdw r1, [r10-0x8]                     
    stxdw [r6+0x28], r1                     
    ldxdw r1, [r10-0x10]                    
    stxdw [r6+0x20], r1                     
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x20]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x8], r1                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_18074                                    if true { pc += 2 }
lbb_18072:
    stw [r6+0x4], 16                        
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_18074:
    stxw [r6+0x0], r1                       
    exit                                    

function_18076:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    syscall [invalid]                       
    jne r0, 0, lbb_18089                            if r0 != (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r1, [r10-0x8]                     
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x10]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x8], r1                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_18091                                    if true { pc += 2 }
lbb_18089:
    stw [r6+0x4], 16                        
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_18091:
    stxw [r6+0x0], r1                       
    exit                                    

function_18093:
    mov64 r4, r2                                    r4 = r2
    ldxdw r1, [r1+0x0]                      
    ldxb r1, [r1+0x0]                       
    jsgt r1, 1, lbb_18105                           if (r1 as i64) > (1 as i32 as i64) { pc += 8 }
    lddw r2, 0x10002dcb0 --> b"TargetAlignmentGreaterAndInputNotAligned"        r2 load str located at 4295154864
    mov64 r3, 40                                    r3 = 40 as i32 as i64 as u64
    jeq r1, 0, lbb_18113                            if r1 == (0 as i32 as i64 as u64) { pc += 12 }
    lddw r2, 0x10002dcd8 --> b"OutputSliceWouldHaveSlop"        r2 load str located at 4295154904
    mov64 r3, 24                                    r3 = 24 as i32 as i64 as u64
    ja lbb_18113                                    if true { pc += 8 }
lbb_18105:
    jeq r1, 2, lbb_18110                            if r1 == (2 as i32 as i64 as u64) { pc += 4 }
    lddw r2, 0x10002dcfc --> b"AlignmentMismatch"        r2 load str located at 4295154940
    mov64 r3, 17                                    r3 = 17 as i32 as i64 as u64
    ja lbb_18113                                    if true { pc += 3 }
lbb_18110:
    lddw r2, 0x10002dcf0 --> b"SizeMismatch"        r2 load str located at 4295154928
    mov64 r3, 12                                    r3 = 12 as i32 as i64 as u64
lbb_18113:
    mov64 r1, r4                                    r1 = r4
    call function_19754                     
    exit                                    

function_18116:
    stxdw [r10-0x48], r1                    
    lddw r1, 0x10002d2c8 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00012345678…        r1 load str located at 4295152328
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100023688 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x…        r1 load str located at 4295112328
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    ldxdw r4, [r2+0x28]                     
    ldxdw r1, [r2+0x20]                     
    mov64 r3, r10                                   r3 = r10
    add64 r3, -64                                   r3 += -64   ///  r3 = r3.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r4                                    r2 = r4
    call function_19230                     
    exit                                    

function_18139:
    mov64 r7, r3                                    r7 = r3
    mov64 r6, r2                                    r6 = r2
    mov64 r8, r1                                    r8 = r1
    mov64 r1, 24                                    r1 = 24 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_11991                     
    jne r0, 0, lbb_18149                            if r0 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    call function_18352                     
lbb_18149:
    stxb [r0+0x10], r8                      
    stxdw [r0+0x8], r7                      
    stxdw [r0+0x0], r6                      
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    exit                                    

function_18154:
    call function_18160                     

function_18155:
    call function_18158                     

function_18156:
    syscall [invalid]                       
    exit                                    

function_18158:
    call custom_panic                       
    syscall [invalid]                       

function_18160:
    syscall [invalid]                       

function_18161:
    lddw r1, 0x10002dd0d --> b"Error: memory allocation failed, out of memory"        r1 load str located at 4295154957
    mov64 r2, 46                                    r2 = 46 as i32 as i64 as u64
    call function_18156                     
    call function_18154                     

function_18166:
    call function_18161                     
    mov64 r3, r2                                    r3 = r2
    lddw r2, 0x10002f740 --> b"\x00\x00\x00\x00\x009\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00…        r2 load str located at 4295161664
    call function_19230                     
    exit                                    

function_18172:
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_18177                            if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r1, [r1+0x8]                      
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_12017                     
lbb_18177:
    exit                                    

function_18178:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x10002dd3b --> b"Error"               r2 load str located at 4295155003
    mov64 r3, 5                                     r3 = 5 as i32 as i64 as u64
    call function_19754                     
    exit                                    

function_18184:
    mov64 r2, r1                                    r2 = r1
    lddw r1, 0x10002f770 --> b"\x00\x00\x00\x00@\xdd\x02\x00\x11\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295161712
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_18698                     

function_18195:
    mov64 r6, r2                                    r6 = r2
    mov64 r2, r1                                    r2 = r1
    ldxdw r3, [r2+0x0]                      
    stdw [r10-0xff8], 1                     
    stdw [r10-0x1000], 1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_18232                     
    ldxdw r1, [r10-0x10]                    
    lddw r2, 0x8000000000000001                     r2 load str located at -9223372036854775807
    jeq r1, r2, lbb_18212                           if r1 == r2 { pc += 3 }
    ldxdw r2, [r10-0x8]                     
    mov64 r3, r6                                    r3 = r6
    call function_18348                     
lbb_18212:
    exit                                    

function_18213:
    mov64 r0, r3                                    r0 = r3
    mov64 r3, r2                                    r3 = r2
    mov64 r2, r1                                    r2 = r1
    stxdw [r10-0xff8], r5                   
    stxdw [r10-0x1000], r4                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r4, r0                                    r4 = r0
    call function_18232                     
    ldxdw r1, [r10-0x10]                    
    lddw r2, 0x8000000000000001                     r2 load str located at -9223372036854775807
    jeq r1, r2, lbb_18231                           if r1 == r2 { pc += 4 }
    ldxdw r2, [r10-0x8]                     
    lddw r3, 0x10002f780 --> b"\x00\x00\x00\x00Q\xdd\x02\x00\x14\x00\x00\x00\x00\x00\x00\x00+\x02\x00\x0…        r3 load str located at 4295161728
    call function_18348                     
lbb_18231:
    exit                                    

function_18232:
    stxdw [r10-0x48], r2                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxdw r7, [r5-0xff8]                    
    jeq r7, 0, lbb_18242                            if r7 == (0 as i32 as i64 as u64) { pc += 6 }
    mov64 r0, r3                                    r0 = r3
    add64 r0, r4                                    r0 += r4   ///  r0 = r0.wrapping_add(r4)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r0, r3, lbb_18241                           if r0 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_18241:
    jne r2, 1, lbb_18245                            if r2 != (1 as i32 as i64 as u64) { pc += 3 }
lbb_18242:
    stxdw [r1+0x8], r0                      
    stxdw [r1+0x0], r6                      
    exit                                    
lbb_18245:
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxdw r2, [r10-0x48]                    
    ldxdw r2, [r2+0x0]                      
    stxdw [r10-0x58], r2                    
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    jgt r2, r4, lbb_18252                           if r2 > r4 { pc += 1 }
    mov64 r2, r4                                    r2 = r4
lbb_18252:
    mov64 r8, 8                                     r8 = 8 as i32 as i64 as u64
    jeq r7, 1, lbb_18256                            if r7 == (1 as i32 as i64 as u64) { pc += 2 }
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jlt r7, 1025, lbb_18309                         if r7 < (1025 as i32 as i64 as u64) { pc += 53 }
lbb_18256:
    stxdw [r10-0x50], r1                    
    ldxdw r9, [r5-0x1000]                   
    jgt r8, r2, lbb_18260                           if r8 > r2 { pc += 1 }
    mov64 r8, r2                                    r8 = r2
lbb_18260:
    mov64 r2, r9                                    r2 = r9
    add64 r2, r7                                    r2 += r7   ///  r2 = r2.wrapping_add(r7)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r8                                    r4 = r8
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r1, [r10-0x38]                    
    jne r1, 0, lbb_18276                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_18276:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    ldxdw r1, [r10-0x50]                    
    jne r2, 0, lbb_18242                            if r2 != (0 as i32 as i64 as u64) { pc += -37 }
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    sub64 r2, r9                                    r2 -= r9   ///  r2 = r2.wrapping_sub(r9)
    ldxdw r3, [r10-0x40]                    
    jgt r3, r2, lbb_18242                           if r3 > r2 { pc += -42 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0x58]                    
    jeq r2, 0, lbb_18293                            if r2 == (0 as i32 as i64 as u64) { pc += 6 }
    mul64 r2, r7                                    r2 *= r7   ///  r2 = r2.wrapping_mul(r7)
    ldxdw r1, [r10-0x48]                    
    ldxdw r1, [r1+0x8]                      
    stxdw [r10-0x8], r2                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r9                                    r1 = r9
lbb_18293:
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -24                                   r4 += -24   ///  r4 = r4.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    call function_18315                     
    ldxdw r1, [r10-0x30]                    
    jne r1, 0, lbb_18311                            if r1 != (0 as i32 as i64 as u64) { pc += 9 }
    ldxdw r1, [r10-0x28]                    
    ldxdw r2, [r10-0x48]                    
    stxdw [r2+0x0], r8                      
    stxdw [r2+0x8], r1                      
    lddw r6, 0x8000000000000001                     r6 load str located at -9223372036854775807
    ja lbb_18313                                    if true { pc += 4 }
lbb_18309:
    mov64 r8, 4                                     r8 = 4 as i32 as i64 as u64
    ja lbb_18256                                    if true { pc += -55 }
lbb_18311:
    ldxdw r0, [r10-0x20]                    
    ldxdw r6, [r10-0x28]                    
lbb_18313:
    ldxdw r1, [r10-0x50]                    
    ja lbb_18242                                    if true { pc += -73 }

function_18315:
    mov64 r8, r3                                    r8 = r3
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r4+0x8]                      
    jeq r1, 0, lbb_18334                            if r1 == (0 as i32 as i64 as u64) { pc += 14 }
    ldxdw r2, [r4+0x10]                     
    jne r2, 0, lbb_18325                            if r2 != (0 as i32 as i64 as u64) { pc += 3 }
    jne r8, 0, lbb_18336                            if r8 != (0 as i32 as i64 as u64) { pc += 13 }
lbb_18323:
    mov64 r0, r7                                    r0 = r7
    ja lbb_18339                                    if true { pc += 14 }
lbb_18325:
    ldxdw r1, [r4+0x0]                      
    mov64 r3, r7                                    r3 = r7
    mov64 r4, r8                                    r4 = r8
    call function_12018                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_18339                            if r1 == (0 as i32 as i64 as u64) { pc += 7 }
    mov64 r0, r1                                    r0 = r1
    ja lbb_18339                                    if true { pc += 5 }
lbb_18334:
    jne r8, 0, lbb_18336                            if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_18323                                    if true { pc += -13 }
lbb_18336:
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r7                                    r2 = r7
    call function_11991                     
lbb_18339:
    stxdw [r6+0x10], r8                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r0, 0, lbb_18343                            if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_18343:
    jeq r0, 0, lbb_18345                            if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, r0                                    r7 = r0
lbb_18345:
    stxdw [r6+0x8], r7                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_18348:
    jne r1, 0, lbb_18351                            if r1 != (0 as i32 as i64 as u64) { pc += 2 }
    mov64 r1, r3                                    r1 = r3
    call function_18184                     
lbb_18351:
    call function_18352                     

function_18352:
    mov64 r3, r1                                    r3 = r1
    mov64 r1, r2                                    r1 = r2
    mov64 r2, r3                                    r2 = r3
    call function_15411                     
    ldxdw r3, [r2+0x10]                     
    stxdw [r1+0x8], r3                      
    ldxdw r2, [r2+0x8]                      
    stxdw [r1+0x0], r2                      
    exit                                    

function_18361:
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r1+0x10]                     
    ldxdw r1, [r1+0x8]                      
    call function_20083                     
    exit                                    

function_18366:
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r1+0x10]                     
    ldxdw r1, [r1+0x8]                      
    call function_19784                     
    exit                                    

function_18371:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r2, [r7+0x8]                      
    jeq r2, 0, lbb_18399                            if r2 == (0 as i32 as i64 as u64) { pc += 24 }
    ldxdw r1, [r7+0x0]                      
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r3, r1                                    r3 = r1
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
lbb_18379:
    ldxdw r8, [r3+0x0]                      
    add64 r8, r4                                    r8 += r4   ///  r8 = r8.wrapping_add(r4)
    add64 r3, 16                                    r3 += 16   ///  r3 = r3.wrapping_add(16 as i32 as i64 as u64)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r4, r8                                    r4 = r8
    jne r2, 0, lbb_18379                            if r2 != (0 as i32 as i64 as u64) { pc += -6 }
    ldxdw r2, [r7+0x18]                     
    jeq r2, 0, lbb_18418                            if r2 == (0 as i32 as i64 as u64) { pc += 31 }
    ldxdw r3, [r1+0x8]                      
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jeq r3, 0, lbb_18392                            if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_18392:
    jlt r8, 16, lbb_18394                           if r8 < (16 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_18394:
    jslt r8, 0, lbb_18399                           if (r8 as i64) < (0 as i32 as i64) { pc += 4 }
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    lsh64 r8, 1                                     r8 <<= 1   ///  r8 = r8.wrapping_shl(1)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jeq r1, 0, lbb_18418                            if r1 == (0 as i32 as i64 as u64) { pc += 19 }
lbb_18399:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_18401:
    stxdw [r10-0x18], r0                    
    stxdw [r10-0x20], r1                    
    stdw [r10-0x10], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    lddw r2, 0x10002f740 --> b"\x00\x00\x00\x00\x009\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00…        r2 load str located at 4295161664
    mov64 r3, r7                                    r3 = r7
    call function_19230                     
    jne r0, 0, lbb_18435                            if r0 != (0 as i32 as i64 as u64) { pc += 24 }
    ldxdw r1, [r10-0x10]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x8], r1                      
    ldxdw r1, [r10-0x20]                    
    stxdw [r6+0x0], r1                      
    exit                                    
lbb_18418:
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jslt r8, 0, lbb_18430                           if (r8 as i64) < (0 as i32 as i64) { pc += 10 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_18401                            if r8 == (0 as i32 as i64 as u64) { pc += -22 }
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    mov64 r1, r8                                    r1 = r8
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_11991                     
    jeq r0, 0, lbb_18430                            if r0 == (0 as i32 as i64 as u64) { pc += 2 }
    mov64 r1, r8                                    r1 = r8
    ja lbb_18401                                    if true { pc += -29 }
lbb_18430:
    mov64 r1, r9                                    r1 = r9
    mov64 r2, r8                                    r2 = r8
    lddw r3, 0x10002f798 --> b"\x00\x00\x00\x00e\xdd\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xea\x01\x00…        r3 load str located at 4295161752
    call function_18348                     
lbb_18435:
    mov64 r3, r10                                   r3 = r10
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    lddw r1, 0x10002dd78 --> b"a formatting trait implementation returned an error when the underlying s…        r1 load str located at 4295155064
    mov64 r2, 86                                    r2 = 86 as i32 as i64 as u64
    lddw r4, 0x10002f7b0 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\…        r4 load str located at 4295161776
    lddw r5, 0x10002f7d0 --> b"\x00\x00\x00\x00\x98\xd2\x02\x00\x10\x00\x00\x00\x00\x00\x00\x00\x88\x02\…        r5 load str located at 4295161808
    call function_18742                     
    mov64 r6, r3                                    r6 = r3
    mov64 r8, r2                                    r8 = r2
    mov64 r7, r1                                    r7 = r1
    ldxdw r9, [r7+0x10]                     
    ldxdw r1, [r7+0x0]                      
    sub64 r1, r9                                    r1 -= r9   ///  r1 = r1.wrapping_sub(r9)
    jge r1, r6, lbb_18459                           if r1 >= r6 { pc += 7 }
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r9                                    r2 = r9
    mov64 r3, r6                                    r3 = r6
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    call function_18213                     
    ldxdw r9, [r7+0x10]                     
lbb_18459:
    ldxdw r1, [r7+0x8]                      
    add64 r1, r9                                    r1 += r9   ///  r1 = r1.wrapping_add(r9)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r6                                    r3 = r6
    call function_21513                     
    add64 r9, r6                                    r9 += r6   ///  r9 = r9.wrapping_add(r6)
    stxdw [r7+0x10], r9                     
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    exit                                    

function_18468:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r7                                    r1 = r7
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jlt r1, 128, lbb_18498                          if r1 < (128 as i32 as i64 as u64) { pc += 24 }
    stw [r10-0x4], 0                        
    jlt r1, 2048, lbb_18511                         if r1 < (2048 as i32 as i64 as u64) { pc += 35 }
    mov64 r1, r7                                    r1 = r7
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jlt r1, 65536, lbb_18519                        if r1 < (65536 as i32 as i64 as u64) { pc += 39 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    rsh64 r2, 18                                    r2 >>= 18   ///  r2 = r2.wrapping_shr(18)
    or64 r2, 240                                    r2 |= 240   ///  r2 = r2.or(240)
    stxb [r10-0x4], r2                      
    mov64 r2, r7                                    r2 = r7
    rsh64 r2, 6                                     r2 >>= 6   ///  r2 = r2.wrapping_shr(6)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    or64 r2, 128                                    r2 |= 128   ///  r2 = r2.or(128)
    stxb [r10-0x2], r2                      
    mov64 r2, r7                                    r2 = r7
    rsh64 r2, 12                                    r2 >>= 12   ///  r2 = r2.wrapping_shr(12)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    or64 r2, 128                                    r2 |= 128   ///  r2 = r2.or(128)
    stxb [r10-0x3], r2                      
    mov64 r8, 4                                     r8 = 4 as i32 as i64 as u64
    ja lbb_18531                                    if true { pc += 33 }
lbb_18498:
    ldxdw r8, [r6+0x10]                     
    ldxdw r1, [r6+0x0]                      
    jne r8, r1, lbb_18505                           if r8 != r1 { pc += 4 }
    mov64 r1, r6                                    r1 = r6
    lddw r2, 0x10002f7e8 --> b"\x00\x00\x00\x00e\xdd\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\x8d\x05\x00…        r2 load str located at 4295161832
    call function_18195                     
lbb_18505:
    ldxdw r1, [r6+0x8]                      
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    stxb [r1+0x0], r7                       
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r6+0x10], r8                     
    ja lbb_18553                                    if true { pc += 42 }
lbb_18511:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -3                                    r1 += -3   ///  r1 = r1.wrapping_add(-3 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    rsh64 r2, 6                                     r2 >>= 6   ///  r2 = r2.wrapping_shr(6)
    or64 r2, 192                                    r2 |= 192   ///  r2 = r2.or(192)
    stxb [r10-0x4], r2                      
    mov64 r8, 2                                     r8 = 2 as i32 as i64 as u64
    ja lbb_18531                                    if true { pc += 12 }
lbb_18519:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2                                    r1 += -2   ///  r1 = r1.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    rsh64 r2, 12                                    r2 >>= 12   ///  r2 = r2.wrapping_shr(12)
    or64 r2, 224                                    r2 |= 224   ///  r2 = r2.or(224)
    stxb [r10-0x4], r2                      
    mov64 r2, r7                                    r2 = r7
    rsh64 r2, 6                                     r2 >>= 6   ///  r2 = r2.wrapping_shr(6)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    or64 r2, 128                                    r2 |= 128   ///  r2 = r2.or(128)
    stxb [r10-0x3], r2                      
    mov64 r8, 3                                     r8 = 3 as i32 as i64 as u64
lbb_18531:
    and64 r7, 63                                    r7 &= 63   ///  r7 = r7.and(63)
    or64 r7, 128                                    r7 |= 128   ///  r7 = r7.or(128)
    stxb [r1+0x0], r7                       
    ldxdw r7, [r6+0x10]                     
    ldxdw r1, [r6+0x0]                      
    sub64 r1, r7                                    r1 -= r7   ///  r1 = r1.wrapping_sub(r7)
    jge r1, r8, lbb_18545                           if r1 >= r8 { pc += 7 }
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r8                                    r3 = r8
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    call function_18213                     
    ldxdw r7, [r6+0x10]                     
lbb_18545:
    ldxdw r1, [r6+0x8]                      
    add64 r1, r7                                    r1 += r7   ///  r1 = r1.wrapping_add(r7)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -4                                    r2 += -4   ///  r2 = r2.wrapping_add(-4 as i32 as i64 as u64)
    mov64 r3, r8                                    r3 = r8
    call function_21513                     
    add64 r7, r8                                    r7 += r8   ///  r7 = r7.wrapping_add(r8)
    stxdw [r6+0x10], r7                     
lbb_18553:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    exit                                    

function_18555:
    mov64 r6, r2                                    r6 = r2
    mov64 r7, r1                                    r7 = r1
    ldxw r1, [r6+0x34]                      
    mov64 r2, r1                                    r2 = r1
    and64 r2, 16                                    r2 &= 16   ///  r2 = r2.and(16)
    jne r2, 0, lbb_18576                            if r2 != (0 as i32 as i64 as u64) { pc += 15 }
    and64 r1, 32                                    r1 &= 32   ///  r1 = r1.and(32)
    jne r1, 0, lbb_18570                            if r1 != (0 as i32 as i64 as u64) { pc += 7 }
    ldxdw r1, [r7+0x0]                      
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r3, r6                                    r3 = r6
    call function_21241                     
    jne r0, 0, lbb_18609                            if r0 != (0 as i32 as i64 as u64) { pc += 40 }
    ja lbb_18581                                    if true { pc += 11 }
lbb_18570:
    ldxdw r2, [r7+0x0]                      
    mov64 r3, r6                                    r3 = r6
    call function_19195                     
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_18609                            if r0 != (0 as i32 as i64 as u64) { pc += 34 }
    ja lbb_18581                                    if true { pc += 5 }
lbb_18576:
    ldxdw r2, [r7+0x0]                      
    mov64 r3, r6                                    r3 = r6
    call function_19035                     
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_18609                            if r0 != (0 as i32 as i64 as u64) { pc += 28 }
lbb_18581:
    ldxdw r1, [r6+0x20]                     
    ldxdw r2, [r6+0x28]                     
    ldxdw r4, [r2+0x18]                     
    lddw r2, 0x10002de6f --> b".."                  r2 load str located at 4295155311
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    callx r4                                
    jne r0, 0, lbb_18609                            if r0 != (0 as i32 as i64 as u64) { pc += 20 }
    ldxw r1, [r6+0x34]                      
    mov64 r2, r1                                    r2 = r1
    and64 r2, 16                                    r2 &= 16   ///  r2 = r2.and(16)
    jne r2, 0, lbb_18601                            if r2 != (0 as i32 as i64 as u64) { pc += 8 }
    and64 r1, 32                                    r1 &= 32   ///  r1 = r1.and(32)
    jeq r1, 0, lbb_18596                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_18605                                    if true { pc += 9 }
lbb_18596:
    ldxdw r1, [r7+0x8]                      
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r3, r6                                    r3 = r6
    call function_21241                     
    ja lbb_18608                                    if true { pc += 7 }
lbb_18601:
    ldxdw r2, [r7+0x8]                      
    mov64 r3, r6                                    r3 = r6
    call function_19035                     
    ja lbb_18608                                    if true { pc += 3 }
lbb_18605:
    ldxdw r2, [r7+0x8]                      
    mov64 r3, r6                                    r3 = r6
    call function_19195                     
lbb_18608:
    mov64 r8, r0                                    r8 = r0
lbb_18609:
    mov64 r0, r8                                    r0 = r8
    exit                                    

function_18611:
    mov64 r4, r1                                    r4 = r1
    ldxb r1, [r4+0x0]                       
    jeq r1, 128, lbb_18624                          if r1 == (128 as i32 as i64 as u64) { pc += 10 }
    ldxb r3, [r4+0xb]                       
    ldxb r1, [r4+0xa]                       
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    ldxdw r1, [r2+0x20]                     
    ldxdw r2, [r2+0x28]                     
    ldxdw r5, [r2+0x18]                     
    mov64 r2, r4                                    r2 = r4
    callx r5                                
    ja lbb_18630                                    if true { pc += 6 }
lbb_18624:
    ldxw r3, [r4+0x4]                       
    ldxdw r1, [r2+0x20]                     
    ldxdw r2, [r2+0x28]                     
    ldxdw r4, [r2+0x20]                     
    mov64 r2, r3                                    r2 = r3
    callx r4                                
lbb_18630:
    exit                                    

function_18631:
    mov64 r3, r1                                    r3 = r1
    lddw r1, 0x10002de82 --> b"called `Option::unwrap()` on a `None` value"        r1 load str located at 4295155330
    mov64 r2, 43                                    r2 = 43 as i32 as i64 as u64
    call function_18704                     

function_18636:
    stxdw [r10-0x68], r1                    
    ldxdw r6, [r2+0x20]                     
    ldxdw r7, [r2+0x28]                     
    ldxdw r8, [r7+0x18]                     
    mov64 r1, r6                                    r1 = r6
    lddw r2, 0x10002dead --> b"panicked at "        r2 load str located at 4295155373
    mov64 r3, 12                                    r3 = 12 as i32 as i64 as u64
    callx r8                                
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_18696                            if r0 != (0 as i32 as i64 as u64) { pc += 49 }
    ldxdw r1, [r10-0x68]                    
    ldxdw r1, [r1+0x8]                      
    lddw r2, 0x10002f850 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r2 load str located at 4295161936
    stxdw [r10-0x60], r2                    
    mov64 r2, r10                                   r2 = r10
    add64 r2, -48                                   r2 += -48   ///  r2 = r2.wrapping_add(-48 as i32 as i64 as u64)
    stxdw [r10-0x50], r2                    
    mov64 r2, r1                                    r2 = r1
    add64 r2, 20                                    r2 += 20   ///  r2 = r2.wrapping_add(20 as i32 as i64 as u64)
    stxdw [r10-0x10], r2                    
    lddw r2, 0x1000295a8 --> b"\xbf#\x00\x00\x00\x00\x00\x00a\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r2 load str located at 4295136680
    stxdw [r10-0x8], r2                     
    stxdw [r10-0x18], r2                    
    mov64 r2, r1                                    r2 = r1
    add64 r2, 16                                    r2 += 16   ///  r2 = r2.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x20], r2                    
    lddw r2, 0x100029bc0 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x13\x08\x00\x00\x00\x00\x00y\x12\x00\x00\x…        r2 load str located at 4295138240
    stxdw [r10-0x28], r2                    
    stxdw [r10-0x30], r1                    
    stdw [r10-0x40], 0                      
    stdw [r10-0x58], 3                      
    stdw [r10-0x48], 3                      
    mov64 r3, r10                                   r3 = r10
    add64 r3, -96                                   r3 += -96   ///  r3 = r3.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r7                                    r2 = r7
    call function_19230                     
    jne r0, 0, lbb_18696                            if r0 != (0 as i32 as i64 as u64) { pc += 18 }
    mov64 r1, r6                                    r1 = r6
    lddw r2, 0x10002deb9 --> b":\x0a"               r2 load str located at 4295155385
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    callx r8                                
    jne r0, 0, lbb_18696                            if r0 != (0 as i32 as i64 as u64) { pc += 12 }
    ldxdw r1, [r10-0x68]                    
    ldxdw r2, [r1+0x0]                      
    mov64 r8, r10                                   r8 = r10
    add64 r8, -48                                   r8 += -48   ///  r8 = r8.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    mov64 r3, 48                                    r3 = 48 as i32 as i64 as u64
    call function_21513                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r8                                    r3 = r8
    call function_19230                     
    mov64 r9, r0                                    r9 = r0
lbb_18696:
    mov64 r0, r9                                    r0 = r9
    exit                                    

function_18698:
    stxdw [r10-0x10], r2                    
    stxdw [r10-0x18], r1                    
    sth [r10-0x8], 1                        
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    call function_18155                     

function_18704:
    mov64 r4, r10                                   r4 = r10
    add64 r4, -16                                   r4 += -16   ///  r4 = r4.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x40], r4                    
    stxdw [r10-0x8], r2                     
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 0                      
    stdw [r10-0x30], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_18698                     

function_18717:
    stxdw [r10-0x58], r2                    
    stxdw [r10-0x60], r1                    
    lddw r1, 0x10002f880 --> b"\x00\x00\x00\x00\xe8\xce\x02\x00 \x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295161984
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x1000298c0 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295137472
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_18698                     

function_18742:
    stxdw [r10-0x68], r2                    
    stxdw [r10-0x70], r1                    
    stxdw [r10-0x58], r4                    
    stxdw [r10-0x60], r3                    
    lddw r1, 0x10002f8a0 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295162016
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    lddw r1, 0x100029b90 --> b"y\x13\x00\x00\x00\x00\x00\x00y\x11\x08\x00\x00\x00\x00\x00y\x14\x18\x00\x…        r1 load str located at 4295138192
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x100029bc0 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x13\x08\x00\x00\x00\x00\x00y\x12\x00\x00\x…        r1 load str located at 4295138240
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r5                                    r2 = r5
    call function_18698                     
    mov64 r7, r3                                    r7 = r3
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    stxdw [r10-0x18], r2                    
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    stxdw [r10-0x48], r2                    
    ldxdw r2, [r1+0x8]                      
    stxdw [r10-0x30], r2                    
    ldxdw r2, [r1+0x0]                      
    stxdw [r10-0x38], r2                    
    ldxdw r1, [r1+0x10]                     
    stxdw [r10-0x28], r1                    
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_18784:
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jne r8, 0, lbb_18880                            if r8 != (0 as i32 as i64 as u64) { pc += 93 }
    jgt r6, r7, lbb_18835                           if r6 > r7 { pc += 47 }
    mov64 r8, r6                                    r8 = r6
lbb_18789:
    ldxdw r3, [r10-0x18]                    
    add64 r3, r8                                    r3 += r8   ///  r3 = r3.wrapping_add(r8)
    mov64 r4, r7                                    r4 = r7
    sub64 r4, r8                                    r4 -= r8   ///  r4 = r4.wrapping_sub(r8)
    jgt r4, 15, lbb_18805                           if r4 > (15 as i32 as i64 as u64) { pc += 11 }
    mov64 r6, r7                                    r6 = r7
    jeq r7, r8, lbb_18835                           if r7 == r8 { pc += 39 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_18797:
    mov64 r2, r3                                    r2 = r3
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxb r2, [r2+0x0]                       
    jeq r2, 10, lbb_18814                           if r2 == (10 as i32 as i64 as u64) { pc += 13 }
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    mov64 r6, r7                                    r6 = r7
    jlt r1, r4, lbb_18797                           if r1 < r4 { pc += -7 }
    ja lbb_18835                                    if true { pc += 30 }
lbb_18805:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r6, r5                                    r6 = r5
    call function_20172                     
    mov64 r5, r6                                    r5 = r6
    ldxdw r1, [r10-0x8]                     
    ldxdw r2, [r10-0x10]                    
    jne r2, 1, lbb_18831                            if r2 != (1 as i32 as i64 as u64) { pc += 17 }
lbb_18814:
    mov64 r2, r8                                    r2 = r8
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r6, r2                                    r6 = r2
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    jlt r2, r7, lbb_18822                           if r2 < r7 { pc += 3 }
lbb_18819:
    mov64 r8, r6                                    r8 = r6
    jgt r6, r7, lbb_18835                           if r6 > r7 { pc += 14 }
    ja lbb_18789                                    if true { pc += -33 }
lbb_18822:
    ldxdw r2, [r10-0x18]                    
    add64 r2, r8                                    r2 += r8   ///  r2 = r2.wrapping_add(r8)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxb r1, [r2+0x0]                       
    stxdw [r10-0x20], r6                    
    mov64 r9, r6                                    r9 = r6
    jeq r1, 10, lbb_18840                           if r1 == (10 as i32 as i64 as u64) { pc += 10 }
    ja lbb_18819                                    if true { pc += -12 }
lbb_18831:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    mov64 r6, r7                                    r6 = r7
    jne r2, 0, lbb_18840                            if r2 != (0 as i32 as i64 as u64) { pc += 5 }
lbb_18835:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    stxdw [r10-0x20], r5                    
    mov64 r9, r7                                    r9 = r7
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r5, r7, lbb_18880                           if r5 == r7 { pc += 40 }
lbb_18840:
    ldxdw r1, [r10-0x28]                    
    ldxb r1, [r1+0x0]                       
    jne r1, 0, lbb_18844                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_18856                                    if true { pc += 12 }
lbb_18844:
    ldxdw r1, [r10-0x30]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r10-0x38]                    
    lddw r2, 0x10002de75 --> b"    "                r2 load str located at 4295155317
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
    stxdw [r10-0x40], r5                    
    callx r4                                
    ldxdw r5, [r10-0x40]                    
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_18880                            if r1 != (0 as i32 as i64 as u64) { pc += 24 }
lbb_18856:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jeq r9, r5, lbb_18864                           if r9 == r5 { pc += 6 }
    ldxdw r1, [r10-0x48]                    
    add64 r1, r9                                    r1 += r9   ///  r1 = r1.wrapping_add(r9)
    ldxb r2, [r1+0x0]                       
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r2, 10, lbb_18864                           if r2 == (10 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_18864:
    ldxdw r2, [r10-0x18]                    
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    sub64 r9, r5                                    r9 -= r5   ///  r9 = r9.wrapping_sub(r5)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    ldxdw r3, [r10-0x28]                    
    stxb [r3+0x0], r1                       
    ldxdw r1, [r10-0x30]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r10-0x38]                    
    mov64 r3, r9                                    r3 = r9
    callx r4                                
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ldxdw r5, [r10-0x20]                    
    jne r1, 0, lbb_18880                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_18784                                    if true { pc += -96 }
lbb_18880:
    exit                                    

function_18881:
    mov64 r6, r2                                    r6 = r2
    ldxdw r8, [r1+0x8]                      
    ldxdw r7, [r1+0x0]                      
    ldxdw r9, [r1+0x10]                     
    ldxb r1, [r9+0x0]                       
    jne r1, 0, lbb_18899                            if r1 != (0 as i32 as i64 as u64) { pc += 12 }
lbb_18887:
    mov64 r2, r6                                    r2 = r6
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r2, 10, lbb_18893                           if r2 == (10 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_18893:
    stxb [r9+0x0], r1                       
    ldxdw r3, [r8+0x20]                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r6                                    r2 = r6
    callx r3                                
    ja lbb_18908                                    if true { pc += 9 }
lbb_18899:
    ldxdw r4, [r8+0x18]                     
    mov64 r1, r7                                    r1 = r7
    lddw r2, 0x10002de75 --> b"    "                r2 load str located at 4295155317
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
    callx r4                                
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_18887                            if r1 == (0 as i32 as i64 as u64) { pc += -21 }
lbb_18908:
    exit                                    

function_18909:
    mov64 r6, r1                                    r6 = r1
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ldxdw r9, [r6+0x0]                      
    ldxb r1, [r6+0x10]                      
    jne r1, 0, lbb_18993                            if r1 != (0 as i32 as i64 as u64) { pc += 79 }
    stxdw [r10-0x68], r2                    
    ldxdw r8, [r6+0x8]                      
    ldxw r1, [r8+0x34]                      
    mov64 r2, r1                                    r2 = r1
    and64 r2, 4                                     r2 &= 4   ///  r2 = r2.and(4)
    stxdw [r10-0x70], r3                    
    jne r2, 0, lbb_18940                            if r2 != (0 as i32 as i64 as u64) { pc += 19 }
    lddw r2, 0x10002ded3 --> b"((\x0a,core/src/fmt/num.rs0x0001020304050607080910111"        r2 load str located at 4295155411
    jeq r9, 0, lbb_18926                            if r9 == (0 as i32 as i64 as u64) { pc += 2 }
    lddw r2, 0x10002decf --> b","                   r2 load str located at 4295155407
lbb_18926:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jeq r9, 0, lbb_18929                            if r9 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
lbb_18929:
    ldxdw r1, [r8+0x20]                     
    ldxdw r4, [r8+0x28]                     
    ldxdw r4, [r4+0x18]                     
    callx r4                                
    ldxdw r1, [r10-0x68]                    
    ldxdw r2, [r10-0x70]                    
    jne r0, 0, lbb_18993                            if r0 != (0 as i32 as i64 as u64) { pc += 57 }
    ldxdw r3, [r2+0x18]                     
    mov64 r2, r8                                    r2 = r8
    callx r3                                
    ja lbb_18992                                    if true { pc += 52 }
lbb_18940:
    jeq r9, 0, lbb_18942                            if r9 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_18951                                    if true { pc += 9 }
lbb_18942:
    ldxdw r1, [r8+0x20]                     
    ldxdw r2, [r8+0x28]                     
    ldxdw r4, [r2+0x18]                     
    lddw r2, 0x10002ded4 --> b"(\x0a"               r2 load str located at 4295155412
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    callx r4                                
    jne r0, 0, lbb_18993                            if r0 != (0 as i32 as i64 as u64) { pc += 43 }
    ldxw r1, [r8+0x34]                      
lbb_18951:
    stb [r10-0x41], 1                       
    ldxdw r2, [r8+0x20]                     
    ldxdw r3, [r8+0x28]                     
    mov64 r4, r10                                   r4 = r10
    add64 r4, -65                                   r4 += -65   ///  r4 = r4.wrapping_add(-65 as i32 as i64 as u64)
    stxdw [r10-0x50], r4                    
    stxdw [r10-0x58], r3                    
    stxdw [r10-0x60], r2                    
    ldxdw r2, [r8+0x0]                      
    ldxdw r3, [r8+0x8]                      
    ldxdw r4, [r8+0x10]                     
    ldxdw r5, [r8+0x18]                     
    ldxw r0, [r8+0x30]                      
    ldxb r8, [r8+0x38]                      
    stxb [r10-0x8], r8                      
    stxw [r10-0x10], r0                     
    stxw [r10-0xc], r1                      
    lddw r1, 0x10002f8c0 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r1 load str located at 4295162048
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stxdw [r10-0x28], r5                    
    stxdw [r10-0x30], r4                    
    stxdw [r10-0x38], r3                    
    stxdw [r10-0x40], r2                    
    ldxdw r1, [r10-0x70]                    
    ldxdw r3, [r1+0x18]                     
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    ldxdw r1, [r10-0x68]                    
    callx r3                                
    jne r0, 0, lbb_18993                            if r0 != (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r1, [r10-0x18]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r10-0x20]                    
    lddw r2, 0x10002ded1 --> b",\x0a"               r2 load str located at 4295155409
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    callx r4                                
lbb_18992:
    mov64 r7, r0                                    r7 = r0
lbb_18993:
    stxb [r6+0x10], r7                      
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r6+0x0], r9                      
    mov64 r0, r6                                    r0 = r6
    exit                                    

function_18998:
    mov64 r6, r1                                    r6 = r1
    ldxb r2, [r6+0x10]                      
    ldxdw r1, [r6+0x0]                      
    mov64 r7, r2                                    r7 = r2
    jeq r1, 0, lbb_19032                            if r1 == (0 as i32 as i64 as u64) { pc += 29 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_19031                            if r2 != (0 as i32 as i64 as u64) { pc += 25 }
    jne r1, 1, lbb_19009                            if r1 != (1 as i32 as i64 as u64) { pc += 2 }
    ldxb r1, [r6+0x11]                      
    jne r1, 0, lbb_19019                            if r1 != (0 as i32 as i64 as u64) { pc += 10 }
lbb_19009:
    ldxdw r2, [r6+0x8]                      
    ldxdw r1, [r2+0x20]                     
    ldxdw r2, [r2+0x28]                     
    ldxdw r4, [r2+0x18]                     
    lddw r2, 0x10002de6e --> b")"                   r2 load str located at 4295155310
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    callx r4                                
    mov64 r7, r0                                    r7 = r0
    ja lbb_19031                                    if true { pc += 12 }
lbb_19019:
    ldxdw r2, [r6+0x8]                      
    ldxw r1, [r2+0x34]                      
    and64 r1, 4                                     r1 &= 4   ///  r1 = r1.and(4)
    jne r1, 0, lbb_19009                            if r1 != (0 as i32 as i64 as u64) { pc += -14 }
    ldxdw r1, [r2+0x20]                     
    ldxdw r2, [r2+0x28]                     
    ldxdw r4, [r2+0x18]                     
    lddw r2, 0x10002ded6 --> b","                   r2 load str located at 4295155414
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    callx r4                                
    jeq r0, 0, lbb_19009                            if r0 == (0 as i32 as i64 as u64) { pc += -22 }
lbb_19031:
    stxb [r6+0x10], r7                      
lbb_19032:
    and64 r7, 1                                     r7 &= 1   ///  r7 = r7.and(1)
    mov64 r0, r7                                    r0 = r7
    exit                                    

function_19035:
    stxdw [r10-0x88], r3                    
    mov64 r7, r2                                    r7 = r2
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64

function_19038:
    mov64 r9, r7                                    r9 = r7
    mov64 r6, r10                                   r6 = r10
    add64 r6, -128                                  r6 += -128   ///  r6 = r6.wrapping_add(-128 as i32 as i64 as u64)
    add64 r6, r8                                    r6 += r8   ///  r6 = r6.wrapping_add(r8)
    mov64 r1, r9                                    r1 = r9
    and64 r1, 15                                    r1 &= 15   ///  r1 = r1.and(15)
    call function_21001                     
    stxb [r6+0x7f], r0                      
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    rsh64 r7, 4                                     r7 >>= 4   ///  r7 = r7.wrapping_shr(4)
    jgt r9, 15, function_19038                      if r9 > (15 as i32 as i64 as u64) { pc += -11 }
    mov64 r1, r8                                    r1 = r8
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    add64 r1, 128                                   r1 += 128   ///  r1 = r1.wrapping_add(128 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    ldxdw r1, [r10-0x88]                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x10002deea --> b"0x"                  r3 load str located at 4295155434
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    call function_19371                     
    exit                                    

function_19065:
    mov64 r6, r3                                    r6 = r3
    mov64 r7, r2                                    r7 = r2
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64

function_19068:
    mov64 r9, r10                                   r9 = r10
    add64 r9, -128                                  r9 += -128   ///  r9 = r9.wrapping_add(-128 as i32 as i64 as u64)
    add64 r9, r8                                    r9 += r8   ///  r9 = r9.wrapping_add(r8)
    mov64 r1, r7                                    r1 = r7
    and64 r1, 15                                    r1 &= 15   ///  r1 = r1.and(15)
    call function_21001                     
    stxb [r9+0x7f], r0                      
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    mov64 r1, r7                                    r1 = r7
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r7, r1                                    r7 = r1
    rsh64 r7, 4                                     r7 >>= 4   ///  r7 = r7.wrapping_shr(4)
    jgt r1, 15, function_19068                      if r1 > (15 as i32 as i64 as u64) { pc += -14 }
    mov64 r1, r8                                    r1 = r8
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    add64 r1, 128                                   r1 += 128   ///  r1 = r1.wrapping_add(128 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x10002deea --> b"0x"                  r3 load str located at 4295155434
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    call function_19371                     
    exit                                    

function_19098:
    mov64 r6, r3                                    r6 = r3
    mov64 r7, r2                                    r7 = r2
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64

function_19101:
    mov64 r9, r10                                   r9 = r10
    add64 r9, -128                                  r9 += -128   ///  r9 = r9.wrapping_add(-128 as i32 as i64 as u64)
    add64 r9, r8                                    r9 += r8   ///  r9 = r9.wrapping_add(r8)
    mov64 r1, r7                                    r1 = r7
    and64 r1, 15                                    r1 &= 15   ///  r1 = r1.and(15)
    call function_21036                     
    stxb [r9+0x7f], r0                      
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    mov64 r1, r7                                    r1 = r7
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r7, r1                                    r7 = r1
    rsh64 r7, 4                                     r7 >>= 4   ///  r7 = r7.wrapping_shr(4)
    jgt r1, 15, function_19101                      if r1 > (15 as i32 as i64 as u64) { pc += -14 }
    mov64 r1, r8                                    r1 = r8
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    add64 r1, 128                                   r1 += 128   ///  r1 = r1.wrapping_add(128 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x10002deea --> b"0x"                  r3 load str located at 4295155434
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    call function_19371                     
    exit                                    

function_19131:
    mov64 r6, r3                                    r6 = r3
    mov64 r7, r2                                    r7 = r2
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64

function_19134:
    mov64 r9, r10                                   r9 = r10
    add64 r9, -128                                  r9 += -128   ///  r9 = r9.wrapping_add(-128 as i32 as i64 as u64)
    add64 r9, r8                                    r9 += r8   ///  r9 = r9.wrapping_add(r8)
    mov64 r1, r7                                    r1 = r7
    and64 r1, 15                                    r1 &= 15   ///  r1 = r1.and(15)
    call function_21001                     
    stxb [r9+0x7f], r0                      
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r7, r1                                    r7 = r1
    rsh64 r7, 4                                     r7 >>= 4   ///  r7 = r7.wrapping_shr(4)
    jgt r1, 15, function_19134                      if r1 > (15 as i32 as i64 as u64) { pc += -13 }
    mov64 r1, r8                                    r1 = r8
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    add64 r1, 128                                   r1 += 128   ///  r1 = r1.wrapping_add(128 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x10002deea --> b"0x"                  r3 load str located at 4295155434
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    call function_19371                     
    exit                                    

function_19163:
    mov64 r6, r3                                    r6 = r3
    mov64 r7, r2                                    r7 = r2
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64

function_19166:
    mov64 r9, r10                                   r9 = r10
    add64 r9, -128                                  r9 += -128   ///  r9 = r9.wrapping_add(-128 as i32 as i64 as u64)
    add64 r9, r8                                    r9 += r8   ///  r9 = r9.wrapping_add(r8)
    mov64 r1, r7                                    r1 = r7
    and64 r1, 15                                    r1 &= 15   ///  r1 = r1.and(15)
    call function_21036                     
    stxb [r9+0x7f], r0                      
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r7, r1                                    r7 = r1
    rsh64 r7, 4                                     r7 >>= 4   ///  r7 = r7.wrapping_shr(4)
    jgt r1, 15, function_19166                      if r1 > (15 as i32 as i64 as u64) { pc += -13 }
    mov64 r1, r8                                    r1 = r8
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    add64 r1, 128                                   r1 += 128   ///  r1 = r1.wrapping_add(128 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x10002deea --> b"0x"                  r3 load str located at 4295155434
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    call function_19371                     
    exit                                    

function_19195:
    stxdw [r10-0x88], r3                    
    mov64 r7, r2                                    r7 = r2
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64

function_19198:
    mov64 r9, r7                                    r9 = r7
    mov64 r6, r10                                   r6 = r10
    add64 r6, -128                                  r6 += -128   ///  r6 = r6.wrapping_add(-128 as i32 as i64 as u64)
    add64 r6, r8                                    r6 += r8   ///  r6 = r6.wrapping_add(r8)
    mov64 r1, r9                                    r1 = r9
    and64 r1, 15                                    r1 &= 15   ///  r1 = r1.and(15)
    call function_21036                     
    stxb [r6+0x7f], r0                      
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    rsh64 r7, 4                                     r7 >>= 4   ///  r7 = r7.wrapping_shr(4)
    jgt r9, 15, function_19198                      if r9 > (15 as i32 as i64 as u64) { pc += -11 }
    mov64 r1, r8                                    r1 = r8
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    add64 r1, 128                                   r1 += 128   ///  r1 = r1.wrapping_add(128 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    ldxdw r1, [r10-0x88]                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x10002deea --> b"0x"                  r3 load str located at 4295155434
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    call function_19371                     
    exit                                    

function_19225:
    mov64 r3, r2                                    r3 = r2
    lddw r2, 0x10002f8c0 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r2 load str located at 4295162048
    call function_19230                     
    exit                                    

function_19230:
    stxdw [r10-0x18], r2                    
    stxdw [r10-0x20], r1                    
    stb [r10-0x8], 3                        
    stdw [r10-0x10], 32                     
    stdw [r10-0x30], 0                      
    stdw [r10-0x40], 0                      
    ldxdw r8, [r3+0x20]                     
    stxdw [r10-0x58], r3                    
    jne r8, 0, lbb_19268                            if r8 != (0 as i32 as i64 as u64) { pc += 29 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r1, [r3+0x18]                     
    jeq r1, 0, lbb_19352                            if r1 == (0 as i32 as i64 as u64) { pc += 110 }
    ldxdw r2, [r10-0x58]                    
    ldxdw r6, [r2+0x10]                     
    lsh64 r1, 4                                     r1 <<= 4   ///  r1 = r1.wrapping_shl(4)
    mov64 r8, r6                                    r8 = r6
    add64 r8, r1                                    r8 += r1   ///  r8 = r8.wrapping_add(r1)
    ldxdw r9, [r2+0x0]                      
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
lbb_19249:
    ldxdw r3, [r9+0x0]                      
    jeq r3, 0, lbb_19257                            if r3 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r1, [r10-0x18]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r2, [r9-0x8]                      
    ldxdw r1, [r10-0x20]                    
    callx r4                                
    jne r0, 0, lbb_19369                            if r0 != (0 as i32 as i64 as u64) { pc += 112 }
lbb_19257:
    ldxdw r3, [r6+0x8]                      
    ldxdw r1, [r6+0x0]                      
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    callx r3                                
    jne r0, 0, lbb_19369                            if r0 != (0 as i32 as i64 as u64) { pc += 106 }
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    add64 r9, 16                                    r9 += 16   ///  r9 = r9.wrapping_add(16 as i32 as i64 as u64)
    add64 r6, 16                                    r6 += 16   ///  r6 = r6.wrapping_add(16 as i32 as i64 as u64)
    jeq r6, r8, lbb_19352                           if r6 == r8 { pc += 85 }
    ja lbb_19249                                    if true { pc += -19 }
lbb_19268:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r1, [r3+0x28]                     
    stxdw [r10-0x50], r1                    
    jeq r1, 0, lbb_19352                            if r1 == (0 as i32 as i64 as u64) { pc += 80 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x50]                    
    mul64 r1, 56                                    r1 *= 56   ///  r1 = r1.wrapping_mul(56 as u64)
    stxdw [r10-0x50], r1                    
    ldxdw r1, [r10-0x58]                    
    ldxdw r2, [r1+0x10]                     
    stxdw [r10-0x48], r2                    
    ldxdw r9, [r1+0x0]                      
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
lbb_19281:
    ldxdw r3, [r9+0x0]                      
    jeq r3, 0, lbb_19289                            if r3 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r1, [r10-0x18]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r2, [r9-0x8]                      
    ldxdw r1, [r10-0x20]                    
    callx r4                                
    jne r0, 0, lbb_19369                            if r0 != (0 as i32 as i64 as u64) { pc += 80 }
lbb_19289:
    mov64 r2, r8                                    r2 = r8
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    ldxw r1, [r2+0x28]                      
    stxw [r10-0x10], r1                     
    ldxb r1, [r2+0x30]                      
    stxb [r10-0x8], r1                      
    ldxw r1, [r2+0x2c]                      
    stxw [r10-0xc], r1                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r5, [r2+0x10]                     
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jeq r5, 2, lbb_19314                            if r5 == (2 as i32 as i64 as u64) { pc += 13 }
    jeq r5, 1, lbb_19304                            if r5 == (1 as i32 as i64 as u64) { pc += 2 }
    add64 r2, 16                                    r2 += 16   ///  r2 = r2.wrapping_add(16 as i32 as i64 as u64)
    ja lbb_19312                                    if true { pc += 8 }
lbb_19304:
    mov64 r2, r8                                    r2 = r8
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    ldxdw r3, [r2+0x18]                     
    lsh64 r3, 4                                     r3 <<= 4   ///  r3 = r3.wrapping_shl(4)
    ldxdw r2, [r10-0x48]                    
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    ldxdw r5, [r2+0x0]                      
    jne r5, 0, lbb_19314                            if r5 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_19312:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r3, [r2+0x8]                      
lbb_19314:
    mov64 r2, r8                                    r2 = r8
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    stxdw [r10-0x38], r3                    
    stxdw [r10-0x40], r4                    
    ldxdw r4, [r2+0x0]                      
    jeq r4, 2, lbb_19332                            if r4 == (2 as i32 as i64 as u64) { pc += 12 }
    jeq r4, 1, lbb_19322                            if r4 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19330                                    if true { pc += 8 }
lbb_19322:
    mov64 r2, r8                                    r2 = r8
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    ldxdw r3, [r2+0x8]                      
    lsh64 r3, 4                                     r3 <<= 4   ///  r3 = r3.wrapping_shl(4)
    ldxdw r2, [r10-0x48]                    
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    ldxdw r4, [r2+0x0]                      
    jne r4, 0, lbb_19332                            if r4 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_19330:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r3, [r2+0x8]                      
lbb_19332:
    stxdw [r10-0x28], r3                    
    stxdw [r10-0x30], r1                    
    mov64 r1, r8                                    r1 = r8
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    ldxdw r1, [r1+0x20]                     
    lsh64 r1, 4                                     r1 <<= 4   ///  r1 = r1.wrapping_shl(4)
    ldxdw r2, [r10-0x48]                    
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxdw r3, [r2+0x8]                      
    ldxdw r1, [r2+0x0]                      
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    callx r3                                
    jne r0, 0, lbb_19369                            if r0 != (0 as i32 as i64 as u64) { pc += 23 }
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    add64 r9, 16                                    r9 += 16   ///  r9 = r9.wrapping_add(16 as i32 as i64 as u64)
    add64 r6, 56                                    r6 += 56   ///  r6 = r6.wrapping_add(56 as i32 as i64 as u64)
    ldxdw r1, [r10-0x50]                    
    jeq r1, r6, lbb_19352                           if r1 == r6 { pc += 1 }
    ja lbb_19281                                    if true { pc += -71 }
lbb_19352:
    ldxdw r1, [r10-0x58]                    
    ldxdw r1, [r1+0x8]                      
    jlt r7, r1, lbb_19356                           if r7 < r1 { pc += 1 }
    ja lbb_19367                                    if true { pc += 11 }
lbb_19356:
    lsh64 r7, 4                                     r7 <<= 4   ///  r7 = r7.wrapping_shl(4)
    ldxdw r1, [r10-0x58]                    
    ldxdw r1, [r1+0x0]                      
    add64 r1, r7                                    r1 += r7   ///  r1 = r1.wrapping_add(r7)
    ldxdw r3, [r1+0x8]                      
    ldxdw r2, [r1+0x0]                      
    ldxdw r1, [r10-0x18]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r10-0x20]                    
    callx r4                                
    jne r0, 0, lbb_19369                            if r0 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_19367:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_19370                                    if true { pc += 1 }
lbb_19369:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_19370:
    exit                                    

function_19371:
    mov64 r6, r1                                    r6 = r1
    ldxdw r0, [r5-0xff8]                    
    jne r2, 0, lbb_19377                            if r2 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r2, 45                                    r2 = 45 as i32 as i64 as u64
    ldxw r7, [r6+0x34]                      
    ja lbb_19400                                    if true { pc += 23 }
lbb_19377:
    mov64 r2, 1114112                               r2 = 1114112 as i32 as i64 as u64
    ldxw r7, [r6+0x34]                      
    mov64 r1, r7                                    r1 = r7
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    mov64 r9, r0                                    r9 = r0
    jne r1, 0, lbb_19399                            if r1 != (0 as i32 as i64 as u64) { pc += 16 }
lbb_19383:
    stxdw [r10-0x28], r0                    
    ldxdw r1, [r5-0x1000]                   
    stxdw [r10-0x30], r1                    
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, r7                                    r1 = r7
    and64 r1, 4                                     r1 &= 4   ///  r1 = r1.and(4)
    jeq r1, 0, lbb_19423                            if r1 == (0 as i32 as i64 as u64) { pc += 33 }
    stxdw [r10-0x40], r2                    
    stxdw [r10-0x48], r6                    
    stxdw [r10-0x38], r4                    
    jlt r4, 32, lbb_19403                           if r4 < (32 as i32 as i64 as u64) { pc += 9 }
    mov64 r8, r3                                    r8 = r3
    mov64 r1, r3                                    r1 = r3
    mov64 r2, r4                                    r2 = r4
    call function_20249                     
    ja lbb_19418                                    if true { pc += 19 }
lbb_19399:
    mov64 r2, 43                                    r2 = 43 as i32 as i64 as u64
lbb_19400:
    mov64 r9, r0                                    r9 = r0
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    ja lbb_19383                                    if true { pc += -20 }
lbb_19403:
    mov64 r8, r3                                    r8 = r3
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r4, 0, lbb_19418                            if r4 == (0 as i32 as i64 as u64) { pc += 12 }
    mov64 r1, r8                                    r1 = r8
    ldxdw r2, [r10-0x38]                    
lbb_19408:
    ldxb r4, [r1+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jsgt r4, -65, lbb_19414                         if (r4 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_19414:
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    jne r2, 0, lbb_19408                            if r2 != (0 as i32 as i64 as u64) { pc += -10 }
lbb_19418:
    add64 r0, r9                                    r0 += r9   ///  r0 = r0.wrapping_add(r9)
    mov64 r9, r0                                    r9 = r0
    ldxdw r6, [r10-0x48]                    
    ldxdw r4, [r10-0x38]                    
    ldxdw r2, [r10-0x40]                    
lbb_19423:
    ldxdw r1, [r6+0x0]                      
    jne r1, 0, lbb_19431                            if r1 != (0 as i32 as i64 as u64) { pc += 6 }
    mov64 r1, r6                                    r1 = r6
    mov64 r3, r8                                    r3 = r8
    call function_19547                     
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_19492                            if r0 != (0 as i32 as i64 as u64) { pc += 62 }
    ja lbb_19485                                    if true { pc += 54 }
lbb_19431:
    ldxdw r3, [r6+0x8]                      
    jgt r3, r9, lbb_19434                           if r3 > r9 { pc += 1 }
    ja lbb_19480                                    if true { pc += 46 }
lbb_19434:
    and64 r7, 8                                     r7 &= 8   ///  r7 = r7.and(8)
    jeq r7, 0, lbb_19437                            if r7 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19495                                    if true { pc += 58 }
lbb_19437:
    stxdw [r10-0x40], r2                    
    stxdw [r10-0x38], r4                    
    sub64 r3, r9                                    r3 -= r9   ///  r3 = r3.wrapping_sub(r9)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r2, r6                                    r2 = r6
    mov64 r9, r6                                    r9 = r6
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_19719                     
    ldxw r1, [r10-0x18]                     
    stxdw [r10-0x48], r1                    
    jeq r1, 1114112, lbb_19492                      if r1 == (1114112 as i32 as i64 as u64) { pc += 42 }
    ldxdw r6, [r10-0x20]                    
    mov64 r1, r9                                    r1 = r9
    ldxdw r2, [r10-0x40]                    
    mov64 r3, r8                                    r3 = r8
    ldxdw r4, [r10-0x38]                    
    call function_19547                     
    jne r0, 0, lbb_19492                            if r0 != (0 as i32 as i64 as u64) { pc += 35 }
    ldxdw r8, [r9+0x20]                     
    ldxdw r9, [r9+0x28]                     
    ldxdw r4, [r9+0x18]                     
    mov64 r1, r8                                    r1 = r8
    ldxdw r2, [r10-0x30]                    
    ldxdw r3, [r10-0x28]                    
    callx r4                                
    jne r0, 0, lbb_19492                            if r0 != (0 as i32 as i64 as u64) { pc += 27 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_19466:
    mov64 r1, r6                                    r1 = r6
    jeq r6, r7, lbb_19476                           if r6 == r7 { pc += 8 }
    ldxdw r3, [r9+0x20]                     
    mov64 r1, r8                                    r1 = r8
    ldxdw r2, [r10-0x48]                    
    callx r3                                
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    jeq r0, 0, lbb_19466                            if r0 == (0 as i32 as i64 as u64) { pc += -8 }
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
lbb_19476:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jlt r1, r6, lbb_19492                           if r1 < r6 { pc += 14 }
lbb_19478:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ja lbb_19492                                    if true { pc += 12 }
lbb_19480:
    mov64 r1, r6                                    r1 = r6
    mov64 r3, r8                                    r3 = r8
    call function_19547                     
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_19492                            if r0 != (0 as i32 as i64 as u64) { pc += 7 }
lbb_19485:
    ldxdw r1, [r6+0x20]                     
    ldxdw r2, [r6+0x28]                     
    ldxdw r4, [r2+0x18]                     
    ldxdw r2, [r10-0x30]                    
    ldxdw r3, [r10-0x28]                    
    callx r4                                
    mov64 r7, r0                                    r7 = r0
lbb_19492:
    and64 r7, 1                                     r7 &= 1   ///  r7 = r7.and(1)
    mov64 r0, r7                                    r0 = r7
    exit                                    
lbb_19495:
    stxdw [r10-0x40], r3                    
    ldxw r1, [r6+0x30]                      
    stxdw [r10-0x50], r1                    
    stw [r6+0x30], 48                       
    ldxb r1, [r6+0x38]                      
    stxdw [r10-0x58], r1                    
    stb [r6+0x38], 1                        
    mov64 r1, r6                                    r1 = r6
    mov64 r3, r8                                    r3 = r8
    call function_19547                     
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_19492                            if r0 != (0 as i32 as i64 as u64) { pc += -15 }
    ldxdw r3, [r10-0x40]                    
    sub64 r3, r9                                    r3 -= r9   ///  r3 = r3.wrapping_sub(r9)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_19719                     
    ldxw r8, [r10-0x8]                      
    jeq r8, 1114112, lbb_19492                      if r8 == (1114112 as i32 as i64 as u64) { pc += -24 }
    ldxdw r1, [r10-0x10]                    
    stxdw [r10-0x38], r1                    
    ldxdw r1, [r6+0x20]                     
    ldxdw r2, [r6+0x28]                     
    stxdw [r10-0x48], r2                    
    ldxdw r4, [r2+0x18]                     
    stxdw [r10-0x40], r1                    
    ldxdw r2, [r10-0x30]                    
    ldxdw r3, [r10-0x28]                    
    callx r4                                
    jne r0, 0, lbb_19492                            if r0 != (0 as i32 as i64 as u64) { pc += -35 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_19528:
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x38]                    
    jeq r2, r9, lbb_19542                           if r2 == r9 { pc += 11 }
    ldxdw r1, [r10-0x48]                    
    ldxdw r3, [r1+0x20]                     
    ldxdw r1, [r10-0x40]                    
    mov64 r2, r8                                    r2 = r8
    callx r3                                
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    jeq r0, 0, lbb_19528                            if r0 == (0 as i32 as i64 as u64) { pc += -10 }
    add64 r9, -1                                    r9 += -1   ///  r9 = r9.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x38]                    
    jlt r9, r2, lbb_19492                           if r9 < r2 { pc += -50 }
lbb_19542:
    ldxdw r2, [r10-0x58]                    
    stxb [r1+0x38], r2                      
    ldxdw r2, [r10-0x50]                    
    stxw [r1+0x30], r2                      
    ja lbb_19478                                    if true { pc += -69 }

function_19547:
    mov64 r6, r4                                    r6 = r4
    mov64 r7, r3                                    r7 = r3
    mov64 r8, r1                                    r8 = r1
    mov64 r1, r2                                    r1 = r2
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jeq r1, 1114112, lbb_19561                      if r1 == (1114112 as i32 as i64 as u64) { pc += 7 }
    ldxdw r1, [r8+0x20]                     
    ldxdw r3, [r8+0x28]                     
    ldxdw r3, [r3+0x20]                     
    callx r3                                
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_19563                            if r1 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_19561:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jne r7, 0, lbb_19564                            if r7 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_19563:
    exit                                    
lbb_19564:
    ldxdw r1, [r8+0x20]                     
    ldxdw r2, [r8+0x28]                     
    ldxdw r4, [r2+0x18]                     
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r6                                    r3 = r6
    callx r4                                
    ja lbb_19563                                    if true { pc += -8 }

function_19571:
    stxdw [r10-0x18], r2                    
    mov64 r8, r1                                    r8 = r1
    ldxdw r2, [r8+0x10]                     
    ldxdw r1, [r8+0x0]                      
    jne r1, 0, lbb_19579                            if r1 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r4, r2                                    r4 = r2
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jeq r4, 0, lbb_19628                            if r4 == (0 as i32 as i64 as u64) { pc += 49 }
lbb_19579:
    stxdw [r10-0x20], r3                    
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_19583                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19646                                    if true { pc += 63 }
lbb_19583:
    ldxdw r0, [r10-0x18]                    
    mov64 r3, r0                                    r3 = r0
    ldxdw r2, [r10-0x20]                    
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r4, [r8+0x18]                     
    jeq r4, 0, lbb_19615                            if r4 == (0 as i32 as i64 as u64) { pc += 25 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r0, [r10-0x18]                    
lbb_19592:
    mov64 r7, r0                                    r7 = r0
    mov64 r6, r2                                    r6 = r2
    jeq r7, r3, lbb_19646                           if r7 == r3 { pc += 51 }
    mov64 r0, r7                                    r0 = r7
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    ldxb r2, [r7+0x0]                       
    mov64 r9, r2                                    r9 = r2
    lsh64 r9, 56                                    r9 <<= 56   ///  r9 = r9.wrapping_shl(56)
    arsh64 r9, 56                                   r9 >>= 56 (signed)   ///  r9 = (r9 as i64).wrapping_shr(56)
    jsgt r9, -1, lbb_19610                          if (r9 as i64) > (-1 as i32 as i64) { pc += 8 }
    mov64 r0, r7                                    r0 = r7
    add64 r0, 2                                     r0 += 2   ///  r0 = r0.wrapping_add(2 as i32 as i64 as u64)
    jlt r2, 224, lbb_19610                          if r2 < (224 as i32 as i64 as u64) { pc += 5 }
    mov64 r0, r7                                    r0 = r7
    add64 r0, 3                                     r0 += 3   ///  r0 = r0.wrapping_add(3 as i32 as i64 as u64)
    jlt r2, 240, lbb_19610                          if r2 < (240 as i32 as i64 as u64) { pc += 2 }
    mov64 r0, r7                                    r0 = r7
    add64 r0, 4                                     r0 += 4   ///  r0 = r0.wrapping_add(4 as i32 as i64 as u64)
lbb_19610:
    add64 r5, 1                                     r5 += 1   ///  r5 = r5.wrapping_add(1 as i32 as i64 as u64)
    mov64 r2, r0                                    r2 = r0
    sub64 r2, r7                                    r2 -= r7   ///  r2 = r2.wrapping_sub(r7)
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    jlt r5, r4, lbb_19592                           if r5 < r4 { pc += -23 }
lbb_19615:
    jeq r0, r3, lbb_19646                           if r0 == r3 { pc += 30 }
    ldxb r3, [r0+0x0]                       
    mov64 r4, r3                                    r4 = r3
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    jsgt r4, -1, lbb_19622                          if (r4 as i64) > (-1 as i32 as i64) { pc += 1 }
    jlt r3, 224, lbb_19622                          if r3 < (224 as i32 as i64 as u64) { pc += 0 }
lbb_19622:
    ldxdw r5, [r10-0x20]                    
    jeq r2, 0, lbb_19640                            if r2 == (0 as i32 as i64 as u64) { pc += 16 }
    jlt r2, r5, lbb_19633                           if r2 < r5 { pc += 8 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jeq r2, r5, lbb_19640                           if r2 == r5 { pc += 13 }
    ja lbb_19641                                    if true { pc += 13 }
lbb_19628:
    ldxdw r1, [r8+0x20]                     
    ldxdw r2, [r8+0x28]                     
    ldxdw r4, [r2+0x18]                     
    ldxdw r2, [r10-0x18]                    
    ja lbb_19714                                    if true { pc += 81 }
lbb_19633:
    ldxdw r4, [r10-0x18]                    
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxb r4, [r4+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    jslt r4, -64, lbb_19641                         if (r4 as i64) < (-64 as i32 as i64) { pc += 1 }
lbb_19640:
    ldxdw r3, [r10-0x18]                    
lbb_19641:
    jeq r3, 0, lbb_19643                            if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, r2                                    r5 = r2
lbb_19643:
    stxdw [r10-0x20], r5                    
    jeq r3, 0, lbb_19646                            if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    stxdw [r10-0x18], r3                    
lbb_19646:
    jne r1, 0, lbb_19653                            if r1 != (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r1, [r8+0x20]                     
    ldxdw r2, [r8+0x28]                     
    ldxdw r4, [r2+0x18]                     
    ldxdw r2, [r10-0x18]                    
    ldxdw r3, [r10-0x20]                    
    ja lbb_19714                                    if true { pc += 61 }
lbb_19653:
    ldxdw r9, [r8+0x8]                      
    ldxdw r7, [r10-0x20]                    
    jlt r7, 32, lbb_19660                           if r7 < (32 as i32 as i64 as u64) { pc += 4 }
    ldxdw r1, [r10-0x18]                    
    mov64 r2, r7                                    r2 = r7
    call function_20249                     
    ja lbb_19674                                    if true { pc += 14 }
lbb_19660:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r7, 0, lbb_19674                            if r7 == (0 as i32 as i64 as u64) { pc += 12 }
    ldxdw r1, [r10-0x18]                    
    mov64 r2, r7                                    r2 = r7
lbb_19664:
    ldxb r4, [r1+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jsgt r4, -65, lbb_19670                         if (r4 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_19670:
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    jne r2, 0, lbb_19664                            if r2 != (0 as i32 as i64 as u64) { pc += -10 }
lbb_19674:
    jle r9, r0, lbb_19709                           if r9 <= r0 { pc += 34 }
    sub64 r9, r0                                    r9 -= r0   ///  r9 = r9.wrapping_sub(r0)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r9                                    r3 = r9
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_19719                     
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    ldxw r6, [r10-0x8]                      
    jeq r6, 1114112, lbb_19716                      if r6 == (1114112 as i32 as i64 as u64) { pc += 31 }
    mov64 r3, r7                                    r3 = r7
    ldxdw r7, [r10-0x10]                    
    ldxdw r1, [r8+0x20]                     
    ldxdw r8, [r8+0x28]                     
    ldxdw r4, [r8+0x18]                     
    stxdw [r10-0x28], r1                    
    ldxdw r2, [r10-0x18]                    
    callx r4                                
    jne r0, 0, lbb_19716                            if r0 != (0 as i32 as i64 as u64) { pc += 22 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_19695:
    mov64 r1, r7                                    r1 = r7
    jeq r7, r9, lbb_19705                           if r7 == r9 { pc += 8 }
    ldxdw r3, [r8+0x20]                     
    ldxdw r1, [r10-0x28]                    
    mov64 r2, r6                                    r2 = r6
    callx r3                                
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    jeq r0, 0, lbb_19695                            if r0 == (0 as i32 as i64 as u64) { pc += -8 }
    add64 r9, -1                                    r9 += -1   ///  r9 = r9.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
lbb_19705:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jlt r1, r7, lbb_19716                           if r1 < r7 { pc += 9 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ja lbb_19716                                    if true { pc += 7 }
lbb_19709:
    ldxdw r1, [r8+0x20]                     
    ldxdw r2, [r8+0x28]                     
    ldxdw r4, [r2+0x18]                     
    ldxdw r2, [r10-0x18]                    
    mov64 r3, r7                                    r3 = r7
lbb_19714:
    callx r4                                
    mov64 r9, r0                                    r9 = r0
lbb_19716:
    and64 r9, 1                                     r9 &= 1   ///  r9 = r9.and(1)
    mov64 r0, r9                                    r0 = r9
    exit                                    

function_19719:
    stxdw [r10-0x10], r1                    
    ldxb r6, [r2+0x38]                      
    jsgt r6, 1, lbb_19724                           if (r6 as i64) > (1 as i32 as i64) { pc += 2 }
    jeq r6, 0, lbb_19735                            if r6 == (0 as i32 as i64 as u64) { pc += 12 }
    ja lbb_19728                                    if true { pc += 4 }
lbb_19724:
    jeq r6, 2, lbb_19731                            if r6 == (2 as i32 as i64 as u64) { pc += 6 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    and64 r4, 255                                   r4 &= 255   ///  r4 = r4.and(255)
    jeq r4, 0, lbb_19735                            if r4 == (0 as i32 as i64 as u64) { pc += 7 }
lbb_19728:
    mov64 r6, r3                                    r6 = r3
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ja lbb_19735                                    if true { pc += 4 }
lbb_19731:
    mov64 r6, r3                                    r6 = r3
    rsh64 r6, 1                                     r6 >>= 1   ///  r6 = r6.wrapping_shr(1)
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
lbb_19735:
    stxdw [r10-0x8], r3                     
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    ldxw r8, [r2+0x30]                      
    ldxdw r7, [r2+0x28]                     
    ldxdw r9, [r2+0x20]                     
lbb_19740:
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    jeq r6, 0, lbb_19749                            if r6 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r3, [r7+0x20]                     
    mov64 r1, r9                                    r1 = r9
    mov64 r2, r8                                    r2 = r8
    callx r3                                
    mov64 r1, 1114112                               r1 = 1114112 as i32 as i64 as u64
    jeq r0, 0, lbb_19740                            if r0 == (0 as i32 as i64 as u64) { pc += -9 }
lbb_19749:
    ldxdw r2, [r10-0x10]                    
    stxw [r2+0x8], r1                       
    ldxdw r1, [r10-0x8]                     
    stxdw [r2+0x0], r1                      
    exit                                    

function_19754:
    ldxdw r4, [r1+0x20]                     
    ldxdw r1, [r1+0x28]                     
    ldxdw r5, [r1+0x18]                     
    mov64 r1, r4                                    r1 = r4
    callx r5                                
    exit                                    

function_19760:
    mov64 r6, r5                                    r6 = r5
    mov64 r7, r4                                    r7 = r4
    mov64 r9, r3                                    r9 = r3
    mov64 r8, r1                                    r8 = r1
    ldxdw r1, [r8+0x28]                     
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r8+0x20]                     
    callx r4                                
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r9, 0, lbb_19771                            if r9 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_19771:
    stxb [r10-0x8], r0                      
    stxdw [r10-0x10], r8                    
    stxb [r10-0x7], r1                      
    stdw [r10-0x18], 0                      
    mov64 r8, r10                                   r8 = r10
    add64 r8, -24                                   r8 += -24   ///  r8 = r8.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r6                                    r3 = r6
    call function_18909                     
    mov64 r1, r8                                    r1 = r8
    call function_18998                     
    exit                                    

function_19784:
    mov64 r9, r2                                    r9 = r2
    mov64 r8, r1                                    r8 = r1
    ldxdw r6, [r3+0x20]                     
    stxdw [r10-0x40], r3                    
    ldxdw r1, [r3+0x28]                     
    stxdw [r10-0x30], r1                    
    ldxdw r7, [r1+0x20]                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 34                                    r2 = 34 as i32 as i64 as u64
    callx r7                                
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_20064                            if r0 != (0 as i32 as i64 as u64) { pc += 268 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, r9                                    r2 = r9
    jeq r2, 0, lbb_20051                            if r2 == (0 as i32 as i64 as u64) { pc += 251 }
    stxdw [r10-0x58], r7                    
    stxdw [r10-0x50], r6                    
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r1, r2                                    r1 = r2
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    stxdw [r10-0x60], r1                    
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x20], r1                    
    stxdw [r10-0x48], r8                    
    mov64 r9, r8                                    r9 = r8
    mov64 r8, r2                                    r8 = r2
    stxdw [r10-0x38], r2                    
lbb_19812:
    mov64 r5, r8                                    r5 = r8
    mov64 r1, r9                                    r1 = r9
    mov64 r8, r1                                    r8 = r1
    add64 r8, r5                                    r8 += r5   ///  r8 = r8.wrapping_add(r5)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_19817:
    mov64 r9, r1                                    r9 = r1
    add64 r9, r6                                    r9 += r6   ///  r9 = r9.wrapping_add(r6)
    ldxb r3, [r9+0x0]                       
    mov64 r4, r3                                    r4 = r3
    add64 r4, -127                                  r4 += -127   ///  r4 = r4.wrapping_add(-127 as i32 as i64 as u64)
    and64 r4, 255                                   r4 &= 255   ///  r4 = r4.and(255)
    jlt r4, 161, lbb_19829                          if r4 < (161 as i32 as i64 as u64) { pc += 5 }
    jeq r3, 34, lbb_19829                           if r3 == (34 as i32 as i64 as u64) { pc += 4 }
    jeq r3, 92, lbb_19829                           if r3 == (92 as i32 as i64 as u64) { pc += 3 }
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    jeq r5, r6, lbb_20023                           if r5 == r6 { pc += 195 }
    ja lbb_19817                                    if true { pc += -12 }
lbb_19829:
    ldxb r3, [r9+0x0]                       
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    lsh64 r2, 56                                    r2 <<= 56   ///  r2 = r2.wrapping_shl(56)
    arsh64 r2, 56                                   r2 >>= 56 (signed)   ///  r2 = (r2 as i64).wrapping_shr(56)
    stxdw [r10-0x28], r0                    
    jsgt r2, -1, lbb_19872                          if (r2 as i64) > (-1 as i32 as i64) { pc += 36 }
    mov64 r5, r1                                    r5 = r1
    add64 r5, r6                                    r5 += r6   ///  r5 = r5.wrapping_add(r6)
    ldxb r2, [r9+0x0]                       
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    mov64 r4, r3                                    r4 = r3
    and64 r4, 31                                    r4 &= 31   ///  r4 = r4.and(31)
    mov64 r7, r4                                    r7 = r4
    lsh64 r7, 6                                     r7 <<= 6   ///  r7 = r7.wrapping_shl(6)
    or64 r7, r2                                     r7 |= r2   ///  r7 = r7.or(r2)
    mov64 r9, r5                                    r9 = r5
    add64 r9, 2                                     r9 += 2   ///  r9 = r9.wrapping_add(2 as i32 as i64 as u64)
    jgt r3, 223, lbb_19849                          if r3 > (223 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19873                                    if true { pc += 24 }
lbb_19849:
    lsh64 r2, 6                                     r2 <<= 6   ///  r2 = r2.wrapping_shl(6)
    ldxb r0, [r9+0x0]                       
    and64 r0, 63                                    r0 &= 63   ///  r0 = r0.and(63)
    or64 r2, r0                                     r2 |= r0   ///  r2 = r2.or(r0)
    add64 r5, 3                                     r5 += 3   ///  r5 = r5.wrapping_add(3 as i32 as i64 as u64)
    mov64 r0, r4                                    r0 = r4
    lsh64 r0, 12                                    r0 <<= 12   ///  r0 = r0.wrapping_shl(12)
    mov64 r7, r2                                    r7 = r2
    or64 r7, r0                                     r7 |= r0   ///  r7 = r7.or(r0)
    mov64 r9, r5                                    r9 = r5
    jlt r3, 240, lbb_19873                          if r3 < (240 as i32 as i64 as u64) { pc += 13 }
    lsh64 r2, 6                                     r2 <<= 6   ///  r2 = r2.wrapping_shl(6)
    ldxb r3, [r5+0x0]                       
    and64 r3, 63                                    r3 &= 63   ///  r3 = r3.and(63)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    lsh64 r4, 18                                    r4 <<= 18   ///  r4 = r4.wrapping_shl(18)
    and64 r4, 1835008                               r4 &= 1835008   ///  r4 = r4.and(1835008)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    add64 r1, 4                                     r1 += 4   ///  r1 = r1.wrapping_add(4 as i32 as i64 as u64)
    mov64 r9, r1                                    r9 = r1
    mov64 r7, r2                                    r7 = r2
    ja lbb_19873                                    if true { pc += 1 }
lbb_19872:
    mov64 r7, r3                                    r7 = r3
lbb_19873:
    jsgt r7, 12, lbb_19883                          if (r7 as i64) > (12 as i32 as i64) { pc += 9 }
    jeq r7, 0, lbb_19914                            if r7 == (0 as i32 as i64 as u64) { pc += 39 }
    jeq r7, 9, lbb_19929                            if r7 == (9 as i32 as i64 as u64) { pc += 53 }
    jeq r7, 10, lbb_19878                           if r7 == (10 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19894                                    if true { pc += 16 }
lbb_19878:
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 28252                   
    ja lbb_19933                                    if true { pc += 50 }
lbb_19883:
    jsgt r7, 38, lbb_19892                          if (r7 as i64) > (38 as i32 as i64) { pc += 8 }
    jeq r7, 13, lbb_19919                           if r7 == (13 as i32 as i64 as u64) { pc += 34 }
    jeq r7, 34, lbb_19887                           if r7 == (34 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19894                                    if true { pc += 7 }
lbb_19887:
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 8796                    
    ja lbb_19933                                    if true { pc += 41 }
lbb_19892:
    jeq r7, 39, lbb_19910                           if r7 == (39 as i32 as i64 as u64) { pc += 17 }
    jeq r7, 92, lbb_19924                           if r7 == (92 as i32 as i64 as u64) { pc += 30 }
lbb_19894:
    jgt r7, 767, lbb_19896                          if r7 > (767 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19910                                    if true { pc += 14 }
lbb_19896:
    mov64 r1, r7                                    r1 = r7
    call function_21413                     
    jeq r0, 0, lbb_19910                            if r0 == (0 as i32 as i64 as u64) { pc += 11 }
lbb_19899:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -12                                   r1 += -12   ///  r1 = r1.wrapping_add(-12 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_20831                     
    ldxh r1, [r10-0x4]                      
    stxh [r10-0x10], r1                     
    ldxdw r1, [r10-0xc]                     
    stxdw [r10-0x18], r1                    
    ldxb r1, [r10-0x1]                      
    ldxb r2, [r10-0x2]                      
    ja lbb_19933                                    if true { pc += 23 }
lbb_19910:
    mov64 r1, r7                                    r1 = r7
    call function_20744                     
    jne r0, 0, lbb_19989                            if r0 != (0 as i32 as i64 as u64) { pc += 76 }
    ja lbb_19899                                    if true { pc += -15 }
lbb_19914:
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 12380                   
    ja lbb_19933                                    if true { pc += 14 }
lbb_19919:
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 29276                   
    ja lbb_19933                                    if true { pc += 9 }
lbb_19924:
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 23644                   
    ja lbb_19933                                    if true { pc += 4 }
lbb_19929:
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 29788                   
lbb_19933:
    stxb [r10-0xd], r1                      
    stxb [r10-0xe], r2                      
    ldxb r3, [r10-0x18]                     
    jeq r3, 128, lbb_19989                          if r3 == (128 as i32 as i64 as u64) { pc += 52 }
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    jeq r1, 1, lbb_19989                            if r1 == (1 as i32 as i64 as u64) { pc += 49 }
    ldxdw r1, [r10-0x20]                    
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    ldxdw r2, [r10-0x38]                    
    ldxdw r5, [r10-0x28]                    
    jgt r5, r1, lbb_20016                           if r5 > r1 { pc += 71 }
    jeq r5, 0, lbb_19957                            if r5 == (0 as i32 as i64 as u64) { pc += 11 }
    jlt r5, r2, lbb_19948                           if r5 < r2 { pc += 1 }
    ja lbb_19955                                    if true { pc += 7 }
lbb_19948:
    ldxdw r3, [r10-0x48]                    
    add64 r3, r5                                    r3 += r5   ///  r3 = r3.wrapping_add(r5)
    ldxb r3, [r3+0x0]                       
    lsh64 r3, 56                                    r3 <<= 56   ///  r3 = r3.wrapping_shl(56)
    arsh64 r3, 56                                   r3 >>= 56 (signed)   ///  r3 = (r3 as i64).wrapping_shr(56)
    jsgt r3, -65, lbb_19957                         if (r3 as i64) > (-65 as i32 as i64) { pc += 3 }
    ja lbb_20016                                    if true { pc += 61 }
lbb_19955:
    jeq r5, r2, lbb_19957                           if r5 == r2 { pc += 1 }
    ja lbb_20016                                    if true { pc += 59 }
lbb_19957:
    jeq r1, 0, lbb_19963                            if r1 == (0 as i32 as i64 as u64) { pc += 5 }
    jlt r1, r2, lbb_20008                           if r1 < r2 { pc += 49 }
    ldxdw r3, [r10-0x60]                    
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    jeq r1, 0, lbb_19963                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_20016                                    if true { pc += 53 }
lbb_19963:
    ldxdw r2, [r10-0x48]                    
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    ldxdw r3, [r10-0x20]                    
    sub64 r3, r5                                    r3 -= r5   ///  r3 = r3.wrapping_sub(r5)
    add64 r3, r6                                    r3 += r6   ///  r3 = r3.wrapping_add(r6)
    ldxdw r1, [r10-0x30]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r10-0x50]                    
    callx r4                                
    jne r0, 0, lbb_20081                            if r0 != (0 as i32 as i64 as u64) { pc += 108 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    ldxdw r2, [r10-0x40]                    
    call function_18611                     
    jne r0, 0, lbb_20081                            if r0 != (0 as i32 as i64 as u64) { pc += 103 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r7, 128, lbb_19985                          if r7 < (128 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    jlt r7, 2048, lbb_19985                         if r7 < (2048 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 3                                     r1 = 3 as i32 as i64 as u64
    jlt r7, 65536, lbb_19985                        if r7 < (65536 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 4                                     r1 = 4 as i32 as i64 as u64
lbb_19985:
    ldxdw r2, [r10-0x20]                    
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    stxdw [r10-0x28], r1                    
lbb_19989:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x38]                    
    jlt r7, 128, lbb_19993                          if r7 < (128 as i32 as i64 as u64) { pc += 1 }
    ja lbb_20002                                    if true { pc += 9 }
lbb_19993:
    ldxdw r1, [r10-0x20]                    
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    mov64 r1, r4                                    r1 = r4
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    stxdw [r10-0x20], r1                    
    sub64 r8, r9                                    r8 -= r9   ///  r8 = r8.wrapping_sub(r9)
    ldxdw r0, [r10-0x28]                    
    jeq r8, 0, lbb_20079                            if r8 == (0 as i32 as i64 as u64) { pc += 78 }
    ja lbb_19812                                    if true { pc += -190 }
lbb_20002:
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    jlt r7, 2048, lbb_19993                         if r7 < (2048 as i32 as i64 as u64) { pc += -11 }
    mov64 r4, 3                                     r4 = 3 as i32 as i64 as u64
    jlt r7, 65536, lbb_19993                        if r7 < (65536 as i32 as i64 as u64) { pc += -13 }
    mov64 r4, 4                                     r4 = 4 as i32 as i64 as u64
    ja lbb_19993                                    if true { pc += -15 }
lbb_20008:
    ldxdw r1, [r10-0x48]                    
    ldxdw r3, [r10-0x20]                    
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    ldxb r1, [r1+0x0]                       
    lsh64 r1, 56                                    r1 <<= 56   ///  r1 = r1.wrapping_shl(56)
    arsh64 r1, 56                                   r1 >>= 56 (signed)   ///  r1 = (r1 as i64).wrapping_shr(56)
    jsgt r1, -65, lbb_19963                         if (r1 as i64) > (-65 as i32 as i64) { pc += -53 }
lbb_20016:
    ldxdw r4, [r10-0x20]                    
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    ldxdw r1, [r10-0x48]                    
    mov64 r3, r5                                    r3 = r5
    lddw r5, 0x10002f8f0 --> b"\x00\x00\x00\x00\xb4\xdf\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xa6\x09\…        r5 load str located at 4295162096
    call function_20412                     
lbb_20023:
    ldxdw r4, [r10-0x20]                    
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
lbb_20025:
    ldxdw r6, [r10-0x50]                    
    ldxdw r7, [r10-0x58]                    
    ldxdw r8, [r10-0x48]                    
    jgt r0, r4, lbb_20074                           if r0 > r4 { pc += 45 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jeq r0, 0, lbb_20045                            if r0 == (0 as i32 as i64 as u64) { pc += 13 }
    jlt r0, r2, lbb_20034                           if r0 < r2 { pc += 1 }
    ja lbb_20042                                    if true { pc += 8 }
lbb_20034:
    mov64 r1, r8                                    r1 = r8
    add64 r1, r0                                    r1 += r0   ///  r1 = r1.wrapping_add(r0)
    ldxb r5, [r1+0x0]                       
    lsh64 r5, 56                                    r5 <<= 56   ///  r5 = r5.wrapping_shl(56)
    arsh64 r5, 56                                   r5 >>= 56 (signed)   ///  r5 = (r5 as i64).wrapping_shr(56)
    mov64 r1, r0                                    r1 = r0
    jsgt r5, -65, lbb_20045                         if (r5 as i64) > (-65 as i32 as i64) { pc += 4 }
    ja lbb_20074                                    if true { pc += 32 }
lbb_20042:
    mov64 r1, r0                                    r1 = r0
    jeq r0, r2, lbb_20045                           if r0 == r2 { pc += 1 }
    ja lbb_20074                                    if true { pc += 29 }
lbb_20045:
    jeq r4, 0, lbb_20051                            if r4 == (0 as i32 as i64 as u64) { pc += 5 }
    jlt r4, r2, lbb_20066                           if r4 < r2 { pc += 19 }
    mov64 r0, r1                                    r0 = r1
    mov64 r3, r4                                    r3 = r4
    jeq r4, r2, lbb_20051                           if r4 == r2 { pc += 1 }
    ja lbb_20074                                    if true { pc += 23 }
lbb_20051:
    add64 r8, r1                                    r8 += r1   ///  r8 = r8.wrapping_add(r1)
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    ldxdw r1, [r10-0x30]                    
    ldxdw r4, [r1+0x18]                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r8                                    r2 = r8
    callx r4                                
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_20064                            if r0 != (0 as i32 as i64 as u64) { pc += 4 }
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 34                                    r2 = 34 as i32 as i64 as u64
    callx r7                                
    mov64 r1, r0                                    r1 = r0
lbb_20064:
    mov64 r0, r1                                    r0 = r1
    exit                                    
lbb_20066:
    mov64 r3, r8                                    r3 = r8
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    ldxb r5, [r3+0x0]                       
    lsh64 r5, 56                                    r5 <<= 56   ///  r5 = r5.wrapping_shl(56)
    arsh64 r5, 56                                   r5 >>= 56 (signed)   ///  r5 = (r5 as i64).wrapping_shr(56)
    mov64 r0, r1                                    r0 = r1
    mov64 r3, r4                                    r3 = r4
    jsgt r5, -65, lbb_20051                         if (r5 as i64) > (-65 as i32 as i64) { pc += -23 }
lbb_20074:
    mov64 r1, r8                                    r1 = r8
    mov64 r3, r0                                    r3 = r0
    lddw r5, 0x10002f908 --> b"\x00\x00\x00\x00\xb4\xdf\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xaf\x09\…        r5 load str located at 4295162120
    call function_20412                     
lbb_20079:
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    ja lbb_20025                                    if true { pc += -56 }
lbb_20081:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ja lbb_20064                                    if true { pc += -19 }

function_20083:
    mov64 r4, r2                                    r4 = r2
    mov64 r2, r1                                    r2 = r1
    mov64 r1, r3                                    r1 = r3
    mov64 r3, r4                                    r3 = r4
    call function_19571                     
    exit                                    

function_20089:
    mov64 r8, r2                                    r8 = r2
    mov64 r9, r1                                    r9 = r1
    ldxdw r6, [r8+0x20]                     
    ldxdw r1, [r8+0x28]                     
    ldxdw r3, [r1+0x20]                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 39                                    r2 = 39 as i32 as i64 as u64
    stxdw [r10-0x20], r3                    
    callx r3                                
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_20170                            if r0 != (0 as i32 as i64 as u64) { pc += 70 }
    ldxw r9, [r9+0x0]                       
    jsgt r9, 12, lbb_20110                          if (r9 as i64) > (12 as i32 as i64) { pc += 8 }
    jeq r9, 0, lbb_20141                            if r9 == (0 as i32 as i64 as u64) { pc += 38 }
    jeq r9, 9, lbb_20157                            if r9 == (9 as i32 as i64 as u64) { pc += 53 }
    jeq r9, 10, lbb_20106                           if r9 == (10 as i32 as i64 as u64) { pc += 1 }
    ja lbb_20116                                    if true { pc += 10 }
lbb_20106:
    sth [r10-0xe], 512                      
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 28252                   
    ja lbb_20160                                    if true { pc += 50 }
lbb_20110:
    jsgt r9, 38, lbb_20114                          if (r9 as i64) > (38 as i32 as i64) { pc += 3 }
    jeq r9, 13, lbb_20145                           if r9 == (13 as i32 as i64 as u64) { pc += 33 }
    jeq r9, 34, lbb_20134                           if r9 == (34 as i32 as i64 as u64) { pc += 21 }
    ja lbb_20116                                    if true { pc += 2 }
lbb_20114:
    jeq r9, 39, lbb_20149                           if r9 == (39 as i32 as i64 as u64) { pc += 34 }
    jeq r9, 92, lbb_20153                           if r9 == (92 as i32 as i64 as u64) { pc += 37 }
lbb_20116:
    jgt r9, 767, lbb_20118                          if r9 > (767 as i32 as i64 as u64) { pc += 1 }
    ja lbb_20134                                    if true { pc += 16 }
lbb_20118:
    mov64 r1, r9                                    r1 = r9
    call function_21413                     
    jeq r0, 0, lbb_20134                            if r0 == (0 as i32 as i64 as u64) { pc += 13 }
lbb_20121:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -12                                   r1 += -12   ///  r1 = r1.wrapping_add(-12 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    call function_20831                     
    ldxh r1, [r10-0x4]                      
    stxh [r10-0x10], r1                     
    ldxdw r1, [r10-0xc]                     
    stxdw [r10-0x18], r1                    
    ldxb r1, [r10-0x1]                      
    stxb [r10-0xd], r1                      
    ldxb r1, [r10-0x2]                      
    stxb [r10-0xe], r1                      
    ja lbb_20160                                    if true { pc += 26 }
lbb_20134:
    mov64 r1, r9                                    r1 = r9
    call function_20744                     
    jne r0, 0, lbb_20138                            if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_20121                                    if true { pc += -17 }
lbb_20138:
    stxw [r10-0x14], r9                     
    stb [r10-0x18], 128                     
    ja lbb_20160                                    if true { pc += 19 }
lbb_20141:
    sth [r10-0xe], 512                      
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 12380                   
    ja lbb_20160                                    if true { pc += 15 }
lbb_20145:
    sth [r10-0xe], 512                      
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 29276                   
    ja lbb_20160                                    if true { pc += 11 }
lbb_20149:
    sth [r10-0xe], 512                      
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 10076                   
    ja lbb_20160                                    if true { pc += 7 }
lbb_20153:
    sth [r10-0xe], 512                      
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 23644                   
    ja lbb_20160                                    if true { pc += 3 }
lbb_20157:
    sth [r10-0xe], 512                      
    stdw [r10-0x16], 0                      
    sth [r10-0x18], 29788                   
lbb_20160:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    call function_18611                     
    jne r0, 0, lbb_20170                            if r0 != (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 39                                    r2 = 39 as i32 as i64 as u64
    ldxdw r3, [r10-0x20]                    
    callx r3                                
    mov64 r7, r0                                    r7 = r0
lbb_20170:
    mov64 r0, r7                                    r0 = r7
    exit                                    

function_20172:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r0, r3                                    r0 = r3
    add64 r0, 7                                     r0 += 7   ///  r0 = r0.wrapping_add(7 as i32 as i64 as u64)
    and64 r0, -8                                    r0 &= -8   ///  r0 = r0.and(-8)
    jeq r0, r3, lbb_20192                           if r0 == r3 { pc += 15 }
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
    jlt r0, r4, lbb_20180                           if r0 < r4 { pc += 1 }
    mov64 r0, r4                                    r0 = r4
lbb_20180:
    jeq r0, 0, lbb_20192                            if r0 == (0 as i32 as i64 as u64) { pc += 11 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
lbb_20183:
    mov64 r5, r3                                    r5 = r3
    add64 r5, r6                                    r5 += r6   ///  r5 = r5.wrapping_add(r6)
    ldxb r8, [r5+0x0]                       
    mov64 r5, r2                                    r5 = r2
    and64 r5, 255                                   r5 &= 255   ///  r5 = r5.and(255)
    jeq r8, r5, lbb_20243                           if r8 == r5 { pc += 54 }
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    mov64 r5, r0                                    r5 = r0
    jlt r6, r0, lbb_20183                           if r6 < r0 { pc += -9 }
lbb_20192:
    stxdw [r10-0x8], r1                     
    mov64 r0, r4                                    r0 = r4
    add64 r0, -16                                   r0 += -16   ///  r0 = r0.wrapping_add(-16 as i32 as i64 as u64)
    jgt r5, r0, lbb_20220                           if r5 > r0 { pc += 24 }
    mov64 r6, r2                                    r6 = r2
    and64 r6, 255                                   r6 &= 255   ///  r6 = r6.and(255)
    lddw r7, 0x101010101010101                      r7 load str located at 72340172838076673
    mul64 r6, r7                                    r6 *= r7   ///  r6 = r6.wrapping_mul(r7)
    lddw r1, 0x8080808080808080                     r1 load str located at -9187201950435737472
lbb_20203:
    mov64 r8, r3                                    r8 = r3
    add64 r8, r5                                    r8 += r5   ///  r8 = r8.wrapping_add(r5)
    ldxdw r9, [r8+0x8]                      
    xor64 r9, r6                                    r9 ^= r6   ///  r9 = r9.xor(r6)
    lddw r7, 0x101010101010100                      r7 load str located at 72340172838076672
    sub64 r7, r9                                    r7 -= r9   ///  r7 = r7.wrapping_sub(r9)
    or64 r7, r9                                     r7 |= r9   ///  r7 = r7.or(r9)
    ldxdw r8, [r8+0x0]                      
    xor64 r8, r6                                    r8 ^= r6   ///  r8 = r8.xor(r6)
    lddw r9, 0x101010101010100                      r9 load str located at 72340172838076672
    sub64 r9, r8                                    r9 -= r8   ///  r9 = r9.wrapping_sub(r8)
    or64 r9, r8                                     r9 |= r8   ///  r9 = r9.or(r8)
    and64 r9, r7                                    r9 &= r7   ///  r9 = r9.and(r7)
    and64 r9, r1                                    r9 &= r1   ///  r9 = r9.and(r1)
    jeq r9, r1, lbb_20227                           if r9 == r1 { pc += 7 }
lbb_20220:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x8]                     
    jeq r4, r5, lbb_20243                           if r4 == r5 { pc += 20 }
    sub64 r4, r5                                    r4 -= r5   ///  r4 = r4.wrapping_sub(r5)
    add64 r3, r5                                    r3 += r5   ///  r3 = r3.wrapping_add(r5)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_20230                                    if true { pc += 3 }
lbb_20227:
    add64 r5, 16                                    r5 += 16   ///  r5 = r5.wrapping_add(16 as i32 as i64 as u64)
    jgt r5, r0, lbb_20220                           if r5 > r0 { pc += -9 }
    ja lbb_20203                                    if true { pc += -27 }
lbb_20230:
    mov64 r6, r3                                    r6 = r3
    add64 r6, r0                                    r6 += r0   ///  r6 = r6.wrapping_add(r0)
    ldxb r7, [r6+0x0]                       
    mov64 r6, r2                                    r6 = r2
    and64 r6, 255                                   r6 &= 255   ///  r6 = r6.and(255)
    jeq r7, r6, lbb_20240                           if r7 == r6 { pc += 4 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    jlt r0, r4, lbb_20230                           if r0 < r4 { pc += -9 }
    ja lbb_20243                                    if true { pc += 3 }
lbb_20240:
    add64 r5, r0                                    r5 += r0   ///  r5 = r5.wrapping_add(r0)
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r6, r5                                    r6 = r5
lbb_20243:
    stxdw [r1+0x8], r6                      
    stxdw [r1+0x0], r7                      
    exit                                    

function_20246:
    call function_21338                     

function_20247:
    call function_21363                     

function_20248:
    call function_21388                     

function_20249:
    mov64 r7, r1                                    r7 = r1
    add64 r7, 7                                     r7 += 7   ///  r7 = r7.wrapping_add(7 as i32 as i64 as u64)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
    mov64 r3, r7                                    r3 = r7
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    jlt r2, r3, lbb_20398                           if r2 < r3 { pc += 143 }
    mov64 r5, r2                                    r5 = r2
    sub64 r5, r3                                    r5 -= r3   ///  r5 = r5.wrapping_sub(r3)
    jlt r5, 8, lbb_20398                            if r5 < (8 as i32 as i64 as u64) { pc += 140 }
    stxdw [r10-0x8], r3                     
    mov64 r2, r5                                    r2 = r5
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jeq r7, r1, lbb_20280                           if r7 == r1 { pc += 16 }
    mov64 r6, r1                                    r6 = r1
    sub64 r6, r7                                    r6 -= r7   ///  r6 = r6.wrapping_sub(r7)
    mov64 r7, r1                                    r7 = r1
lbb_20267:
    ldxb r4, [r7+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jsgt r4, -65, lbb_20274                         if (r4 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_20274:
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    jeq r6, 0, lbb_20277                            if r6 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_20277:
    add64 r3, r9                                    r3 += r9   ///  r3 = r3.wrapping_add(r9)
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    jne r8, 1, lbb_20267                            if r8 != (1 as i32 as i64 as u64) { pc += -13 }
lbb_20280:
    ldxdw r4, [r10-0x8]                     
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    jeq r2, 0, lbb_20298                            if r2 == (0 as i32 as i64 as u64) { pc += 15 }
    mov64 r0, r5                                    r0 = r5
    and64 r0, -8                                    r0 &= -8   ///  r0 = r0.and(-8)
    mov64 r4, r1                                    r4 = r1
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_20288:
    ldxb r7, [r4+0x0]                       
    lsh64 r7, 56                                    r7 <<= 56   ///  r7 = r7.wrapping_shl(56)
    arsh64 r7, 56                                   r7 >>= 56 (signed)   ///  r7 = (r7 as i64).wrapping_shr(56)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jsgt r7, -65, lbb_20294                         if (r7 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_20294:
    add64 r0, r6                                    r0 += r6   ///  r0 = r0.wrapping_add(r6)
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    jne r2, 0, lbb_20288                            if r2 != (0 as i32 as i64 as u64) { pc += -10 }
lbb_20298:
    rsh64 r5, 3                                     r5 >>= 3   ///  r5 = r5.wrapping_shr(3)
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    lddw r9, 0x101010101010101                      r9 load str located at 72340172838076673
lbb_20302:
    mov64 r3, r5                                    r3 = r5
    jeq r3, 0, lbb_20411                            if r3 == (0 as i32 as i64 as u64) { pc += 107 }
    stxdw [r10-0x8], r1                     
    mov64 r5, r3                                    r5 = r3
    jlt r3, 192, lbb_20308                          if r3 < (192 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 192                                   r5 = 192 as i32 as i64 as u64
lbb_20308:
    stxdw [r10-0x10], r5                    
    lsh64 r5, 3                                     r5 <<= 3   ///  r5 = r5.wrapping_shl(3)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jlt r3, 4, lbb_20337                            if r3 < (4 as i32 as i64 as u64) { pc += 25 }
    mov64 r2, r5                                    r2 = r5
    and64 r2, 2016                                  r2 &= 2016   ///  r2 = r2.and(2016)
    ldxdw r6, [r10-0x8]                     
    mov64 r1, r6                                    r1 = r6
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, r6                                    r2 = r6
    ja lbb_20321                                    if true { pc += 2 }
lbb_20319:
    add64 r2, 32                                    r2 += 32   ///  r2 = r2.wrapping_add(32 as i32 as i64 as u64)
    jeq r2, r1, lbb_20337                           if r2 == r1 { pc += 16 }
lbb_20321:
    mov64 r6, r4                                    r6 = r4
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_20323:
    mov64 r4, r2                                    r4 = r2
    add64 r4, r7                                    r4 += r7   ///  r4 = r4.wrapping_add(r7)
    ldxdw r4, [r4+0x0]                      
    mov64 r8, r4                                    r8 = r4
    rsh64 r8, 6                                     r8 >>= 6   ///  r8 = r8.wrapping_shr(6)
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    rsh64 r4, 7                                     r4 >>= 7   ///  r4 = r4.wrapping_shr(7)
    or64 r4, r8                                     r4 |= r8   ///  r4 = r4.or(r8)
    and64 r4, r9                                    r4 &= r9   ///  r4 = r4.and(r9)
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    mov64 r6, r4                                    r6 = r4
    jeq r7, 32, lbb_20319                           if r7 == (32 as i32 as i64 as u64) { pc += -17 }
    ja lbb_20323                                    if true { pc += -14 }
lbb_20337:
    ldxdw r1, [r10-0x8]                     
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    ldxdw r7, [r10-0x10]                    
    mov64 r2, r7                                    r2 = r7
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    mov64 r8, r3                                    r8 = r3
    sub64 r3, r7                                    r3 -= r7   ///  r3 = r3.wrapping_sub(r7)
    mov64 r6, r4                                    r6 = r4
    lddw r5, 0xff00ff00ff00ff                       r5 load str located at 71777214294589695
    and64 r6, r5                                    r6 &= r5   ///  r6 = r6.and(r5)
    rsh64 r4, 8                                     r4 >>= 8   ///  r4 = r4.wrapping_shr(8)
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    mov64 r5, r3                                    r5 = r3
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    lddw r6, 0x1000100010001                        r6 load str located at 281479271743489
    mul64 r4, r6                                    r4 *= r6   ///  r4 = r4.wrapping_mul(r6)
    rsh64 r4, 48                                    r4 >>= 48   ///  r4 = r4.wrapping_shr(48)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    mov64 r0, r4                                    r0 = r4
    jeq r2, 0, lbb_20302                            if r2 == (0 as i32 as i64 as u64) { pc += -57 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r10-0x8]                     
    jeq r2, 0, lbb_20385                            if r2 == (0 as i32 as i64 as u64) { pc += 23 }
    and64 r7, 252                                   r7 &= 252   ///  r7 = r7.and(252)
    lsh64 r7, 3                                     r7 <<= 3   ///  r7 = r7.wrapping_shl(3)
    jlt r8, 192, lbb_20366                          if r8 < (192 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, 192                                   r8 = 192 as i32 as i64 as u64
lbb_20366:
    add64 r6, r7                                    r6 += r7   ///  r6 = r6.wrapping_add(r7)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    and64 r8, 3                                     r8 &= 3   ///  r8 = r8.and(3)
    lsh64 r8, 3                                     r8 <<= 3   ///  r8 = r8.wrapping_shl(3)
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
lbb_20372:
    ldxdw r0, [r6+0x0]                      
    mov64 r5, r0                                    r5 = r0
    rsh64 r5, 6                                     r5 >>= 6   ///  r5 = r5.wrapping_shr(6)
    xor64 r0, -1                                    r0 ^= -1   ///  r0 = r0.xor(-1)
    rsh64 r0, 7                                     r0 >>= 7   ///  r0 = r0.wrapping_shr(7)
    or64 r0, r5                                     r0 |= r5   ///  r0 = r0.or(r5)
    and64 r0, r1                                    r0 &= r1   ///  r0 = r0.and(r1)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    add64 r8, -8                                    r8 += -8   ///  r8 = r8.wrapping_add(-8 as i32 as i64 as u64)
    mov64 r2, r0                                    r2 = r0
    jeq r8, 0, lbb_20385                            if r8 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_20372                                    if true { pc += -13 }
lbb_20385:
    lddw r1, 0xff00ff00ff00ff                       r1 load str located at 71777214294589695
    mov64 r2, r0                                    r2 = r0
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    and64 r0, r1                                    r0 &= r1   ///  r0 = r0.and(r1)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    lddw r1, 0x1000100010001                        r1 load str located at 281479271743489
    mul64 r0, r1                                    r0 *= r1   ///  r0 = r0.wrapping_mul(r1)
    rsh64 r0, 48                                    r0 >>= 48   ///  r0 = r0.wrapping_shr(48)
    add64 r0, r4                                    r0 += r4   ///  r0 = r0.wrapping_add(r4)
    ja lbb_20411                                    if true { pc += 13 }
lbb_20398:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r2, 0, lbb_20411                            if r2 == (0 as i32 as i64 as u64) { pc += 11 }
lbb_20400:
    ldxb r4, [r1+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jsgt r4, -65, lbb_20406                         if (r4 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_20406:
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    jeq r2, 0, lbb_20411                            if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_20400                                    if true { pc += -11 }
lbb_20411:
    exit                                    

function_20412:
    call function_20413                     

function_20413:
    stxdw [r10-0xc8], r4                    
    stxdw [r10-0xd0], r3                    
    mov64 r0, r2                                    r0 = r2
    jlt r2, 257, lbb_20430                          if r2 < (257 as i32 as i64 as u64) { pc += 13 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r6, r1                                    r6 = r1
    add64 r6, 256                                   r6 += 256   ///  r6 = r6.wrapping_add(256 as i32 as i64 as u64)
lbb_20420:
    jeq r0, -4, lbb_20429                           if r0 == (-4 as i32 as i64 as u64) { pc += 8 }
    mov64 r7, r6                                    r7 = r6
    add64 r7, r0                                    r7 += r0   ///  r7 = r7.wrapping_add(r0)
    add64 r0, -1                                    r0 += -1   ///  r0 = r0.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r7, [r7+0x0]                       
    lsh64 r7, 56                                    r7 <<= 56   ///  r7 = r7.wrapping_shl(56)
    arsh64 r7, 56                                   r7 >>= 56 (signed)   ///  r7 = (r7 as i64).wrapping_shr(56)
    jslt r7, -64, lbb_20420                         if (r7 as i64) < (-64 as i32 as i64) { pc += -8 }
    add64 r0, 4                                     r0 += 4   ///  r0 = r0.wrapping_add(4 as i32 as i64 as u64)
lbb_20429:
    add64 r0, 253                                   r0 += 253   ///  r0 = r0.wrapping_add(253 as i32 as i64 as u64)
lbb_20430:
    jeq r0, 0, lbb_20434                            if r0 == (0 as i32 as i64 as u64) { pc += 3 }
    jlt r0, r2, lbb_20557                           if r0 < r2 { pc += 125 }
    jeq r0, r2, lbb_20434                           if r0 == r2 { pc += 1 }
    ja lbb_20563                                    if true { pc += 129 }
lbb_20434:
    lddw r6, 0x10002dfda --> b"[...]begin <= end (`byte index  is not a char boun"        r6 load str located at 4295155674
    jlt r0, r2, lbb_20438                           if r0 < r2 { pc += 1 }
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
lbb_20438:
    mov64 r7, 5                                     r7 = 5 as i32 as i64 as u64
    jlt r0, r2, lbb_20441                           if r0 < r2 { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_20441:
    stxdw [r10-0xb8], r0                    
    stxdw [r10-0xc0], r1                    
    stxdw [r10-0xa8], r7                    
    stxdw [r10-0xb0], r6                    
    jgt r3, r2, lbb_20627                           if r3 > r2 { pc += 181 }
    jgt r4, r2, lbb_20627                           if r4 > r2 { pc += 180 }
    jle r3, r4, lbb_20478                           if r3 <= r4 { pc += 30 }
    lddw r1, 0x10002f920 --> b"\x00\x00\x00\x00\xdf\xdf\x02\x00\x0e\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295162144
    stxdw [r10-0x80], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    stxdw [r10-0x70], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100029bc0 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x13\x08\x00\x00\x00\x00\x00y\x12\x00\x00\x…        r1 load str located at 4295138240
    stxdw [r10-0x18], r1                    
    stxdw [r10-0x28], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -192                                  r1 += -192   ///  r1 = r1.wrapping_add(-192 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -200                                  r1 += -200   ///  r1 = r1.wrapping_add(-200 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    lddw r1, 0x1000298c0 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295137472
    stxdw [r10-0x38], r1                    
    stxdw [r10-0x48], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    stxdw [r10-0x50], r1                    
    stdw [r10-0x60], 0                      
    stdw [r10-0x78], 4                      
    stdw [r10-0x68], 4                      
    ja lbb_20623                                    if true { pc += 145 }
lbb_20478:
    jeq r3, 0, lbb_20487                            if r3 == (0 as i32 as i64 as u64) { pc += 8 }
    jge r3, r2, lbb_20487                           if r3 >= r2 { pc += 7 }
    mov64 r0, r1                                    r0 = r1
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    ldxb r0, [r0+0x0]                       
    lsh64 r0, 56                                    r0 <<= 56   ///  r0 = r0.wrapping_shl(56)
    arsh64 r0, 56                                   r0 >>= 56 (signed)   ///  r0 = (r0 as i64).wrapping_shr(56)
    jsgt r0, -65, lbb_20487                         if (r0 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r4, r3                                    r4 = r3
lbb_20487:
    stxdw [r10-0xa0], r4                    
    mov64 r3, r2                                    r3 = r2
    jge r4, r2, lbb_20517                           if r4 >= r2 { pc += 27 }
    mov64 r3, r4                                    r3 = r4
    add64 r3, -3                                    r3 += -3   ///  r3 = r3.wrapping_add(-3 as i32 as i64 as u64)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jgt r3, r4, lbb_20496                           if r3 > r4 { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_20496:
    jne r6, 0, lbb_20498                            if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, r3                                    r0 = r3
lbb_20498:
    mov64 r3, r4                                    r3 = r4
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    jge r3, r0, lbb_20506                           if r3 >= r0 { pc += 5 }
    mov64 r1, r0                                    r1 = r0
    mov64 r2, r3                                    r2 = r3
    lddw r3, 0x10002f9e0 --> b"\x00\x00\x00\x00\xc7\xdf\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\xf4\x00\…        r3 load str located at 4295162336
    call function_20248                     
lbb_20506:
    mov64 r6, r1                                    r6 = r1
    add64 r6, r4                                    r6 += r4   ///  r6 = r6.wrapping_add(r4)
    sub64 r3, r0                                    r3 -= r0   ///  r3 = r3.wrapping_sub(r0)
lbb_20509:
    jeq r3, 0, lbb_20516                            if r3 == (0 as i32 as i64 as u64) { pc += 6 }
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r4, [r6+0x0]                       
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    jslt r4, -64, lbb_20509                         if (r4 as i64) < (-64 as i32 as i64) { pc += -7 }
lbb_20516:
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
lbb_20517:
    jeq r3, 0, lbb_20521                            if r3 == (0 as i32 as i64 as u64) { pc += 3 }
    jlt r3, r2, lbb_20566                           if r3 < r2 { pc += 47 }
    jeq r3, r2, lbb_20521                           if r3 == r2 { pc += 1 }
    ja lbb_20572                                    if true { pc += 51 }
lbb_20521:
    jeq r3, r2, lbb_20555                           if r3 == r2 { pc += 33 }
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    ldxb r0, [r1+0x0]                       
    mov64 r2, r0                                    r2 = r0
    lsh64 r2, 56                                    r2 <<= 56   ///  r2 = r2.wrapping_shl(56)
    arsh64 r2, 56                                   r2 >>= 56 (signed)   ///  r2 = (r2 as i64).wrapping_shr(56)
    jsgt r2, -1, lbb_20574                          if (r2 as i64) > (-1 as i32 as i64) { pc += 46 }
    ldxb r2, [r1+0x1]                       
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    mov64 r4, r0                                    r4 = r0
    and64 r4, 31                                    r4 &= 31   ///  r4 = r4.and(31)
    mov64 r6, r4                                    r6 = r4
    lsh64 r6, 6                                     r6 <<= 6   ///  r6 = r6.wrapping_shl(6)
    or64 r6, r2                                     r6 |= r2   ///  r6 = r6.or(r2)
    jgt r0, 223, lbb_20537                          if r0 > (223 as i32 as i64 as u64) { pc += 1 }
    ja lbb_20575                                    if true { pc += 38 }
lbb_20537:
    lsh64 r2, 6                                     r2 <<= 6   ///  r2 = r2.wrapping_shl(6)
    ldxb r6, [r1+0x2]                       
    and64 r6, 63                                    r6 &= 63   ///  r6 = r6.and(63)
    or64 r2, r6                                     r2 |= r6   ///  r2 = r2.or(r6)
    mov64 r7, r4                                    r7 = r4
    lsh64 r7, 12                                    r7 <<= 12   ///  r7 = r7.wrapping_shl(12)
    mov64 r6, r2                                    r6 = r2
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    jlt r0, 240, lbb_20575                          if r0 < (240 as i32 as i64 as u64) { pc += 29 }
    lsh64 r2, 6                                     r2 <<= 6   ///  r2 = r2.wrapping_shl(6)
    ldxb r1, [r1+0x3]                       
    and64 r1, 63                                    r1 &= 63   ///  r1 = r1.and(63)
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    lsh64 r4, 18                                    r4 <<= 18   ///  r4 = r4.wrapping_shl(18)
    and64 r4, 1835008                               r4 &= 1835008   ///  r4 = r4.and(1835008)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    mov64 r6, r2                                    r6 = r2
    ja lbb_20575                                    if true { pc += 20 }
lbb_20555:
    mov64 r1, r5                                    r1 = r5
    call function_18631                     
lbb_20557:
    mov64 r6, r1                                    r6 = r1
    add64 r6, r0                                    r6 += r0   ///  r6 = r6.wrapping_add(r0)
    ldxb r6, [r6+0x0]                       
    lsh64 r6, 56                                    r6 <<= 56   ///  r6 = r6.wrapping_shl(56)
    arsh64 r6, 56                                   r6 >>= 56 (signed)   ///  r6 = (r6 as i64).wrapping_shr(56)
    jsgt r6, -65, lbb_20434                         if (r6 as i64) > (-65 as i32 as i64) { pc += -129 }
lbb_20563:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r0                                    r4 = r0
    call function_20412                     
lbb_20566:
    mov64 r4, r1                                    r4 = r1
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxb r4, [r4+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    jsgt r4, -65, lbb_20521                         if (r4 as i64) > (-65 as i32 as i64) { pc += -51 }
lbb_20572:
    mov64 r4, r2                                    r4 = r2
    call function_20412                     
lbb_20574:
    mov64 r6, r0                                    r6 = r0
lbb_20575:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxw [r10-0x94], r6                     
    jlt r6, 128, lbb_20583                          if r6 < (128 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    jlt r6, 2048, lbb_20583                         if r6 < (2048 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 3                                     r1 = 3 as i32 as i64 as u64
    jlt r6, 65536, lbb_20583                        if r6 < (65536 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 4                                     r1 = 4 as i32 as i64 as u64
lbb_20583:
    stxdw [r10-0x90], r3                    
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    stxdw [r10-0x88], r1                    
    lddw r1, 0x10002f960 --> b"\x00\x00\x00\x00\xee\xdf\x02\x00\x0b\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295162208
    stxdw [r10-0x80], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    stxdw [r10-0x70], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x100029bc0 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x13\x08\x00\x00\x00\x00\x00y\x12\x00\x00\x…        r1 load str located at 4295138240
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -192                                  r1 += -192   ///  r1 = r1.wrapping_add(-192 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x1000244f8 --> b"\xbf&\x00\x00\x00\x00\x00\x00\xbf\x17\x00\x00\x00\x00\x00\x00aa4\x00\x00\…        r1 load str located at 4295116024
    stxdw [r10-0x28], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x1000274e8 --> b"\xbf(\x00\x00\x00\x00\x00\x00\xbf\x19\x00\x00\x00\x00\x00\x00y\x86 \x00\x…        r1 load str located at 4295128296
    stxdw [r10-0x38], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -148                                  r1 += -148   ///  r1 = r1.wrapping_add(-148 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    lddw r1, 0x1000298c0 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295137472
    stxdw [r10-0x48], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -160                                  r1 += -160   ///  r1 = r1.wrapping_add(-160 as i32 as i64 as u64)
    stxdw [r10-0x50], r1                    
    stdw [r10-0x60], 0                      
    stdw [r10-0x78], 5                      
    stdw [r10-0x68], 5                      
lbb_20623:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r2, r5                                    r2 = r5
    call function_18698                     
lbb_20627:
    jgt r3, r2, lbb_20629                           if r3 > r2 { pc += 1 }
    mov64 r3, r4                                    r3 = r4
lbb_20629:
    stxdw [r10-0x90], r3                    
    lddw r1, 0x10002f9b0 --> b"\x00\x00\x00\x00\xee\xdf\x02\x00\x0b\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295162288
    stxdw [r10-0x80], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    stxdw [r10-0x70], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100029bc0 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x13\x08\x00\x00\x00\x00\x00y\x12\x00\x00\x…        r1 load str located at 4295138240
    stxdw [r10-0x28], r1                    
    stxdw [r10-0x38], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -192                                  r1 += -192   ///  r1 = r1.wrapping_add(-192 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    lddw r1, 0x1000298c0 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295137472
    stxdw [r10-0x48], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    stxdw [r10-0x50], r1                    
    stdw [r10-0x60], 0                      
    stdw [r10-0x78], 3                      
    stdw [r10-0x68], 3                      
    ja lbb_20623                                    if true { pc += -33 }

function_20656:
    mov64 r9, r1                                    r9 = r1
    ldxdw r1, [r5-0xff0]                    
    stxdw [r10-0x10], r1                    
    ldxdw r6, [r5-0xff8]                    
    jeq r3, 0, lbb_20702                            if r3 == (0 as i32 as i64 as u64) { pc += 41 }
    lsh64 r3, 1                                     r3 <<= 1   ///  r3 = r3.wrapping_shl(1)
    mov64 r1, r2                                    r1 = r2
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    stxdw [r10-0x8], r1                     
    ldxdw r8, [r5-0x1000]                   
    mov64 r3, r9                                    r3 = r9
    and64 r3, 65280                                 r3 &= 65280   ///  r3 = r3.and(65280)
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    stxdw [r10-0x18], r4                    
    stxdw [r10-0x20], r8                    
lbb_20672:
    ldxb r7, [r2+0x1]                       
    mov64 r5, r0                                    r5 = r0
    add64 r5, r7                                    r5 += r7   ///  r5 = r5.wrapping_add(r7)
    ldxb r1, [r2+0x0]                       
    add64 r2, 2                                     r2 += 2   ///  r2 = r2.wrapping_add(2 as i32 as i64 as u64)
    jeq r1, r3, lbb_20679                           if r1 == r3 { pc += 1 }
    ja lbb_20691                                    if true { pc += 12 }
lbb_20679:
    jlt r5, r0, lbb_20731                           if r5 < r0 { pc += 51 }
    jgt r5, r8, lbb_20736                           if r5 > r8 { pc += 55 }
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
lbb_20682:
    jeq r7, 0, lbb_20696                            if r7 == (0 as i32 as i64 as u64) { pc += 13 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    ldxb r8, [r4+0x0]                       
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    jeq r8, r1, lbb_20729                           if r8 == r1 { pc += 39 }
    ja lbb_20682                                    if true { pc += -9 }
lbb_20691:
    jgt r1, r3, lbb_20702                           if r1 > r3 { pc += 10 }
    mov64 r0, r5                                    r0 = r5
    ldxdw r1, [r10-0x8]                     
    jeq r2, r1, lbb_20702                           if r2 == r1 { pc += 7 }
    ja lbb_20672                                    if true { pc += -24 }
lbb_20696:
    mov64 r0, r5                                    r0 = r5
    ldxdw r4, [r10-0x18]                    
    ldxdw r8, [r10-0x20]                    
    ldxdw r1, [r10-0x8]                     
    jeq r2, r1, lbb_20702                           if r2 == r1 { pc += 1 }
    ja lbb_20672                                    if true { pc += -30 }
lbb_20702:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ldxdw r1, [r10-0x10]                    
    jeq r1, 0, lbb_20729                            if r1 == (0 as i32 as i64 as u64) { pc += 24 }
    mov64 r2, r6                                    r2 = r6
    ldxdw r1, [r10-0x10]                    
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    and64 r9, 65535                                 r9 &= 65535   ///  r9 = r9.and(65535)
lbb_20709:
    mov64 r4, r6                                    r4 = r6
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    ldxb r3, [r6+0x0]                       
    lsh64 r3, 56                                    r3 <<= 56   ///  r3 = r3.wrapping_shl(56)
    arsh64 r3, 56                                   r3 >>= 56 (signed)   ///  r3 = (r3 as i64).wrapping_shr(56)
    jslt r3, 0, lbb_20717                           if (r3 as i64) < (0 as i32 as i64) { pc += 2 }
    mov64 r6, r4                                    r6 = r4
    ja lbb_20723                                    if true { pc += 6 }
lbb_20717:
    jeq r4, r2, lbb_20741                           if r4 == r2 { pc += 23 }
    and64 r3, 127                                   r3 &= 127   ///  r3 = r3.and(127)
    lsh64 r3, 8                                     r3 <<= 8   ///  r3 = r3.wrapping_shl(8)
    ldxb r4, [r6+0x1]                       
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    add64 r6, 2                                     r6 += 2   ///  r6 = r6.wrapping_add(2 as i32 as i64 as u64)
lbb_20723:
    sub64 r9, r3                                    r9 -= r3   ///  r9 = r9.wrapping_sub(r3)
    lsh64 r9, 32                                    r9 <<= 32   ///  r9 = r9.wrapping_shl(32)
    arsh64 r9, 32                                   r9 >>= 32 (signed)   ///  r9 = (r9 as i64).wrapping_shr(32)
    jslt r9, 0, lbb_20729                           if (r9 as i64) < (0 as i32 as i64) { pc += 2 }
    xor64 r0, 1                                     r0 ^= 1   ///  r0 = r0.xor(1)
    jne r6, r2, lbb_20709                           if r6 != r2 { pc += -20 }
lbb_20729:
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    exit                                    
lbb_20731:
    mov64 r1, r0                                    r1 = r0
    mov64 r2, r5                                    r2 = r5
    lddw r3, 0x10002fa10 --> b"\x00\x00\x00\x00;\xe0\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x0a\x00\x00…        r3 load str located at 4295162384
    call function_20248                     
lbb_20736:
    mov64 r1, r5                                    r1 = r5
    mov64 r2, r8                                    r2 = r8
    lddw r3, 0x10002fa10 --> b"\x00\x00\x00\x00;\xe0\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x0a\x00\x00…        r3 load str located at 4295162384
    call function_20247                     
lbb_20741:
    lddw r1, 0x10002f9f8 --> b"\x00\x00\x00\x00;\xe0\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x1a\x00\x00…        r1 load str located at 4295162360
    call function_18631                     

function_20744:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 32, lbb_20781                           if r2 < (32 as i32 as i64 as u64) { pc += 32 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r2, 127, lbb_20781                          if r2 < (127 as i32 as i64 as u64) { pc += 30 }
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 65536, lbb_20769                        if r2 < (65536 as i32 as i64 as u64) { pc += 14 }
    jlt r2, 131072, lbb_20757                       if r2 < (131072 as i32 as i64 as u64) { pc += 1 }
    ja lbb_20783                                    if true { pc += 26 }
lbb_20757:
    lddw r2, 0x10002e180 --> b"^"{\x05\x03\x04-\x03f\x03\x01/.\x80\x82\x1d\x031\x0f\x1c\x04$\x09\x1e\x05…        r2 load str located at 4295156096
    stxdw [r10-0xff8], r2                   
    stdw [r10-0xff0], 486                   
    stdw [r10-0x1000], 208                  
    mov64 r5, r10                                   r5 = r10
    lddw r2, 0x10002e058 --> b"\x00\x06\x01\x01\x03\x01\x04\x02\x05\x07\x07\x02\x08\x08\x09\x02\x0a\x05\…        r2 load str located at 4295155800
    mov64 r3, 44                                    r3 = 44 as i32 as i64 as u64
    lddw r4, 0x10002e0b0 --> b"\x0c';>NO\x8f\x9e\x9e\x9f{\x8b\x93\x96\xa2\xb2\xba\x86\xb1\x06\x07\x096=>…        r4 load str located at 4295155888
    ja lbb_20780                                    if true { pc += 11 }
lbb_20769:
    lddw r2, 0x10002e4d8 --> b"\x00 _"\x82\xdf\x04\x82D\x08\x1b\x04\x06\x11\x81\xac\x0e\x80\xab\x05\x1f\…        r2 load str located at 4295156952
    stxdw [r10-0xff8], r2                   
    stdw [r10-0xff0], 297                   
    stdw [r10-0x1000], 290                  
    mov64 r5, r10                                   r5 = r10
    lddw r2, 0x10002e366 --> b"\x00\x01\x03\x05\x05\x06\x06\x02\x07\x06\x08\x07\x09\x11\x0a\x1c\x0b\x19\…        r2 load str located at 4295156582
    mov64 r3, 40                                    r3 = 40 as i32 as i64 as u64
    lddw r4, 0x10002e3b6 --> b"\xadxy\x8b\x8d\xa20WX\x8b\x8c\x90\x1c\xdd\x0e\x0fKL\xfb\xfc./?\]_\xe2\x84…        r4 load str located at 4295156662
lbb_20780:
    call function_20656                     
lbb_20781:
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    exit                                    
lbb_20783:
    mov64 r2, r1                                    r2 = r1
    and64 r2, 2097150                               r2 &= 2097150   ///  r2 = r2.and(2097150)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r2, 178206, lbb_20781                       if r2 == (178206 as i32 as i64 as u64) { pc += -6 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 2097120                               r2 &= 2097120   ///  r2 = r2.and(2097120)
    jeq r2, 173792, lbb_20781                       if r2 == (173792 as i32 as i64 as u64) { pc += -9 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -177978                               r2 += -177978   ///  r2 = r2.wrapping_add(-177978 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 6, lbb_20781                            if r2 < (6 as i32 as i64 as u64) { pc += -14 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -183970                               r2 += -183970   ///  r2 = r2.wrapping_add(-183970 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 14, lbb_20781                           if r2 < (14 as i32 as i64 as u64) { pc += -19 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -191457                               r2 += -191457   ///  r2 = r2.wrapping_add(-191457 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 15, lbb_20781                           if r2 < (15 as i32 as i64 as u64) { pc += -24 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -192094                               r2 += -192094   ///  r2 = r2.wrapping_add(-192094 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 2466, lbb_20781                         if r2 < (2466 as i32 as i64 as u64) { pc += -29 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -195102                               r2 += -195102   ///  r2 = r2.wrapping_add(-195102 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 1506, lbb_20781                         if r2 < (1506 as i32 as i64 as u64) { pc += -34 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -201547                               r2 += -201547   ///  r2 = r2.wrapping_add(-201547 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 5, lbb_20781                            if r2 < (5 as i32 as i64 as u64) { pc += -39 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -205744                               r2 += -205744   ///  r2 = r2.wrapping_add(-205744 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 712016, lbb_20781                       if r2 < (712016 as i32 as i64 as u64) { pc += -44 }
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r1, 918000, lbb_20781                       if r1 < (918000 as i32 as i64 as u64) { pc += -48 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_20781                                    if true { pc += -50 }

function_20831:
    lddw r3, 0xfffffffc                             r3 load str located at 4294967292
    mov64 r4, r2                                    r4 = r2
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    mov64 r3, r2                                    r3 = r2
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    lddw r4, 0xfffffff8                             r4 load str located at 4294967288
    mov64 r5, r3                                    r5 = r3
    and64 r5, r4                                    r5 &= r4   ///  r5 = r5.and(r4)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    or64 r3, r5                                     r3 |= r5   ///  r3 = r3.or(r5)
    lddw r4, 0xffffffe0                             r4 load str located at 4294967264
    mov64 r5, r3                                    r5 = r3
    and64 r5, r4                                    r5 &= r4   ///  r5 = r5.and(r4)
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    or64 r3, r5                                     r3 |= r5   ///  r3 = r3.or(r5)
    lddw r5, 0xfff00000                             r5 load str located at 4293918720
    mov64 r4, r2                                    r4 = r2
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    rsh64 r4, 20                                    r4 >>= 20   ///  r4 = r4.wrapping_shr(20)
    mov64 r6, r2                                    r6 = r2
    rsh64 r6, 16                                    r6 >>= 16   ///  r6 = r6.wrapping_shr(16)
    and64 r6, 15                                    r6 &= 15   ///  r6 = r6.and(15)
    mov64 r7, r2                                    r7 = r2
    rsh64 r7, 4                                     r7 >>= 4   ///  r7 = r7.wrapping_shr(4)
    and64 r7, 15                                    r7 &= 15   ///  r7 = r7.and(15)
    lddw r0, 0x10002d2d8 --> b"0123456789abcdefVaultInitSettingsprograms/amm/src/"        r0 load str located at 4295152344
    lddw r5, 0x10002d2d8 --> b"0123456789abcdefVaultInitSettingsprograms/amm/src/"        r5 load str located at 4295152344
    add64 r5, r7                                    r5 += r7   ///  r5 = r5.wrapping_add(r7)
    mov64 r8, r2                                    r8 = r2
    rsh64 r8, 8                                     r8 >>= 8   ///  r8 = r8.wrapping_shr(8)
    and64 r8, 15                                    r8 &= 15   ///  r8 = r8.and(15)
    mov64 r9, r2                                    r9 = r2
    rsh64 r9, 12                                    r9 >>= 12   ///  r9 = r9.wrapping_shr(12)
    and64 r9, 15                                    r9 &= 15   ///  r9 = r9.and(15)
    lddw r7, 0x10002d2d8 --> b"0123456789abcdefVaultInitSettingsprograms/amm/src/"        r7 load str located at 4295152344
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    lddw r4, 0x10002d2d8 --> b"0123456789abcdefVaultInitSettingsprograms/amm/src/"        r4 load str located at 4295152344
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    lddw r6, 0x10002d2d8 --> b"0123456789abcdefVaultInitSettingsprograms/amm/src/"        r6 load str located at 4295152344
    add64 r6, r9                                    r6 += r9   ///  r6 = r6.wrapping_add(r9)
    lddw r9, 0x10002d2d8 --> b"0123456789abcdefVaultInitSettingsprograms/amm/src/"        r9 load str located at 4295152344
    add64 r9, r8                                    r9 += r8   ///  r9 = r9.wrapping_add(r8)
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    ldxb r2, [r0+0x0]                       
    stxb [r10-0x2], r2                      
    ldxb r2, [r5+0x0]                       
    stxb [r10-0x3], r2                      
    ldxb r2, [r9+0x0]                       
    stxb [r10-0x4], r2                      
    ldxb r2, [r6+0x0]                       
    stxb [r10-0x5], r2                      
    ldxb r2, [r4+0x0]                       
    stxb [r10-0x6], r2                      
    ldxb r2, [r7+0x0]                       
    stxb [r10-0x7], r2                      
    lddw r2, 0xfffffe00                             r2 load str located at 4294966784
    mov64 r4, r3                                    r4 = r3
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    rsh64 r4, 8                                     r4 >>= 8   ///  r4 = r4.wrapping_shr(8)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    lddw r2, 0xfffe0000                             r2 load str located at 4294836224
    mov64 r4, r3                                    r4 = r3
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    rsh64 r4, 16                                    r4 >>= 16   ///  r4 = r4.wrapping_shr(16)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    mov64 r4, r3                                    r4 = r3
    and64 r4, -2                                    r4 &= -2   ///  r4 = r4.and(-2)
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    and64 r3, 1431655765                            r3 &= 1431655765   ///  r3 = r3.and(1431655765)
    sub64 r4, r3                                    r4 -= r3   ///  r4 = r4.wrapping_sub(r3)
    mov64 r2, r4                                    r2 = r4
    and64 r2, 858993459                             r2 &= 858993459   ///  r2 = r2.and(858993459)
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    and64 r4, 858993459                             r4 &= 858993459   ///  r4 = r4.and(858993459)
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    stb [r10-0x8], 0                        
    sth [r10-0xa], 0                        
    stb [r10-0x1], 125                      
    and64 r2, 252645135                             r2 &= 252645135   ///  r2 = r2.and(252645135)
    mul64 r2, 16843009                              r2 *= 16843009   ///  r2 = r2.wrapping_mul(16843009 as u64)
    rsh64 r2, 26                                    r2 >>= 26   ///  r2 = r2.wrapping_shr(26)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -10                                   r3 += -10   ///  r3 = r3.wrapping_add(-10 as i32 as i64 as u64)
    mov64 r4, r3                                    r4 = r3
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    stb [r4+0x0], 123                       
    add64 r2, -2                                    r2 += -2   ///  r2 = r2.wrapping_add(-2 as i32 as i64 as u64)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    sth [r3+0x0], 30044                     
    ldxh r3, [r10-0x2]                      
    stxh [r1+0x8], r3                       
    ldxdw r3, [r10-0xa]                     
    stxdw [r1+0x0], r3                      
    stxb [r1+0xa], r2                       
    stb [r1+0xb], 10                        
    exit                                    

function_20946:
    mov64 r2, r1                                    r2 = r1
    lddw r1, 0x10002f800 --> b"\x00\x00\x00\x00\xce\xdd\x02\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295161856
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_18698                     

function_20957:
    mov64 r2, r1                                    r2 = r1
    lddw r1, 0x10002f810 --> b"\x00\x00\x00\x00\xea\xdd\x02\x00!\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295161872
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_18698                     

function_20968:
    mov64 r2, r1                                    r2 = r1
    lddw r1, 0x10002f820 --> b"\x00\x00\x00\x00\x0b\xde\x02\x00!\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295161888
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_18698                     

function_20979:
    mov64 r2, r1                                    r2 = r1
    lddw r1, 0x10002f830 --> b"\x00\x00\x00\x00,\xde\x02\x00\x1f\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295161904
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_18698                     

function_20990:
    mov64 r2, r1                                    r2 = r1
    lddw r1, 0x10002f840 --> b"\x00\x00\x00\x00K\xde\x02\x00#\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x0…        r1 load str located at 4295161920
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_18698                     

function_21001:
    mov64 r2, r1                                    r2 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jlt r2, 10, lbb_21008                           if r2 < (10 as i32 as i64 as u64) { pc += 4 }
    jlt r2, 16, lbb_21006                           if r2 < (16 as i32 as i64 as u64) { pc += 1 }
    ja lbb_21011                                    if true { pc += 5 }
lbb_21006:
    add64 r1, 87                                    r1 += 87   ///  r1 = r1.wrapping_add(87 as i32 as i64 as u64)
    ja lbb_21009                                    if true { pc += 1 }
lbb_21008:
    or64 r1, 48                                     r1 |= 48   ///  r1 = r1.or(48)
lbb_21009:
    mov64 r0, r1                                    r0 = r1
    exit                                    
lbb_21011:
    stxb [r10-0x51], r1                     
    lddw r1, 0x10002fa58 --> b"\x00\x00\x00\x00\x01\xe6\x02\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295162456
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -81                                   r1 += -81   ///  r1 = r1.wrapping_add(-81 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x100029418 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295136280
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    lddw r1, 0x10002e61d --> b"\x0frange start index  out of range for slice of leng"        r1 load str located at 4295157277
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    lddw r2, 0x10002fa78 --> b"\x00\x00\x00\x00\xd7\xde\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\x8c\x00\…        r2 load str located at 4295162488
    call function_18698                     

function_21036:
    mov64 r2, r1                                    r2 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jlt r2, 10, lbb_21043                           if r2 < (10 as i32 as i64 as u64) { pc += 4 }
    jlt r2, 16, lbb_21041                           if r2 < (16 as i32 as i64 as u64) { pc += 1 }
    ja lbb_21046                                    if true { pc += 5 }
lbb_21041:
    add64 r1, 55                                    r1 += 55   ///  r1 = r1.wrapping_add(55 as i32 as i64 as u64)
    ja lbb_21044                                    if true { pc += 1 }
lbb_21043:
    or64 r1, 48                                     r1 |= 48   ///  r1 = r1.or(48)
lbb_21044:
    mov64 r0, r1                                    r0 = r1
    exit                                    
lbb_21046:
    stxb [r10-0x51], r1                     
    lddw r1, 0x10002fa58 --> b"\x00\x00\x00\x00\x01\xe6\x02\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295162456
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -81                                   r1 += -81   ///  r1 = r1.wrapping_add(-81 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x100029418 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295136280
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    lddw r1, 0x10002e61d --> b"\x0frange start index  out of range for slice of leng"        r1 load str located at 4295157277
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    lddw r2, 0x10002fa90 --> b"\x00\x00\x00\x00\xd7\xde\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\x8d\x00\…        r2 load str located at 4295162512
    call function_18698                     

function_21071:
    mov64 r3, r2                                    r3 = r2
    ldxb r2, [r1+0x0]                       
    call function_19131                     
    exit                                    

function_21075:
    mov64 r3, r2                                    r3 = r2
    ldxb r2, [r1+0x0]                       
    call function_19163                     
    exit                                    

function_21079:
    mov64 r3, r2                                    r3 = r2
    ldxw r2, [r1+0x0]                       
    call function_19065                     
    exit                                    

function_21083:
    mov64 r3, r2                                    r3 = r2
    ldxw r2, [r1+0x0]                       
    call function_19098                     
    exit                                    

function_21087:
    mov64 r3, r2                                    r3 = r2
    ldxb r1, [r1+0x0]                       
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_21092                     
    exit                                    

function_21092:
    mov64 r5, r1                                    r5 = r1
    and64 r5, 255                                   r5 &= 255   ///  r5 = r5.and(255)
    jlt r5, 100, lbb_21114                          if r5 < (100 as i32 as i64 as u64) { pc += 19 }
    div64 r5, 100                                   r5 /= 100   ///  r5 = r5 / (100 as u64)
    mov64 r4, r5                                    r4 = r5
    mul64 r4, 100                                   r4 *= 100   ///  r4 = r4.wrapping_mul(100 as u64)
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    and64 r1, 254                                   r1 &= 254   ///  r1 = r1.and(254)
    lddw r4, 0x10002deec --> b"00010203040506070809101112131415161718192021222324"        r4 load str located at 4295155436
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    ldxh r1, [r4+0x0]                       
    stxh [r10-0x2], r1                      
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r1, r5                                    r1 = r5
lbb_21108:
    mov64 r5, r10                                   r5 = r10
    add64 r5, -3                                    r5 += -3   ///  r5 = r5.wrapping_add(-3 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    or64 r1, 48                                     r1 |= 48   ///  r1 = r1.or(48)
    stxb [r5+0x0], r1                       
    ja lbb_21124                                    if true { pc += 10 }
lbb_21114:
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    jlt r5, 10, lbb_21108                           if r5 < (10 as i32 as i64 as u64) { pc += -8 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    and64 r1, 254                                   r1 &= 254   ///  r1 = r1.and(254)
    lddw r4, 0x10002deec --> b"00010203040506070809101112131415161718192021222324"        r4 load str located at 4295155436
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    ldxh r1, [r4+0x0]                       
    stxh [r10-0x2], r1                      
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
lbb_21124:
    mov64 r1, r4                                    r1 = r4
    xor64 r1, 3                                     r1 ^= 3   ///  r1 = r1.xor(3)
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -3                                    r1 += -3   ///  r1 = r1.wrapping_add(-3 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r3                                    r1 = r3
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_19371                     
    exit                                    

function_21137:
    mov64 r3, r2                                    r3 = r2
    ldxw r1, [r1+0x0]                       
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_21142                     
    exit                                    

function_21142:
    mov64 r4, 10                                    r4 = 10 as i32 as i64 as u64
    mov64 r5, r1                                    r5 = r1
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    jlt r5, 10000, lbb_21183                        if r5 < (10000 as i32 as i64 as u64) { pc += 36 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_21148:
    mov64 r0, r1                                    r0 = r1
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r5, r0                                    r5 = r0
    div64 r5, 10000                                 r5 /= 10000   ///  r5 = r5 / (10000 as u64)
    mov64 r6, r5                                    r6 = r5
    mul64 r6, 10000                                 r6 *= 10000   ///  r6 = r6.wrapping_mul(10000 as u64)
    sub64 r1, r6                                    r1 -= r6   ///  r1 = r1.wrapping_sub(r6)
    mov64 r6, r1                                    r6 = r1
    and64 r6, 65535                                 r6 &= 65535   ///  r6 = r6.and(65535)
    div64 r6, 100                                   r6 /= 100   ///  r6 = r6 / (100 as u64)
    mov64 r7, r6                                    r7 = r6
    mul64 r7, 100                                   r7 *= 100   ///  r7 = r7.wrapping_mul(100 as u64)
    sub64 r1, r7                                    r1 -= r7   ///  r1 = r1.wrapping_sub(r7)
    mov64 r7, r10                                   r7 = r10
    add64 r7, -10                                   r7 += -10   ///  r7 = r7.wrapping_add(-10 as i32 as i64 as u64)
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    lsh64 r6, 1                                     r6 <<= 1   ///  r6 = r6.wrapping_shl(1)
    lddw r8, 0x10002deec --> b"00010203040506070809101112131415161718192021222324"        r8 load str located at 4295155436
    add64 r8, r6                                    r8 += r6   ///  r8 = r8.wrapping_add(r6)
    ldxh r6, [r8+0x0]                       
    stxh [r7+0x6], r6                       
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    and64 r1, 65534                                 r1 &= 65534   ///  r1 = r1.and(65534)
    lddw r6, 0x10002deec --> b"00010203040506070809101112131415161718192021222324"        r6 load str located at 4295155436
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    ldxh r1, [r6+0x0]                       
    stxh [r7+0x8], r1                       
    add64 r4, -4                                    r4 += -4   ///  r4 = r4.wrapping_add(-4 as i32 as i64 as u64)
    mov64 r1, r5                                    r1 = r5
    jgt r0, 99999999, lbb_21148                     if r0 > (99999999 as i32 as i64 as u64) { pc += -33 }
    add64 r4, 10                                    r4 += 10   ///  r4 = r4.wrapping_add(10 as i32 as i64 as u64)
    mov64 r1, r5                                    r1 = r5
lbb_21183:
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jgt r1, 99, lbb_21187                           if r1 > (99 as i32 as i64 as u64) { pc += 1 }
    ja lbb_21205                                    if true { pc += 18 }
lbb_21187:
    mov64 r5, r1                                    r5 = r1
    and64 r5, 65535                                 r5 &= 65535   ///  r5 = r5.and(65535)
    div64 r5, 100                                   r5 /= 100   ///  r5 = r5 / (100 as u64)
    mov64 r0, r5                                    r0 = r5
    mul64 r0, 100                                   r0 *= 100   ///  r0 = r0.wrapping_mul(100 as u64)
    sub64 r1, r0                                    r1 -= r0   ///  r1 = r1.wrapping_sub(r0)
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    and64 r1, 65534                                 r1 &= 65534   ///  r1 = r1.and(65534)
    lddw r0, 0x10002deec --> b"00010203040506070809101112131415161718192021222324"        r0 load str located at 4295155436
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -10                                   r1 += -10   ///  r1 = r1.wrapping_add(-10 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    ldxh r0, [r0+0x0]                       
    stxh [r1+0x0], r0                       
    mov64 r1, r5                                    r1 = r5
lbb_21205:
    jlt r1, 10, lbb_21217                           if r1 < (10 as i32 as i64 as u64) { pc += 11 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    lddw r5, 0x10002deec --> b"00010203040506070809101112131415161718192021222324"        r5 load str located at 4295155436
    add64 r5, r1                                    r5 += r1   ///  r5 = r5.wrapping_add(r1)
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -10                                   r1 += -10   ///  r1 = r1.wrapping_add(-10 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    ldxh r5, [r5+0x0]                       
    stxh [r1+0x0], r5                       
    ja lbb_21223                                    if true { pc += 6 }
lbb_21217:
    add64 r4, -1                                    r4 += -1   ///  r4 = r4.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -10                                   r5 += -10   ///  r5 = r5.wrapping_add(-10 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    or64 r1, 48                                     r1 |= 48   ///  r1 = r1.or(48)
    stxb [r5+0x0], r1                       
lbb_21223:
    mov64 r1, 10                                    r1 = 10 as i32 as i64 as u64
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -10                                   r1 += -10   ///  r1 = r1.wrapping_add(-10 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r3                                    r1 = r3
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_19371                     
    exit                                    

function_21236:
    mov64 r3, r2                                    r3 = r2
    ldxdw r1, [r1+0x0]                      
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_21241                     
    exit                                    

function_21241:
    mov64 r4, 20                                    r4 = 20 as i32 as i64 as u64
    jlt r1, 10000, lbb_21275                        if r1 < (10000 as i32 as i64 as u64) { pc += 32 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_21244:
    mov64 r5, r1                                    r5 = r1
    div64 r1, 10000                                 r1 /= 10000   ///  r1 = r1 / (10000 as u64)
    mov64 r6, r1                                    r6 = r1
    mul64 r6, 10000                                 r6 *= 10000   ///  r6 = r6.wrapping_mul(10000 as u64)
    mov64 r0, r5                                    r0 = r5
    sub64 r0, r6                                    r0 -= r6   ///  r0 = r0.wrapping_sub(r6)
    mov64 r6, r0                                    r6 = r0
    and64 r6, 65535                                 r6 &= 65535   ///  r6 = r6.and(65535)
    div64 r6, 100                                   r6 /= 100   ///  r6 = r6 / (100 as u64)
    mov64 r7, r6                                    r7 = r6
    mul64 r7, 100                                   r7 *= 100   ///  r7 = r7.wrapping_mul(100 as u64)
    sub64 r0, r7                                    r0 -= r7   ///  r0 = r0.wrapping_sub(r7)
    mov64 r7, r10                                   r7 = r10
    add64 r7, -20                                   r7 += -20   ///  r7 = r7.wrapping_add(-20 as i32 as i64 as u64)
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    lsh64 r6, 1                                     r6 <<= 1   ///  r6 = r6.wrapping_shl(1)
    lddw r8, 0x10002deec --> b"00010203040506070809101112131415161718192021222324"        r8 load str located at 4295155436
    add64 r8, r6                                    r8 += r6   ///  r8 = r8.wrapping_add(r6)
    ldxh r6, [r8+0x0]                       
    stxh [r7+0x10], r6                      
    lsh64 r0, 1                                     r0 <<= 1   ///  r0 = r0.wrapping_shl(1)
    and64 r0, 65534                                 r0 &= 65534   ///  r0 = r0.and(65534)
    lddw r6, 0x10002deec --> b"00010203040506070809101112131415161718192021222324"        r6 load str located at 4295155436
    add64 r6, r0                                    r6 += r0   ///  r6 = r6.wrapping_add(r0)
    ldxh r0, [r6+0x0]                       
    stxh [r7+0x12], r0                      
    add64 r4, -4                                    r4 += -4   ///  r4 = r4.wrapping_add(-4 as i32 as i64 as u64)
    jgt r5, 99999999, lbb_21244                     if r5 > (99999999 as i32 as i64 as u64) { pc += -30 }
    add64 r4, 20                                    r4 += 20   ///  r4 = r4.wrapping_add(20 as i32 as i64 as u64)
lbb_21275:
    jgt r1, 99, lbb_21277                           if r1 > (99 as i32 as i64 as u64) { pc += 1 }
    ja lbb_21295                                    if true { pc += 18 }
lbb_21277:
    mov64 r5, r1                                    r5 = r1
    and64 r5, 65535                                 r5 &= 65535   ///  r5 = r5.and(65535)
    div64 r5, 100                                   r5 /= 100   ///  r5 = r5 / (100 as u64)
    mov64 r0, r5                                    r0 = r5
    mul64 r0, 100                                   r0 *= 100   ///  r0 = r0.wrapping_mul(100 as u64)
    sub64 r1, r0                                    r1 -= r0   ///  r1 = r1.wrapping_sub(r0)
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    and64 r1, 65534                                 r1 &= 65534   ///  r1 = r1.and(65534)
    lddw r0, 0x10002deec --> b"00010203040506070809101112131415161718192021222324"        r0 load str located at 4295155436
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -20                                   r1 += -20   ///  r1 = r1.wrapping_add(-20 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    ldxh r0, [r0+0x0]                       
    stxh [r1+0x0], r0                       
    mov64 r1, r5                                    r1 = r5
lbb_21295:
    jlt r1, 10, lbb_21307                           if r1 < (10 as i32 as i64 as u64) { pc += 11 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    lddw r5, 0x10002deec --> b"00010203040506070809101112131415161718192021222324"        r5 load str located at 4295155436
    add64 r5, r1                                    r5 += r1   ///  r5 = r5.wrapping_add(r1)
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -20                                   r1 += -20   ///  r1 = r1.wrapping_add(-20 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    ldxh r5, [r5+0x0]                       
    stxh [r1+0x0], r5                       
    ja lbb_21313                                    if true { pc += 6 }
lbb_21307:
    add64 r4, -1                                    r4 += -1   ///  r4 = r4.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -20                                   r5 += -20   ///  r5 = r5.wrapping_add(-20 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    or64 r1, 48                                     r1 |= 48   ///  r1 = r1.or(48)
    stxb [r5+0x0], r1                       
lbb_21313:
    mov64 r1, 20                                    r1 = 20 as i32 as i64 as u64
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -20                                   r1 += -20   ///  r1 = r1.wrapping_add(-20 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r3                                    r1 = r3
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_19371                     
    exit                                    

function_21326:
    ldxdw r3, [r1+0x0]                      
    ldxdw r1, [r1+0x8]                      
    ldxdw r4, [r1+0x18]                     
    mov64 r1, r3                                    r1 = r3
    callx r4                                
    exit                                    

function_21332:
    mov64 r4, r2                                    r4 = r2
    ldxdw r3, [r1+0x8]                      
    ldxdw r2, [r1+0x0]                      
    mov64 r1, r4                                    r1 = r4
    call function_19571                     
    exit                                    

function_21338:
    stxdw [r10-0x58], r2                    
    stxdw [r10-0x60], r1                    
    lddw r1, 0x10002faa8 --> b"\x00\x00\x00\x00\x1e\xe6\x02\x00\x12\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295162536
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x1000298c0 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295137472
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_18698                     

function_21363:
    stxdw [r10-0x58], r2                    
    stxdw [r10-0x60], r1                    
    lddw r1, 0x10002fac8 --> b"\x00\x00\x00\x00\xb8\xd2\x02\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295162568
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x1000298c0 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295137472
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_18698                     

function_21388:
    stxdw [r10-0x58], r2                    
    stxdw [r10-0x60], r1                    
    lddw r1, 0x10002fae8 --> b"\x00\x00\x00\x00R\xe6\x02\x00\x16\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295162600
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x1000298c0 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295137472
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_18698                     

function_21413:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r4, 34                                    r4 = 34 as i32 as i64 as u64
    mov64 r3, r1                                    r3 = r1
    lsh64 r3, 11                                    r3 <<= 11   ///  r3 = r3.wrapping_shl(11)

function_21417:
    mov64 r5, r4                                    r5 = r4
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    mov64 r0, r5                                    r0 = r5
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    mov64 r6, r0                                    r6 = r0
    lsh64 r6, 2                                     r6 <<= 2   ///  r6 = r6.wrapping_shl(2)
    lddw r7, 0x10002e678 --> b"\x00\x03\x00\x00\x83\x04 \x00\x91\x05`\x00]\x13\xa0\x00\x12\x17 \x1f\x0c …        r7 load str located at 4295157368
    add64 r7, r6                                    r7 += r6   ///  r7 = r7.wrapping_add(r6)
    ldxw r6, [r7+0x0]                       
    lsh64 r6, 11                                    r6 <<= 11   ///  r6 = r6.wrapping_shl(11)
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    mov64 r7, r3                                    r7 = r3
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    jgt r6, r7, lbb_21435                           if r6 > r7 { pc += 1 }
    mov64 r2, r0                                    r2 = r0
lbb_21435:
    sub64 r4, r5                                    r4 -= r5   ///  r4 = r4.wrapping_sub(r5)
    jgt r4, 1, function_21417                       if r4 > (1 as i32 as i64 as u64) { pc += -20 }
    mov64 r4, r2                                    r4 = r2
    lsh64 r4, 2                                     r4 <<= 2   ///  r4 = r4.wrapping_shl(2)
    lddw r5, 0x10002e678 --> b"\x00\x03\x00\x00\x83\x04 \x00\x91\x05`\x00]\x13\xa0\x00\x12\x17 \x1f\x0c …        r5 load str located at 4295157368
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxw r4, [r5+0x0]                       
    lsh64 r4, 11                                    r4 <<= 11   ///  r4 = r4.wrapping_shl(11)
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r4, r3, lbb_21452                           if r4 == r3 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_21452:
    jlt r4, r3, lbb_21454                           if r4 < r3 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_21454:
    add64 r2, r0                                    r2 += r0   ///  r2 = r2.wrapping_add(r0)
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    jgt r2, 33, lbb_21508                           if r2 > (33 as i32 as i64 as u64) { pc += 51 }
    mov64 r4, r2                                    r4 = r2
    lsh64 r4, 2                                     r4 <<= 2   ///  r4 = r4.wrapping_shl(2)
    lddw r6, 0x10002e678 --> b"\x00\x03\x00\x00\x83\x04 \x00\x91\x05`\x00]\x13\xa0\x00\x12\x17 \x1f\x0c …        r6 load str located at 4295157368
    lddw r7, 0x10002e678 --> b"\x00\x03\x00\x00\x83\x04 \x00\x91\x05`\x00]\x13\xa0\x00\x12\x17 \x1f\x0c …        r7 load str located at 4295157368
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    mov64 r3, 751                                   r3 = 751 as i32 as i64 as u64
    ldxw r0, [r7+0x0]                       
    rsh64 r0, 21                                    r0 >>= 21   ///  r0 = r0.wrapping_shr(21)
    jeq r2, 33, lbb_21472                           if r2 == (33 as i32 as i64 as u64) { pc += 4 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxw r3, [r7+0x4]                       
    rsh64 r3, 21                                    r3 >>= 21   ///  r3 = r3.wrapping_shr(21)
    jeq r2, 0, lbb_21475                            if r2 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_21472:
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    ldxw r5, [r4-0x4]                       
    and64 r5, 2097151                               r5 &= 2097151   ///  r5 = r5.and(2097151)
lbb_21475:
    mov64 r2, r0                                    r2 = r0
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    jeq r3, 0, lbb_21501                            if r3 == (0 as i32 as i64 as u64) { pc += 22 }
    sub64 r1, r5                                    r1 -= r5   ///  r1 = r1.wrapping_sub(r5)
    lddw r5, 0x10002e700 --> b"\x00p\x00\x07\x00-\x01\x01\x01\x02\x01\x02\x01\x01H\x0b0\x15\x10\x01e\x07…        r5 load str located at 4295157504
    add64 r5, r0                                    r5 += r0   ///  r5 = r5.wrapping_add(r0)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_21487:
    mov64 r2, r0                                    r2 = r0
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    jgt r2, 750, lbb_21503                          if r2 > (750 as i32 as i64 as u64) { pc += 13 }
    mov64 r2, r5                                    r2 = r5
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    ldxb r2, [r2+0x0]                       
    add64 r6, r2                                    r6 += r2   ///  r6 = r6.wrapping_add(r2)
    mov64 r2, r6                                    r2 = r6
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jgt r2, r1, lbb_21500                           if r2 > r1 { pc += 2 }
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    jlt r4, r3, lbb_21487                           if r4 < r3 { pc += -13 }
lbb_21500:
    add64 r0, r4                                    r0 += r4   ///  r0 = r0.wrapping_add(r4)
lbb_21501:
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    exit                                    
lbb_21503:
    mov64 r1, r2                                    r1 = r2
    mov64 r2, 751                                   r2 = 751 as i32 as i64 as u64
    lddw r3, 0x10002fa40 --> b"\x00\x00\x00\x00\xe8\xcd\x02\x00 \x00\x00\x00\x00\x00\x00\x00Z\x00\x00\x0…        r3 load str located at 4295162432
    call function_18717                     
lbb_21508:
    mov64 r1, r2                                    r1 = r2
    mov64 r2, 34                                    r2 = 34 as i32 as i64 as u64
    lddw r3, 0x10002fa28 --> b"\x00\x00\x00\x00\xe8\xcd\x02\x00 \x00\x00\x00\x00\x00\x00\x00N\x00\x00\x0…        r3 load str located at 4295162408
    call function_18717                     

function_21513:
    mov64 r6, r1                                    r6 = r1
    syscall [invalid]                       
    mov64 r0, r6                                    r0 = r6
    exit                                    

function_21517:
    mov64 r6, r1                                    r6 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    syscall [invalid]                       
    mov64 r0, r6                                    r0 = r6
    exit                                    

function_21522:
    stw [r10-0x4], 0                        
    mov64 r4, r10                                   r4 = r10
    add64 r4, -4                                    r4 += -4   ///  r4 = r4.wrapping_add(-4 as i32 as i64 as u64)
    syscall [invalid]                       
    ldxw r0, [r10-0x4]                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    exit                                    

function_21530:
    call function_21532                     
    exit                                    

function_21532:
    mov64 r0, r1                                    r0 = r1
    mov64 r9, r2                                    r9 = r2
    xor64 r9, r0                                    r9 ^= r0   ///  r9 = r9.xor(r0)
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    and64 r9, r1                                    r9 &= r1   ///  r9 = r9.and(r1)
    lddw r1, 0xfffffffffffff                        r1 load str located at 4503599627370495
    mov64 r5, r2                                    r5 = r2
    and64 r5, r1                                    r5 &= r1   ///  r5 = r5.and(r1)
    mov64 r7, r0                                    r7 = r0
    and64 r7, r1                                    r7 &= r1   ///  r7 = r7.and(r1)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 52                                    r3 >>= 52   ///  r3 = r3.wrapping_shr(52)
    and64 r3, 2047                                  r3 &= 2047   ///  r3 = r3.and(2047)
    mov64 r4, r0                                    r4 = r0
    rsh64 r4, 52                                    r4 >>= 52   ///  r4 = r4.wrapping_shr(52)
    and64 r4, 2047                                  r4 &= 2047   ///  r4 = r4.and(2047)
    mov64 r1, r4                                    r1 = r4
    sub64 r1, r3                                    r1 -= r3   ///  r1 = r1.wrapping_sub(r3)
    mov64 r8, r1                                    r8 = r1
    add64 r8, 1023                                  r8 += 1023   ///  r8 = r8.wrapping_add(1023 as i32 as i64 as u64)
    add64 r4, -2047                                 r4 += -2047   ///  r4 = r4.wrapping_add(-2047 as i32 as i64 as u64)
    jlt r4, -2046, lbb_21631                        if r4 < (-2046 as i32 as i64 as u64) { pc += 75 }
    add64 r3, -2047                                 r3 += -2047   ///  r3 = r3.wrapping_add(-2047 as i32 as i64 as u64)
    jlt r3, -2046, lbb_21631                        if r3 < (-2046 as i32 as i64 as u64) { pc += 73 }
lbb_21558:
    stxdw [r10-0x20], r9                    
    lddw r1, 0x10000000000000                       r1 load str located at 4503599627370496
    or64 r5, r1                                     r5 |= r1   ///  r5 = r5.or(r1)
    mov64 r4, 3                                     r4 = 3 as i32 as i64 as u64
    stxdw [r10-0x18], r5                    
    mov64 r1, r5                                    r1 = r5
    lsh64 r1, 11                                    r1 <<= 11   ///  r1 = r1.wrapping_shl(11)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    mov64 r5, 1963258675                            r5 = 1963258675 as i32 as i64 as u64
    sub64 r5, r3                                    r5 -= r3   ///  r5 = r5.wrapping_sub(r3)
lbb_21570:
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    mov64 r2, r5                                    r2 = r5
    mul64 r2, r3                                    r2 *= r3   ///  r2 = r2.wrapping_mul(r3)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mul64 r2, r5                                    r2 *= r5   ///  r2 = r2.wrapping_mul(r5)
    rsh64 r2, 31                                    r2 >>= 31   ///  r2 = r2.wrapping_shr(31)
    add64 r4, -1                                    r4 += -1   ///  r4 = r4.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r5, r2                                    r5 = r2
    jeq r4, 0, lbb_21584                            if r4 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_21570                                    if true { pc += -14 }
lbb_21584:
    lddw r4, 0xfffff800                             r4 load str located at 4294965248
    and64 r1, r4                                    r1 &= r4   ///  r1 = r1.and(r4)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mov64 r4, r2                                    r4 = r2
    mul64 r4, r3                                    r4 *= r3   ///  r4 = r4.wrapping_mul(r3)
    mov64 r3, r2                                    r3 = r2
    mul64 r3, r1                                    r3 *= r1   ///  r3 = r3.wrapping_mul(r1)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    lddw r3, 0x10000000000000                       r3 load str located at 4503599627370496
    mov64 r9, r7                                    r9 = r7
    or64 r9, r3                                     r9 |= r3   ///  r9 = r9.or(r3)
    mov64 r3, r1                                    r3 = r1
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    mul64 r3, r2                                    r3 *= r2   ///  r3 = r3.wrapping_mul(r2)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mul64 r2, r1                                    r2 *= r1   ///  r2 = r2.wrapping_mul(r1)
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    rsh64 r3, 31                                    r3 >>= 31   ///  r3 = r3.wrapping_shr(31)
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    add64 r2, -225                                  r2 += -225   ///  r2 = r2.wrapping_add(-225 as i32 as i64 as u64)
    mov64 r6, r9                                    r6 = r9
    lsh64 r6, 1                                     r6 <<= 1   ///  r6 = r6.wrapping_shl(1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    ldxdw r0, [r10-0x8]                     
    lddw r1, 0x20000000000000                       r1 load str located at 9007199254740992
    jlt r0, r1, lbb_21648                           if r0 < r1 { pc += 24 }
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    mov64 r1, r0                                    r1 = r0
    ldxdw r4, [r10-0x18]                    
    mul64 r1, r4                                    r1 *= r4   ///  r1 = r1.wrapping_mul(r4)
    lsh64 r7, 52                                    r7 <<= 52   ///  r7 = r7.wrapping_shl(52)
    sub64 r7, r1                                    r7 -= r1   ///  r7 = r7.wrapping_sub(r1)
    ja lbb_21655                                    if true { pc += 24 }
lbb_21631:
    stxdw [r10-0x18], r5                    
    lddw r5, 0x7fffffffffffffff                     r5 load str located at 9223372036854775807
    mov64 r4, r0                                    r4 = r0
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    lddw r6, 0x7ff0000000000000                     r6 load str located at 9218868437227405312
    jgt r4, r6, lbb_21670                           if r4 > r6 { pc += 31 }
    mov64 r3, r2                                    r3 = r2
    and64 r3, r5                                    r3 &= r5   ///  r3 = r3.and(r5)
    jgt r3, r6, lbb_21643                           if r3 > r6 { pc += 1 }
    ja lbb_21674                                    if true { pc += 31 }
lbb_21643:
    lddw r1, 0x8000000000000                        r1 load str located at 2251799813685248
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    mov64 r0, r2                                    r0 = r2
    ja lbb_21715                                    if true { pc += 67 }
lbb_21648:
    ldxdw r4, [r10-0x18]                    
    mov64 r1, r4                                    r1 = r4
    mul64 r1, r0                                    r1 *= r0   ///  r1 = r1.wrapping_mul(r0)
    lsh64 r7, 53                                    r7 <<= 53   ///  r7 = r7.wrapping_shl(53)
    sub64 r7, r1                                    r7 -= r1   ///  r7 = r7.wrapping_sub(r1)
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r9, r6                                    r9 = r6
lbb_21655:
    jsgt r8, 2046, lbb_21666                        if (r8 as i64) > (2046 as i32 as i64) { pc += 10 }
    jsgt r8, 0, lbb_21658                           if (r8 as i64) > (0 as i32 as i64) { pc += 1 }
    ja lbb_21685                                    if true { pc += 27 }
lbb_21658:
    lddw r1, 0xfffffffffffff                        r1 load str located at 4503599627370495
    and64 r0, r1                                    r0 &= r1   ///  r0 = r0.and(r1)
    lsh64 r8, 52                                    r8 <<= 52   ///  r8 = r8.wrapping_shl(52)
    or64 r8, r0                                     r8 |= r0   ///  r8 = r8.or(r0)
    lsh64 r7, 1                                     r7 <<= 1   ///  r7 = r7.wrapping_shl(1)
    mov64 r0, r8                                    r0 = r8
    ja lbb_21700                                    if true { pc += 34 }
lbb_21666:
    lddw r1, 0x7ff0000000000000                     r1 load str located at 9218868437227405312
    ldxdw r0, [r10-0x20]                    
    ja lbb_21672                                    if true { pc += 2 }
lbb_21670:
    lddw r1, 0x8000000000000                        r1 load str located at 2251799813685248
lbb_21672:
    or64 r0, r1                                     r0 |= r1   ///  r0 = r0.or(r1)
    ja lbb_21715                                    if true { pc += 41 }
lbb_21674:
    lddw r2, 0x7ff0000000000000                     r2 load str located at 9218868437227405312
    jeq r4, r2, lbb_21678                           if r4 == r2 { pc += 1 }
    ja lbb_21710                                    if true { pc += 32 }
lbb_21678:
    lddw r0, 0x7ff8000000000000                     r0 load str located at 9221120237041090560
    jeq r3, r2, lbb_21715                           if r3 == r2 { pc += 34 }
lbb_21681:
    lddw r1, 0x7ff0000000000000                     r1 load str located at 9218868437227405312
    or64 r9, r1                                     r9 |= r1   ///  r9 = r9.or(r1)
    ja lbb_21712                                    if true { pc += 27 }
lbb_21685:
    jslt r8, -52, lbb_21714                         if (r8 as i64) < (-52 as i32 as i64) { pc += 28 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    sub64 r1, r8                                    r1 -= r8   ///  r1 = r1.wrapping_sub(r8)
    add64 r8, 52                                    r8 += 52   ///  r8 = r8.wrapping_add(52 as i32 as i64 as u64)
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    rsh64 r8, 32                                    r8 >>= 32   ///  r8 = r8.wrapping_shr(32)
    lsh64 r9, r8                                    r9 <<= r8   ///  r9 = r9.wrapping_shl(r8 as u32)
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    rsh64 r0, r1                                    r0 >>= r1   ///  r0 = r0.wrapping_shr(r1 as u32)
    mov64 r1, r4                                    r1 = r4
    mul64 r1, r0                                    r1 *= r0   ///  r1 = r1.wrapping_mul(r0)
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    sub64 r9, r1                                    r9 -= r1   ///  r9 = r9.wrapping_sub(r1)
    mov64 r7, r9                                    r7 = r9
lbb_21700:
    mov64 r2, r0                                    r2 = r0
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    add64 r2, r7                                    r2 += r7   ///  r2 = r2.wrapping_add(r7)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0x20]                    
    jgt r2, r4, lbb_21707                           if r2 > r4 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_21707:
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    ja lbb_21715                                    if true { pc += 5 }
lbb_21710:
    jeq r3, r2, lbb_21712                           if r3 == r2 { pc += 1 }
    ja lbb_21716                                    if true { pc += 4 }
lbb_21712:
    mov64 r0, r9                                    r0 = r9
    ja lbb_21715                                    if true { pc += 1 }
lbb_21714:
    ldxdw r0, [r10-0x20]                    
lbb_21715:
    exit                                    
lbb_21716:
    jeq r4, 0, lbb_21831                            if r4 == (0 as i32 as i64 as u64) { pc += 114 }
    jeq r3, 0, lbb_21681                            if r3 == (0 as i32 as i64 as u64) { pc += -37 }
    lddw r2, 0x10000000000000                       r2 load str located at 4503599627370496
    ldxdw r5, [r10-0x18]                    
    jlt r4, r2, lbb_21723                           if r4 < r2 { pc += 1 }
    ja lbb_21775                                    if true { pc += 52 }
lbb_21723:
    mov64 r4, 64                                    r4 = 64 as i32 as i64 as u64
    jeq r7, 0, lbb_21769                            if r7 == (0 as i32 as i64 as u64) { pc += 44 }
    mov64 r4, r7                                    r4 = r7
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    mov64 r2, r7                                    r2 = r7
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    mov64 r4, r2                                    r4 = r2
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    mov64 r4, r2                                    r4 = r2
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    mov64 r4, r2                                    r4 = r2
    rsh64 r4, 8                                     r4 >>= 8   ///  r4 = r4.wrapping_shr(8)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    mov64 r4, r2                                    r4 = r2
    rsh64 r4, 16                                    r4 >>= 16   ///  r4 = r4.wrapping_shr(16)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    mov64 r4, r2                                    r4 = r2
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    lddw r4, 0x5555555555555555                     r4 load str located at 6148914691236517205
    mov64 r5, r2                                    r5 = r2
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    and64 r5, r4                                    r5 &= r4   ///  r5 = r5.and(r4)
    sub64 r2, r5                                    r2 -= r5   ///  r2 = r2.wrapping_sub(r5)
    lddw r5, 0x3333333333333333                     r5 load str located at 3689348814741910323
    mov64 r4, r2                                    r4 = r2
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    and64 r2, r5                                    r2 &= r5   ///  r2 = r2.and(r5)
    ldxdw r5, [r10-0x18]                    
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    mov64 r2, r4                                    r2 = r4
    rsh64 r2, 4                                     r2 >>= 4   ///  r2 = r2.wrapping_shr(4)
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    lddw r2, 0xf0f0f0f0f0f0f0f                      r2 load str located at 1085102592571150095
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    lddw r2, 0x101010101010101                      r2 load str located at 72340172838076673
    mul64 r4, r2                                    r4 *= r2   ///  r4 = r4.wrapping_mul(r2)
    rsh64 r4, 56                                    r4 >>= 56   ///  r4 = r4.wrapping_shr(56)
lbb_21769:
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    add64 r4, 53                                    r4 += 53   ///  r4 = r4.wrapping_add(53 as i32 as i64 as u64)
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    lsh64 r7, r4                                    r7 <<= r4   ///  r7 = r7.wrapping_shl(r4 as u32)
    add64 r1, 1035                                  r1 += 1035   ///  r1 = r1.wrapping_add(1035 as i32 as i64 as u64)
    mov64 r8, r1                                    r8 = r1
lbb_21775:
    lddw r1, 0xfffffffffffff                        r1 load str located at 4503599627370495
    jgt r3, r1, lbb_21558                           if r3 > r1 { pc += -220 }
    mov64 r2, 64                                    r2 = 64 as i32 as i64 as u64
    jeq r5, 0, lbb_21823                            if r5 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r2, r5                                    r2 = r5
    rsh64 r2, 1                                     r2 >>= 1   ///  r2 = r2.wrapping_shr(1)
    mov64 r1, r5                                    r1 = r5
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 4                                     r2 >>= 4   ///  r2 = r2.wrapping_shr(4)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 8                                     r2 >>= 8   ///  r2 = r2.wrapping_shr(8)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 16                                    r2 >>= 16   ///  r2 = r2.wrapping_shr(16)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    xor64 r1, -1                                    r1 ^= -1   ///  r1 = r1.xor(-1)
    lddw r2, 0x5555555555555555                     r2 load str located at 6148914691236517205
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    and64 r3, r2                                    r3 &= r2   ///  r3 = r3.and(r2)
    sub64 r1, r3                                    r1 -= r3   ///  r1 = r1.wrapping_sub(r3)
    lddw r3, 0x3333333333333333                     r3 load str located at 3689348814741910323
    mov64 r2, r1                                    r2 = r1
    and64 r2, r3                                    r2 &= r3   ///  r2 = r2.and(r3)
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    lddw r1, 0xf0f0f0f0f0f0f0f                      r1 load str located at 1085102592571150095
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
    mul64 r2, r1                                    r2 *= r1   ///  r2 = r2.wrapping_mul(r1)
    rsh64 r2, 56                                    r2 >>= 56   ///  r2 = r2.wrapping_shr(56)
lbb_21823:
    mov64 r1, r2                                    r1 = r2
    add64 r1, 53                                    r1 += 53   ///  r1 = r1.wrapping_add(53 as i32 as i64 as u64)
    and64 r1, 63                                    r1 &= 63   ///  r1 = r1.and(63)
    lsh64 r5, r1                                    r5 <<= r1   ///  r5 = r5.wrapping_shl(r1 as u32)
    add64 r2, r8                                    r2 += r8   ///  r2 = r2.wrapping_add(r8)
    add64 r2, -12                                   r2 += -12   ///  r2 = r2.wrapping_add(-12 as i32 as i64 as u64)
    mov64 r8, r2                                    r8 = r2
    ja lbb_21558                                    if true { pc += -273 }
lbb_21831:
    lddw r0, 0x7ff8000000000000                     r0 load str located at 9221120237041090560
    jeq r3, 0, lbb_21715                            if r3 == (0 as i32 as i64 as u64) { pc += -119 }
    ja lbb_21712                                    if true { pc += -123 }

function_21835:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_21844                     
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x8]                     
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_21844:
    mov64 r5, r4                                    r5 = r4
    and64 r5, 64                                    r5 &= 64   ///  r5 = r5.and(64)
    jne r5, 0, lbb_21862                            if r5 != (0 as i32 as i64 as u64) { pc += 15 }
    mov64 r5, r4                                    r5 = r4
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    jeq r5, 0, lbb_21866                            if r5 == (0 as i32 as i64 as u64) { pc += 15 }
    mov64 r5, r4                                    r5 = r4
    and64 r5, 63                                    r5 &= 63   ///  r5 = r5.and(63)
    rsh64 r2, r5                                    r2 >>= r5   ///  r2 = r2.wrapping_shr(r5 as u32)
    neg64 r4                                        r4 = -r4   ///  r4 = (r4 as i64).wrapping_neg() as u64
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    mov64 r0, r3                                    r0 = r3
    lsh64 r0, r4                                    r0 <<= r4   ///  r0 = r0.wrapping_shl(r4 as u32)
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    rsh64 r3, r5                                    r3 >>= r5   ///  r3 = r3.wrapping_shr(r5 as u32)
    mov64 r2, r0                                    r2 = r0
    ja lbb_21866                                    if true { pc += 4 }
lbb_21862:
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    rsh64 r3, r4                                    r3 >>= r4   ///  r3 = r3.wrapping_shr(r4 as u32)
    mov64 r2, r3                                    r2 = r3
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_21866:
    stxdw [r1+0x0], r2                      
    stxdw [r1+0x8], r3                      
    exit                                    

function_21869:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_21878                     
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x8]                     
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_21878:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    lddw r4, 0x3ff0000000000000                     r4 load str located at 4607182418800017408
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jlt r2, r4, lbb_21910                           if r2 < r4 { pc += 26 }
    lddw r3, 0x47f0000000000000                     r3 load str located at 5183643171103440896
    jlt r2, r3, lbb_21895                           if r2 < r3 { pc += 8 }
    lddw r4, 0x7ff0000000000001                     r4 load str located at 9218868437227405313
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jlt r2, r4, lbb_21892                           if r2 < r4 { pc += 1 }
    ja lbb_21910                                    if true { pc += 18 }
lbb_21892:
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
    mov64 r3, -1                                    r3 = -1 as i32 as i64 as u64
    ja lbb_21910                                    if true { pc += 15 }
lbb_21895:
    mov64 r3, r2                                    r3 = r2
    lsh64 r3, 11                                    r3 <<= 11   ///  r3 = r3.wrapping_shl(11)
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    rsh64 r2, 52                                    r2 >>= 52   ///  r2 = r2.wrapping_shr(52)
    mov64 r4, 126                                   r4 = 126 as i32 as i64 as u64
    sub64 r4, r2                                    r4 -= r2   ///  r4 = r4.wrapping_sub(r2)
    and64 r4, 127                                   r4 &= 127   ///  r4 = r4.and(127)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_21835                     
    ldxdw r3, [r10-0x8]                     
    ldxdw r1, [r10-0x10]                    
lbb_21910:
    stxdw [r6+0x8], r3                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_21913:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    jeq r3, 0, lbb_21982                            if r3 == (0 as i32 as i64 as u64) { pc += 63 }
    mov64 r3, r2                                    r3 = r2
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    xor64 r1, r3                                    r1 ^= r3   ///  r1 = r1.xor(r3)
    sub64 r1, r3                                    r1 -= r3   ///  r1 = r1.wrapping_sub(r3)
    mov64 r3, r1                                    r3 = r1
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    or64 r1, r4                                     r1 |= r4   ///  r1 = r1.or(r4)
    lddw r4, 0xfffffffc                             r4 load str located at 4294967292
    mov64 r5, r1                                    r5 = r1
    and64 r5, r4                                    r5 &= r4   ///  r5 = r5.and(r4)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    or64 r1, r5                                     r1 |= r5   ///  r1 = r1.or(r5)
    lddw r4, 0xfffffff0                             r4 load str located at 4294967280
    mov64 r5, r1                                    r5 = r1
    and64 r5, r4                                    r5 &= r4   ///  r5 = r5.and(r4)
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    or64 r1, r5                                     r1 |= r5   ///  r1 = r1.or(r5)
    lddw r4, 0xffffff00                             r4 load str located at 4294967040
    mov64 r5, r1                                    r5 = r1
    and64 r5, r4                                    r5 &= r4   ///  r5 = r5.and(r4)
    rsh64 r5, 8                                     r5 >>= 8   ///  r5 = r5.wrapping_shr(8)
    or64 r1, r5                                     r1 |= r5   ///  r1 = r1.or(r5)
    lddw r4, 0xffff0000                             r4 load str located at 4294901760
    mov64 r5, r1                                    r5 = r1
    and64 r5, r4                                    r5 &= r4   ///  r5 = r5.and(r4)
    rsh64 r5, 16                                    r5 >>= 16   ///  r5 = r5.wrapping_shr(16)
    or64 r1, r5                                     r1 |= r5   ///  r1 = r1.or(r5)
    xor64 r1, -1                                    r1 ^= -1   ///  r1 = r1.xor(-1)
    mov64 r4, r1                                    r4 = r1
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    and64 r4, 1431655765                            r4 &= 1431655765   ///  r4 = r4.and(1431655765)
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    mov64 r4, r1                                    r4 = r1
    and64 r4, 858993459                             r4 &= 858993459   ///  r4 = r4.and(858993459)
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    and64 r1, 858993459                             r1 &= 858993459   ///  r1 = r1.and(858993459)
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    and64 r4, 252645135                             r4 &= 252645135   ///  r4 = r4.and(252645135)
    mul64 r4, 16843009                              r4 *= 16843009   ///  r4 = r4.wrapping_mul(16843009 as u64)
    lddw r1, 0xff000000                             r1 load str located at 4278190080
    and64 r4, r1                                    r4 &= r1   ///  r4 = r4.and(r1)
    rsh64 r4, 24                                    r4 >>= 24   ///  r4 = r4.wrapping_shr(24)
    mov64 r1, r4                                    r1 = r4
    add64 r1, 21                                    r1 += 21   ///  r1 = r1.wrapping_add(21 as i32 as i64 as u64)
    lsh64 r3, r1                                    r3 <<= r1   ///  r3 = r3.wrapping_shl(r1 as u32)
    mov64 r0, 1054                                  r0 = 1054 as i32 as i64 as u64
    sub64 r0, r4                                    r0 -= r4   ///  r0 = r0.wrapping_sub(r4)
    lsh64 r0, 52                                    r0 <<= 52   ///  r0 = r0.wrapping_shl(52)
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    lddw r1, 0xfff0000000000000                     r1 load str located at -4503599627370496
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
lbb_21982:
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    exit                                    

function_21987:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jlt r2, r3, lbb_22127                           if r2 < r3 { pc += 138 }
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    mov64 r6, r3                                    r6 = r3
    or64 r6, r4                                     r6 |= r4   ///  r6 = r6.or(r4)
    mov64 r4, r6                                    r4 = r6
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    or64 r6, r4                                     r6 |= r4   ///  r6 = r6.or(r4)
    mov64 r4, r6                                    r4 = r6
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    or64 r6, r4                                     r6 |= r4   ///  r6 = r6.or(r4)
    mov64 r4, r6                                    r4 = r6
    rsh64 r4, 8                                     r4 >>= 8   ///  r4 = r4.wrapping_shr(8)
    or64 r6, r4                                     r6 |= r4   ///  r6 = r6.or(r4)
    mov64 r4, r6                                    r4 = r6
    rsh64 r4, 16                                    r4 >>= 16   ///  r4 = r4.wrapping_shr(16)
    or64 r6, r4                                     r6 |= r4   ///  r6 = r6.or(r4)
    mov64 r4, r6                                    r4 = r6
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    or64 r6, r4                                     r6 |= r4   ///  r6 = r6.or(r4)
    xor64 r6, -1                                    r6 ^= -1   ///  r6 = r6.xor(-1)
    lddw r0, 0x5555555555555555                     r0 load str located at 6148914691236517205
    mov64 r4, r6                                    r4 = r6
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    and64 r4, r0                                    r4 &= r0   ///  r4 = r4.and(r0)
    sub64 r6, r4                                    r6 -= r4   ///  r6 = r6.wrapping_sub(r4)
    lddw r5, 0x3333333333333333                     r5 load str located at 3689348814741910323
    mov64 r4, r6                                    r4 = r6
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    rsh64 r6, 2                                     r6 >>= 2   ///  r6 = r6.wrapping_shr(2)
    and64 r6, r5                                    r6 &= r5   ///  r6 = r6.and(r5)
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    mov64 r6, r4                                    r6 = r4
    rsh64 r6, 4                                     r6 >>= 4   ///  r6 = r6.wrapping_shr(4)
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    lddw r6, 0xf0f0f0f0f0f0f0f                      r6 load str located at 1085102592571150095
    and64 r4, r6                                    r4 &= r6   ///  r4 = r4.and(r6)
    lddw r7, 0x101010101010101                      r7 load str located at 72340172838076673
    mul64 r4, r7                                    r4 *= r7   ///  r4 = r4.wrapping_mul(r7)
    mov64 r9, 64                                    r9 = 64 as i32 as i64 as u64
    rsh64 r4, 56                                    r4 >>= 56   ///  r4 = r4.wrapping_shr(56)
    jeq r2, 0, lbb_22069                            if r2 == (0 as i32 as i64 as u64) { pc += 35 }
    mov64 r9, r2                                    r9 = r2
    rsh64 r9, 1                                     r9 >>= 1   ///  r9 = r9.wrapping_shr(1)
    mov64 r8, r2                                    r8 = r2
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    mov64 r9, r8                                    r9 = r8
    rsh64 r9, 2                                     r9 >>= 2   ///  r9 = r9.wrapping_shr(2)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    mov64 r9, r8                                    r9 = r8
    rsh64 r9, 4                                     r9 >>= 4   ///  r9 = r9.wrapping_shr(4)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    mov64 r9, r8                                    r9 = r8
    rsh64 r9, 8                                     r9 >>= 8   ///  r9 = r9.wrapping_shr(8)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    mov64 r9, r8                                    r9 = r8
    rsh64 r9, 16                                    r9 >>= 16   ///  r9 = r9.wrapping_shr(16)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    mov64 r9, r8                                    r9 = r8
    rsh64 r9, 32                                    r9 >>= 32   ///  r9 = r9.wrapping_shr(32)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    xor64 r8, -1                                    r8 ^= -1   ///  r8 = r8.xor(-1)
    mov64 r9, r8                                    r9 = r8
    rsh64 r9, 1                                     r9 >>= 1   ///  r9 = r9.wrapping_shr(1)
    and64 r9, r0                                    r9 &= r0   ///  r9 = r9.and(r0)
    sub64 r8, r9                                    r8 -= r9   ///  r8 = r8.wrapping_sub(r9)
    mov64 r9, r8                                    r9 = r8
    and64 r9, r5                                    r9 &= r5   ///  r9 = r9.and(r5)
    rsh64 r8, 2                                     r8 >>= 2   ///  r8 = r8.wrapping_shr(2)
    and64 r8, r5                                    r8 &= r5   ///  r8 = r8.and(r5)
    add64 r9, r8                                    r9 += r8   ///  r9 = r9.wrapping_add(r8)
    mov64 r5, r9                                    r5 = r9
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    add64 r9, r5                                    r9 += r5   ///  r9 = r9.wrapping_add(r5)
    and64 r9, r6                                    r9 &= r6   ///  r9 = r9.and(r6)
    mul64 r9, r7                                    r9 *= r7   ///  r9 = r9.wrapping_mul(r7)
    rsh64 r9, 56                                    r9 >>= 56   ///  r9 = r9.wrapping_shr(56)
lbb_22069:
    sub64 r4, r9                                    r4 -= r9   ///  r4 = r4.wrapping_sub(r9)
    mov64 r5, r4                                    r5 = r4
    and64 r5, 63                                    r5 &= 63   ///  r5 = r5.and(63)
    mov64 r0, r3                                    r0 = r3
    lsh64 r0, r5                                    r0 <<= r5   ///  r0 = r0.wrapping_shl(r5 as u32)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jgt r0, r2, lbb_22078                           if r0 > r2 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_22078:
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    sub64 r4, r5                                    r4 -= r5   ///  r4 = r4.wrapping_sub(r5)
    mov64 r0, r4                                    r0 = r4
    and64 r0, 63                                    r0 &= 63   ///  r0 = r0.and(63)
    lsh64 r8, r0                                    r8 <<= r0   ///  r8 = r8.wrapping_shl(r0 as u32)
    mov64 r5, r3                                    r5 = r3
    lsh64 r5, r0                                    r5 <<= r0   ///  r5 = r5.wrapping_shl(r0 as u32)
    sub64 r2, r5                                    r2 -= r5   ///  r2 = r2.wrapping_sub(r5)
    jlt r2, r3, lbb_22127                           if r2 < r3 { pc += 39 }
    mov64 r0, r8                                    r0 = r8
    mov64 r6, r8                                    r6 = r8
    mov64 r7, r2                                    r7 = r2
    jsgt r5, -1, lbb_22109                          if (r5 as i64) > (-1 as i32 as i64) { pc += 17 }
    add64 r4, -1                                    r4 += -1   ///  r4 = r4.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r6, r4                                    r6 = r4
    and64 r6, 63                                    r6 &= 63   ///  r6 = r6.and(63)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    lsh64 r0, r6                                    r0 <<= r6   ///  r0 = r0.wrapping_shl(r6 as u32)
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    mov64 r7, r2                                    r7 = r2
    sub64 r7, r5                                    r7 -= r5   ///  r7 = r7.wrapping_sub(r5)
    mov64 r6, r0                                    r6 = r0
    jsgt r7, -1, lbb_22103                          if (r7 as i64) > (-1 as i32 as i64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_22103:
    jsgt r7, -1, lbb_22105                          if (r7 as i64) > (-1 as i32 as i64) { pc += 1 }
    mov64 r7, r2                                    r7 = r2
lbb_22105:
    or64 r6, r8                                     r6 |= r8   ///  r6 = r6.or(r8)
    mov64 r2, r7                                    r2 = r7
    mov64 r8, r6                                    r8 = r6
    jlt r7, r3, lbb_22127                           if r7 < r3 { pc += 18 }
lbb_22109:
    add64 r0, -1                                    r0 += -1   ///  r0 = r0.wrapping_add(-1 as i32 as i64 as u64)
    jeq r4, 0, lbb_22121                            if r4 == (0 as i32 as i64 as u64) { pc += 10 }
    mov64 r2, r4                                    r2 = r4
lbb_22112:
    lsh64 r7, 1                                     r7 <<= 1   ///  r7 = r7.wrapping_shl(1)
    mov64 r3, r7                                    r3 = r7
    sub64 r3, r5                                    r3 -= r5   ///  r3 = r3.wrapping_sub(r5)
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    jslt r3, 0, lbb_22118                           if (r3 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r7, r3                                    r7 = r3
lbb_22118:
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    jeq r2, 0, lbb_22121                            if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_22112                                    if true { pc += -9 }
lbb_22121:
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    mov64 r2, r7                                    r2 = r7
    rsh64 r2, r4                                    r2 >>= r4   ///  r2 = r2.wrapping_shr(r4 as u32)
    and64 r7, r0                                    r7 &= r0   ///  r7 = r7.and(r0)
    or64 r7, r6                                     r7 |= r6   ///  r7 = r7.or(r6)
    mov64 r8, r7                                    r8 = r7
lbb_22127:
    stxdw [r1+0x8], r2                      
    stxdw [r1+0x0], r8                      
    exit                                    

function_22130:
    mov64 r0, 3                                     r0 = 3 as i32 as i64 as u64
    lddw r5, 0x7fffffffffffffff                     r5 load str located at 9223372036854775807
    mov64 r3, r1                                    r3 = r1
    and64 r3, r5                                    r3 &= r5   ///  r3 = r3.and(r5)
    lddw r6, 0x7ff0000000000000                     r6 load str located at 9218868437227405312
    jgt r3, r6, lbb_22157                           if r3 > r6 { pc += 19 }
    mov64 r4, r2                                    r4 = r2
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    jgt r4, r6, lbb_22157                           if r4 > r6 { pc += 16 }
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r4, 0, lbb_22157                            if r4 == (0 as i32 as i64 as u64) { pc += 13 }
    mov64 r3, r2                                    r3 = r2
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    jsgt r3, -1, lbb_22152                          if (r3 as i64) > (-1 as i32 as i64) { pc += 5 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jsgt r1, r2, lbb_22157                          if (r1 as i64) > (r2 as i64) { pc += 8 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_22157                           if r1 == r2 { pc += 6 }
    ja lbb_22156                                    if true { pc += 4 }
lbb_22152:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jslt r1, r2, lbb_22157                          if (r1 as i64) < (r2 as i64) { pc += 3 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_22157                           if r1 == r2 { pc += 1 }
lbb_22156:
    mov64 r0, 2                                     r0 = 2 as i32 as i64 as u64
lbb_22157:
    exit                                    

function_22158:
    call function_22130                     
    mov64 r1, r0                                    r1 = r0
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    jsgt r1, 1, lbb_22167                           if (r1 as i64) > (1 as i32 as i64) { pc += 3 }
    jeq r1, 0, lbb_22170                            if r1 == (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_22170                                    if true { pc += 3 }
lbb_22167:
    jeq r1, 2, lbb_22169                            if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_22170                                    if true { pc += 1 }
lbb_22169:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_22170:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    exit                                    

function_22173:
    call function_22130                     
    mov64 r1, r0                                    r1 = r0
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    jsgt r1, 1, lbb_22182                           if (r1 as i64) > (1 as i32 as i64) { pc += 5 }
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    jeq r1, 0, lbb_22184                            if r1 == (0 as i32 as i64 as u64) { pc += 4 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_22184                                    if true { pc += 2 }
lbb_22182:
    jeq r1, 2, lbb_22183                            if r1 == (2 as i32 as i64 as u64) { pc += 0 }
lbb_22183:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_22184:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    exit                                    

function_22187:
    stxdw [r10-0x10], r3                    
    stxdw [r10-0x8], r1                     
    mov64 r1, r2                                    r1 = r2
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r7, r4                                    r7 = r4
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r6, r2                                    r6 = r2
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    mov64 r3, r7                                    r3 = r7
    mul64 r3, r1                                    r3 *= r1   ///  r3 = r3.wrapping_mul(r1)
    mul64 r7, r6                                    r7 *= r6   ///  r7 = r7.wrapping_mul(r6)
    mov64 r0, r4                                    r0 = r4
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r9, r0                                    r9 = r0
    mul64 r9, r1                                    r9 *= r1   ///  r9 = r9.wrapping_mul(r1)
    mov64 r1, r9                                    r1 = r9
    add64 r1, r7                                    r1 += r7   ///  r1 = r1.wrapping_add(r7)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jlt r1, r9, lbb_22209                           if r1 < r9 { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_22209:
    mov64 r9, r1                                    r9 = r1
    lsh64 r9, 32                                    r9 <<= 32   ///  r9 = r9.wrapping_shl(32)
    mov64 r7, r3                                    r7 = r3
    add64 r7, r9                                    r7 += r9   ///  r7 = r7.wrapping_add(r9)
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jlt r7, r3, lbb_22216                           if r7 < r3 { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_22216:
    ldxdw r3, [r10-0x8]                     
    stxdw [r3+0x0], r7                      
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    or64 r8, r1                                     r8 |= r1   ///  r8 = r8.or(r1)
    ldxdw r1, [r10-0x10]                    
    mul64 r4, r1                                    r4 *= r1   ///  r4 = r4.wrapping_mul(r1)
    mul64 r5, r2                                    r5 *= r2   ///  r5 = r5.wrapping_mul(r2)
    mul64 r0, r6                                    r0 *= r6   ///  r0 = r0.wrapping_mul(r6)
    add64 r0, r8                                    r0 += r8   ///  r0 = r0.wrapping_add(r8)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    add64 r0, r9                                    r0 += r9   ///  r0 = r0.wrapping_add(r9)
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    stxdw [r3+0x8], r0                      
    exit                                    

function_22231:
    mov64 r6, r2                                    r6 = r2
    mov64 r7, r1                                    r7 = r1
    arsh64 r1, 63                                   r1 >>= 63 (signed)   ///  r1 = (r1 as i64).wrapping_shr(63)
    mov64 r2, r7                                    r2 = r7
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    sub64 r2, r1                                    r2 -= r1   ///  r2 = r2.wrapping_sub(r1)
    mov64 r1, r6                                    r1 = r6
    arsh64 r1, 63                                   r1 >>= 63 (signed)   ///  r1 = (r1 as i64).wrapping_shr(63)
    mov64 r3, r6                                    r3 = r6
    xor64 r3, r1                                    r3 ^= r1   ///  r3 = r3.xor(r1)
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_21987                     
    xor64 r6, r7                                    r6 ^= r7   ///  r6 = r6.xor(r7)
    ldxdw r1, [r10-0x10]                    
    mov64 r0, r1                                    r0 = r1
    neg64 r0                                        r0 = -r0   ///  r0 = (r0 as i64).wrapping_neg() as u64
    jslt r6, 0, lbb_22251                           if (r6 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r0, r1                                    r0 = r1
lbb_22251:
    exit                                    

function_22252:
    call function_22173                     
    exit                                    

function_22254:
    call function_22256                     
    exit                                    

function_22256:
    mov64 r3, r2                                    r3 = r2
    mov64 r0, r1                                    r0 = r1
    mov64 r6, r3                                    r6 = r3
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    and64 r6, r1                                    r6 &= r1   ///  r6 = r6.and(r1)
    lddw r1, 0xfffffffffffff                        r1 load str located at 4503599627370495
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    mov64 r4, r0                                    r4 = r0
    and64 r4, r1                                    r4 &= r1   ///  r4 = r4.and(r1)
    mov64 r7, r3                                    r7 = r3
    rsh64 r7, 52                                    r7 >>= 52   ///  r7 = r7.wrapping_shr(52)
    and64 r7, 2047                                  r7 &= 2047   ///  r7 = r7.and(2047)
    mov64 r8, r0                                    r8 = r0
    rsh64 r8, 52                                    r8 >>= 52   ///  r8 = r8.wrapping_shr(52)
    and64 r8, 2047                                  r8 &= 2047   ///  r8 = r8.and(2047)
    mov64 r1, r8                                    r1 = r8
    add64 r1, -2047                                 r1 += -2047   ///  r1 = r1.wrapping_add(-2047 as i32 as i64 as u64)
    jlt r1, -2046, lbb_22309                        if r1 < (-2046 as i32 as i64 as u64) { pc += 32 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r1, r7                                    r1 = r7
    add64 r1, -2047                                 r1 += -2047   ///  r1 = r1.wrapping_add(-2047 as i32 as i64 as u64)
    jlt r1, -2046, lbb_22309                        if r1 < (-2046 as i32 as i64 as u64) { pc += 28 }
lbb_22281:
    stxdw [r10-0x18], r6                    
    lsh64 r2, 11                                    r2 <<= 11   ///  r2 = r2.wrapping_shl(11)
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    lddw r6, 0x10000000000000                       r6 load str located at 4503599627370496
    or64 r4, r6                                     r4 |= r6   ///  r4 = r4.or(r6)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_22896                     
    add64 r7, r8                                    r7 += r8   ///  r7 = r7.wrapping_add(r8)
    add64 r7, r9                                    r7 += r9   ///  r7 = r7.wrapping_add(r9)
    ldxdw r1, [r10-0x8]                     
    mov64 r3, r1                                    r3 = r1
    and64 r3, r6                                    r3 &= r6   ///  r3 = r3.and(r6)
    ldxdw r2, [r10-0x10]                    
    jeq r3, 0, lbb_22302                            if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_22325                                    if true { pc += 23 }
lbb_22302:
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 63                                    r3 >>= 63   ///  r3 = r3.wrapping_shr(63)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    add64 r7, -1023                                 r7 += -1023   ///  r7 = r7.wrapping_add(-1023 as i32 as i64 as u64)
    ja lbb_22326                                    if true { pc += 17 }
lbb_22309:
    lddw r9, 0x7fffffffffffffff                     r9 load str located at 9223372036854775807
    mov64 r5, r0                                    r5 = r0
    and64 r5, r9                                    r5 &= r9   ///  r5 = r5.and(r9)
    lddw r1, 0x7ff0000000000000                     r1 load str located at 9218868437227405312
    jgt r5, r1, lbb_22336                           if r5 > r1 { pc += 20 }
    mov64 r0, r3                                    r0 = r3
    and64 r0, r9                                    r0 &= r9   ///  r0 = r0.and(r9)
    jgt r0, r1, lbb_22320                           if r0 > r1 { pc += 1 }
    ja lbb_22340                                    if true { pc += 20 }
lbb_22320:
    lddw r1, 0x8000000000000                        r1 load str located at 2251799813685248
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r0, r3                                    r0 = r3
    ja lbb_22379                                    if true { pc += 54 }
lbb_22325:
    add64 r7, -1022                                 r7 += -1022   ///  r7 = r7.wrapping_add(-1022 as i32 as i64 as u64)
lbb_22326:
    ldxdw r6, [r10-0x18]                    
    jsgt r7, 2046, lbb_22346                        if (r7 as i64) > (2046 as i32 as i64) { pc += 18 }
    jslt r7, 1, lbb_22351                           if (r7 as i64) < (1 as i32 as i64) { pc += 22 }
    lddw r3, 0xfffffffffffff                        r3 load str located at 4503599627370495
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    lsh64 r7, 52                                    r7 <<= 52   ///  r7 = r7.wrapping_shl(52)
    or64 r7, r1                                     r7 |= r1   ///  r7 = r7.or(r1)
    mov64 r1, r7                                    r1 = r7
    ja lbb_22368                                    if true { pc += 32 }
lbb_22336:
    lddw r1, 0x8000000000000                        r1 load str located at 2251799813685248
    or64 r0, r1                                     r0 |= r1   ///  r0 = r0.or(r1)
    ja lbb_22379                                    if true { pc += 39 }
lbb_22340:
    jeq r5, r1, lbb_22342                           if r5 == r1 { pc += 1 }
    ja lbb_22380                                    if true { pc += 38 }
lbb_22342:
    mov64 r1, r0                                    r1 = r0
    lddw r0, 0x7ff8000000000000                     r0 load str located at 9221120237041090560
    jeq r1, 0, lbb_22379                            if r1 == (0 as i32 as i64 as u64) { pc += 33 }
lbb_22346:
    lddw r1, 0x7ff0000000000000                     r1 load str located at 9218868437227405312
lbb_22348:
    or64 r6, r1                                     r6 |= r1   ///  r6 = r6.or(r1)
lbb_22349:
    mov64 r0, r6                                    r0 = r6
    ja lbb_22379                                    if true { pc += 28 }
lbb_22351:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    sub64 r3, r7                                    r3 -= r7   ///  r3 = r3.wrapping_sub(r7)
    jgt r3, 63, lbb_22349                           if r3 > (63 as i32 as i64 as u64) { pc += -6 }
    add64 r7, 63                                    r7 += 63   ///  r7 = r7.wrapping_add(63 as i32 as i64 as u64)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r5, r2                                    r5 = r2
    lsh64 r5, r7                                    r5 <<= r7   ///  r5 = r5.wrapping_shl(r7 as u32)
    jne r5, 0, lbb_22362                            if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_22362:
    rsh64 r2, r3                                    r2 >>= r3   ///  r2 = r2.wrapping_shr(r3 as u32)
    mov64 r5, r1                                    r5 = r1
    lsh64 r5, r7                                    r5 <<= r7   ///  r5 = r5.wrapping_shl(r7 as u32)
    or64 r2, r5                                     r2 |= r5   ///  r2 = r2.or(r5)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    rsh64 r1, r3                                    r1 >>= r3   ///  r1 = r1.wrapping_shr(r3 as u32)
lbb_22368:
    mov64 r0, r1                                    r0 = r1
    or64 r0, r6                                     r0 |= r6   ///  r0 = r0.or(r6)
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    jgt r2, r3, lbb_22374                           if r2 > r3 { pc += 1 }
    ja lbb_22376                                    if true { pc += 2 }
lbb_22374:
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    ja lbb_22379                                    if true { pc += 3 }
lbb_22376:
    jne r2, r3, lbb_22379                           if r2 != r3 { pc += 2 }
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
lbb_22379:
    exit                                    
lbb_22380:
    jeq r0, r1, lbb_22382                           if r0 == r1 { pc += 1 }
    ja lbb_22386                                    if true { pc += 4 }
lbb_22382:
    lddw r0, 0x7ff8000000000000                     r0 load str located at 9221120237041090560
    jeq r5, 0, lbb_22379                            if r5 == (0 as i32 as i64 as u64) { pc += -6 }
    ja lbb_22348                                    if true { pc += -38 }
lbb_22386:
    jeq r5, 0, lbb_22349                            if r5 == (0 as i32 as i64 as u64) { pc += -38 }
    jeq r0, 0, lbb_22349                            if r0 == (0 as i32 as i64 as u64) { pc += -39 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    lddw r1, 0x10000000000000                       r1 load str located at 4503599627370496
    jlt r5, r1, lbb_22393                           if r5 < r1 { pc += 1 }
    ja lbb_22443                                    if true { pc += 50 }
lbb_22393:
    mov64 r5, 64                                    r5 = 64 as i32 as i64 as u64
    jeq r4, 0, lbb_22438                            if r4 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
    mov64 r3, r4                                    r3 = r4
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 8                                     r1 >>= 8   ///  r1 = r1.wrapping_shr(8)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 16                                    r1 >>= 16   ///  r1 = r1.wrapping_shr(16)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    lddw r1, 0x5555555555555555                     r1 load str located at 6148914691236517205
    mov64 r5, r3                                    r5 = r3
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    and64 r5, r1                                    r5 &= r1   ///  r5 = r5.and(r1)
    sub64 r3, r5                                    r3 -= r5   ///  r3 = r3.wrapping_sub(r5)
    lddw r1, 0x3333333333333333                     r1 load str located at 3689348814741910323
    mov64 r5, r3                                    r5 = r3
    and64 r5, r1                                    r5 &= r1   ///  r5 = r5.and(r1)
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    add64 r5, r3                                    r5 += r3   ///  r5 = r5.wrapping_add(r3)
    mov64 r1, r5                                    r1 = r5
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    add64 r5, r1                                    r5 += r1   ///  r5 = r5.wrapping_add(r1)
    lddw r1, 0xf0f0f0f0f0f0f0f                      r1 load str located at 1085102592571150095
    and64 r5, r1                                    r5 &= r1   ///  r5 = r5.and(r1)
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
    mul64 r5, r1                                    r5 *= r1   ///  r5 = r5.wrapping_mul(r1)
    rsh64 r5, 56                                    r5 >>= 56   ///  r5 = r5.wrapping_shr(56)
lbb_22438:
    mov64 r9, 12                                    r9 = 12 as i32 as i64 as u64
    sub64 r9, r5                                    r9 -= r5   ///  r9 = r9.wrapping_sub(r5)
    add64 r5, 53                                    r5 += 53   ///  r5 = r5.wrapping_add(53 as i32 as i64 as u64)
    and64 r5, 63                                    r5 &= 63   ///  r5 = r5.and(63)
    lsh64 r4, r5                                    r4 <<= r5   ///  r4 = r4.wrapping_shl(r5 as u32)
lbb_22443:
    lddw r1, 0xfffffffffffff                        r1 load str located at 4503599627370495
    jgt r0, r1, lbb_22281                           if r0 > r1 { pc += -165 }
    mov64 r3, 64                                    r3 = 64 as i32 as i64 as u64
    jeq r2, 0, lbb_22491                            if r2 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    mov64 r1, r2                                    r1 = r2
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 16                                    r3 >>= 16   ///  r3 = r3.wrapping_shr(16)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    xor64 r1, -1                                    r1 ^= -1   ///  r1 = r1.xor(-1)
    lddw r3, 0x5555555555555555                     r3 load str located at 6148914691236517205
    mov64 r5, r1                                    r5 = r1
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    and64 r5, r3                                    r5 &= r3   ///  r5 = r5.and(r3)
    sub64 r1, r5                                    r1 -= r5   ///  r1 = r1.wrapping_sub(r5)
    lddw r5, 0x3333333333333333                     r5 load str located at 3689348814741910323
    mov64 r3, r1                                    r3 = r1
    and64 r3, r5                                    r3 &= r5   ///  r3 = r3.and(r5)
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    and64 r1, r5                                    r1 &= r5   ///  r1 = r1.and(r5)
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    lddw r1, 0xf0f0f0f0f0f0f0f                      r1 load str located at 1085102592571150095
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
    mul64 r3, r1                                    r3 *= r1   ///  r3 = r3.wrapping_mul(r1)
    rsh64 r3, 56                                    r3 >>= 56   ///  r3 = r3.wrapping_shr(56)
lbb_22491:
    sub64 r9, r3                                    r9 -= r3   ///  r9 = r9.wrapping_sub(r3)
    add64 r3, 53                                    r3 += 53   ///  r3 = r3.wrapping_add(53 as i32 as i64 as u64)
    and64 r3, 63                                    r3 &= 63   ///  r3 = r3.and(63)
    lsh64 r2, r3                                    r2 <<= r3   ///  r2 = r2.wrapping_shl(r3 as u32)
    add64 r9, 12                                    r9 += 12   ///  r9 = r9.wrapping_add(12 as i32 as i64 as u64)
    ja lbb_22281                                    if true { pc += -216 }

function_22497:
    lddw r3, 0x7fffffffffffffff                     r3 load str located at 9223372036854775807
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    and64 r2, r3                                    r2 &= r3   ///  r2 = r2.and(r3)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    lddw r4, 0x7ff0000000000000                     r4 load str located at 9218868437227405312
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jgt r2, r4, lbb_22507                           if r2 > r4 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_22507:
    jgt r1, r4, lbb_22509                           if r1 > r4 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_22509:
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    exit                                    

function_22511:
    call function_22158                     
    exit                                    

function_22513:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_22575                            if r1 == (0 as i32 as i64 as u64) { pc += 60 }
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    mov64 r2, r1                                    r2 = r1
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 16                                    r3 >>= 16   ///  r3 = r3.wrapping_shr(16)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    lddw r3, 0x5555555555555555                     r3 load str located at 6148914691236517205
    mov64 r4, r2                                    r4 = r2
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    sub64 r2, r4                                    r2 -= r4   ///  r2 = r2.wrapping_sub(r4)
    lddw r4, 0x3333333333333333                     r4 load str located at 3689348814741910323
    mov64 r3, r2                                    r3 = r2
    and64 r3, r4                                    r3 &= r4   ///  r3 = r3.and(r4)
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    and64 r2, r4                                    r2 &= r4   ///  r2 = r2.and(r4)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    mov64 r2, r3                                    r2 = r3
    rsh64 r2, 4                                     r2 >>= 4   ///  r2 = r2.wrapping_shr(4)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    lddw r2, 0xf0f0f0f0f0f0f0f                      r2 load str located at 1085102592571150095
    and64 r3, r2                                    r3 &= r2   ///  r3 = r3.and(r2)
    lddw r2, 0x101010101010101                      r2 load str located at 72340172838076673
    mul64 r3, r2                                    r3 *= r2   ///  r3 = r3.wrapping_mul(r2)
    rsh64 r3, 56                                    r3 >>= 56   ///  r3 = r3.wrapping_shr(56)
    lsh64 r1, r3                                    r1 <<= r3   ///  r1 = r1.wrapping_shl(r3 as u32)
    lsh64 r3, 52                                    r3 <<= 52   ///  r3 = r3.wrapping_shl(52)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 11                                    r2 >>= 11   ///  r2 = r2.wrapping_shr(11)
    mov64 r0, r2                                    r0 = r2
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    lsh64 r1, 53                                    r1 <<= 53   ///  r1 = r1.wrapping_shl(53)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 63                                    r3 >>= 63   ///  r3 = r3.wrapping_shr(63)
    and64 r3, r2                                    r3 &= r2   ///  r3 = r3.and(r2)
    sub64 r1, r3                                    r1 -= r3   ///  r1 = r1.wrapping_sub(r3)
    rsh64 r1, 63                                    r1 >>= 63   ///  r1 = r1.wrapping_shr(63)
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
    lddw r1, 0x43d0000000000000                     r1 load str located at 4886405595696988160
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
lbb_22575:
    exit                                    

function_22576:
    call function_22578                     
    exit                                    

function_22578:
    lddw r3, 0x7fffffffffffffff                     r3 load str located at 9223372036854775807
    mov64 r4, r2                                    r4 = r2
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    mov64 r5, r1                                    r5 = r1
    and64 r5, r3                                    r5 &= r3   ///  r5 = r5.and(r3)
    lddw r3, 0x8010000000000000                     r3 load str located at -9218868437227405312
    mov64 r6, r5                                    r6 = r5
    add64 r6, r3                                    r6 += r3   ///  r6 = r6.wrapping_add(r3)
    lddw r0, 0x8010000000000001                     r0 load str located at -9218868437227405311
    jlt r6, r0, lbb_22750                           if r6 < r0 { pc += 159 }
    mov64 r6, r4                                    r6 = r4
    add64 r6, r3                                    r6 += r3   ///  r6 = r6.wrapping_add(r3)
    jlt r6, r0, lbb_22750                           if r6 < r0 { pc += 156 }
lbb_22594:
    mov64 r3, r2                                    r3 = r2
    jgt r4, r5, lbb_22597                           if r4 > r5 { pc += 1 }
    mov64 r3, r1                                    r3 = r1
lbb_22597:
    mov64 r0, r1                                    r0 = r1
    jgt r4, r5, lbb_22600                           if r4 > r5 { pc += 1 }
    mov64 r0, r2                                    r0 = r2
lbb_22600:
    lddw r7, 0xfffffffffffff                        r7 load str located at 4503599627370495
    mov64 r5, r3                                    r5 = r3
    and64 r5, r7                                    r5 &= r7   ///  r5 = r5.and(r7)
    mov64 r6, r0                                    r6 = r0
    rsh64 r6, 52                                    r6 >>= 52   ///  r6 = r6.wrapping_shr(52)
    and64 r6, 2047                                  r6 &= 2047   ///  r6 = r6.and(2047)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 52                                    r4 >>= 52   ///  r4 = r4.wrapping_shr(52)
    and64 r4, 2047                                  r4 &= 2047   ///  r4 = r4.and(2047)
    jeq r4, 0, lbb_22612                            if r4 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_22662                                    if true { pc += 50 }
lbb_22612:
    mov64 r8, 64                                    r8 = 64 as i32 as i64 as u64
    jeq r5, 0, lbb_22657                            if r5 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r8, r5                                    r8 = r5
    rsh64 r8, 1                                     r8 >>= 1   ///  r8 = r8.wrapping_shr(1)
    mov64 r4, r5                                    r4 = r5
    or64 r4, r8                                     r4 |= r8   ///  r4 = r4.or(r8)
    mov64 r8, r4                                    r8 = r4
    rsh64 r8, 2                                     r8 >>= 2   ///  r8 = r8.wrapping_shr(2)
    or64 r4, r8                                     r4 |= r8   ///  r4 = r4.or(r8)
    mov64 r8, r4                                    r8 = r4
    rsh64 r8, 4                                     r8 >>= 4   ///  r8 = r8.wrapping_shr(4)
    or64 r4, r8                                     r4 |= r8   ///  r4 = r4.or(r8)
    mov64 r8, r4                                    r8 = r4
    rsh64 r8, 8                                     r8 >>= 8   ///  r8 = r8.wrapping_shr(8)
    or64 r4, r8                                     r4 |= r8   ///  r4 = r4.or(r8)
    mov64 r8, r4                                    r8 = r4
    rsh64 r8, 16                                    r8 >>= 16   ///  r8 = r8.wrapping_shr(16)
    or64 r4, r8                                     r4 |= r8   ///  r4 = r4.or(r8)
    mov64 r8, r4                                    r8 = r4
    rsh64 r8, 32                                    r8 >>= 32   ///  r8 = r8.wrapping_shr(32)
    or64 r4, r8                                     r4 |= r8   ///  r4 = r4.or(r8)
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    lddw r8, 0x5555555555555555                     r8 load str located at 6148914691236517205
    mov64 r9, r4                                    r9 = r4
    rsh64 r9, 1                                     r9 >>= 1   ///  r9 = r9.wrapping_shr(1)
    and64 r9, r8                                    r9 &= r8   ///  r9 = r9.and(r8)
    sub64 r4, r9                                    r4 -= r9   ///  r4 = r4.wrapping_sub(r9)
    lddw r9, 0x3333333333333333                     r9 load str located at 3689348814741910323
    mov64 r8, r4                                    r8 = r4
    and64 r8, r9                                    r8 &= r9   ///  r8 = r8.and(r9)
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    and64 r4, r9                                    r4 &= r9   ///  r4 = r4.and(r9)
    add64 r8, r4                                    r8 += r4   ///  r8 = r8.wrapping_add(r4)
    mov64 r4, r8                                    r4 = r8
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    add64 r8, r4                                    r8 += r4   ///  r8 = r8.wrapping_add(r4)
    lddw r4, 0xf0f0f0f0f0f0f0f                      r4 load str located at 1085102592571150095
    and64 r8, r4                                    r8 &= r4   ///  r8 = r8.and(r4)
    lddw r4, 0x101010101010101                      r4 load str located at 72340172838076673
    mul64 r8, r4                                    r8 *= r4   ///  r8 = r8.wrapping_mul(r4)
    rsh64 r8, 56                                    r8 >>= 56   ///  r8 = r8.wrapping_shr(56)
lbb_22657:
    mov64 r4, 12                                    r4 = 12 as i32 as i64 as u64
    sub64 r4, r8                                    r4 -= r8   ///  r4 = r4.wrapping_sub(r8)
    add64 r8, 53                                    r8 += 53   ///  r8 = r8.wrapping_add(53 as i32 as i64 as u64)
    and64 r8, 63                                    r8 &= 63   ///  r8 = r8.and(63)
    lsh64 r5, r8                                    r5 <<= r8   ///  r5 = r5.wrapping_shl(r8 as u32)
lbb_22662:
    and64 r0, r7                                    r0 &= r7   ///  r0 = r0.and(r7)
    jne r6, 0, lbb_22714                            if r6 != (0 as i32 as i64 as u64) { pc += 50 }
    mov64 r7, 64                                    r7 = 64 as i32 as i64 as u64
    jeq r0, 0, lbb_22709                            if r0 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r7, r0                                    r7 = r0
    rsh64 r7, 1                                     r7 >>= 1   ///  r7 = r7.wrapping_shr(1)
    mov64 r6, r0                                    r6 = r0
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    mov64 r7, r6                                    r7 = r6
    rsh64 r7, 2                                     r7 >>= 2   ///  r7 = r7.wrapping_shr(2)
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    mov64 r7, r6                                    r7 = r6
    rsh64 r7, 4                                     r7 >>= 4   ///  r7 = r7.wrapping_shr(4)
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    mov64 r7, r6                                    r7 = r6
    rsh64 r7, 8                                     r7 >>= 8   ///  r7 = r7.wrapping_shr(8)
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    mov64 r7, r6                                    r7 = r6
    rsh64 r7, 16                                    r7 >>= 16   ///  r7 = r7.wrapping_shr(16)
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    mov64 r7, r6                                    r7 = r6
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    xor64 r6, -1                                    r6 ^= -1   ///  r6 = r6.xor(-1)
    lddw r7, 0x5555555555555555                     r7 load str located at 6148914691236517205
    mov64 r8, r6                                    r8 = r6
    rsh64 r8, 1                                     r8 >>= 1   ///  r8 = r8.wrapping_shr(1)
    and64 r8, r7                                    r8 &= r7   ///  r8 = r8.and(r7)
    sub64 r6, r8                                    r6 -= r8   ///  r6 = r6.wrapping_sub(r8)
    lddw r8, 0x3333333333333333                     r8 load str located at 3689348814741910323
    mov64 r7, r6                                    r7 = r6
    and64 r7, r8                                    r7 &= r8   ///  r7 = r7.and(r8)
    rsh64 r6, 2                                     r6 >>= 2   ///  r6 = r6.wrapping_shr(2)
    and64 r6, r8                                    r6 &= r8   ///  r6 = r6.and(r8)
    add64 r7, r6                                    r7 += r6   ///  r7 = r7.wrapping_add(r6)
    mov64 r6, r7                                    r6 = r7
    rsh64 r6, 4                                     r6 >>= 4   ///  r6 = r6.wrapping_shr(4)
    add64 r7, r6                                    r7 += r6   ///  r7 = r7.wrapping_add(r6)
    lddw r6, 0xf0f0f0f0f0f0f0f                      r6 load str located at 1085102592571150095
    and64 r7, r6                                    r7 &= r6   ///  r7 = r7.and(r6)
    lddw r6, 0x101010101010101                      r6 load str located at 72340172838076673
    mul64 r7, r6                                    r7 *= r6   ///  r7 = r7.wrapping_mul(r6)
    rsh64 r7, 56                                    r7 >>= 56   ///  r7 = r7.wrapping_shr(56)
lbb_22709:
    mov64 r6, 12                                    r6 = 12 as i32 as i64 as u64
    sub64 r6, r7                                    r6 -= r7   ///  r6 = r6.wrapping_sub(r7)
    add64 r7, 53                                    r7 += 53   ///  r7 = r7.wrapping_add(53 as i32 as i64 as u64)
    and64 r7, 63                                    r7 &= 63   ///  r7 = r7.and(63)
    lsh64 r0, r7                                    r0 <<= r7   ///  r0 = r0.wrapping_shl(r7 as u32)
lbb_22714:
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    lsh64 r5, 3                                     r5 <<= 3   ///  r5 = r5.wrapping_shl(3)
    lsh64 r0, 3                                     r0 <<= 3   ///  r0 = r0.wrapping_shl(3)
    lddw r7, 0x80000000000000                       r7 load str located at 36028797018963968
    or64 r0, r7                                     r0 |= r7   ///  r0 = r0.or(r7)
    mov64 r8, r0                                    r8 = r0
    jeq r4, r6, lbb_22728                           if r4 == r6 { pc += 4 }
    mov64 r9, r4                                    r9 = r4
    sub64 r9, r6                                    r9 -= r6   ///  r9 = r9.wrapping_sub(r6)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jlt r9, 64, lbb_22853                           if r9 < (64 as i32 as i64 as u64) { pc += 125 }
lbb_22728:
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    or64 r5, r7                                     r5 |= r7   ///  r5 = r5.or(r7)
    jsgt r2, -1, lbb_22732                          if (r2 as i64) > (-1 as i32 as i64) { pc += 1 }
    ja lbb_22764                                    if true { pc += 32 }
lbb_22732:
    mov64 r1, r8                                    r1 = r8
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    lddw r2, 0x100000000000000                      r2 load str located at 72057594037927936
    mov64 r5, r1                                    r5 = r1
    and64 r5, r2                                    r5 &= r2   ///  r5 = r5.and(r2)
    jeq r5, 0, lbb_22743                            if r5 == (0 as i32 as i64 as u64) { pc += 4 }
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
    or64 r1, r8                                     r1 |= r8   ///  r1 = r1.or(r8)
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
lbb_22743:
    jsgt r4, 2046, lbb_22745                        if (r4 as i64) > (2046 as i32 as i64) { pc += 1 }
    ja lbb_22820                                    if true { pc += 75 }
lbb_22745:
    lddw r1, 0x7ff0000000000000                     r1 load str located at 9218868437227405312
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r0, r3                                    r0 = r3
    ja lbb_22867                                    if true { pc += 117 }
lbb_22750:
    lddw r3, 0x7ff0000000000000                     r3 load str located at 9218868437227405312
    jgt r5, r3, lbb_22759                           if r5 > r3 { pc += 6 }
    jgt r4, r3, lbb_22755                           if r4 > r3 { pc += 1 }
    ja lbb_22868                                    if true { pc += 113 }
lbb_22755:
    lddw r1, 0x8000000000000                        r1 load str located at 2251799813685248
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    ja lbb_22866                                    if true { pc += 107 }
lbb_22759:
    lddw r1, 0x8000000000000                        r1 load str located at 2251799813685248
    or64 r5, r1                                     r5 |= r1   ///  r5 = r5.or(r1)
    mov64 r0, r5                                    r0 = r5
    ja lbb_22867                                    if true { pc += 103 }
lbb_22764:
    sub64 r5, r8                                    r5 -= r8   ///  r5 = r5.wrapping_sub(r8)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r5, 0, lbb_22867                            if r5 == (0 as i32 as i64 as u64) { pc += 100 }
    lddw r2, 0x7fffffffffffff                       r2 load str located at 36028797018963967
    mov64 r1, r5                                    r1 = r5
    jgt r5, r2, lbb_22743                           if r5 > r2 { pc += -28 }
    mov64 r2, r5                                    r2 = r5
    rsh64 r2, 1                                     r2 >>= 1   ///  r2 = r2.wrapping_shr(1)
    mov64 r1, r5                                    r1 = r5
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 4                                     r2 >>= 4   ///  r2 = r2.wrapping_shr(4)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 8                                     r2 >>= 8   ///  r2 = r2.wrapping_shr(8)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 16                                    r2 >>= 16   ///  r2 = r2.wrapping_shr(16)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    xor64 r1, -1                                    r1 ^= -1   ///  r1 = r1.xor(-1)
    lddw r2, 0x5555555555555555                     r2 load str located at 6148914691236517205
    mov64 r0, r1                                    r0 = r1
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r2                                    r0 &= r2   ///  r0 = r0.and(r2)
    sub64 r1, r0                                    r1 -= r0   ///  r1 = r1.wrapping_sub(r0)
    lddw r0, 0x3333333333333333                     r0 load str located at 3689348814741910323
    mov64 r2, r1                                    r2 = r1
    and64 r2, r0                                    r2 &= r0   ///  r2 = r2.and(r0)
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    and64 r1, r0                                    r1 &= r0   ///  r1 = r1.and(r0)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    lddw r1, 0xf0f0f0f0f0f0f0f                      r1 load str located at 1085102592571150095
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
    mul64 r2, r1                                    r2 *= r1   ///  r2 = r2.wrapping_mul(r1)
    rsh64 r2, 56                                    r2 >>= 56   ///  r2 = r2.wrapping_shr(56)
    add64 r2, -8                                    r2 += -8   ///  r2 = r2.wrapping_add(-8 as i32 as i64 as u64)
    sub64 r4, r2                                    r4 -= r2   ///  r4 = r4.wrapping_sub(r2)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    lsh64 r5, r2                                    r5 <<= r2   ///  r5 = r5.wrapping_shl(r2 as u32)
    mov64 r1, r5                                    r1 = r5
lbb_22820:
    jslt r4, 1, lbb_22822                           if (r4 as i64) < (1 as i32 as i64) { pc += 1 }
    ja lbb_22836                                    if true { pc += 14 }
lbb_22822:
    mov64 r2, r4                                    r2 = r4
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    mov64 r0, r1                                    r0 = r1
    lsh64 r0, r2                                    r0 <<= r2   ///  r0 = r0.wrapping_shl(r2 as u32)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_22831                            if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_22831:
    sub64 r2, r4                                    r2 -= r4   ///  r2 = r2.wrapping_sub(r4)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    rsh64 r1, r2                                    r1 >>= r2   ///  r1 = r1.wrapping_shr(r2 as u32)
    or64 r1, r5                                     r1 |= r5   ///  r1 = r1.or(r5)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_22836:
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 3                                     r2 >>= 3   ///  r2 = r2.wrapping_shr(3)
    lddw r5, 0xfffffffffffff                        r5 load str located at 4503599627370495
    mov64 r0, r2                                    r0 = r2
    and64 r0, r5                                    r0 &= r5   ///  r0 = r0.and(r5)
    lsh64 r4, 52                                    r4 <<= 52   ///  r4 = r4.wrapping_shl(52)
    or64 r4, r0                                     r4 |= r0   ///  r4 = r4.or(r0)
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jgt r1, 4, lbb_22865                            if r1 > (4 as i32 as i64 as u64) { pc += 18 }
    mov64 r0, r4                                    r0 = r4
    jeq r1, 4, lbb_22850                            if r1 == (4 as i32 as i64 as u64) { pc += 1 }
    ja lbb_22867                                    if true { pc += 17 }
lbb_22850:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    ja lbb_22866                                    if true { pc += 13 }
lbb_22853:
    mov64 r6, r9                                    r6 = r9
    neg64 r6                                        r6 = -r6   ///  r6 = (r6 as i64).wrapping_neg() as u64
    and64 r6, 63                                    r6 &= 63   ///  r6 = r6.and(63)
    mov64 r8, r0                                    r8 = r0
    lsh64 r8, r6                                    r8 <<= r6   ///  r8 = r8.wrapping_shl(r6 as u32)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jne r8, 0, lbb_22861                            if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_22861:
    rsh64 r0, r9                                    r0 >>= r9   ///  r0 = r0.wrapping_shr(r9 as u32)
    or64 r0, r6                                     r0 |= r6   ///  r0 = r0.or(r6)
    mov64 r8, r0                                    r8 = r0
    ja lbb_22728                                    if true { pc += -137 }
lbb_22865:
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
lbb_22866:
    mov64 r0, r4                                    r0 = r4
lbb_22867:
    exit                                    
lbb_22868:
    jeq r5, r3, lbb_22870                           if r5 == r3 { pc += 1 }
    ja lbb_22879                                    if true { pc += 9 }
lbb_22870:
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    mov64 r0, r1                                    r0 = r1
    jeq r2, r3, lbb_22876                           if r2 == r3 { pc += 1 }
    ja lbb_22867                                    if true { pc += -9 }
lbb_22876:
    lddw r0, 0x7ff8000000000000                     r0 load str located at 9221120237041090560
    ja lbb_22867                                    if true { pc += -12 }
lbb_22879:
    mov64 r0, r2                                    r0 = r2
    jeq r4, r3, lbb_22867                           if r4 == r3 { pc += -14 }
    jeq r5, 0, lbb_22885                            if r5 == (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r0, r1                                    r0 = r1
    jeq r4, 0, lbb_22867                            if r4 == (0 as i32 as i64 as u64) { pc += -17 }
    ja lbb_22594                                    if true { pc += -291 }
lbb_22885:
    mov64 r0, r2                                    r0 = r2
    jeq r4, 0, lbb_22888                            if r4 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_22867                                    if true { pc += -21 }
lbb_22888:
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    mov64 r0, r2                                    r0 = r2
    ja lbb_22867                                    if true { pc += -24 }

function_22891:
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    call function_22578                     
    exit                                    

function_22896:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_22187                     
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x8]                     
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_22905:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    lddw r2, 0x3ff0000000000000                     r2 load str located at 4607182418800017408
    jlt r1, r2, lbb_22928                           if r1 < r2 { pc += 19 }
    lddw r2, 0x43f0000000000000                     r2 load str located at 4895412794951729152
    jlt r1, r2, lbb_22918                           if r1 < r2 { pc += 6 }
    lddw r2, 0x7ff0000000000001                     r2 load str located at 9218868437227405313
    jlt r1, r2, lbb_22916                           if r1 < r2 { pc += 1 }
    ja lbb_22928                                    if true { pc += 12 }
lbb_22916:
    mov64 r0, -1                                    r0 = -1 as i32 as i64 as u64
    ja lbb_22928                                    if true { pc += 10 }
lbb_22918:
    mov64 r0, r1                                    r0 = r1
    lsh64 r0, 11                                    r0 <<= 11   ///  r0 = r0.wrapping_shl(11)
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    rsh64 r1, 52                                    r1 >>= 52   ///  r1 = r1.wrapping_shr(52)
    mov64 r2, 62                                    r2 = 62 as i32 as i64 as u64
    sub64 r2, r1                                    r2 -= r1   ///  r2 = r2.wrapping_sub(r1)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    rsh64 r0, r2                                    r0 >>= r2   ///  r0 = r0.wrapping_shr(r2 as u32)
lbb_22928:
    exit                                    

function_22929:
    call function_22158                     
    exit                                    

function_22931:
    call function_21913                     
    exit                                    
