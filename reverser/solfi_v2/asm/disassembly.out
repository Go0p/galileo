function_0:
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r2+0x50]                     
    jne r1, 165, lbb_13                             if r1 != (165 as i32 as i64 as u64) { pc += 10 }
    mov64 r7, r2                                    r7 = r2
    mov64 r1, r2                                    r1 = r2
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x1000333c8 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295177160
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_16                               if r0 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_13:
    stdw [r6+0x0], 0                        
    stw [r6+0x8], 3                         
lbb_15:
    exit                                    
lbb_16:
    mov64 r1, r7                                    r1 = r7
    ldxb r2, [r1+0x0]                       
    mov64 r3, r2                                    r3 = r2
    and64 r3, 8                                     r3 &= 8   ///  r3 = r3.and(8)
    jne r3, 0, lbb_31                               if r3 != (0 as i32 as i64 as u64) { pc += 10 }
    mov64 r3, r2                                    r3 = r2
    and64 r3, 7                                     r3 &= 7   ///  r3 = r3.and(7)
    jeq r3, 7, lbb_31                               if r3 == (7 as i32 as i64 as u64) { pc += 7 }
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    stxb [r1+0x0], r2                       
    stxdw [r6+0x8], r1                      
    add64 r1, 88                                    r1 += 88   ///  r1 = r1.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r6+0x0], r1                      
    stb [r6+0x10], 0                        
    ja lbb_15                                       if true { pc += -16 }
lbb_31:
    stdw [r6+0x0], 0                        
    stdw [r6+0x8], 11                       
    ja lbb_15                                       if true { pc += -19 }

function_34:
    lddw r2, 0x300000000                            r2 load str located at 12884901888
    ldxdw r3, [r2+0x0]                      
    mov64 r2, r3                                    r2 = r3
    add64 r2, -24                                   r2 += -24   ///  r2 = r2.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jgt r2, r3, lbb_43                              if r2 > r3 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_43:
    jne r5, 0, lbb_45                               if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r2                                    r4 = r2
lbb_45:
    lddw r2, 0x300007fe8                            r2 load str located at 12884934632
    jeq r3, 0, lbb_50                               if r3 == (0 as i32 as i64 as u64) { pc += 2 }
    and64 r4, -8                                    r4 &= -8   ///  r4 = r4.and(-8)
    mov64 r2, r4                                    r2 = r4
lbb_50:
    lddw r3, 0x300000007                            r3 load str located at 12884901895
    jgt r2, r3, lbb_56                              if r2 > r3 { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    call function_20094                     
lbb_56:
    lddw r3, 0x300000000                            r3 load str located at 12884901888
    stxdw [r3+0x0], r2                      
    ldxdw r3, [r1+0x10]                     
    stxdw [r2+0x10], r3                     
    ldxdw r3, [r1+0x8]                      
    stxdw [r2+0x8], r3                      
    ldxdw r1, [r1+0x0]                      
    stxdw [r2+0x0], r1                      
    mov64 r1, 20                                    r1 = 20 as i32 as i64 as u64
    lddw r3, 0x100034778 --> b"\x00\x00\x00\x00\x88\x06\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r3 load str located at 4295182200
    call function_19910                     
    exit                                    

function_70:
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r2, [r1+0x0]                      
    mov64 r1, r2                                    r1 = r2
    add64 r1, -18                                   r1 += -18   ///  r1 = r1.wrapping_add(-18 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jgt r1, r2, lbb_79                              if r1 > r2 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_79:
    jne r4, 0, lbb_81                               if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, r1                                    r3 = r1
lbb_81:
    lddw r1, 0x300007fee                            r1 load str located at 12884934638
    jeq r2, 0, lbb_85                               if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r3                                    r1 = r3
lbb_85:
    lddw r2, 0x300000007                            r2 load str located at 12884901895
    jgt r1, r2, lbb_91                              if r1 > r2 { pc += 3 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 18                                    r2 = 18 as i32 as i64 as u64
    call function_20091                     
lbb_91:
    lddw r2, 0x300000000                            r2 load str located at 12884901888
    stxdw [r2+0x0], r1                      
    lddw r3, 0x6572207365747962                     r3 load str located at 7309940825171196258
    stxdw [r1+0x8], r3                      
    lddw r3, 0x206c6c6120746f4e                     r3 load str located at 2336361471110573902
    stxdw [r1+0x0], r3                      
    sth [r1+0x10], 25697                    
    ldxdw r3, [r2+0x0]                      
    mov64 r2, r3                                    r2 = r3
    add64 r2, -24                                   r2 += -24   ///  r2 = r2.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jgt r2, r3, lbb_108                             if r2 > r3 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_108:
    jne r5, 0, lbb_110                              if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r2                                    r4 = r2
lbb_110:
    lddw r2, 0x300007fe8                            r2 load str located at 12884934632
    jeq r3, 0, lbb_115                              if r3 == (0 as i32 as i64 as u64) { pc += 2 }
    and64 r4, -8                                    r4 &= -8   ///  r4 = r4.and(-8)
    mov64 r2, r4                                    r2 = r4
lbb_115:
    lddw r3, 0x300000007                            r3 load str located at 12884901895
    jgt r2, r3, lbb_121                             if r2 > r3 { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    call function_20094                     
lbb_121:
    lddw r3, 0x300000000                            r3 load str located at 12884901888
    stxdw [r3+0x0], r2                      
    stxdw [r2+0x8], r1                      
    stdw [r2+0x10], 18                      
    stdw [r2+0x0], 18                       
    mov64 r1, 21                                    r1 = 21 as i32 as i64 as u64
    lddw r3, 0x100034778 --> b"\x00\x00\x00\x00\x88\x06\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r3 load str located at 4295182200
    call function_19910                     
    exit                                    

function_132:
    ldxw r3, [r2+0x34]                      
    mov64 r4, r3                                    r4 = r3
    and64 r4, 16                                    r4 &= 16   ///  r4 = r4.and(16)
    jne r4, 0, lbb_141                              if r4 != (0 as i32 as i64 as u64) { pc += 5 }
    and64 r3, 32                                    r3 &= 32   ///  r3 = r3.and(32)
    jeq r3, 0, lbb_139                              if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_143                                      if true { pc += 4 }
lbb_139:
    call function_22975                     
    ja lbb_144                                      if true { pc += 3 }
lbb_141:
    call function_22723                     
    ja lbb_144                                      if true { pc += 1 }
lbb_143:
    call function_22767                     
lbb_144:
    exit                                    

function_145:
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jeq r2, 2, lbb_148                              if r2 == (2 as i32 as i64 as u64) { pc += 1 }
lbb_147:
    exit                                    
lbb_148:
    mov64 r2, r1                                    r2 = r1
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    mov64 r3, r2                                    r3 = r2
    add64 r3, -2                                    r3 += -2   ///  r3 = r3.wrapping_add(-2 as i32 as i64 as u64)
    jlt r3, 2, lbb_147                              if r3 < (2 as i32 as i64 as u64) { pc += -6 }
    jeq r2, 0, lbb_147                              if r2 == (0 as i32 as i64 as u64) { pc += -7 }
    ldxdw r2, [r1-0x1]                      
    ldxdw r1, [r1+0x7]                      
    ldxdw r3, [r1+0x0]                      
    mov64 r1, r2                                    r1 = r2
    callx r3                                
    ja lbb_147                                      if true { pc += -13 }

function_160:
    jne r1, 0, lbb_162                              if r1 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_161:
    exit                                    
lbb_162:
    mov64 r1, r2                                    r1 = r2
    and64 r1, 3                                     r1 &= 3   ///  r1 = r1.and(3)
    mov64 r3, r1                                    r3 = r1
    add64 r3, -2                                    r3 += -2   ///  r3 = r3.wrapping_add(-2 as i32 as i64 as u64)
    jlt r3, 2, lbb_161                              if r3 < (2 as i32 as i64 as u64) { pc += -6 }
    jeq r1, 0, lbb_161                              if r1 == (0 as i32 as i64 as u64) { pc += -7 }
    ldxdw r1, [r2-0x1]                      
    ldxdw r2, [r2+0x7]                      
    ldxdw r2, [r2+0x0]                      
    callx r2                                
    ja lbb_161                                      if true { pc += -12 }

function_173:
    exit                                    

function_174:
    mov64 r2, r1                                    r2 = r1
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    mov64 r3, r2                                    r3 = r2
    add64 r3, -2                                    r3 += -2   ///  r3 = r3.wrapping_add(-2 as i32 as i64 as u64)
    jlt r3, 2, lbb_185                              if r3 < (2 as i32 as i64 as u64) { pc += 6 }
    jeq r2, 0, lbb_185                              if r2 == (0 as i32 as i64 as u64) { pc += 5 }
    ldxdw r2, [r1-0x1]                      
    ldxdw r1, [r1+0x7]                      
    ldxdw r3, [r1+0x0]                      
    mov64 r1, r2                                    r1 = r2
    callx r3                                
lbb_185:
    exit                                    

function_186:
    stdw [r1+0x0], 0                        
    exit                                    

function_188:
    exit                                    

function_189:
    lddw r2, 0x1b204ea073ccb1b7                     r2 load str located at 1954648689323323831
    stxdw [r1+0x8], r2                      
    lddw r2, 0xe61d2cabdb5e3d                       r2 load str located at 64771322342497853
    stxdw [r1+0x0], r2                      
    exit                                    

function_196:
    jeq r2, 0, lbb_205                              if r2 == (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r5, [r4+0x8]                      
    jeq r5, 0, lbb_243                              if r5 == (0 as i32 as i64 as u64) { pc += 44 }
    ldxdw r5, [r4+0x10]                     
    jne r5, 0, lbb_207                              if r5 != (0 as i32 as i64 as u64) { pc += 6 }
    jne r3, 0, lbb_267                              if r3 != (0 as i32 as i64 as u64) { pc += 65 }
lbb_202:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r6, r2                                    r6 = r2
    ja lbb_292                                      if true { pc += 87 }
lbb_205:
    stdw [r1+0x8], 0                        
    ja lbb_307                                      if true { pc += 100 }
lbb_207:
    lddw r0, 0x300000000                            r0 load str located at 12884901888
    ldxdw r0, [r0+0x0]                      
    lddw r7, 0x300008000                            r7 load str located at 12884934656
    jeq r0, 0, lbb_214                              if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, r0                                    r7 = r0
lbb_214:
    mov64 r0, r7                                    r0 = r7
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jgt r0, r7, lbb_220                             if r0 > r7 { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_220:
    jne r8, 0, lbb_222                              if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r0                                    r6 = r0
lbb_222:
    mov64 r0, r2                                    r0 = r2
    neg64 r0                                        r0 = -r0   ///  r0 = (r0 as i64).wrapping_neg() as u64
    and64 r6, r0                                    r6 &= r0   ///  r6 = r6.and(r0)
    lddw r0, 0x300000008                            r0 load str located at 12884901896
    jlt r6, r0, lbb_301                             if r6 < r0 { pc += 73 }
    ldxdw r4, [r4+0x0]                      
    lddw r0, 0x300000000                            r0 load str located at 12884901888
    stxdw [r0+0x0], r6                      
    mov64 r7, r1                                    r7 = r1
    mov64 r1, r6                                    r1 = r6
    mov64 r8, r2                                    r8 = r2
    mov64 r2, r4                                    r2 = r4
    mov64 r9, r3                                    r9 = r3
    mov64 r3, r5                                    r3 = r5
    call function_23152                     
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r9                                    r3 = r9
    mov64 r1, r7                                    r1 = r7
    ja lbb_291                                      if true { pc += 48 }
lbb_243:
    jne r3, 0, lbb_245                              if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_202                                      if true { pc += -43 }
lbb_245:
    lddw r4, 0x300000000                            r4 load str located at 12884901888
    ldxdw r4, [r4+0x0]                      
    lddw r5, 0x300008000                            r5 load str located at 12884934656
    jeq r4, 0, lbb_252                              if r4 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, r4                                    r5 = r4
lbb_252:
    mov64 r4, r5                                    r4 = r5
    sub64 r4, r3                                    r4 -= r3   ///  r4 = r4.wrapping_sub(r3)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jgt r4, r5, lbb_258                             if r4 > r5 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_258:
    jne r0, 0, lbb_260                              if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r4                                    r6 = r4
lbb_260:
    mov64 r4, r2                                    r4 = r2
    neg64 r4                                        r4 = -r4   ///  r4 = (r4 as i64).wrapping_neg() as u64
    and64 r6, r4                                    r6 &= r4   ///  r6 = r6.and(r4)
    lddw r4, 0x300000008                            r4 load str located at 12884901896
    jlt r6, r4, lbb_301                             if r6 < r4 { pc += 35 }
    ja lbb_288                                      if true { pc += 21 }
lbb_267:
    lddw r4, 0x300000000                            r4 load str located at 12884901888
    ldxdw r4, [r4+0x0]                      
    lddw r5, 0x300008000                            r5 load str located at 12884934656
    jeq r4, 0, lbb_274                              if r4 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, r4                                    r5 = r4
lbb_274:
    mov64 r4, r5                                    r4 = r5
    sub64 r4, r3                                    r4 -= r3   ///  r4 = r4.wrapping_sub(r3)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jgt r4, r5, lbb_280                             if r4 > r5 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_280:
    jne r0, 0, lbb_282                              if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r4                                    r6 = r4
lbb_282:
    mov64 r4, r2                                    r4 = r2
    neg64 r4                                        r4 = -r4   ///  r4 = (r4 as i64).wrapping_neg() as u64
    and64 r6, r4                                    r6 &= r4   ///  r6 = r6.and(r4)
    lddw r4, 0x300000008                            r4 load str located at 12884901896
    jlt r6, r4, lbb_301                             if r6 < r4 { pc += 13 }
lbb_288:
    lddw r4, 0x300000000                            r4 load str located at 12884901888
    stxdw [r4+0x0], r6                      
lbb_291:
    mov64 r4, r3                                    r4 = r3
lbb_292:
    mov64 r5, r1                                    r5 = r1
    add64 r5, 16                                    r5 += 16   ///  r5 = r5.wrapping_add(16 as i32 as i64 as u64)
    mov64 r0, r1                                    r0 = r1
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
    jeq r6, 0, lbb_305                              if r6 == (0 as i32 as i64 as u64) { pc += 8 }
    stxdw [r0+0x0], r6                      
    stxdw [r5+0x0], r4                      
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ja lbb_308                                      if true { pc += 7 }
lbb_301:
    mov64 r5, r1                                    r5 = r1
    add64 r5, 16                                    r5 += 16   ///  r5 = r5.wrapping_add(16 as i32 as i64 as u64)
    mov64 r0, r1                                    r0 = r1
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
lbb_305:
    stxdw [r0+0x0], r2                      
    stxdw [r5+0x0], r3                      
lbb_307:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_308:
    stxdw [r1+0x0], r2                      
    exit                                    

function_310:
    mov64 r6, r1                                    r6 = r1
    ldxdw r4, [r6+0x0]                      
    mov64 r3, r4                                    r3 = r4
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r3, 0, lbb_318                              if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_318:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_349                              if r5 != (0 as i32 as i64 as u64) { pc += 29 }
    mov64 r7, r4                                    r7 = r4
    lsh64 r7, 1                                     r7 <<= 1   ///  r7 = r7.wrapping_shl(1)
    jgt r7, r3, lbb_324                             if r7 > r3 { pc += 1 }
    mov64 r7, r3                                    r7 = r3
lbb_324:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x1000000000000000                     r3 load str located at 1152921504606846976
    jlt r7, r3, lbb_329                             if r7 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_329:
    jgt r7, 4, lbb_331                              if r7 > (4 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 4                                     r7 = 4 as i32 as i64 as u64
lbb_331:
    mov64 r3, r7                                    r3 = r7
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    jeq r4, 0, lbb_339                              if r4 == (0 as i32 as i64 as u64) { pc += 5 }
    ldxdw r1, [r6+0x8]                      
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    stxdw [r10-0x8], r4                     
    stxdw [r10-0x18], r1                    
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_339:
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -24                                   r4 += -24   ///  r4 = r4.wrapping_add(-24 as i32 as i64 as u64)
    call function_196                       
    ldxdw r1, [r10-0x30]                    
    jeq r1, 0, lbb_350                              if r1 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r2, [r10-0x20]                    
    ldxdw r1, [r10-0x28]                    
lbb_349:
    call function_20091                     
lbb_350:
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x0], r7                      
    stxdw [r6+0x8], r1                      
    exit                                    

function_354:
    mov64 r6, r1                                    r6 = r1
    jeq r3, 0, lbb_383                              if r3 == (0 as i32 as i64 as u64) { pc += 27 }
    ldxb r1, [r2+0x0]                       
    stxb [r10-0x59], r1                     
    jeq r1, 0, lbb_387                              if r1 == (0 as i32 as i64 as u64) { pc += 28 }
    lddw r1, 0x1000347f8 --> b"\x00\x00\x00\x00\xe04\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295182328
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100000540 --> b"a#4\x00\x00\x00\x00\x00\xbf4\x00\x00\x00\x00\x00\x00W\x04\x00\x00\x10\x00…        r1 load str located at 4294968640
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_20113                     
    mov64 r1, r7                                    r1 = r7
    call function_34                        
    ja lbb_393                                      if true { pc += 10 }
lbb_383:
    lddw r1, 0x1000347d0 --> b"\x00\x00\x00\x00\xa84\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182288
    call function_19705                     
    ja lbb_393                                      if true { pc += 6 }
lbb_387:
    jeq r3, 1, lbb_389                              if r3 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_392                                      if true { pc += 3 }
lbb_389:
    stb [r6+0x1], 0                         
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_395                                      if true { pc += 3 }
lbb_392:
    call function_70                        
lbb_393:
    stxdw [r6+0x8], r0                      
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_395:
    stxb [r6+0x0], r1                       
    exit                                    

function_397:
    mov64 r6, r1                                    r6 = r1
    jlt r3, 8, lbb_432                              if r3 < (8 as i32 as i64 as u64) { pc += 33 }
    mov64 r1, r3                                    r1 = r3
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    jeq r1, 8, lbb_432                              if r1 == (8 as i32 as i64 as u64) { pc += 30 }
    jeq r3, 16, lbb_432                             if r3 == (16 as i32 as i64 as u64) { pc += 29 }
    ldxdw r4, [r2+0x0]                      
    ldxdw r5, [r2+0x8]                      
    ldxb r1, [r2+0x10]                      
    stxb [r10-0x59], r1                     
    jlt r1, 2, lbb_439                              if r1 < (2 as i32 as i64 as u64) { pc += 31 }
    lddw r1, 0x1000347e8 --> b"\x00\x00\x00\x00\xc34\x03\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295182312
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x10002cf18 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295151384
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_20113                     
    mov64 r1, r7                                    r1 = r7
    call function_34                        
    ja lbb_435                                      if true { pc += 3 }
lbb_432:
    lddw r1, 0x1000347d0 --> b"\x00\x00\x00\x00\xa84\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182288
    call function_19705                     
lbb_435:
    stxdw [r6+0x0], r0                      
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
lbb_437:
    stxb [r6+0x10], r1                      
    exit                                    
lbb_439:
    jeq r3, 17, lbb_441                             if r3 == (17 as i32 as i64 as u64) { pc += 1 }
    ja lbb_444                                      if true { pc += 3 }
lbb_441:
    stxdw [r6+0x8], r5                      
    stxdw [r6+0x0], r4                      
    ja lbb_437                                      if true { pc += -7 }
lbb_444:
    call function_70                        
    ja lbb_435                                      if true { pc += -11 }

function_446:
    mov64 r6, r1                                    r6 = r1
    jlt r3, 8, lbb_459                              if r3 < (8 as i32 as i64 as u64) { pc += 11 }
    mov64 r1, r3                                    r1 = r3
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    jeq r1, 8, lbb_459                              if r1 == (8 as i32 as i64 as u64) { pc += 8 }
    jeq r3, 16, lbb_453                             if r3 == (16 as i32 as i64 as u64) { pc += 1 }
    ja lbb_466                                      if true { pc += 13 }
lbb_453:
    ldxdw r1, [r2+0x0]                      
    ldxdw r2, [r2+0x8]                      
    stxdw [r6+0x10], r2                     
    stxdw [r6+0x8], r1                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_464                                      if true { pc += 5 }
lbb_459:
    lddw r1, 0x1000347d0 --> b"\x00\x00\x00\x00\xa84\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182288
    call function_19705                     
lbb_462:
    stxdw [r6+0x8], r0                      
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_464:
    stxdw [r6+0x0], r1                      
    exit                                    
lbb_466:
    call function_70                        
    ja lbb_462                                      if true { pc += -6 }

function_468:
    mov64 r6, r1                                    r6 = r1
    jlt r3, 8, lbb_511                              if r3 < (8 as i32 as i64 as u64) { pc += 41 }
    mov64 r1, r3                                    r1 = r3
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    jeq r1, 8, lbb_511                              if r1 == (8 as i32 as i64 as u64) { pc += 38 }
    jeq r3, 16, lbb_511                             if r3 == (16 as i32 as i64 as u64) { pc += 37 }
    ldxdw r1, [r2+0x0]                      
    ldxdw r5, [r2+0x8]                      
    ldxb r0, [r2+0x10]                      
    stxb [r10-0x59], r0                     
    jlt r0, 2, lbb_503                              if r0 < (2 as i32 as i64 as u64) { pc += 24 }
lbb_479:
    lddw r1, 0x1000347e8 --> b"\x00\x00\x00\x00\xc34\x03\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295182312
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x10002cf18 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295151384
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_20113                     
    mov64 r1, r7                                    r1 = r7
    call function_34                        
    ja lbb_514                                      if true { pc += 11 }
lbb_503:
    jeq r3, 17, lbb_511                             if r3 == (17 as i32 as i64 as u64) { pc += 7 }
    ldxb r4, [r2+0x11]                      
    stxb [r10-0x59], r4                     
    jlt r4, 2, lbb_508                              if r4 < (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_479                                      if true { pc += -29 }
lbb_508:
    mov64 r7, r3                                    r7 = r3
    add64 r7, -18                                   r7 += -18   ///  r7 = r7.wrapping_add(-18 as i32 as i64 as u64)
    jgt r7, 5, lbb_518                              if r7 > (5 as i32 as i64 as u64) { pc += 7 }
lbb_511:
    lddw r1, 0x1000347d0 --> b"\x00\x00\x00\x00\xa84\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182288
    call function_19705                     
lbb_514:
    stxdw [r6+0x0], r0                      
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
lbb_516:
    stxb [r6+0x11], r4                      
    exit                                    
lbb_518:
    jeq r3, 24, lbb_520                             if r3 == (24 as i32 as i64 as u64) { pc += 1 }
    ja lbb_529                                      if true { pc += 9 }
lbb_520:
    add64 r2, 18                                    r2 += 18   ///  r2 = r2.wrapping_add(18 as i32 as i64 as u64)
    ldxh r3, [r2+0x4]                       
    stxh [r6+0x16], r3                      
    ldxw r2, [r2+0x0]                       
    stxw [r6+0x12], r2                      
    stxb [r6+0x10], r0                      
    stxdw [r6+0x8], r5                      
    stxdw [r6+0x0], r1                      
    ja lbb_516                                      if true { pc += -13 }
lbb_529:
    call function_70                        
    ja lbb_514                                      if true { pc += -17 }

function_531:
    mov64 r6, r1                                    r6 = r1
    ldxdw r3, [r2+0x8]                      
    jlt r3, 4, lbb_633                              if r3 < (4 as i32 as i64 as u64) { pc += 99 }
    mov64 r1, r3                                    r1 = r3
    add64 r1, -4                                    r1 += -4   ///  r1 = r1.wrapping_add(-4 as i32 as i64 as u64)
    ldxdw r4, [r2+0x0]                      
    ldxw r5, [r4+0x0]                       
    stxdw [r2+0x8], r1                      
    add64 r4, 4                                     r4 += 4   ///  r4 = r4.wrapping_add(4 as i32 as i64 as u64)
    stxdw [r10-0x28], r4                    
    stxdw [r2+0x0], r4                      
    jeq r5, 0, lbb_641                              if r5 == (0 as i32 as i64 as u64) { pc += 98 }
    mov64 r7, r3                                    r7 = r3
    stxdw [r10-0x38], r2                    
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r4, 0x300008000                            r4 load str located at 12884934656
    jeq r1, 0, lbb_552                              if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r1                                    r4 = r1
lbb_552:
    mov64 r3, r5                                    r3 = r5
    stxdw [r10-0x40], r5                    
    jlt r5, 512, lbb_556                            if r5 < (512 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 512                                   r3 = 512 as i32 as i64 as u64
lbb_556:
    mov64 r2, r3                                    r2 = r3
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r5, r4                                    r5 = r4
    sub64 r5, r2                                    r5 -= r2   ///  r5 = r5.wrapping_sub(r2)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jgt r5, r4, lbb_564                             if r5 > r4 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_564:
    jne r0, 0, lbb_566                              if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r5                                    r1 = r5
lbb_566:
    stxdw [r10-0x48], r6                    
    lddw r4, 0x300000008                            r4 load str located at 12884901896
    jlt r1, r4, lbb_661                             if r1 < r4 { pc += 91 }
    lddw r2, 0x300000000                            r2 load str located at 12884901888
    stxdw [r2+0x0], r1                      
    stxdw [r10-0x18], r3                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    stxdw [r10-0x10], r1                    
    mov64 r4, r7                                    r4 = r7
    add64 r4, -12                                   r4 += -12   ///  r4 = r4.wrapping_add(-12 as i32 as i64 as u64)
    stdw [r10-0x8], 0                       
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ldxdw r3, [r10-0x38]                    
    ldxdw r5, [r10-0x40]                    
    ja lbb_610                                      if true { pc += 27 }
lbb_583:
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    mov64 r0, r1                                    r0 = r1
    ldxdw r1, [r10-0x20]                    
    or64 r1, r8                                     r1 |= r8   ///  r1 = r1.or(r8)
    lsh64 r7, 48                                    r7 <<= 48   ///  r7 = r7.wrapping_shl(48)
    mov64 r2, r7                                    r2 = r7
    lddw r8, 0xff000000000000                       r8 load str located at 71776119061217280
    and64 r2, r8                                    r2 &= r8   ///  r2 = r2.and(r8)
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    mov64 r1, r0                                    r1 = r0
    lddw r8, 0xff00000000000000                     r8 load str located at -72057594037927936
    and64 r7, r8                                    r7 &= r8   ///  r7 = r7.and(r8)
    or64 r7, r2                                     r7 |= r2   ///  r7 = r7.or(r2)
    mov64 r2, r1                                    r2 = r1
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    stxdw [r2+0x0], r7                      
    add64 r4, -8                                    r4 += -8   ///  r4 = r4.wrapping_add(-8 as i32 as i64 as u64)
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x8], r9                     
    mov64 r2, r9                                    r2 = r9
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, r5, lbb_610                             if r2 < r5 { pc += 1 }
    ja lbb_653                                      if true { pc += 43 }
lbb_610:
    mov64 r2, r4                                    r2 = r4
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jlt r2, 8, lbb_645                              if r2 < (8 as i32 as i64 as u64) { pc += 32 }
    ldxdw r2, [r10-0x28]                    
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    ldxh r7, [r2+0x6]                       
    ldxw r8, [r2+0x0]                       
    stxdw [r10-0x20], r8                    
    ldxh r8, [r2+0x4]                       
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r3+0x0], r2                      
    stxdw [r3+0x8], r4                      
    ldxdw r2, [r10-0x18]                    
    jne r9, r2, lbb_583                             if r9 != r2 { pc += -41 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    stxdw [r10-0x30], r4                    
    call function_310                       
    ldxdw r5, [r10-0x40]                    
    ldxdw r4, [r10-0x30]                    
    ldxdw r3, [r10-0x38]                    
    ldxdw r1, [r10-0x10]                    
    ja lbb_583                                      if true { pc += -50 }
lbb_633:
    lddw r1, 0x1000347d0 --> b"\x00\x00\x00\x00\xa84\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182288
    call function_19705                     
    stxdw [r6+0x8], r0                      
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    stxdw [r6+0x0], r1                      
    ja lbb_660                                      if true { pc += 19 }
lbb_641:
    stdw [r6+0x10], 0                       
    stdw [r6+0x8], 1                        
    stdw [r6+0x0], 0                        
    ja lbb_660                                      if true { pc += 15 }
lbb_645:
    lddw r1, 0x1000347d0 --> b"\x00\x00\x00\x00\xa84\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182288
    call function_19705                     
    ldxdw r2, [r10-0x48]                    
    stxdw [r2+0x8], r0                      
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    ja lbb_659                                      if true { pc += 6 }
lbb_653:
    ldxdw r1, [r10-0x8]                     
    ldxdw r2, [r10-0x48]                    
    stxdw [r2+0x10], r1                     
    ldxdw r1, [r10-0x10]                    
    stxdw [r2+0x8], r1                      
    ldxdw r1, [r10-0x18]                    
lbb_659:
    stxdw [r2+0x0], r1                      
lbb_660:
    exit                                    
lbb_661:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    call function_20091                     

function_663:
    mov64 r0, 10                                    r0 = 10 as i32 as i64 as u64
    ldxdw r9, [r1+0x20]                     
    jlt r9, 4, lbb_685                              if r9 < (4 as i32 as i64 as u64) { pc += 19 }
    stxdw [r10-0x120], r3                   
    stxdw [r10-0x118], r4                   
    stxdw [r10-0x128], r1                   
    ldxdw r6, [r1+0x18]                     
    ldxdw r3, [r6+0x0]                      
    stxdw [r10-0x110], r2                   
    ldxdw r1, [r2+0x0]                      
    ldxdw r7, [r1+0x0]                      
    mov64 r8, r7                                    r8 = r7
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r3                                    r2 = r3
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jeq r1, 0, lbb_686                              if r1 == (0 as i32 as i64 as u64) { pc += 1 }
lbb_685:
    exit                                    
lbb_686:
    ldxb r2, [r6+0x8]                       
    mov64 r1, -120                                  r1 = -120 as i32 as i64 as u64
    jeq r2, 0, lbb_690                              if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
lbb_690:
    ldxb r2, [r7+0x0]                       
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r0, 11                                    r0 = 11 as i32 as i64 as u64
    jne r1, 0, lbb_685                              if r1 != (0 as i32 as i64 as u64) { pc += -10 }
    ldxb r1, [r7+0x1]                       
    ldxb r2, [r7+0x2]                       
    ldxb r4, [r7+0x3]                       
    ldxdw r3, [r7+0x50]                     
    mov64 r5, r7                                    r5 = r7
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xe8], r5                    
    mov64 r5, r7                                    r5 = r7
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xf0], r5                    
    stxdw [r10-0xf8], r3                    
    add64 r7, 72                                    r7 += 72   ///  r7 = r7.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x100], r7                   
    stxdw [r10-0x108], r8                   
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_712                              if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_712:
    stxb [r10-0xd6], r3                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0x110]                   
    jne r2, 0, lbb_717                              if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_717:
    stxb [r10-0xd7], r4                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_721                              if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_721:
    stxb [r10-0xd8], r2                     
    stdw [r10-0xe0], 0                      
    ldxdw r2, [r6+0x10]                     
    ldxdw r1, [r3+0x8]                      
    ldxdw r7, [r1+0x0]                      
    mov64 r8, r7                                    r8 = r7
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, r0                                    r1 = r0
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_685                              if r1 != (0 as i32 as i64 as u64) { pc += -51 }
    ldxb r2, [r6+0x18]                      
    mov64 r1, -120                                  r1 = -120 as i32 as i64 as u64
    jeq r2, 0, lbb_740                              if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
lbb_740:
    ldxb r2, [r7+0x0]                       
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r0, 11                                    r0 = 11 as i32 as i64 as u64
    jne r1, 0, lbb_685                              if r1 != (0 as i32 as i64 as u64) { pc += -60 }
    ldxb r1, [r7+0x1]                       
    ldxb r2, [r7+0x2]                       
    ldxb r4, [r7+0x3]                       
    ldxdw r3, [r7+0x50]                     
    mov64 r5, r7                                    r5 = r7
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xb0], r5                    
    mov64 r5, r7                                    r5 = r7
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xb8], r5                    
    stxdw [r10-0xc0], r3                    
    add64 r7, 72                                    r7 += 72   ///  r7 = r7.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0xc8], r7                    
    stxdw [r10-0xd0], r8                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_762                              if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_762:
    stxb [r10-0x9e], r3                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0x110]                   
    jne r2, 0, lbb_767                              if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_767:
    stxb [r10-0x9f], r4                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_771                              if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_771:
    stxb [r10-0xa0], r2                     
    stdw [r10-0xa8], 0                      
    ldxdw r2, [r6+0x20]                     
    ldxdw r1, [r3+0x10]                     
    ldxdw r7, [r1+0x0]                      
    mov64 r8, r7                                    r8 = r7
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, r0                                    r1 = r0
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_685                              if r1 != (0 as i32 as i64 as u64) { pc += -101 }
    ldxb r2, [r6+0x28]                      
    mov64 r1, -120                                  r1 = -120 as i32 as i64 as u64
    jeq r2, 0, lbb_790                              if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
lbb_790:
    ldxb r2, [r7+0x0]                       
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r0, 11                                    r0 = 11 as i32 as i64 as u64
    jne r1, 0, lbb_685                              if r1 != (0 as i32 as i64 as u64) { pc += -110 }
    ldxb r1, [r7+0x1]                       
    ldxb r2, [r7+0x2]                       
    ldxb r4, [r7+0x3]                       
    ldxdw r3, [r7+0x50]                     
    mov64 r5, r7                                    r5 = r7
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r5                    
    mov64 r5, r7                                    r5 = r7
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r5                    
    stxdw [r10-0x88], r3                    
    add64 r7, 72                                    r7 += 72   ///  r7 = r7.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r7                    
    stxdw [r10-0x98], r8                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_812                              if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_812:
    stxb [r10-0x66], r3                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0x110]                   
    jne r2, 0, lbb_817                              if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_817:
    stxb [r10-0x67], r4                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_821                              if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_821:
    stxb [r10-0x68], r2                     
    stdw [r10-0x70], 0                      
    ldxdw r2, [r6+0x30]                     
    ldxdw r1, [r3+0x18]                     
    ldxdw r7, [r1+0x0]                      
    mov64 r8, r7                                    r8 = r7
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, r0                                    r1 = r0
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_685                              if r1 != (0 as i32 as i64 as u64) { pc += -151 }
    ldxb r2, [r6+0x38]                      
    mov64 r1, -120                                  r1 = -120 as i32 as i64 as u64
    jeq r2, 0, lbb_840                              if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
lbb_840:
    ldxb r2, [r7+0x0]                       
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r0, 11                                    r0 = 11 as i32 as i64 as u64
    jne r1, 0, lbb_685                              if r1 != (0 as i32 as i64 as u64) { pc += -160 }
    ldxb r3, [r7+0x1]                       
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_850                              if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_850:
    ldxb r5, [r7+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0x128]                   
    jne r5, 0, lbb_855                              if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_855:
    ldxb r5, [r7+0x3]                       
    jne r5, 0, lbb_858                              if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_858:
    ldxdw r5, [r7+0x50]                     
    mov64 r0, r7                                    r0 = r7
    add64 r0, 40                                    r0 += 40   ///  r0 = r0.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r0                    
    mov64 r0, r7                                    r0 = r7
    add64 r0, 88                                    r0 += 88   ///  r0 = r0.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r0                    
    stxdw [r10-0x50], r5                    
    add64 r7, 72                                    r7 += 72   ///  r7 = r7.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r7                    
    stxdw [r10-0x60], r8                    
    stxb [r10-0x2e], r2                     
    stxb [r10-0x2f], r4                     
    stxb [r10-0x30], r1                     
    stdw [r10-0x38], 0                      
    ldxdw r1, [r3+0x0]                      
    ldxdw r2, [r3+0x8]                      
    ldxdw r3, [r3+0x10]                     
    stxdw [r10-0x8], r3                     
    stxdw [r10-0x10], r2                    
    stxdw [r10-0x18], r9                    
    stxdw [r10-0x20], r6                    
    stxdw [r10-0x28], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -264                                  r2 += -264   ///  r2 = r2.wrapping_add(-264 as i32 as i64 as u64)
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
    ldxdw r4, [r10-0x120]                   
    ldxdw r5, [r10-0x118]                   
    syscall [invalid]                       
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
    ja lbb_685                                      if true { pc += -206 }

function_891:
    mov64 r0, 10                                    r0 = 10 as i32 as i64 as u64
    ldxdw r7, [r1+0x20]                     
    jlt r7, 3, lbb_911                              if r7 < (3 as i32 as i64 as u64) { pc += 17 }
    stxdw [r10-0xe0], r1                    
    ldxdw r8, [r1+0x18]                     
    ldxdw r3, [r8+0x0]                      
    stxdw [r10-0xd8], r2                    
    ldxdw r1, [r2+0x0]                      
    ldxdw r9, [r1+0x0]                      
    mov64 r6, r9                                    r6 = r9
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r3                                    r2 = r3
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jeq r1, 0, lbb_912                              if r1 == (0 as i32 as i64 as u64) { pc += 1 }
lbb_911:
    exit                                    
lbb_912:
    ldxb r2, [r8+0x8]                       
    mov64 r1, -120                                  r1 = -120 as i32 as i64 as u64
    jeq r2, 0, lbb_916                              if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
lbb_916:
    ldxb r2, [r9+0x0]                       
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r0, 11                                    r0 = 11 as i32 as i64 as u64
    jne r1, 0, lbb_911                              if r1 != (0 as i32 as i64 as u64) { pc += -10 }
    ldxb r1, [r9+0x1]                       
    ldxb r2, [r9+0x2]                       
    ldxb r4, [r9+0x3]                       
    ldxdw r3, [r9+0x50]                     
    mov64 r5, r9                                    r5 = r9
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xb0], r5                    
    mov64 r5, r9                                    r5 = r9
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xb8], r5                    
    stxdw [r10-0xc0], r3                    
    add64 r9, 72                                    r9 += 72   ///  r9 = r9.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0xc8], r9                    
    stxdw [r10-0xd0], r6                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_938                              if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_938:
    stxb [r10-0x9e], r3                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0xd8]                    
    jne r2, 0, lbb_943                              if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_943:
    stxb [r10-0x9f], r4                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_947                              if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_947:
    stxb [r10-0xa0], r2                     
    stdw [r10-0xa8], 0                      
    ldxdw r2, [r8+0x10]                     
    ldxdw r1, [r3+0x8]                      
    ldxdw r9, [r1+0x0]                      
    mov64 r6, r9                                    r6 = r9
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, r0                                    r1 = r0
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_911                              if r1 != (0 as i32 as i64 as u64) { pc += -51 }
    ldxb r2, [r8+0x18]                      
    mov64 r1, -120                                  r1 = -120 as i32 as i64 as u64
    jeq r2, 0, lbb_966                              if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
lbb_966:
    ldxb r2, [r9+0x0]                       
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r0, 11                                    r0 = 11 as i32 as i64 as u64
    jne r1, 0, lbb_911                              if r1 != (0 as i32 as i64 as u64) { pc += -60 }
    ldxb r1, [r9+0x1]                       
    ldxb r2, [r9+0x2]                       
    ldxb r4, [r9+0x3]                       
    ldxdw r3, [r9+0x50]                     
    mov64 r5, r9                                    r5 = r9
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r5                    
    mov64 r5, r9                                    r5 = r9
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r5                    
    stxdw [r10-0x88], r3                    
    add64 r9, 72                                    r9 += 72   ///  r9 = r9.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r9                    
    stxdw [r10-0x98], r6                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_988                              if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_988:
    stxb [r10-0x66], r3                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0xd8]                    
    jne r2, 0, lbb_993                              if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_993:
    stxb [r10-0x67], r4                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_997                              if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_997:
    stxb [r10-0x68], r2                     
    stdw [r10-0x70], 0                      
    ldxdw r2, [r8+0x20]                     
    ldxdw r1, [r3+0x10]                     
    ldxdw r9, [r1+0x0]                      
    mov64 r6, r9                                    r6 = r9
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, r0                                    r1 = r0
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_911                              if r1 != (0 as i32 as i64 as u64) { pc += -101 }
    ldxb r2, [r8+0x28]                      
    mov64 r1, -120                                  r1 = -120 as i32 as i64 as u64
    jeq r2, 0, lbb_1016                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
lbb_1016:
    ldxb r2, [r9+0x0]                       
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r0, 11                                    r0 = 11 as i32 as i64 as u64
    jne r1, 0, lbb_911                              if r1 != (0 as i32 as i64 as u64) { pc += -110 }
    ldxb r1, [r9+0x1]                       
    ldxb r2, [r9+0x2]                       
    ldxb r5, [r9+0x3]                       
    ldxdw r3, [r9+0x50]                     
    mov64 r4, r9                                    r4 = r9
    add64 r4, 40                                    r4 += 40   ///  r4 = r4.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r4                    
    mov64 r4, r9                                    r4 = r9
    add64 r4, 88                                    r4 += 88   ///  r4 = r4.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r4                    
    stxdw [r10-0x50], r3                    
    add64 r9, 72                                    r9 += 72   ///  r9 = r9.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r9                    
    stxdw [r10-0x60], r6                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r5, 0, lbb_1039                             if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_1039:
    stxb [r10-0x2e], r4                     
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    ldxdw r4, [r10-0xe0]                    
    jne r2, 0, lbb_1044                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_1044:
    stxb [r10-0x2f], r5                     
    jne r1, 0, lbb_1047                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_1047:
    stxb [r10-0x30], r3                     
    stdw [r10-0x38], 0                      
    ldxdw r1, [r4+0x0]                      
    ldxdw r2, [r4+0x8]                      
    ldxdw r3, [r4+0x10]                     
    stxdw [r10-0x8], r3                     
    stxdw [r10-0x10], r2                    
    stxdw [r10-0x18], r7                    
    stxdw [r10-0x20], r8                    
    stxdw [r10-0x28], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -208                                  r2 += -208   ///  r2 = r2.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r3, 3                                     r3 = 3 as i32 as i64 as u64
    mov64 r4, 8                                     r4 = 8 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    syscall [invalid]                       
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
    ja lbb_911                                      if true { pc += -156 }

function_1067:
    mov64 r9, 10                                    r9 = 10 as i32 as i64 as u64
    jne r3, 13, lbb_1259                            if r3 != (13 as i32 as i64 as u64) { pc += 190 }
    mov64 r9, 2                                     r9 = 2 as i32 as i64 as u64
    jne r5, 3, lbb_1259                             if r5 != (3 as i32 as i64 as u64) { pc += 188 }
    mov64 r6, 46                                    r6 = 46 as i32 as i64 as u64
    ldxb r3, [r4+0x1]                       
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jne r3, 0, lbb_1259                             if r3 != (0 as i32 as i64 as u64) { pc += 184 }
    mov64 r9, 7                                     r9 = 7 as i32 as i64 as u64
    ldxdw r3, [r2+0x0]                      
    ldxb r3, [r3+0x1]                       
    jeq r3, 0, lbb_1259                             if r3 == (0 as i32 as i64 as u64) { pc += 180 }
    stxdw [r10-0x148], r1                   
    ldxb r6, [r4+0x2]                       
    ldxb r7, [r4+0x0]                       
    mov64 r3, r2                                    r3 = r2
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -288                                  r1 += -288   ///  r1 = r1.wrapping_add(-288 as i32 as i64 as u64)
    mov64 r8, r2                                    r8 = r2
    call function_6494                      
    ldxw r9, [r10-0x120]                    
    jne r9, 26, lbb_1257                            if r9 != (26 as i32 as i64 as u64) { pc += 167 }
    stxdw [r10-0x160], r7                   
    stxdw [r10-0x158], r6                   
    mov64 r6, r8                                    r6 = r8
    ldxdw r7, [r6+0x48]                     
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    lddw r2, 0x1000333c8 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295177160
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r2, r6                                    r2 = r6
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_1116                             if r0 == (0 as i32 as i64 as u64) { pc += 12 }
    mov64 r1, r7                                    r1 = r7
    lddw r2, 0x100033488 --> b"\x06\xdd\xf6\xe1\xeeu\x8f\xde\x18B]\xbc\xe4l\xcd\xda\xb6\x1a\xfcM\x83\xb9…        r2 load str located at 4295177352
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r2, r8                                    r2 = r8
    ldxdw r1, [r10-0x148]                   
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jne r0, 0, lbb_1259                             if r0 != (0 as i32 as i64 as u64) { pc += 143 }
lbb_1116:
    ldxdw r1, [r2+0x50]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x150], r1                   
    lddw r2, 0x1000333c8 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295177160
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r2, r8                                    r2 = r8
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_1139                             if r0 == (0 as i32 as i64 as u64) { pc += 12 }
    ldxdw r1, [r10-0x150]                   
    lddw r2, 0x100033488 --> b"\x06\xdd\xf6\xe1\xeeu\x8f\xde\x18B]\xbc\xe4l\xcd\xda\xb6\x1a\xfcM\x83\xb9…        r2 load str located at 4295177352
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r2, r8                                    r2 = r8
    ldxdw r1, [r10-0x148]                   
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jne r0, 0, lbb_1259                             if r0 != (0 as i32 as i64 as u64) { pc += 120 }
lbb_1139:
    ldxdw r1, [r2+0x58]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x100033388 --> b"\x8c\x97%\x8fN$\x89\xf1\xbb=\x10)\x14\x8e\x0d\x83\x0bZ\x13\x99\xda\xff\x1…        r2 load str located at 4295177096
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r2, r8                                    r2 = r8
    ldxdw r1, [r10-0x148]                   
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jne r0, 0, lbb_1259                             if r0 != (0 as i32 as i64 as u64) { pc += 107 }
    ldxdw r1, [r2+0x60]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x100033408 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r2 load str located at 4295177224
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r2, r8                                    r2 = r8
    ldxdw r1, [r10-0x148]                   
    mov64 r6, 2                                     r6 = 2 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_1259                             if r0 != (0 as i32 as i64 as u64) { pc += 95 }
    ldxdw r1, [r2+0x28]                     
    stxdw [r10-0x168], r1                   
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r2, r8                                    r2 = r8
    ldxdw r1, [r10-0x148]                   
    mov64 r6, 35                                    r6 = 35 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_1259                             if r0 != (0 as i32 as i64 as u64) { pc += 83 }
    ldxdw r1, [r2+0x30]                     
    stxdw [r10-0x170], r1                   
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    ldxdw r2, [r10-0x150]                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r2, r8                                    r2 = r8
    ldxdw r1, [r10-0x148]                   
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_1259                             if r0 != (0 as i32 as i64 as u64) { pc += 72 }
    ldxdw r1, [r2+0x18]                     
    stxdw [r10-0x178], r1                   
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033428 --> b"\xc1\xa4\x86\xeb\x03\xafw\xa7B\x17\xc6\x96"\xcf\x9e5\x0ea\xbbBK?_\x11?\x9…        r2 load str located at 4295177256
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r2, r8                                    r2 = r8
    ldxdw r1, [r10-0x148]                   
    mov64 r6, 29                                    r6 = 29 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_1259                             if r0 != (0 as i32 as i64 as u64) { pc += 59 }
    ldxdw r3, [r10-0x178]                   
    ldxdw r3, [r3+0x50]                     
    jne r3, 168, lbb_1259                           if r3 != (168 as i32 as i64 as u64) { pc += 56 }
    mov64 r6, 4                                     r6 = 4 as i32 as i64 as u64
    ldxdw r2, [r2+0x10]                     
    stxdw [r10-0x180], r2                   
    ldxdw r2, [r2+0x50]                     
    jne r2, 0, lbb_1259                             if r2 != (0 as i32 as i64 as u64) { pc += 51 }
    ldxdw r1, [r10-0x178]                   
    ldxdw r2, [r10-0x168]                   
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r3, [r10-0x170]                   
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    stxdw [r10-0x168], r2                   
    stxdw [r10-0x170], r3                   
    ldxdw r4, [r10-0x160]                   
    call function_17371                     
    ldxdw r1, [r10-0x98]                    
    stxdw [r10-0x100], r1                   
    ldxdw r1, [r10-0xa0]                    
    stxdw [r10-0x108], r1                   
    ldxdw r1, [r10-0xa8]                    
    stxdw [r10-0x110], r1                   
    ldxdw r1, [r10-0xb0]                    
    stxdw [r10-0x118], r1                   
    ldxb r1, [r10-0x90]                     
    stxdw [r10-0x188], r1                   
    ldxdw r2, [r10-0x180]                   
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -280                                  r1 += -280   ///  r1 = r1.wrapping_add(-280 as i32 as i64 as u64)
    stxdw [r10-0x190], r2                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    ldxdw r1, [r10-0x158]                   
    ldxdw r2, [r10-0x188]                   
    jne r2, r1, lbb_1256                            if r2 != r1 { pc += 17 }
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_1256                             if r0 != (0 as i32 as i64 as u64) { pc += 14 }
    ldxdw r1, [r8+0x20]                     
    stxdw [r10-0x188], r1                   
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033328 --> b"\x06\x8747\xb4\xfb\xc9\xf4\x047\xdd<\xe6?\x9c\xfe\x8c$\x84\x03U\x93\xa1'\…        r2 load str located at 4295177000
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r6, 38                                    r6 = 38 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_1256                             if r0 != (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r1, [r10-0x188]                   
    ldxdw r1, [r1+0x50]                     
    jeq r1, 1048576, lbb_1262                       if r1 == (1048576 as i32 as i64 as u64) { pc += 6 }
lbb_1256:
    ja lbb_1258                                     if true { pc += 1 }
lbb_1257:
    ldxw r6, [r10-0x11c]                    
lbb_1258:
    ldxdw r1, [r10-0x148]                   
lbb_1259:
    stxw [r1+0x4], r6                       
    stxw [r1+0x0], r9                       
    exit                                    
lbb_1262:
    ldxdw r1, [r10-0x168]                   
    ldxdw r2, [r10-0x170]                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r6, 4                                     r6 = 4 as i32 as i64 as u64
    jeq r0, 0, lbb_1256                             if r0 == (0 as i32 as i64 as u64) { pc += -14 }
    mov64 r1, r8                                    r1 = r8
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x1a0], r1                   
    call function_5683                      
    mov64 r9, r0                                    r9 = r0
    mov64 r1, r9                                    r1 = r9
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    stxdw [r10-0x198], r1                   
    mov64 r1, r9                                    r1 = r9
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jne r1, 26, lbb_1364                            if r1 != (26 as i32 as i64 as u64) { pc += 82 }
    mov64 r1, r8                                    r1 = r8
    add64 r1, 48                                    r1 += 48   ///  r1 = r1.wrapping_add(48 as i32 as i64 as u64)
    stxdw [r10-0x1b0], r1                   
    call function_5683                      
    mov64 r9, r0                                    r9 = r0
    mov64 r1, r9                                    r1 = r9
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    stxdw [r10-0x1a8], r1                   
    mov64 r1, r9                                    r1 = r9
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jeq r1, 26, lbb_1295                            if r1 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1366                                     if true { pc += 71 }
lbb_1295:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    ldxdw r6, [r10-0x190]                   
    mov64 r2, r6                                    r2 = r6
    ldxdw r3, [r10-0x168]                   
    mov64 r4, r7                                    r4 = r7
    call function_17416                     
    ldxdw r1, [r10-0x98]                    
    stxdw [r10-0xe0], r1                    
    ldxdw r1, [r10-0xa0]                    
    stxdw [r10-0xe8], r1                    
    ldxdw r1, [r10-0xa8]                    
    stxdw [r10-0xf0], r1                    
    ldxdw r1, [r10-0xb0]                    
    stxdw [r10-0xf8], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    ldxdw r3, [r10-0x170]                   
    ldxdw r4, [r10-0x150]                   
    call function_17416                     
    ldxdw r1, [r10-0x98]                    
    stxdw [r10-0xc0], r1                    
    ldxdw r1, [r10-0xa0]                    
    stxdw [r10-0xc8], r1                    
    ldxdw r1, [r10-0xa8]                    
    stxdw [r10-0xd0], r1                    
    ldxdw r1, [r10-0xb0]                    
    stxdw [r10-0xd8], r1                    
    ldxdw r1, [r8+0x38]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -248                                  r2 += -248   ///  r2 = r2.wrapping_add(-248 as i32 as i64 as u64)
    stxdw [r10-0x1b8], r1                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r6, 7                                     r6 = 7 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_1337                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1256                                     if true { pc += -81 }
lbb_1337:
    ldxdw r1, [r8+0x40]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -216                                  r2 += -216   ///  r2 = r2.wrapping_add(-216 as i32 as i64 as u64)
    stxdw [r10-0x1c0], r1                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_1256                             if r0 != (0 as i32 as i64 as u64) { pc += -92 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    call function_19884                     
    ldxw r1, [r10-0xb0]                     
    jne r1, 0, lbb_1368                             if r1 != (0 as i32 as i64 as u64) { pc += 15 }
    mov64 r1, r8                                    r1 = r8
    add64 r1, 16                                    r1 += 16   ///  r1 = r1.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x1c8], r1                   
    ldxdw r1, [r10-0xa8]                    
    ldxdw r9, [r10-0xa0]                    
    lddw r2, 0x4000000000000000                     r2 load str located at 4611686018427387904
    jeq r9, r2, lbb_1362                            if r9 == r2 { pc += 1 }
    ja lbb_1371                                     if true { pc += 9 }
lbb_1362:
    mul64 r1, 3712                                  r1 *= 3712   ///  r1 = r1.wrapping_mul(3712 as u64)
    ja lbb_1394                                     if true { pc += 30 }
lbb_1364:
    ldxdw r6, [r10-0x198]                   
    ja lbb_1258                                     if true { pc += -108 }
lbb_1366:
    ldxdw r6, [r10-0x1a8]                   
    ja lbb_1258                                     if true { pc += -110 }
lbb_1368:
    ldxw r6, [r10-0xa8]                     
    ldxw r9, [r10-0xac]                     
    ja lbb_1258                                     if true { pc += -113 }
lbb_1371:
    mul64 r1, 1856                                  r1 *= 1856   ///  r1 = r1.wrapping_mul(1856 as u64)
    call function_23211                     
    mov64 r1, r9                                    r1 = r9
    mov64 r2, r0                                    r2 = r0
    call function_23463                     
    mov64 r9, r0                                    r9 = r0
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r1, r9                                    r1 = r9
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_26086                     
    stxdw [r10-0x1d0], r0                   
    mov64 r1, r9                                    r1 = r9
    call function_23128                     
    ldxdw r1, [r10-0x1d0]                   
    jslt r1, 0, lbb_1387                            if (r1 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r6, r0                                    r6 = r0
lbb_1387:
    mov64 r1, r9                                    r1 = r9
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_25957                     
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
    jsgt r0, 0, lbb_1394                            if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r1, r6                                    r1 = r6
lbb_1394:
    ldxdw r2, [r10-0x160]                   
    stxb [r10-0xb2], r2                     
    ldxdw r2, [r10-0x158]                   
    stxb [r10-0xb1], r2                     
    mov64 r2, r10                                   r2 = r10
    add64 r2, -177                                  r2 += -177   ///  r2 = r2.wrapping_add(-177 as i32 as i64 as u64)
    stxdw [r10-0x70], r2                    
    mov64 r2, r10                                   r2 = r10
    add64 r2, -178                                  r2 += -178   ///  r2 = r2.wrapping_add(-178 as i32 as i64 as u64)
    stxdw [r10-0x80], r2                    
    ldxdw r2, [r10-0x170]                   
    stxdw [r10-0x90], r2                    
    ldxdw r2, [r10-0x168]                   
    stxdw [r10-0xa0], r2                    
    lddw r2, 0x1000334fa --> b"marketconfigprograms/solfi-v2-program/src/instruct"        r2 load str located at 4295177466
    stxdw [r10-0xb0], r2                    
    stdw [r10-0x68], 1                      
    stdw [r10-0x78], 1                      
    stdw [r10-0x88], 32                     
    stdw [r10-0x98], 32                     
    stdw [r10-0xa8], 6                      
    mov64 r2, r10                                   r2 = r10
    add64 r2, -176                                  r2 += -176   ///  r2 = r2.wrapping_add(-176 as i32 as i64 as u64)
    stxdw [r10-0x60], r2                    
    stdw [r10-0x58], 5                      
    stxdw [r10-0x18], r1                    
    ldxdw r1, [r10-0x1c8]                   
    stxdw [r10-0x28], r1                    
    stxdw [r10-0x30], r8                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x961634be972aba81                     r1 load str located at -7631854525348136319
    stxdw [r10-0x38], r1                    
    lddw r1, 0x27a193550384248c                     r1 load str located at 2855725632070100108
    stxdw [r10-0x40], r1                    
    lddw r1, 0xfe9c3fe63cdd3704                     r1 load str located at -100134833612835068
    stxdw [r10-0x48], r1                    
    lddw r1, 0xf4c9fbb437348706                     r1 load str located at -807837906697419002
    stxdw [r10-0x50], r1                    
    stdw [r10-0x10], 1728                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -296                                  r1 += -296   ///  r1 = r1.wrapping_add(-296 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -48                                   r2 += -48   ///  r2 = r2.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -96                                   r3 += -96   ///  r3 = r3.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_7957                      
    ldxw r9, [r10-0x128]                    
    jne r9, 26, lbb_1609                            if r9 != (26 as i32 as i64 as u64) { pc += 159 }
    mov64 r9, 11                                    r9 = 11 as i32 as i64 as u64
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x180]                   
    ldxb r1, [r1+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    jeq r2, 0, lbb_1458                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1256                                     if true { pc += -202 }
lbb_1458:
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    ldxdw r2, [r10-0x180]                   
    stxb [r2+0x0], r1                       
    ldxdw r3, [r2+0x50]                     
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_15574                     
    ldxw r1, [r10-0x30]                     
    jne r1, 0, lbb_1611                             if r1 != (0 as i32 as i64 as u64) { pc += 143 }
    ldxdw r2, [r10-0x28]                    
    ldxdw r5, [r10-0x178]                   
    ldxdw r1, [r5+0x8]                      
    ldxdw r3, [r5+0x10]                     
    ldxdw r4, [r5+0x18]                     
    ldxdw r5, [r5+0x20]                     
    stxdw [r2+0x30], r5                     
    stxdw [r2+0x28], r4                     
    stxdw [r2+0x20], r3                     
    stxdw [r2+0x18], r1                     
    ldxdw r5, [r10-0x188]                   
    ldxdw r1, [r5+0x8]                      
    ldxdw r3, [r5+0x10]                     
    ldxdw r4, [r5+0x18]                     
    ldxdw r5, [r5+0x20]                     
    stxdw [r2+0x118], r5                    
    stxdw [r2+0x110], r4                    
    stxdw [r2+0x108], r3                    
    stxdw [r2+0x100], r1                    
    ldxdw r5, [r10-0x168]                   
    ldxdw r1, [r5+0x0]                      
    ldxdw r3, [r5+0x8]                      
    ldxdw r4, [r5+0x10]                     
    ldxdw r5, [r5+0x18]                     
    stxdw [r2+0x50], r5                     
    stxdw [r2+0x48], r4                     
    stxdw [r2+0x40], r3                     
    stxdw [r2+0x38], r1                     
    ldxdw r5, [r10-0x170]                   
    ldxdw r1, [r5+0x0]                      
    ldxdw r3, [r5+0x8]                      
    ldxdw r4, [r5+0x10]                     
    ldxdw r5, [r5+0x18]                     
    stxdw [r2+0x70], r5                     
    stxdw [r2+0x68], r4                     
    stxdw [r2+0x60], r3                     
    stxdw [r2+0x58], r1                     
    ldxdw r1, [r10-0x1a8]                   
    stxb [r2+0xf9], r1                      
    ldxdw r1, [r10-0x198]                   
    stxb [r2+0xf8], r1                      
    ldxdw r5, [r10-0x1b8]                   
    ldxdw r1, [r5+0x0]                      
    ldxdw r3, [r5+0x8]                      
    ldxdw r4, [r5+0x10]                     
    ldxdw r5, [r5+0x18]                     
    stxdw [r2+0x90], r5                     
    stxdw [r2+0x88], r4                     
    stxdw [r2+0x80], r3                     
    stxdw [r2+0x78], r1                     
    ldxdw r5, [r10-0x1c0]                   
    ldxdw r1, [r5+0x0]                      
    ldxdw r3, [r5+0x8]                      
    ldxdw r4, [r5+0x10]                     
    ldxdw r5, [r5+0x18]                     
    stxdw [r2+0xb0], r5                     
    stxdw [r2+0xa8], r4                     
    stxdw [r2+0xa0], r3                     
    stxdw [r2+0x98], r1                     
    ldxdw r1, [r7+0x0]                      
    ldxdw r3, [r7+0x8]                      
    ldxdw r4, [r7+0x10]                     
    ldxdw r5, [r7+0x18]                     
    stxdw [r2+0xd0], r5                     
    stxdw [r2+0xc8], r4                     
    stxdw [r2+0xc0], r3                     
    stxdw [r2+0xb8], r1                     
    ldxdw r5, [r10-0x150]                   
    ldxdw r1, [r5+0x0]                      
    ldxdw r3, [r5+0x8]                      
    ldxdw r4, [r5+0x10]                     
    ldxdw r5, [r5+0x18]                     
    stxdw [r2+0xf0], r5                     
    stxdw [r2+0xe8], r4                     
    stxdw [r2+0xe0], r3                     
    stxdw [r2+0xd8], r1                     
    ldxdw r1, [r10-0x160]                   
    stxb [r2+0x1], r1                       
    ldxdw r1, [r10-0x158]                   
    stxb [r2+0x0], r1                       
    stb [r2+0x2], 0                         
    mov64 r1, r10                                   r1 = r10
    add64 r1, -304                                  r1 += -304   ///  r1 = r1.wrapping_add(-304 as i32 as i64 as u64)
    ldxdw r3, [r10-0x190]                   
    call function_15520                     
    ldxw r9, [r10-0x130]                    
    jeq r9, 26, lbb_1556                            if r9 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1614                                     if true { pc += 58 }
lbb_1556:
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r1                                    r2 = r1
    add64 r2, 72                                    r2 += 72   ///  r2 = r2.wrapping_add(72 as i32 as i64 as u64)
    mov64 r7, r1                                    r7 = r1
    add64 r7, 96                                    r7 += 96   ///  r7 = r7.wrapping_add(96 as i32 as i64 as u64)
    mov64 r3, r1                                    r3 = r1
    add64 r3, 56                                    r3 += 56   ///  r3 = r3.wrapping_add(56 as i32 as i64 as u64)
    ldxdw r5, [r10-0x180]                   
    ldxb r4, [r5+0x0]                       
    and64 r4, 247                                   r4 &= 247   ///  r4 = r4.and(247)
    stxb [r5+0x0], r4                       
    stxdw [r10-0x8], r2                     
    stxdw [r10-0x10], r7                    
    ldxdw r2, [r10-0x1a0]                   
    stxdw [r10-0x18], r2                    
    ldxdw r2, [r10-0x1c8]                   
    stxdw [r10-0x20], r2                    
    stxdw [r10-0x28], r3                    
    stxdw [r10-0x30], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -312                                  r1 += -312   ///  r1 = r1.wrapping_add(-312 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -48                                   r2 += -48   ///  r2 = r2.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_17934                     
    ldxw r6, [r10-0x134]                    
    ldxw r9, [r10-0x138]                    
    jne r9, 26, lbb_1256                            if r9 != (26 as i32 as i64 as u64) { pc += -329 }
    mov64 r1, r8                                    r1 = r8
    add64 r1, 80                                    r1 += 80   ///  r1 = r1.wrapping_add(80 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    add64 r2, 64                                    r2 += 64   ///  r2 = r2.wrapping_add(64 as i32 as i64 as u64)
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x10], r7                    
    ldxdw r1, [r10-0x1b0]                   
    stxdw [r10-0x18], r1                    
    ldxdw r1, [r10-0x1c8]                   
    stxdw [r10-0x20], r1                    
    stxdw [r10-0x28], r2                    
    stxdw [r10-0x30], r8                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -320                                  r1 += -320   ///  r1 = r1.wrapping_add(-320 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -48                                   r2 += -48   ///  r2 = r2.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_17934                     
    ldxw r6, [r10-0x13c]                    
    ldxw r9, [r10-0x140]                    
    jne r9, 26, lbb_1256                            if r9 != (26 as i32 as i64 as u64) { pc += -351 }
    mov64 r9, 26                                    r9 = 26 as i32 as i64 as u64
    ja lbb_1258                                     if true { pc += -351 }
lbb_1609:
    ldxw r6, [r10-0x124]                    
    ja lbb_1258                                     if true { pc += -353 }
lbb_1611:
    ldxw r6, [r10-0x28]                     
    ldxw r9, [r10-0x2c]                     
    ja lbb_1615                                     if true { pc += 1 }
lbb_1614:
    ldxw r6, [r10-0x12c]                    
lbb_1615:
    ldxdw r2, [r10-0x180]                   
    ldxb r1, [r2+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r2+0x0], r1                       
    ja lbb_1258                                     if true { pc += -362 }

function_1620:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r1, 10                                    r1 = 10 as i32 as i64 as u64
    jne r3, 3, lbb_1667                             if r3 != (3 as i32 as i64 as u64) { pc += 43 }
    mov64 r8, r7                                    r8 = r7
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r8                                    r3 = r8
    call function_6425                      
    ldxw r1, [r10-0x18]                     
    jne r1, 26, lbb_1666                            if r1 != (26 as i32 as i64 as u64) { pc += 33 }
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r8, [r8+0x0]                      
    ldxb r3, [r8+0x0]                       
    mov64 r4, r3                                    r4 = r3
    and64 r4, 15                                    r4 &= 15   ///  r4 = r4.and(15)
    jeq r4, 0, lbb_1641                             if r4 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1667                                     if true { pc += 26 }
lbb_1641:
    or64 r3, 8                                      r3 |= 8   ///  r3 = r3.or(8)
    stxb [r8+0x0], r3                       
    ldxdw r3, [r8+0x50]                     
    mov64 r2, r8                                    r2 = r8
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_10307                     
    ldxw r1, [r10-0x10]                     
    jne r1, 0, lbb_1670                             if r1 != (0 as i32 as i64 as u64) { pc += 19 }
    ldxdw r1, [r10-0x8]                     
    ldxdw r2, [r7+0x10]                     
    ldxdw r3, [r2+0x8]                      
    ldxdw r4, [r2+0x10]                     
    ldxdw r5, [r2+0x18]                     
    ldxdw r2, [r2+0x20]                     
    stxdw [r1+0x18], r2                     
    stxdw [r1+0x10], r5                     
    stxdw [r1+0x8], r4                      
    stxdw [r1+0x0], r3                      
    ldxb r1, [r8+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r8+0x0], r1                       
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ja lbb_1667                                     if true { pc += 1 }
lbb_1666:
    ldxw r2, [r10-0x14]                     
lbb_1667:
    stxw [r6+0x4], r2                       
    stxw [r6+0x0], r1                       
    exit                                    
lbb_1670:
    ldxw r2, [r10-0x8]                      
    ldxw r1, [r10-0xc]                      
    ldxb r3, [r8+0x0]                       
    and64 r3, 247                                   r3 &= 247   ///  r3 = r3.and(247)
    stxb [r8+0x0], r3                       
    ja lbb_1667                                     if true { pc += -9 }

function_1676:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r1, 10                                    r1 = 10 as i32 as i64 as u64
    jne r3, 3, lbb_1723                             if r3 != (3 as i32 as i64 as u64) { pc += 43 }
    mov64 r8, r7                                    r8 = r7
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r8                                    r3 = r8
    call function_6425                      
    ldxw r1, [r10-0x18]                     
    jne r1, 26, lbb_1722                            if r1 != (26 as i32 as i64 as u64) { pc += 33 }
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r8, [r8+0x0]                      
    ldxb r3, [r8+0x0]                       
    mov64 r4, r3                                    r4 = r3
    and64 r4, 15                                    r4 &= 15   ///  r4 = r4.and(15)
    jeq r4, 0, lbb_1697                             if r4 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1723                                     if true { pc += 26 }
lbb_1697:
    or64 r3, 8                                      r3 |= 8   ///  r3 = r3.or(8)
    stxb [r8+0x0], r3                       
    ldxdw r3, [r8+0x50]                     
    mov64 r2, r8                                    r2 = r8
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_10307                     
    ldxw r1, [r10-0x10]                     
    jne r1, 0, lbb_1726                             if r1 != (0 as i32 as i64 as u64) { pc += 19 }
    ldxdw r1, [r10-0x8]                     
    ldxdw r2, [r7+0x10]                     
    ldxdw r3, [r2+0x8]                      
    ldxdw r4, [r2+0x10]                     
    ldxdw r5, [r2+0x18]                     
    ldxdw r2, [r2+0x20]                     
    stxdw [r1+0x38], r2                     
    stxdw [r1+0x30], r5                     
    stxdw [r1+0x28], r4                     
    stxdw [r1+0x20], r3                     
    ldxb r1, [r8+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r8+0x0], r1                       
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ja lbb_1723                                     if true { pc += 1 }
lbb_1722:
    ldxw r2, [r10-0x14]                     
lbb_1723:
    stxw [r6+0x4], r2                       
    stxw [r6+0x0], r1                       
    exit                                    
lbb_1726:
    ldxw r2, [r10-0x8]                      
    ldxw r1, [r10-0xc]                      
    ldxb r3, [r8+0x0]                       
    and64 r3, 247                                   r3 &= 247   ///  r3 = r3.and(247)
    stxb [r8+0x0], r3                       
    ja lbb_1723                                     if true { pc += -9 }

function_1732:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r8, 10                                    r8 = 10 as i32 as i64 as u64
    jne r3, 11, lbb_1905                            if r3 != (11 as i32 as i64 as u64) { pc += 169 }
    stxdw [r10-0x88], r4                    
    stxdw [r10-0x80], r5                    
    mov64 r3, r7                                    r3 = r7
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_6425                      
    ldxw r8, [r10-0x50]                     
    jne r8, 26, lbb_1904                            if r8 != (26 as i32 as i64 as u64) { pc += 158 }
    ldxdw r9, [r7+0x10]                     
    mov64 r1, r9                                    r1 = r9
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033328 --> b"\x06\x8747\xb4\xfb\xc9\xf4\x047\xdd<\xe6?\x9c\xfe\x8c$\x84\x03U\x93\xa1'\…        r2 load str located at 4295177000
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, 4                                     r1 = 4 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_1759                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1905                                     if true { pc += 146 }
lbb_1759:
    mov64 r8, 11                                    r8 = 11 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxb r2, [r9+0x0]                       
    mov64 r3, r2                                    r3 = r2
    and64 r3, 15                                    r3 &= 15   ///  r3 = r3.and(15)
    jne r3, 0, lbb_1905                             if r3 != (0 as i32 as i64 as u64) { pc += 140 }
    or64 r2, 8                                      r2 |= 8   ///  r2 = r2.or(8)
    stxb [r9+0x0], r2                       
    ldxdw r3, [r9+0x50]                     
    mov64 r2, r9                                    r2 = r9
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    call function_15585                     
    ldxw r1, [r10-0x28]                     
    mov64 r8, r9                                    r8 = r9
    jne r1, 0, lbb_1908                             if r1 != (0 as i32 as i64 as u64) { pc += 132 }
    ldxdw r1, [r10-0x20]                    
    stxdw [r10-0x90], r1                    
    ldxb r4, [r1+0x1]                       
    ldxdw r3, [r7+0x40]                     
    ldxdw r2, [r7+0x38]                     
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    call function_17371                     
    ldxdw r1, [r10-0x10]                    
    stxdw [r10-0x30], r1                    
    ldxdw r1, [r10-0x18]                    
    stxdw [r10-0x38], r1                    
    ldxdw r1, [r10-0x20]                    
    stxdw [r10-0x40], r1                    
    ldxdw r1, [r10-0x28]                    
    stxdw [r10-0x48], r1                    
    mov64 r9, r8                                    r9 = r8
    mov64 r2, r8                                    r2 = r8
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    stxdw [r10-0x98], r2                    
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, 4                                     r1 = 4 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_1808                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1911                                     if true { pc += 103 }
lbb_1808:
    ldxdw r2, [r7+0x18]                     
    ldxdw r1, [r10-0x90]                    
    add64 r1, 120                                   r1 += 120   ///  r1 = r1.wrapping_add(120 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 7                                     r1 = 7 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_1911                             if r0 != (0 as i32 as i64 as u64) { pc += 93 }
    ldxdw r2, [r7+0x20]                     
    ldxdw r1, [r10-0x90]                    
    add64 r1, 152                                   r1 += 152   ///  r1 = r1.wrapping_add(152 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_1911                             if r0 != (0 as i32 as i64 as u64) { pc += 83 }
    ldxdw r1, [r10-0x90]                    
    mov64 r1, r7                                    r1 = r7
    add64 r1, 16                                    r1 += 16   ///  r1 = r1.wrapping_add(16 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    add64 r2, 56                                    r2 += 56   ///  r2 = r2.wrapping_add(56 as i32 as i64 as u64)
    mov64 r3, r7                                    r3 = r7
    add64 r3, 24                                    r3 += 24   ///  r3 = r3.wrapping_add(24 as i32 as i64 as u64)
    stxdw [r10-0xa0], r1                    
    stxdw [r10-0x1000], r1                  
    mov64 r1, r7                                    r1 = r7
    add64 r1, 72                                    r1 += 72   ///  r1 = r1.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0xff0], r1                   
    stxdw [r10-0xff8], r7                   
    stdw [r10-0xfe8], 1                     
    mov64 r4, r7                                    r4 = r7
    add64 r4, 40                                    r4 += 40   ///  r4 = r4.wrapping_add(40 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    stxdw [r10-0xb0], r2                    
    stxdw [r10-0xa8], r3                    
    stxdw [r10-0xb8], r4                    
    call function_5754                      
    ldxw r8, [r10-0x58]                     
    jne r8, 26, lbb_1915                            if r8 != (26 as i32 as i64 as u64) { pc += 62 }
    mov64 r2, r7                                    r2 = r7
    add64 r2, 64                                    r2 += 64   ///  r2 = r2.wrapping_add(64 as i32 as i64 as u64)
    mov64 r3, r7                                    r3 = r7
    add64 r3, 32                                    r3 += 32   ///  r3 = r3.wrapping_add(32 as i32 as i64 as u64)
    ldxdw r1, [r10-0xa0]                    
    stxdw [r10-0x1000], r1                  
    mov64 r1, r7                                    r1 = r7
    add64 r1, 80                                    r1 += 80   ///  r1 = r1.wrapping_add(80 as i32 as i64 as u64)
    stxdw [r10-0xff0], r1                   
    stxdw [r10-0xff8], r7                   
    stdw [r10-0xfe8], 0                     
    mov64 r4, r7                                    r4 = r7
    add64 r4, 48                                    r4 += 48   ///  r4 = r4.wrapping_add(48 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    stxdw [r10-0xc0], r2                    
    stxdw [r10-0xa0], r3                    
    stxdw [r10-0xc8], r4                    
    call function_5754                      
    ldxw r8, [r10-0x60]                     
    jeq r8, 26, lbb_1876                            if r8 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1917                                     if true { pc += 41 }
lbb_1876:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    ldxdw r2, [r10-0x90]                    
    ldxdw r3, [r10-0x98]                    
    call function_15520                     
    ldxw r8, [r10-0x68]                     
    jeq r8, 26, lbb_1884                            if r8 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_1919                                     if true { pc += 35 }
lbb_1884:
    ldxdw r2, [r10-0x90]                    
    ldxb r1, [r2+0xf9]                      
    stxdw [r10-0x98], r1                    
    ldxb r1, [r2+0xf8]                      
    stxdw [r10-0x90], r1                    
    ldxb r1, [r9+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r9+0x0], r1                       
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    ldxdw r2, [r10-0x88]                    
    ldxdw r3, [r10-0x80]                    
    call function_446                       
    ldxdw r1, [r10-0x28]                    
    jeq r1, 0, lbb_1921                             if r1 == (0 as i32 as i64 as u64) { pc += 22 }
    ldxdw r2, [r10-0x20]                    
    call function_160                       
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, 32                                    r1 = 32 as i32 as i64 as u64
    ja lbb_1905                                     if true { pc += 1 }
lbb_1904:
    ldxw r1, [r10-0x4c]                     
lbb_1905:
    stxw [r6+0x4], r1                       
    stxw [r6+0x0], r8                       
    exit                                    
lbb_1908:
    mov64 r9, r8                                    r9 = r8
    ldxw r1, [r10-0x20]                     
    ldxw r8, [r10-0x24]                     
lbb_1911:
    ldxb r2, [r9+0x0]                       
    and64 r2, 247                                   r2 &= 247   ///  r2 = r2.and(247)
    stxb [r9+0x0], r2                       
    ja lbb_1905                                     if true { pc += -10 }
lbb_1915:
    ldxw r1, [r10-0x54]                     
    ja lbb_1911                                     if true { pc += -6 }
lbb_1917:
    ldxw r1, [r10-0x5c]                     
    ja lbb_1911                                     if true { pc += -8 }
lbb_1919:
    ldxw r1, [r10-0x64]                     
    ja lbb_1911                                     if true { pc += -10 }
lbb_1921:
    ldxdw r9, [r10-0x18]                    
    ldxdw r8, [r10-0x20]                    
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, r8                                    r2 = r8
    call function_160                       
    jeq r8, 0, lbb_1946                             if r8 == (0 as i32 as i64 as u64) { pc += 19 }
    ldxdw r1, [r7+0x48]                     
    stxdw [r10-0xff0], r8                   
    ldxdw r2, [r10-0x90]                    
    stxdw [r10-0xfe8], r2                   
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0xb0]                    
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0xb8]                    
    ldxdw r3, [r10-0xa8]                    
    mov64 r4, r7                                    r4 = r7
    call function_4870                      
    ldxw r8, [r10-0x70]                     
    jeq r8, 26, lbb_1946                            if r8 == (26 as i32 as i64 as u64) { pc += 2 }
    ldxw r1, [r10-0x6c]                     
    ja lbb_1905                                     if true { pc += -41 }
lbb_1946:
    jeq r9, 0, lbb_1966                             if r9 == (0 as i32 as i64 as u64) { pc += 19 }
    ldxdw r1, [r7+0x50]                     
    stxdw [r10-0xff0], r9                   
    ldxdw r2, [r10-0x98]                    
    stxdw [r10-0xfe8], r2                   
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0xc0]                    
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -120                                  r1 += -120   ///  r1 = r1.wrapping_add(-120 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0xc8]                    
    ldxdw r3, [r10-0xa0]                    
    mov64 r4, r7                                    r4 = r7
    call function_4870                      
    ldxw r8, [r10-0x78]                     
    jeq r8, 26, lbb_1966                            if r8 == (26 as i32 as i64 as u64) { pc += 2 }
    ldxw r1, [r10-0x74]                     
    ja lbb_1905                                     if true { pc += -61 }
lbb_1966:
    mov64 r8, 26                                    r8 = 26 as i32 as i64 as u64
    ja lbb_1905                                     if true { pc += -63 }

function_1968:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r1, 10                                    r1 = 10 as i32 as i64 as u64
    jne r3, 4, lbb_2151                             if r3 != (4 as i32 as i64 as u64) { pc += 179 }
    mov64 r1, 7                                     r1 = 7 as i32 as i64 as u64
    ldxdw r9, [r7+0x0]                      
    ldxb r3, [r9+0x1]                       
    jeq r3, 0, lbb_2151                             if r3 == (0 as i32 as i64 as u64) { pc += 175 }
    ldxdw r1, [r7+0x18]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x100033408 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r2 load str located at 4295177224
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 3                                     r1 = 3 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2151                             if r0 != (0 as i32 as i64 as u64) { pc += 165 }
    ldxdw r8, [r7+0x8]                      
    mov64 r2, r8                                    r2 = r8
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    lddw r1, 0x100033448 --> b"\xa4\xa7M\x03\xd49\xb1\xf5\xd9\xdd0\x98\xcf\xfe\x87\xf0\xfb\x9c\xeey\xea\…        r1 load str located at 4295177288
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 3                                     r2 = 3 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2151                             if r0 != (0 as i32 as i64 as u64) { pc += 153 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    call function_19884                     
    ldxw r1, [r10-0x48]                     
    jne r1, 0, lbb_2015                             if r1 != (0 as i32 as i64 as u64) { pc += 12 }
    stxdw [r10-0x90], r8                    
    mov64 r3, r7                                    r3 = r7
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r1, [r10-0x40]                    
    ldxdw r8, [r10-0x38]                    
    lddw r2, 0x4000000000000000                     r2 load str located at 4611686018427387904
    jeq r8, r2, lbb_2012                            if r8 == r2 { pc += 1 }
    ja lbb_2018                                     if true { pc += 6 }
lbb_2012:
    mul64 r1, 960                                   r1 *= 960   ///  r1 = r1.wrapping_mul(960 as u64)
    ldxdw r8, [r10-0x90]                    
    ja lbb_2045                                     if true { pc += 30 }
lbb_2015:
    ldxw r2, [r10-0x40]                     
    ldxw r1, [r10-0x44]                     
    ja lbb_2151                                     if true { pc += 133 }
lbb_2018:
    stxdw [r10-0x98], r3                    
    mul64 r1, 480                                   r1 *= 480   ///  r1 = r1.wrapping_mul(480 as u64)
    call function_23211                     
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r0                                    r2 = r0
    call function_23463                     
    mov64 r8, r0                                    r8 = r0
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xa0], r1                    
    mov64 r1, r8                                    r1 = r8
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_26086                     
    stxdw [r10-0xa8], r0                    
    mov64 r1, r8                                    r1 = r8
    call function_23128                     
    ldxdw r1, [r10-0xa8]                    
    jslt r1, 0, lbb_2036                            if (r1 as i64) < (0 as i32 as i64) { pc += 1 }
    stxdw [r10-0xa0], r0                    
lbb_2036:
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_25957                     
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
    jsgt r0, 0, lbb_2043                            if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    ldxdw r1, [r10-0xa0]                    
lbb_2043:
    ldxdw r8, [r10-0x90]                    
    ldxdw r3, [r10-0x98]                    
lbb_2045:
    stb [r10-0x79], 255                     
    mov64 r2, r10                                   r2 = r10
    add64 r2, -121                                  r2 += -121   ///  r2 = r2.wrapping_add(-121 as i32 as i64 as u64)
    stxdw [r10-0x68], r2                    
    lddw r2, 0x100033500 --> b"configprograms/solfi-v2-program/src/instructions/u"        r2 load str located at 4295177472
    stxdw [r10-0x78], r2                    
    stdw [r10-0x60], 1                      
    stdw [r10-0x70], 6                      
    mov64 r2, r10                                   r2 = r10
    add64 r2, -120                                  r2 += -120   ///  r2 = r2.wrapping_add(-120 as i32 as i64 as u64)
    stxdw [r10-0x58], r2                    
    stdw [r10-0x50], 2                      
    stxdw [r10-0x30], r1                    
    stxdw [r10-0x40], r3                    
    stxdw [r10-0x48], r7                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x38], r1                    
    lddw r1, 0x961634be972aba81                     r1 load str located at -7631854525348136319
    stxdw [r10-0x8], r1                     
    lddw r1, 0x27a193550384248c                     r1 load str located at 2855725632070100108
    stxdw [r10-0x10], r1                    
    lddw r1, 0xfe9c3fe63cdd3704                     r1 load str located at -100134833612835068
    stxdw [r10-0x18], r1                    
    lddw r1, 0xf4c9fbb437348706                     r1 load str located at -807837906697419002
    stxdw [r10-0x20], r1                    
    stdw [r10-0x28], 352                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -136                                  r1 += -136   ///  r1 = r1.wrapping_add(-136 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -72                                   r2 += -72   ///  r2 = r2.wrapping_add(-72 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -88                                   r3 += -88   ///  r3 = r3.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_7957                      
    ldxw r1, [r10-0x88]                     
    jne r1, 26, lbb_2143                            if r1 != (26 as i32 as i64 as u64) { pc += 56 }
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxb r3, [r8+0x0]                       
    mov64 r4, r3                                    r4 = r3
    and64 r4, 15                                    r4 &= 15   ///  r4 = r4.and(15)
    jeq r4, 0, lbb_2094                             if r4 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2151                                     if true { pc += 57 }
lbb_2094:
    or64 r3, 8                                      r3 |= 8   ///  r3 = r3.or(8)
    stxb [r8+0x0], r3                       
    ldxdw r3, [r8+0x50]                     
    mov64 r2, r8                                    r2 = r8
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    call function_10307                     
    ldxw r1, [r10-0x48]                     
    jne r1, 0, lbb_2145                             if r1 != (0 as i32 as i64 as u64) { pc += 41 }
    ldxdw r8, [r10-0x40]                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x38], 0                      
    stdw [r10-0x40], 0                      
    stdw [r10-0x48], 0                      
    mov64 r2, r10                                   r2 = r10
    add64 r2, -72                                   r2 += -72   ///  r2 = r2.wrapping_add(-72 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 3                                     r2 = 3 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_2120                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2147                                     if true { pc += 27 }
lbb_2120:
    ldxdw r1, [r9+0x8]                      
    ldxdw r2, [r9+0x10]                     
    ldxdw r3, [r9+0x18]                     
    ldxdw r4, [r9+0x20]                     
    stxdw [r8+0x18], r4                     
    stxdw [r8+0x10], r3                     
    stxdw [r8+0x8], r2                      
    stxdw [r8+0x0], r1                      
    ldxdw r1, [r7+0x10]                     
    ldxdw r2, [r1+0x8]                      
    ldxdw r3, [r1+0x10]                     
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r1+0x20]                     
    stxdw [r8+0x38], r1                     
    stxdw [r8+0x30], r4                     
    stxdw [r8+0x28], r3                     
    stxdw [r8+0x20], r2                     
    ldxdw r2, [r10-0x90]                    
    ldxb r1, [r2+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r2+0x0], r1                       
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ja lbb_2151                                     if true { pc += 8 }
lbb_2143:
    ldxw r2, [r10-0x84]                     
    ja lbb_2151                                     if true { pc += 6 }
lbb_2145:
    ldxw r2, [r10-0x40]                     
    ldxw r1, [r10-0x44]                     
lbb_2147:
    ldxdw r4, [r10-0x90]                    
    ldxb r3, [r4+0x0]                       
    and64 r3, 247                                   r3 &= 247   ///  r3 = r3.and(247)
    stxb [r4+0x0], r3                       
lbb_2151:
    stxw [r6+0x4], r2                       
    stxw [r6+0x0], r1                       
    exit                                    

function_2154:
    mov64 r9, 10                                    r9 = 10 as i32 as i64 as u64
    jne r3, 4, lbb_2200                             if r3 != (4 as i32 as i64 as u64) { pc += 44 }
    mov64 r9, 2                                     r9 = 2 as i32 as i64 as u64
    jne r4, 0, lbb_2200                             if r4 != (0 as i32 as i64 as u64) { pc += 42 }
    mov64 r8, r1                                    r8 = r1
    mov64 r3, r2                                    r3 = r2
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -56                                   r1 += -56   ///  r1 = r1.wrapping_add(-56 as i32 as i64 as u64)
    stxdw [r10-0x48], r2                    
    call function_6494                      
    ldxw r9, [r10-0x38]                     
    jne r9, 26, lbb_2198                            if r9 != (26 as i32 as i64 as u64) { pc += 31 }
    ldxdw r1, [r10-0x48]                    
    ldxdw r7, [r1+0x10]                     
    ldxdw r2, [r1+0x18]                     
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r6, 38                                    r6 = 38 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r1, r8                                    r1 = r8
    jeq r0, 0, lbb_2182                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2200                                     if true { pc += 18 }
lbb_2182:
    ldxdw r2, [r7+0x50]                     
    jne r2, 1048576, lbb_2200                       if r2 != (1048576 as i32 as i64 as u64) { pc += 16 }
    stxdw [r10-0x50], r7                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    call function_19884                     
    ldxw r1, [r10-0x20]                     
    jne r1, 0, lbb_2203                             if r1 != (0 as i32 as i64 as u64) { pc += 13 }
    ldxdw r2, [r10-0x18]                    
    ldxdw r7, [r10-0x10]                    
    lddw r1, 0x4000000000000000                     r1 load str located at 4611686018427387904
    jeq r7, r1, lbb_2196                            if r7 == r1 { pc += 1 }
    ja lbb_2206                                     if true { pc += 10 }
lbb_2196:
    mul64 r2, 2097408                               r2 *= 2097408   ///  r2 = r2.wrapping_mul(2097408 as u64)
    ja lbb_2232                                     if true { pc += 34 }
lbb_2198:
    ldxw r6, [r10-0x34]                     
lbb_2199:
    mov64 r1, r8                                    r1 = r8
lbb_2200:
    stxw [r1+0x4], r6                       
    stxw [r1+0x0], r9                       
    exit                                    
lbb_2203:
    ldxw r6, [r10-0x18]                     
    ldxw r9, [r10-0x1c]                     
    ja lbb_2199                                     if true { pc += -7 }
lbb_2206:
    mul64 r2, 1048704                               r2 *= 1048704   ///  r2 = r2.wrapping_mul(1048704 as u64)
    mov64 r1, r2                                    r1 = r2
    call function_23211                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r0                                    r2 = r0
    call function_23463                     
    mov64 r7, r0                                    r7 = r0
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x58], r1                    
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_26086                     
    stxdw [r10-0x68], r0                    
    stxdw [r10-0x60], r7                    
    mov64 r1, r7                                    r1 = r7
    call function_23128                     
    ldxdw r1, [r10-0x68]                    
    jslt r1, 0, lbb_2225                            if (r1 as i64) < (0 as i32 as i64) { pc += 1 }
    stxdw [r10-0x58], r0                    
lbb_2225:
    ldxdw r1, [r10-0x60]                    
    lddw r2, 0x43efffffffffffff                     r2 load str located at 4895412794951729151
    call function_25957                     
    mov64 r2, -1                                    r2 = -1 as i32 as i64 as u64
    jsgt r0, 0, lbb_2232                            if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    ldxdw r2, [r10-0x58]                    
lbb_2232:
    mov64 r1, r8                                    r1 = r8
    ldxdw r3, [r10-0x48]                    
    ldxdw r4, [r10-0x50]                    
    ldxdw r4, [r4+0x48]                     
    jne r4, r2, lbb_2200                            if r4 != r2 { pc += -37 }
    mov64 r1, r3                                    r1 = r3
    add64 r1, 16                                    r1 += 16   ///  r1 = r1.wrapping_add(16 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -32                                   r2 += -32   ///  r2 = r2.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x28], r2                    
    stxdw [r10-0x30], r1                    
    lddw r1, 0x961634be972aba81                     r1 load str located at -7631854525348136319
    stxdw [r10-0x8], r1                     
    lddw r1, 0x27a193550384248c                     r1 load str located at 2855725632070100108
    stxdw [r10-0x10], r1                    
    lddw r1, 0xfe9c3fe63cdd3704                     r1 load str located at -100134833612835068
    stxdw [r10-0x18], r1                    
    lddw r1, 0xf4c9fbb437348706                     r1 load str located at -807837906697419002
    stxdw [r10-0x20], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -48                                   r2 += -48   ///  r2 = r2.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_7885                      
    ldxw r9, [r10-0x40]                     
    jne r9, 26, lbb_2306                            if r9 != (26 as i32 as i64 as u64) { pc += 41 }
    mov64 r9, 11                                    r9 = 11 as i32 as i64 as u64
    ldxdw r7, [r10-0x50]                    
    ldxb r2, [r7+0x0]                       
    mov64 r3, r2                                    r3 = r2
    and64 r3, 15                                    r3 &= 15   ///  r3 = r3.and(15)
    mov64 r1, r8                                    r1 = r8
    jeq r3, 0, lbb_2273                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2200                                     if true { pc += -73 }
lbb_2273:
    or64 r2, 8                                      r2 |= 8   ///  r2 = r2.or(8)
    stxb [r7+0x0], r2                       
    mov64 r6, r7                                    r6 = r7
    add64 r6, 88                                    r6 += 88   ///  r6 = r6.wrapping_add(88 as i32 as i64 as u64)
    ldxdw r9, [r7+0x50]                     
    jeq r9, 0, lbb_2283                             if r9 == (0 as i32 as i64 as u64) { pc += 4 }
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r3, r9                                    r3 = r9
    call function_23160                     
lbb_2283:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r9                                    r3 = r9
    call function_13476                     
    ldxw r1, [r10-0x20]                     
    jne r1, 0, lbb_2308                             if r1 != (0 as i32 as i64 as u64) { pc += 18 }
    ldxdw r1, [r10-0x18]                    
    ldxdw r2, [r10-0x48]                    
    ldxdw r2, [r2+0x0]                      
    ldxdw r3, [r2+0x8]                      
    ldxdw r4, [r2+0x10]                     
    ldxdw r5, [r2+0x18]                     
    ldxdw r2, [r2+0x20]                     
    stxdw [r1+0x20], r2                     
    stxdw [r1+0x18], r5                     
    stxdw [r1+0x10], r4                     
    stxdw [r1+0x8], r3                      
    ldxb r1, [r7+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r7+0x0], r1                       
    mov64 r9, 26                                    r9 = 26 as i32 as i64 as u64
    ja lbb_2199                                     if true { pc += -107 }
lbb_2306:
    ldxw r6, [r10-0x3c]                     
    ja lbb_2199                                     if true { pc += -109 }
lbb_2308:
    ldxw r6, [r10-0x18]                     
    ldxw r9, [r10-0x1c]                     
    ldxb r1, [r7+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r7+0x0], r1                       
    ja lbb_2199                                     if true { pc += -115 }

function_2314:
    mov64 r9, r5                                    r9 = r5
    mov64 r7, r4                                    r7 = r4
    mov64 r4, r1                                    r4 = r1
    mov64 r6, 10                                    r6 = 10 as i32 as i64 as u64
    jne r3, 13, lbb_2618                            if r3 != (13 as i32 as i64 as u64) { pc += 299 }
    mov64 r6, 7                                     r6 = 7 as i32 as i64 as u64
    ldxdw r1, [r2+0x0]                      
    stxdw [r10-0xa10], r1                   
    ldxb r1, [r1+0x1]                       
    jeq r1, 0, lbb_2618                             if r1 == (0 as i32 as i64 as u64) { pc += 294 }
    ldxdw r1, [r2+0x8]                      
    stxdw [r10-0xa18], r1                   
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xa08], r2                   
    lddw r2, 0x100033328 --> b"\x06\x8747\xb4\xfb\xc9\xf4\x047\xdd<\xe6?\x9c\xfe\x8c$\x84\x03U\x93\xa1'\…        r2 load str located at 4295177000
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    stxdw [r10-0xa00], r4                   
    call function_23165                     
    ldxdw r1, [r10-0xa08]                   
    ldxdw r4, [r10-0xa00]                   
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r8, 4                                     r8 = 4 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2618                             if r0 != (0 as i32 as i64 as u64) { pc += 278 }
    ldxdw r1, [r1+0x60]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x100033348 --> b"\x06\xa7\xd5\x17\x18{\xd1f5\xda\xd4\x04U\xfd\xc2\xc0\xc1$\xc6\x8f!Vu\xa5\…        r2 load str located at 4295177032
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    ldxdw r4, [r10-0xa00]                   
    mov64 r6, 3                                     r6 = 3 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2618                             if r0 != (0 as i32 as i64 as u64) { pc += 267 }
    mov64 r6, 11                                    r6 = 11 as i32 as i64 as u64
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0xa18]                   
    ldxb r1, [r1+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    jne r2, 0, lbb_2618                             if r2 != (0 as i32 as i64 as u64) { pc += 260 }
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    ldxdw r6, [r10-0xa18]                   
    stxb [r6+0x0], r1                       
    ldxdw r3, [r6+0x50]                     
    add64 r6, 88                                    r6 += 88   ///  r6 = r6.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    call function_15574                     
    ldxw r1, [r10-0x818]                    
    jne r1, 0, lbb_2611                             if r1 != (0 as i32 as i64 as u64) { pc += 242 }
    stxdw [r10-0xa20], r6                   
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r8, 46                                    r8 = 46 as i32 as i64 as u64
    ldxdw r2, [r10-0x810]                   
    ldxb r1, [r2+0x2]                       
    ldxdw r4, [r10-0xa00]                   
    jeq r1, 0, lbb_2377                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2614                                     if true { pc += 237 }
lbb_2377:
    mov64 r8, r2                                    r8 = r2
    add64 r2, 704                                   r2 += 704   ///  r2 = r2.wrapping_add(704 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    call function_15984                     
    ldxw r1, [r10-0x818]                    
    jne r1, 0, lbb_2611                             if r1 != (0 as i32 as i64 as u64) { pc += 227 }
    mov64 r2, r8                                    r2 = r8
    mov64 r8, 16                                    r8 = 16 as i32 as i64 as u64
    ldxdw r3, [r10-0x810]                   
    ldxb r1, [r3+0x0]                       
    ldxdw r4, [r10-0xa00]                   
    jeq r1, 1, lbb_2391                             if r1 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2614                                     if true { pc += 223 }
lbb_2391:
    stxdw [r10-0xa38], r3                   
    ldxb r4, [r2+0x1]                       
    ldxdw r1, [r10-0xa08]                   
    ldxdw r3, [r1+0x48]                     
    stxdw [r10-0xa28], r2                   
    ldxdw r2, [r1+0x40]                     
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    stxdw [r10-0xa40], r2                   
    stxdw [r10-0xa48], r3                   
    call function_17371                     
    ldxdw r1, [r10-0x800]                   
    stxdw [r10-0x980], r1                   
    ldxdw r1, [r10-0x808]                   
    stxdw [r10-0x988], r1                   
    ldxdw r1, [r10-0x810]                   
    stxdw [r10-0x990], r1                   
    ldxdw r1, [r10-0x818]                   
    stxdw [r10-0x998], r1                   
    ldxb r1, [r10-0x7f8]                    
    stxdw [r10-0xa30], r1                   
    ldxdw r2, [r10-0xa18]                   
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2456                                 r1 += -2456   ///  r1 = r1.wrapping_add(-2456 as i32 as i64 as u64)
    stxdw [r10-0xa50], r2                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r8, 4                                     r8 = 4 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_2426                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
lbb_2425:
    ja lbb_2613                                     if true { pc += 187 }
lbb_2426:
    ldxdw r1, [r10-0xa28]                   
    ldxb r1, [r1+0x0]                       
    ldxdw r2, [r10-0xa30]                   
    jne r2, r1, lbb_2425                            if r2 != r1 { pc += -5 }
    ldxdw r1, [r10-0xa28]                   
    ldxdw r8, [r10-0xa08]                   
    mov64 r1, r8                                    r1 = r8
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    add64 r2, 64                                    r2 += 64   ///  r2 = r2.wrapping_add(64 as i32 as i64 as u64)
    stxdw [r10-0xa30], r1                   
    stxdw [r10-0x1000], r1                  
    stxdw [r10-0xff8], r8                   
    mov64 r1, r8                                    r1 = r8
    add64 r1, 80                                    r1 += 80   ///  r1 = r1.wrapping_add(80 as i32 as i64 as u64)
    stxdw [r10-0xa70], r1                   
    stxdw [r10-0xff0], r1                   
    stdw [r10-0xfe8], 1                     
    mov64 r3, r8                                    r3 = r8
    add64 r3, 32                                    r3 += 32   ///  r3 = r3.wrapping_add(32 as i32 as i64 as u64)
    mov64 r4, r8                                    r4 = r8
    add64 r4, 48                                    r4 += 48   ///  r4 = r4.wrapping_add(48 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2464                                 r1 += -2464   ///  r1 = r1.wrapping_add(-2464 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    stxdw [r10-0xa68], r2                   
    stxdw [r10-0xa60], r3                   
    stxdw [r10-0xa58], r4                   
    call function_5754                      
    ldxw r1, [r10-0x9a0]                    
    jne r1, 26, lbb_2546                            if r1 != (26 as i32 as i64 as u64) { pc += 89 }
    mov64 r2, r8                                    r2 = r8
    add64 r2, 72                                    r2 += 72   ///  r2 = r2.wrapping_add(72 as i32 as i64 as u64)
    ldxdw r1, [r10-0xa30]                   
    stxdw [r10-0x1000], r1                  
    stxdw [r10-0xff8], r8                   
    mov64 r1, r8                                    r1 = r8
    add64 r1, 88                                    r1 += 88   ///  r1 = r1.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xa90], r1                   
    stxdw [r10-0xff0], r1                   
    stdw [r10-0xfe8], 0                     
    mov64 r3, r8                                    r3 = r8
    add64 r3, 40                                    r3 += 40   ///  r3 = r3.wrapping_add(40 as i32 as i64 as u64)
    mov64 r4, r8                                    r4 = r8
    add64 r4, 56                                    r4 += 56   ///  r4 = r4.wrapping_add(56 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2472                                 r1 += -2472   ///  r1 = r1.wrapping_add(-2472 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    stxdw [r10-0xa88], r2                   
    stxdw [r10-0xa80], r3                   
    stxdw [r10-0xa78], r4                   
    call function_5754                      
    ldxw r1, [r10-0x9a8]                    
    jeq r1, 26, lbb_2481                            if r1 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2549                                     if true { pc += 68 }
lbb_2481:
    ldxdw r1, [r10-0xa08]                   
    ldxdw r1, [r1+0x18]                     
    stxdw [r10-0xa98], r1                   
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033328 --> b"\x06\x8747\xb4\xfb\xc9\xf4\x047\xdd<\xe6?\x9c\xfe\x8c$\x84\x03U\x93\xa1'\…        r2 load str located at 4295177000
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r8, 38                                    r8 = 38 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    ldxdw r1, [r10-0xa28]                   
    jeq r0, 0, lbb_2495                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2425                                     if true { pc += -70 }
lbb_2495:
    ldxdw r1, [r10-0xa98]                   
    ldxdw r1, [r1+0x50]                     
    jne r1, 1048576, lbb_2425                       if r1 != (1048576 as i32 as i64 as u64) { pc += -73 }
    ldxdw r1, [r10-0xa98]                   
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r2, [r10-0xa28]                   
    add64 r2, 256                                   r2 += 256   ///  r2 = r2.wrapping_add(256 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2425                             if r0 != (0 as i32 as i64 as u64) { pc += -82 }
    ldxdw r1, [r10-0xa08]                   
    ldxdw r1, [r1+0x10]                     
    stxdw [r10-0xa98], r1                   
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033428 --> b"\xc1\xa4\x86\xeb\x03\xafw\xa7B\x17\xc6\x96"\xcf\x9e5\x0ea\xbbBK?_\x11?\x9…        r2 load str located at 4295177256
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r8, 29                                    r8 = 29 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2425                             if r0 != (0 as i32 as i64 as u64) { pc += -94 }
    ldxdw r1, [r10-0xa98]                   
    ldxdw r1, [r1+0x50]                     
    jne r1, 168, lbb_2425                           if r1 != (168 as i32 as i64 as u64) { pc += -97 }
    ldxdw r1, [r10-0xa28]                   
    add64 r1, 24                                    r1 += 24   ///  r1 = r1.wrapping_add(24 as i32 as i64 as u64)
    ldxdw r2, [r10-0xa98]                   
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_2425                             if r0 != (0 as i32 as i64 as u64) { pc += -106 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2288                                 r1 += -2288   ///  r1 = r1.wrapping_add(-2288 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r9                                    r3 = r9
    call function_468                       
    ldxb r8, [r10-0x8df]                    
    jeq r8, 2, lbb_2552                             if r8 == (2 as i32 as i64 as u64) { pc += 14 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ldxb r1, [r10-0x8e0]                    
    stxdw [r10-0xaa0], r1                   
    ldxdw r1, [r10-0x8e8]                   
    stxdw [r10-0xab0], r1                   
    ldxdw r1, [r10-0x8f0]                   
    stxdw [r10-0xaa8], r1                   
    ja lbb_2577                                     if true { pc += 31 }
lbb_2546:
    ldxw r8, [r10-0x99c]                    
    mov64 r6, r1                                    r6 = r1
    ja lbb_2613                                     if true { pc += 64 }
lbb_2549:
    ldxw r8, [r10-0x9a4]                    
    mov64 r6, r1                                    r6 = r1
    ja lbb_2613                                     if true { pc += 61 }
lbb_2552:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r9                                    r3 = r9
    call function_397                       
    ldxb r1, [r10-0x808]                    
    stxdw [r10-0xaa0], r1                   
    jeq r1, 2, lbb_2561                             if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2569                                     if true { pc += 8 }
lbb_2561:
    ldxdw r1, [r10-0x818]                   
    mov64 r2, 2                                     r2 = 2 as i32 as i64 as u64
    call function_145                       
    ldxdw r1, [r10-0x8f0]                   
    mov64 r2, 2                                     r2 = 2 as i32 as i64 as u64
    call function_145                       
    mov64 r8, 34                                    r8 = 34 as i32 as i64 as u64
    ja lbb_2613                                     if true { pc += 44 }
lbb_2569:
    ldxdw r1, [r10-0x810]                   
    stxdw [r10-0xab0], r1                   
    ldxdw r1, [r10-0x818]                   
    stxdw [r10-0xaa8], r1                   
    ldxdw r2, [r10-0xaa0]                   
    call function_145                       
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x8f0]                   
lbb_2577:
    mov64 r2, r8                                    r2 = r8
    call function_145                       
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    ldxdw r2, [r10-0xa58]                   
    call function_6140                      
    ldxw r1, [r10-0x818]                    
    jeq r1, 0, lbb_2586                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2611                                     if true { pc += 25 }
lbb_2586:
    ldxdw r9, [r10-0x810]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    ldxdw r2, [r10-0xa78]                   
    call function_6140                      
    ldxw r1, [r10-0x818]                    
    jeq r1, 0, lbb_2594                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2611                                     if true { pc += 17 }
lbb_2594:
    ldxdw r1, [r10-0x810]                   
    stxdw [r10-0xab8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    ldxdw r2, [r10-0xa60]                   
    call function_6140                      
    ldxw r1, [r10-0x818]                    
    jeq r1, 0, lbb_2603                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2611                                     if true { pc += 8 }
lbb_2603:
    ldxdw r1, [r10-0x810]                   
    stxdw [r10-0xac0], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    ldxdw r2, [r10-0xa80]                   
    call function_6140                      
    ldxw r1, [r10-0x818]                    
    jeq r1, 0, lbb_2621                             if r1 == (0 as i32 as i64 as u64) { pc += 10 }
lbb_2611:
    ldxw r8, [r10-0x810]                    
    ldxw r6, [r10-0x814]                    
lbb_2613:
    ldxdw r4, [r10-0xa00]                   
lbb_2614:
    ldxdw r2, [r10-0xa18]                   
    ldxb r1, [r2+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r2+0x0], r1                       
lbb_2618:
    stxw [r4+0x4], r8                       
    stxw [r4+0x0], r6                       
    exit                                    
lbb_2621:
    ldxdw r1, [r10-0x810]                   
    stxdw [r10-0xac8], r1                   
    ldxdw r1, [r10-0xaa0]                   
    jne r1, 0, lbb_2626                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2650                                     if true { pc += 24 }
lbb_2626:
    mov64 r8, 17                                    r8 = 17 as i32 as i64 as u64
    ldxdw r1, [r10-0xaa8]                   
    ldxdw r2, [r10-0xab8]                   
    jgt r1, r2, lbb_2425                            if r1 > r2 { pc += -205 }
lbb_2630:
    ldxdw r1, [r10-0xa98]                   
    ldxb r1, [r1+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 8                                     r2 &= 8   ///  r2 = r2.and(8)
    jne r2, 0, lbb_2654                             if r2 != (0 as i32 as i64 as u64) { pc += 19 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    jeq r2, 7, lbb_2654                             if r2 == (7 as i32 as i64 as u64) { pc += 16 }
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    ldxdw r2, [r10-0xa98]                   
    stxb [r2+0x0], r1                       
    ldxdw r3, [r2+0x50]                     
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    call function_18741                     
    ldxb r1, [r10-0x818]                    
    jeq r1, 0, lbb_2657                             if r1 == (0 as i32 as i64 as u64) { pc += 9 }
    mov64 r8, 39                                    r8 = 39 as i32 as i64 as u64
    ja lbb_2681                                     if true { pc += 31 }
lbb_2650:
    mov64 r8, 17                                    r8 = 17 as i32 as i64 as u64
    ldxdw r1, [r10-0xaa8]                   
    jgt r1, r9, lbb_2425                            if r1 > r9 { pc += -228 }
    ja lbb_2630                                     if true { pc += -24 }
lbb_2654:
    mov64 r6, 11                                    r6 = 11 as i32 as i64 as u64
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ja lbb_2613                                     if true { pc += -44 }
lbb_2657:
    ldxdw r1, [r10-0x7e8]                   
    stxdw [r10-0xab8], r1                   
    ldxdw r1, [r10-0x7f0]                   
    stxdw [r10-0xae8], r1                   
    ldxdw r1, [r10-0x7f8]                   
    stxdw [r10-0xad0], r1                   
    ldxdw r1, [r10-0x800]                   
    stxdw [r10-0xae0], r1                   
    ldxdw r1, [r10-0x808]                   
    stxdw [r10-0xad8], r1                   
    ldxdw r9, [r10-0x810]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2424                                 r1 += -2424   ///  r1 = r1.wrapping_add(-2424 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2016                                 r2 += -2016   ///  r2 = r2.wrapping_add(-2016 as i32 as i64 as u64)
    mov64 r3, 120                                   r3 = 120 as i32 as i64 as u64
    call function_23152                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    call function_19856                     
    ldxw r1, [r10-0x818]                    
    jeq r1, 0, lbb_2686                             if r1 == (0 as i32 as i64 as u64) { pc += 7 }
    ldxw r8, [r10-0x810]                    
    ldxw r6, [r10-0x814]                    
lbb_2681:
    ldxdw r2, [r10-0xa98]                   
    ldxb r1, [r2+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxb [r2+0x0], r1                       
    ja lbb_2613                                     if true { pc += -73 }
lbb_2686:
    mov64 r8, 23                                    r8 = 23 as i32 as i64 as u64
    ldxdw r2, [r10-0x810]                   
    ldxdw r1, [r10-0xab8]                   
    stxdw [r10-0xaf0], r2                   
    jgt r2, r1, lbb_2681                            if r2 > r1 { pc += -10 }
    ldxdw r2, [r10-0xa10]                   
    ldxdw r1, [r2+0x20]                     
    stxdw [r10-0x800], r1                   
    ldxdw r1, [r2+0x18]                     
    stxdw [r10-0x808], r1                   
    ldxdw r1, [r2+0x10]                     
    stxdw [r10-0x810], r1                   
    ldxdw r1, [r2+0x8]                      
    stxdw [r10-0x818], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    call function_17304                     
    stxdw [r10-0xa10], r0                   
    ldxdw r1, [r10-0xa38]                   
    ldxdw r1, [r1+0x10]                     
    ldxdw r2, [r10-0xad0]                   
    jge r1, r2, lbb_2717                            if r1 >= r2 { pc += 9 }
    ldxdw r3, [r10-0xa38]                   
    ldxdw r1, [r10-0xad0]                   
    stxdw [r3+0x10], r1                     
    lddw r1, 0x44dd228877ee1166                     r1 load str located at 4962160333955141990
    ldxdw r2, [r10-0xad8]                   
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r3+0x8], r2                      
    stdw [r3+0x240], 0                      
lbb_2717:
    ldxdw r8, [r10-0xa08]                   
    mov64 r6, r8                                    r6 = r8
    add64 r6, 96                                    r6 += 96   ///  r6 = r6.wrapping_add(96 as i32 as i64 as u64)
    add64 r8, 24                                    r8 += 24   ///  r8 = r8.wrapping_add(24 as i32 as i64 as u64)
    ldxdw r1, [r10-0xab8]                   
    stxdw [r10-0x7f0], r1                   
    ldxdw r1, [r10-0xae8]                   
    stxdw [r10-0x7f8], r1                   
    ldxdw r1, [r10-0xad0]                   
    stxdw [r10-0x800], r1                   
    ldxdw r1, [r10-0xae0]                   
    stxdw [r10-0x808], r1                   
    ldxdw r1, [r10-0xad8]                   
    stxdw [r10-0x810], r1                   
    stxdw [r10-0x818], r9                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2024                                 r1 += -2024   ///  r1 = r1.wrapping_add(-2024 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2424                                 r2 += -2424   ///  r2 = r2.wrapping_add(-2424 as i32 as i64 as u64)
    mov64 r3, 120                                   r3 = 120 as i32 as i64 as u64
    call function_23152                     
    ldxdw r1, [r10-0xaa0]                   
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_2742                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_2742:
    stxdw [r10-0xfd0], r7                   
    ldxdw r1, [r10-0xa10]                   
    stxdw [r10-0xfc8], r1                   
    stxdw [r10-0xfd8], r6                   
    stxdw [r10-0xfe0], r8                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    stxdw [r10-0xfe8], r1                   
    ldxdw r1, [r10-0xac8]                   
    stxdw [r10-0xff0], r1                   
    ldxdw r1, [r10-0xac0]                   
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0xaf0]                   
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2288                                 r1 += -2288   ///  r1 = r1.wrapping_add(-2288 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0xa38]                   
    ldxdw r3, [r10-0xaa8]                   
    call function_15596                     
    ldxb r1, [r10-0x8e8]                    
    mov64 r7, r1                                    r7 = r1
    jeq r1, 2, lbb_2802                             if r1 == (2 as i32 as i64 as u64) { pc += 37 }
    ldxdw r1, [r10-0x8e0]                   
    stxdw [r10-0x8f9], r1                   
    ldxdw r1, [r10-0x8e7]                   
    stxdw [r10-0x900], r1                   
    ldxdw r1, [r10-0x900]                   
    stxdw [r10-0x10], r1                    
    ldxw r1, [r10-0x8f9]                    
    stxw [r10-0x9], r1                      
    ldxdw r1, [r10-0x8f0]                   
    stxdw [r10-0xa10], r1                   
    ldxdw r1, [r10-0xaa0]                   
    jne r1, 0, lbb_2778                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2805                                     if true { pc += 27 }
lbb_2778:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2504                                 r1 += -2504   ///  r1 = r1.wrapping_add(-2504 as i32 as i64 as u64)
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0xac0]                   
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, 110                                   r4 = 110 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_25903                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x9c0]                   
    jne r2, 0, lbb_2790                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_2790:
    mov64 r6, 23                                    r6 = 23 as i32 as i64 as u64
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_2681                             if r1 != (0 as i32 as i64 as u64) { pc += -112 }
    ldxdw r1, [r10-0x9c8]                   
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r8, 18                                    r8 = 18 as i32 as i64 as u64
    div64 r1, 100                                   r1 /= 100   ///  r1 = r1 / (100 as u64)
    ldxdw r2, [r10-0xac0]                   
    stxdw [r10-0xac8], r2                   
    ldxdw r2, [r10-0xa10]                   
    jgt r2, r1, lbb_2681                            if r2 > r1 { pc += -120 }
    ja lbb_2826                                     if true { pc += 24 }
lbb_2802:
    ldxw r8, [r10-0x8ec]                    
    ldxw r6, [r10-0x8f0]                    
    ja lbb_2681                                     if true { pc += -124 }
lbb_2805:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2488                                 r1 += -2488   ///  r1 = r1.wrapping_add(-2488 as i32 as i64 as u64)
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0xac8]                   
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, 110                                   r4 = 110 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_25903                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x9b0]                   
    jne r2, 0, lbb_2817                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_2817:
    mov64 r6, 23                                    r6 = 23 as i32 as i64 as u64
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_2681                             if r1 != (0 as i32 as i64 as u64) { pc += -139 }
    ldxdw r1, [r10-0x9b8]                   
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r8, 18                                    r8 = 18 as i32 as i64 as u64
    div64 r1, 100                                   r1 /= 100   ///  r1 = r1 / (100 as u64)
    ldxdw r2, [r10-0xa10]                   
    jgt r2, r1, lbb_2681                            if r2 > r1 { pc += -145 }
lbb_2826:
    ldxdw r1, [r10-0xa10]                   
    ldxdw r2, [r10-0xac8]                   
    jlt r1, r2, lbb_2831                            if r1 < r2 { pc += 2 }
    ldxdw r1, [r10-0xac8]                   
    stxdw [r10-0xa10], r1                   
lbb_2831:
    ldxdw r1, [r10-0xab0]                   
    ldxdw r2, [r10-0xa10]                   
    jlt r2, r1, lbb_2681                            if r2 < r1 { pc += -153 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2512                                 r1 += -2512   ///  r1 = r1.wrapping_add(-2512 as i32 as i64 as u64)
    ldxdw r2, [r10-0xa28]                   
    ldxdw r3, [r10-0xa50]                   
    call function_15520                     
    ldxw r6, [r10-0x9d0]                    
    jne r6, 26, lbb_2923                            if r6 != (26 as i32 as i64 as u64) { pc += 82 }
    ldxdw r2, [r10-0xa28]                   
    ldxb r1, [r2+0x1]                       
    stxb [r10-0x8f1], r1                    
    ldxb r1, [r2+0x0]                       
    stxb [r10-0x900], r1                    
    ldxb r8, [r2+0xf9]                      
    ldxb r1, [r2+0xf8]                      
    stxdw [r10-0xa28], r1                   
    ldxdw r2, [r10-0xa18]                   
    ldxb r1, [r2+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r2+0x0], r1                       
    ldxdw r1, [r10-0xaa0]                   
    jne r1, 0, lbb_2856                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_2925                                     if true { pc += 69 }
lbb_2856:
    ldxdw r4, [r10-0xa08]                   
    ldxdw r1, [r4+0x58]                     
    ldxdw r2, [r10-0xaa8]                   
    stxdw [r10-0xff0], r2                   
    stxdw [r10-0xfe8], r8                   
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0xa88]                   
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2536                                 r1 += -2536   ///  r1 = r1.wrapping_add(-2536 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0xa78]                   
    ldxdw r3, [r10-0xa80]                   
    call function_4870                      
    ldxw r6, [r10-0x9e8]                    
    jeq r6, 26, lbb_2874                            if r6 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3089                                     if true { pc += 215 }
lbb_2874:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2304                                 r1 += -2304   ///  r1 = r1.wrapping_add(-2304 as i32 as i64 as u64)
    stxdw [r10-0x7d8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2289                                 r1 += -2289   ///  r1 = r1.wrapping_add(-2289 as i32 as i64 as u64)
    stxdw [r10-0x7e8], r1                   
    ldxdw r1, [r10-0xa48]                   
    stxdw [r10-0x7f8], r1                   
    ldxdw r1, [r10-0xa40]                   
    stxdw [r10-0x808], r1                   
    lddw r1, 0x1000334fa --> b"marketconfigprograms/solfi-v2-program/src/instruct"        r1 load str located at 4295177466
    stxdw [r10-0x818], r1                   
    stdw [r10-0x7d0], 1                     
    stdw [r10-0x7e0], 1                     
    stdw [r10-0x7f0], 32                    
    stdw [r10-0x800], 32                    
    stdw [r10-0x810], 6                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    stxdw [r10-0x8f0], r1                   
    stdw [r10-0x8e8], 5                     
    ldxdw r1, [r10-0xa70]                   
    ldxdw r1, [r1+0x0]                      
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2288                                 r2 += -2288   ///  r2 = r2.wrapping_add(-2288 as i32 as i64 as u64)
    stxdw [r10-0xfe0], r2                   
    ldxdw r2, [r10-0xa28]                   
    stxdw [r10-0xfe8], r2                   
    ldxdw r2, [r10-0xa10]                   
    stxdw [r10-0xff0], r2                   
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0xa68]                   
    stxdw [r10-0x1000], r1                  
    stdw [r10-0xfd8], 1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2544                                 r1 += -2544   ///  r1 = r1.wrapping_add(-2544 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0xa60]                   
    ldxdw r3, [r10-0xa58]                   
    ldxdw r4, [r10-0xa30]                   
    call function_5273                      
    ldxw r6, [r10-0x9f0]                    
    jeq r6, 26, lbb_2920                            if r6 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3093                                     if true { pc += 173 }
lbb_2920:
    ldxdw r1, [r10-0xaa8]                   
    stxdw [r10-0xa10], r1                   
    ja lbb_2989                                     if true { pc += 66 }
lbb_2923:
    ldxw r8, [r10-0x9cc]                    
    ja lbb_2681                                     if true { pc += -244 }
lbb_2925:
    ldxdw r4, [r10-0xa08]                   
    ldxdw r1, [r4+0x50]                     
    ldxdw r2, [r10-0xaa8]                   
    stxdw [r10-0xff0], r2                   
    ldxdw r2, [r10-0xa28]                   
    stxdw [r10-0xfe8], r2                   
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0xa68]                   
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2520                                 r1 += -2520   ///  r1 = r1.wrapping_add(-2520 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0xa58]                   
    ldxdw r3, [r10-0xa60]                   
    call function_4870                      
    ldxw r6, [r10-0x9d8]                    
    jeq r6, 26, lbb_2944                            if r6 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3091                                     if true { pc += 147 }
lbb_2944:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2304                                 r1 += -2304   ///  r1 = r1.wrapping_add(-2304 as i32 as i64 as u64)
    stxdw [r10-0x7d8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2289                                 r1 += -2289   ///  r1 = r1.wrapping_add(-2289 as i32 as i64 as u64)
    stxdw [r10-0x7e8], r1                   
    ldxdw r1, [r10-0xa48]                   
    stxdw [r10-0x7f8], r1                   
    ldxdw r1, [r10-0xa40]                   
    stxdw [r10-0x808], r1                   
    lddw r1, 0x1000334fa --> b"marketconfigprograms/solfi-v2-program/src/instruct"        r1 load str located at 4295177466
    stxdw [r10-0x818], r1                   
    stdw [r10-0x7d0], 1                     
    stdw [r10-0x7e0], 1                     
    stdw [r10-0x7f0], 32                    
    stdw [r10-0x800], 32                    
    stdw [r10-0x810], 6                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    stxdw [r10-0x8f0], r1                   
    stdw [r10-0x8e8], 5                     
    ldxdw r1, [r10-0xa90]                   
    ldxdw r1, [r1+0x0]                      
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2288                                 r2 += -2288   ///  r2 = r2.wrapping_add(-2288 as i32 as i64 as u64)
    stxdw [r10-0xfe0], r2                   
    stxdw [r10-0xfe8], r8                   
    ldxdw r2, [r10-0xa10]                   
    stxdw [r10-0xff0], r2                   
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0xa88]                   
    stxdw [r10-0x1000], r1                  
    stdw [r10-0xfd8], 1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2528                                 r1 += -2528   ///  r1 = r1.wrapping_add(-2528 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0xa80]                   
    ldxdw r3, [r10-0xa78]                   
    ldxdw r4, [r10-0xa30]                   
    call function_5273                      
    ldxw r6, [r10-0x9e0]                    
    jeq r6, 26, lbb_2989                            if r6 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3095                                     if true { pc += 106 }
lbb_2989:
    mov64 r6, 11                                    r6 = 11 as i32 as i64 as u64
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0xa18]                   
    ldxb r1, [r1+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    jeq r2, 0, lbb_2997                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3096                                     if true { pc += 99 }
lbb_2997:
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    ldxdw r2, [r10-0xa18]                   
    stxb [r2+0x0], r1                       
    ldxdw r3, [r2+0x50]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    ldxdw r2, [r10-0xa20]                   
    call function_15574                     
    ldxw r1, [r10-0x818]                    
    jne r1, 0, lbb_3082                             if r1 != (0 as i32 as i64 as u64) { pc += 75 }
    mov64 r6, 19                                    r6 = 19 as i32 as i64 as u64
    ldxdw r1, [r10-0x810]                   
    stxdw [r10-0xa08], r1                   
    ldxb r1, [r1+0x2]                       
    jeq r1, 0, lbb_3013                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3084                                     if true { pc += 71 }
lbb_3013:
    ldxdw r2, [r10-0xa08]                   
    add64 r2, 704                                   r2 += 704   ///  r2 = r2.wrapping_add(704 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    call function_15984                     
    ldxw r1, [r10-0x818]                    
    jne r1, 0, lbb_3082                             if r1 != (0 as i32 as i64 as u64) { pc += 62 }
    ldxdw r8, [r10-0x810]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    ldxdw r2, [r10-0xa60]                   
    call function_6140                      
    ldxw r1, [r10-0x818]                    
    jeq r1, 0, lbb_3028                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3082                                     if true { pc += 54 }
lbb_3028:
    ldxdw r6, [r10-0x810]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    ldxdw r2, [r10-0xa80]                   
    call function_6140                      
    ldxw r1, [r10-0x818]                    
    jeq r1, 0, lbb_3036                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3082                                     if true { pc += 46 }
lbb_3036:
    ldxdw r1, [r10-0x810]                   
    ldxdw r2, [r10-0xaa0]                   
    stxb [r10-0x81c], r2                    
    ldxdw r2, [r10-0xaf0]                   
    stxdw [r10-0x8f0], r2                   
    ldxdw r2, [r10-0xab8]                   
    stxdw [r10-0x8a8], r2                   
    ldxdw r2, [r10-0xae8]                   
    stxdw [r10-0x8b0], r2                   
    ldxdw r2, [r10-0xad0]                   
    stxdw [r10-0x8b8], r2                   
    ldxdw r2, [r10-0xae0]                   
    stxdw [r10-0x8c0], r2                   
    ldxdw r2, [r10-0xad8]                   
    stxdw [r10-0x8c8], r2                   
    stxdw [r10-0x8d0], r9                   
    stxdw [r10-0x8d8], r1                   
    stxdw [r10-0x8e0], r6                   
    ldxdw r1, [r10-0xa10]                   
    stxdw [r10-0x8e8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2208                                 r1 += -2208   ///  r1 = r1.wrapping_add(-2208 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2424                                 r2 += -2424   ///  r2 = r2.wrapping_add(-2424 as i32 as i64 as u64)
    mov64 r3, 120                                   r3 = 120 as i32 as i64 as u64
    call function_23152                     
    stxb [r10-0x828], r7                    
    ldxdw r1, [r10-0x10]                    
    stxdw [r10-0x827], r1                   
    ldxw r1, [r10-0x9]                      
    stxw [r10-0x820], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2552                                 r1 += -2552   ///  r1 = r1.wrapping_add(-2552 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -2288                                 r3 += -2288   ///  r3 = r3.wrapping_add(-2288 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    call function_15911                     
    ldxw r6, [r10-0x9f8]                    
    jeq r6, 26, lbb_3076                            if r6 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3102                                     if true { pc += 26 }
lbb_3076:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2072                                 r1 += -2072   ///  r1 = r1.wrapping_add(-2072 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    call function_15897                     
    ldxw r1, [r10-0x818]                    
    jeq r1, 0, lbb_3104                             if r1 == (0 as i32 as i64 as u64) { pc += 22 }
lbb_3082:
    ldxw r8, [r10-0x810]                    
    ldxw r6, [r10-0x814]                    
lbb_3084:
    ldxdw r2, [r10-0xa18]                   
    ldxb r1, [r2+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r2+0x0], r1                       
    ja lbb_3096                                     if true { pc += 7 }
lbb_3089:
    ldxw r8, [r10-0x9e4]                    
    ja lbb_3096                                     if true { pc += 5 }
lbb_3091:
    ldxw r8, [r10-0x9d4]                    
    ja lbb_3096                                     if true { pc += 3 }
lbb_3093:
    ldxw r8, [r10-0x9ec]                    
    ja lbb_3096                                     if true { pc += 1 }
lbb_3095:
    ldxw r8, [r10-0x9dc]                    
lbb_3096:
    ldxdw r2, [r10-0xa98]                   
    ldxb r1, [r2+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxb [r2+0x0], r1                       
lbb_3100:
    ldxdw r4, [r10-0xa00]                   
    ja lbb_2618                                     if true { pc += -484 }
lbb_3102:
    ldxw r8, [r10-0x9f4]                    
    ja lbb_3084                                     if true { pc += -20 }
lbb_3104:
    ldxw r6, [r10-0x810]                    
    mov64 r8, r10                                   r8 = r10
    add64 r8, -1036                                 r8 += -1036   ///  r8 = r8.wrapping_add(-1036 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2060                                 r2 += -2060   ///  r2 = r2.wrapping_add(-2060 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    mov64 r3, 1020                                  r3 = 1020 as i32 as i64 as u64
    call function_23152                     
    ldxdw r1, [r10-0xa08]                   
    stxw [r1+0x2c0], r6                     
    add64 r1, 708                                   r1 += 708   ///  r1 = r1.wrapping_add(708 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 1020                                  r3 = 1020 as i32 as i64 as u64
    call function_23152                     
    ldxdw r2, [r10-0xa18]                   
    ldxb r1, [r2+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r2+0x0], r1                       
    ldxdw r2, [r10-0xa98]                   
    ldxb r1, [r2+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxb [r2+0x0], r1                       
    mov64 r6, 26                                    r6 = 26 as i32 as i64 as u64
    ja lbb_3100                                     if true { pc += -28 }

function_3128:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r8, 10                                    r8 = 10 as i32 as i64 as u64
    jne r3, 3, lbb_3188                             if r3 != (3 as i32 as i64 as u64) { pc += 56 }
    stxdw [r10-0x20], r5                    
    stxdw [r10-0x28], r4                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -40                                   r2 += -40   ///  r2 = r2.wrapping_add(-40 as i32 as i64 as u64)
    call function_531                       
    ldxdw r0, [r10-0x10]                    
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    ldxdw r2, [r10-0x18]                    
    jeq r2, r1, lbb_3147                            if r2 == r1 { pc += 3 }
    ldxdw r1, [r10-0x20]                    
    jeq r1, 0, lbb_3159                             if r1 == (0 as i32 as i64 as u64) { pc += 13 }
    call function_70                        
lbb_3147:
    mov64 r8, 2                                     r8 = 2 as i32 as i64 as u64
    mov64 r2, r0                                    r2 = r0
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    mov64 r3, r2                                    r3 = r2
    add64 r3, -2                                    r3 += -2   ///  r3 = r3.wrapping_add(-2 as i32 as i64 as u64)
    jlt r3, 2, lbb_3188                             if r3 < (2 as i32 as i64 as u64) { pc += 35 }
    jeq r2, 0, lbb_3188                             if r2 == (0 as i32 as i64 as u64) { pc += 34 }
    ldxdw r1, [r0-0x1]                      
    ldxdw r2, [r0+0x7]                      
    ldxdw r2, [r2+0x0]                      
    callx r2                                
lbb_3158:
    ja lbb_3188                                     if true { pc += 29 }
lbb_3159:
    stxdw [r10-0x38], r0                    
    ldxdw r9, [r10-0x8]                     
    mov64 r3, r7                                    r3 = r7
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_6494                      
    ldxw r8, [r10-0x30]                     
    jeq r8, 26, lbb_3170                            if r8 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3187                                     if true { pc += 17 }
lbb_3170:
    ldxdw r8, [r7+0x10]                     
    mov64 r1, r8                                    r1 = r8
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033328 --> b"\x06\x8747\xb4\xfb\xc9\xf4\x047\xdd<\xe6?\x9c\xfe\x8c$\x84\x03U\x93\xa1'\…        r2 load str located at 4295177000
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r4, r8                                    r4 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, 38                                    r1 = 38 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_3184                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3186                                     if true { pc += 2 }
lbb_3184:
    ldxdw r2, [r4+0x50]                     
    jeq r2, 1048576, lbb_3191                       if r2 == (1048576 as i32 as i64 as u64) { pc += 5 }
lbb_3186:
    ja lbb_3188                                     if true { pc += 1 }
lbb_3187:
    ldxw r1, [r10-0x2c]                     
lbb_3188:
    stxw [r6+0x4], r1                       
    stxw [r6+0x0], r8                       
    exit                                    
lbb_3191:
    mov64 r8, 11                                    r8 = 11 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxb r2, [r4+0x0]                       
    mov64 r3, r2                                    r3 = r2
    and64 r3, 15                                    r3 &= 15   ///  r3 = r3.and(15)
    jne r3, 0, lbb_3186                             if r3 != (0 as i32 as i64 as u64) { pc += -11 }
    or64 r2, 8                                      r2 |= 8   ///  r2 = r2.or(8)
    stxb [r4+0x0], r2                       
    stxdw [r10-0x40], r4                    
    mov64 r2, r4                                    r2 = r4
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r3, 1048576                               r3 = 1048576 as i32 as i64 as u64
    call function_13476                     
    ldxw r1, [r10-0x18]                     
    jne r1, 0, lbb_3260                             if r1 != (0 as i32 as i64 as u64) { pc += 52 }
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r7+0x0]                      
    mov64 r7, r1                                    r7 = r1
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, 12                                    r1 = 12 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_3221                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3262                                     if true { pc += 41 }
lbb_3221:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_13502                     
    ldxw r1, [r10-0x18]                     
    jne r1, 0, lbb_3260                             if r1 != (0 as i32 as i64 as u64) { pc += 33 }
    ldxdw r5, [r10-0x38]                    
    jeq r9, 0, lbb_3254                             if r9 == (0 as i32 as i64 as u64) { pc += 25 }
    ldxdw r2, [r10-0x10]                    
    lsh64 r9, 3                                     r9 <<= 3   ///  r9 = r9.wrapping_shl(3)
    ldxdw r1, [r2+0x0]                      
    mov64 r3, r2                                    r3 = r2
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
lbb_3234:
    jlt r1, 2559, lbb_3236                          if r1 < (2559 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3267                                     if true { pc += 31 }
lbb_3236:
    lsh64 r1, 3                                     r1 <<= 3   ///  r1 = r1.wrapping_shl(3)
    mov64 r4, r3                                    r4 = r3
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    ldxdw r1, [r5+0x0]                      
    stxdw [r4+0x0], r1                      
    ldxdw r1, [r2+0x0]                      
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_3246                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_3246:
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jne r4, 0, lbb_3271                             if r4 != (0 as i32 as i64 as u64) { pc += 23 }
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    mod64 r1, 2559                                  r1 %= 2559   ///  r1 = r1 % (2559 as u64)
    stxdw [r2+0x0], r1                      
    add64 r9, -8                                    r9 += -8   ///  r9 = r9.wrapping_add(-8 as i32 as i64 as u64)
    jeq r9, 0, lbb_3254                             if r9 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3234                                     if true { pc += -20 }
lbb_3254:
    ldxdw r2, [r10-0x40]                    
    ldxb r1, [r2+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r2+0x0], r1                       
    mov64 r8, 26                                    r8 = 26 as i32 as i64 as u64
    ja lbb_3158                                     if true { pc += -102 }
lbb_3260:
    ldxw r1, [r10-0x10]                     
    ldxw r8, [r10-0x14]                     
lbb_3262:
    ldxdw r3, [r10-0x40]                    
    ldxb r2, [r3+0x0]                       
    and64 r2, 247                                   r2 &= 247   ///  r2 = r2.and(247)
    stxb [r3+0x0], r2                       
    ja lbb_3188                                     if true { pc += -79 }
lbb_3267:
    mov64 r2, 2559                                  r2 = 2559 as i32 as i64 as u64
    lddw r3, 0x100034808 --> b"\x00\x00\x00\x00\x065\x03\x00B\x00\x00\x00\x00\x00\x00\x00(\x00\x00\x00\x…        r3 load str located at 4295182344
    call function_20679                     
lbb_3271:
    lddw r1, 0x100034820 --> b"\x00\x00\x00\x00\x065\x03\x00B\x00\x00\x00\x00\x00\x00\x00)\x00\x00\x00\x…        r1 load str located at 4295182368
    call function_22712                     

function_3274:
    mov64 r6, r1                                    r6 = r1
    mov64 r9, 10                                    r9 = 10 as i32 as i64 as u64
    jne r3, 7, lbb_3428                             if r3 != (7 as i32 as i64 as u64) { pc += 151 }
    jeq r5, 0, lbb_3289                             if r5 == (0 as i32 as i64 as u64) { pc += 11 }
    ldxb r1, [r4+0x0]                       
    stxb [r10-0x18], r1                     
    jsgt r1, 5, lbb_3293                            if (r1 as i64) > (5 as i32 as i64) { pc += 12 }
    jsgt r1, 2, lbb_3300                            if (r1 as i64) > (2 as i32 as i64) { pc += 18 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_3325                             if r1 == (0 as i32 as i64 as u64) { pc += 41 }
    jeq r1, 1, lbb_3318                             if r1 == (1 as i32 as i64 as u64) { pc += 33 }
    jeq r1, 2, lbb_3287                             if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3431                                     if true { pc += 144 }
lbb_3287:
    mov64 r7, 2                                     r7 = 2 as i32 as i64 as u64
    ja lbb_3325                                     if true { pc += 36 }
lbb_3289:
    lddw r1, 0x1000347d0 --> b"\x00\x00\x00\x00\xa84\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182288
    call function_19705                     
    ja lbb_3327                                     if true { pc += 34 }
lbb_3293:
    jsgt r1, 8, lbb_3306                            if (r1 as i64) > (8 as i32 as i64) { pc += 12 }
    jeq r1, 6, lbb_3312                             if r1 == (6 as i32 as i64 as u64) { pc += 17 }
    jeq r1, 7, lbb_3320                             if r1 == (7 as i32 as i64 as u64) { pc += 24 }
    jeq r1, 8, lbb_3298                             if r1 == (8 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3431                                     if true { pc += 133 }
lbb_3298:
    mov64 r7, 8                                     r7 = 8 as i32 as i64 as u64
    ja lbb_3325                                     if true { pc += 25 }
lbb_3300:
    jeq r1, 3, lbb_3314                             if r1 == (3 as i32 as i64 as u64) { pc += 13 }
    jeq r1, 4, lbb_3322                             if r1 == (4 as i32 as i64 as u64) { pc += 20 }
    jeq r1, 5, lbb_3304                             if r1 == (5 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3431                                     if true { pc += 127 }
lbb_3304:
    mov64 r7, 5                                     r7 = 5 as i32 as i64 as u64
    ja lbb_3325                                     if true { pc += 19 }
lbb_3306:
    jeq r1, 9, lbb_3316                             if r1 == (9 as i32 as i64 as u64) { pc += 9 }
    jeq r1, 10, lbb_3324                            if r1 == (10 as i32 as i64 as u64) { pc += 16 }
    jeq r1, 11, lbb_3310                            if r1 == (11 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3431                                     if true { pc += 121 }
lbb_3310:
    mov64 r7, 11                                    r7 = 11 as i32 as i64 as u64
    ja lbb_3325                                     if true { pc += 13 }
lbb_3312:
    mov64 r7, 6                                     r7 = 6 as i32 as i64 as u64
    ja lbb_3325                                     if true { pc += 11 }
lbb_3314:
    mov64 r7, 3                                     r7 = 3 as i32 as i64 as u64
    ja lbb_3325                                     if true { pc += 9 }
lbb_3316:
    mov64 r7, 9                                     r7 = 9 as i32 as i64 as u64
    ja lbb_3325                                     if true { pc += 7 }
lbb_3318:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ja lbb_3325                                     if true { pc += 5 }
lbb_3320:
    mov64 r7, 7                                     r7 = 7 as i32 as i64 as u64
    ja lbb_3325                                     if true { pc += 3 }
lbb_3322:
    mov64 r7, 4                                     r7 = 4 as i32 as i64 as u64
    ja lbb_3325                                     if true { pc += 1 }
lbb_3324:
    mov64 r7, 10                                    r7 = 10 as i32 as i64 as u64
lbb_3325:
    jeq r5, 1, lbb_3339                             if r5 == (1 as i32 as i64 as u64) { pc += 13 }
    call function_70                        
lbb_3327:
    mov64 r9, 2                                     r9 = 2 as i32 as i64 as u64
    mov64 r2, r0                                    r2 = r0
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    mov64 r3, r2                                    r3 = r2
    add64 r3, -2                                    r3 += -2   ///  r3 = r3.wrapping_add(-2 as i32 as i64 as u64)
    jlt r3, 2, lbb_3428                             if r3 < (2 as i32 as i64 as u64) { pc += 95 }
    jeq r2, 0, lbb_3428                             if r2 == (0 as i32 as i64 as u64) { pc += 94 }
    ldxdw r1, [r0-0x1]                      
    ldxdw r2, [r0+0x7]                      
    ldxdw r2, [r2+0x0]                      
    callx r2                                
lbb_3338:
    ja lbb_3428                                     if true { pc += 89 }
lbb_3339:
    mov64 r3, r2                                    r3 = r2
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -232                                  r1 += -232   ///  r1 = r1.wrapping_add(-232 as i32 as i64 as u64)
    stxdw [r10-0xf0], r2                    
    call function_6494                      
    ldxw r9, [r10-0xe8]                     
    jeq r9, 26, lbb_3348                            if r9 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3427                                     if true { pc += 79 }
lbb_3348:
    ldxdw r9, [r10-0xf0]                    
    ldxdw r8, [r9+0x18]                     
    mov64 r1, r8                                    r1 = r8
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033368 --> b"\xe4\xd1\xd98\xec\xff\xe2<\x8c\x94\xb1\xc3\xa2\xc8\x8bz\xd8\xbdn\xff\x8e\…        r2 load str located at 4295177064
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r2, r9                                    r2 = r9
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r1, 44                                    r1 = 44 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_3363                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3428                                     if true { pc += 65 }
lbb_3363:
    stxdw [r10-0xf8], r8                    
    ldxdw r8, [r2+0x10]                     
    mov64 r1, r8                                    r1 = r8
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033328 --> b"\x06\x8747\xb4\xfb\xc9\xf4\x047\xdd<\xe6?\x9c\xfe\x8c$\x84\x03U\x93\xa1'\…        r2 load str located at 4295177000
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r4, r8                                    r4 = r8
    mov64 r1, 38                                    r1 = 38 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_3428                             if r0 != (0 as i32 as i64 as u64) { pc += 52 }
    ldxdw r2, [r4+0x50]                     
    jne r2, 1048576, lbb_3428                       if r2 != (1048576 as i32 as i64 as u64) { pc += 50 }
    mov64 r9, 11                                    r9 = 11 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxb r2, [r4+0x0]                       
    mov64 r3, r2                                    r3 = r2
    and64 r3, 15                                    r3 &= 15   ///  r3 = r3.and(15)
    jne r3, 0, lbb_3428                             if r3 != (0 as i32 as i64 as u64) { pc += 44 }
    or64 r2, 8                                      r2 |= 8   ///  r2 = r2.or(8)
    stxb [r4+0x0], r2                       
    mov64 r8, r4                                    r8 = r4
    mov64 r2, r4                                    r2 = r4
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -224                                  r1 += -224   ///  r1 = r1.wrapping_add(-224 as i32 as i64 as u64)
    mov64 r3, 1048576                               r3 = 1048576 as i32 as i64 as u64
    call function_13476                     
    ldxw r1, [r10-0xe0]                     
    jne r1, 0, lbb_3455                             if r1 != (0 as i32 as i64 as u64) { pc += 60 }
    ldxdw r3, [r10-0xd8]                    
    ldxdw r1, [r10-0xf0]                    
    ldxdw r2, [r1+0x0]                      
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x100], r3                   
    mov64 r1, r3                                    r1 = r3
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x108], r2                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r1, 12                                    r1 = 12 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_3411                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3457                                     if true { pc += 46 }
lbb_3411:
    jsgt r7, 5, lbb_3461                            if (r7 as i64) > (5 as i32 as i64) { pc += 49 }
    jsgt r7, 2, lbb_3466                            if (r7 as i64) > (2 as i32 as i64) { pc += 53 }
    mov64 r2, 3                                     r2 = 3 as i32 as i64 as u64
    mov64 r4, 672                                   r4 = 672 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x110], r1                   
    stxdw [r10-0x118], r1                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jeq r7, 0, lbb_3502                             if r7 == (0 as i32 as i64 as u64) { pc += 80 }
    jeq r7, 1, lbb_3480                             if r7 == (1 as i32 as i64 as u64) { pc += 57 }
    mov64 r2, 2                                     r2 = 2 as i32 as i64 as u64
    mov64 r4, 3080                                  r4 = 3080 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    ja lbb_3483                                     if true { pc += 56 }
lbb_3427:
    ldxw r1, [r10-0xe4]                     
lbb_3428:
    stxw [r6+0x4], r1                       
    stxw [r6+0x0], r9                       
    exit                                    
lbb_3431:
    lddw r1, 0x1000347f8 --> b"\x00\x00\x00\x00\xe04\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295182328
    stxdw [r10-0xe0], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -56                                   r1 += -56   ///  r1 = r1.wrapping_add(-56 as i32 as i64 as u64)
    stxdw [r10-0xd0], r1                    
    lddw r1, 0x100000540 --> b"a#4\x00\x00\x00\x00\x00\xbf4\x00\x00\x00\x00\x00\x00W\x04\x00\x00\x10\x00…        r1 load str located at 4294968640
    stxdw [r10-0x30], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    stxdw [r10-0x38], r1                    
    stdw [r10-0xc0], 0                      
    stdw [r10-0xd8], 1                      
    stdw [r10-0xc8], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -96                                   r7 += -96   ///  r7 = r7.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -224                                  r2 += -224   ///  r2 = r2.wrapping_add(-224 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_20113                     
    mov64 r1, r7                                    r1 = r7
    call function_34                        
    ja lbb_3327                                     if true { pc += -128 }
lbb_3455:
    ldxw r1, [r10-0xd8]                     
    ldxw r9, [r10-0xdc]                     
lbb_3457:
    ldxb r2, [r8+0x0]                       
    and64 r2, 247                                   r2 &= 247   ///  r2 = r2.and(247)
    stxb [r8+0x0], r2                       
    ja lbb_3428                                     if true { pc += -33 }
lbb_3461:
    jsgt r7, 8, lbb_3470                            if (r7 as i64) > (8 as i32 as i64) { pc += 8 }
    jeq r7, 6, lbb_3474                             if r7 == (6 as i32 as i64 as u64) { pc += 11 }
    jeq r7, 7, lbb_3487                             if r7 == (7 as i32 as i64 as u64) { pc += 23 }
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    ja lbb_3494                                     if true { pc += 28 }
lbb_3466:
    jeq r7, 3, lbb_3476                             if r7 == (3 as i32 as i64 as u64) { pc += 9 }
    jeq r7, 4, lbb_3493                             if r7 == (4 as i32 as i64 as u64) { pc += 25 }
    mov64 r2, 7                                     r2 = 7 as i32 as i64 as u64
    ja lbb_3494                                     if true { pc += 24 }
lbb_3470:
    jeq r7, 9, lbb_3478                             if r7 == (9 as i32 as i64 as u64) { pc += 7 }
    jeq r7, 10, lbb_3537                            if r7 == (10 as i32 as i64 as u64) { pc += 65 }
    mov64 r2, 13                                    r2 = 13 as i32 as i64 as u64
    ja lbb_3494                                     if true { pc += 20 }
lbb_3474:
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    ja lbb_3494                                     if true { pc += 18 }
lbb_3476:
    mov64 r2, 5                                     r2 = 5 as i32 as i64 as u64
    ja lbb_3494                                     if true { pc += 16 }
lbb_3478:
    mov64 r2, 11                                    r2 = 11 as i32 as i64 as u64
    ja lbb_3494                                     if true { pc += 14 }
lbb_3480:
    mov64 r2, 4                                     r2 = 4 as i32 as i64 as u64
    mov64 r4, 3696                                  r4 = 3696 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_3483:
    stxdw [r10-0x110], r1                   
    stxdw [r10-0x118], r1                   
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_3502                                     if true { pc += 15 }
lbb_3487:
    mov64 r2, 9                                     r2 = 9 as i32 as i64 as u64
    mov64 r4, 136                                   r4 = 136 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x118], r1                   
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_3498                                     if true { pc += 5 }
lbb_3493:
    mov64 r2, 6                                     r2 = 6 as i32 as i64 as u64
lbb_3494:
    mov64 r4, 12                                    r4 = 12 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x118], r1                   
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_3498:
    stxdw [r10-0x110], r1                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_3502:
    ldxdw r1, [r10-0xf8]                    
    ldxdw r7, [r1+0x50]                     
    mov64 r1, 43                                    r1 = 43 as i32 as i64 as u64
    jne r7, r4, lbb_3457                            if r7 != r4 { pc += -49 }
    mov64 r9, 11                                    r9 = 11 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0xf8]                    
    ldxb r4, [r4+0x0]                       
    mov64 r7, r4                                    r7 = r4
    and64 r7, 8                                     r7 &= 8   ///  r7 = r7.and(8)
    jne r7, 0, lbb_3457                             if r7 != (0 as i32 as i64 as u64) { pc += -56 }
    mov64 r7, r4                                    r7 = r4
    and64 r7, 7                                     r7 &= 7   ///  r7 = r7.and(7)
    jeq r7, 7, lbb_3457                             if r7 == (7 as i32 as i64 as u64) { pc += -59 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r1, 44                                    r1 = 44 as i32 as i64 as u64
    ldxdw r7, [r10-0xf8]                    
    ldxb r7, [r7+0x58]                      
    jne r7, r2, lbb_3457                            if r7 != r2 { pc += -64 }
    mov64 r1, r4                                    r1 = r4
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    ldxdw r7, [r10-0xf8]                    
    stxb [r7+0x0], r1                       
    jsgt r2, 7, lbb_3547                            if (r2 as i64) > (7 as i32 as i64) { pc += 21 }
    jsgt r2, 4, lbb_3558                            if (r2 as i64) > (4 as i32 as i64) { pc += 31 }
    jeq r2, 2, lbb_3578                             if r2 == (2 as i32 as i64 as u64) { pc += 50 }
    jeq r2, 3, lbb_3610                             if r2 == (3 as i32 as i64 as u64) { pc += 81 }
    jne r5, 0, lbb_3531                             if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3661                                     if true { pc += 130 }
lbb_3531:
    ldxdw r1, [r10-0x100]                   
    add64 r1, 4152                                  r1 += 4152   ///  r1 = r1.wrapping_add(4152 as i32 as i64 as u64)
    ldxdw r2, [r10-0xf8]                    
    add64 r2, 96                                    r2 += 96   ///  r2 = r2.wrapping_add(96 as i32 as i64 as u64)
    mov64 r3, 3688                                  r3 = 3688 as i32 as i64 as u64
    ja lbb_3617                                     if true { pc += 80 }
lbb_3537:
    mov64 r4, 12                                    r4 = 12 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x118], r1                   
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x110], r1                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r2, 12                                    r2 = 12 as i32 as i64 as u64
    ja lbb_3502                                     if true { pc += -45 }
lbb_3547:
    jsgt r2, 10, lbb_3568                           if (r2 as i64) > (10 as i32 as i64) { pc += 20 }
    jeq r2, 8, lbb_3586                             if r2 == (8 as i32 as i64 as u64) { pc += 37 }
    jeq r2, 9, lbb_3619                             if r2 == (9 as i32 as i64 as u64) { pc += 69 }
    ldxdw r1, [r10-0x110]                   
    jne r1, 0, lbb_3553                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3661                                     if true { pc += 108 }
lbb_3553:
    ldxdw r1, [r10-0xf8]                    
    ldxw r1, [r1+0x60]                      
    ldxdw r2, [r10-0x100]                   
    stxw [r2+0x3058], r1                    
    ja lbb_3669                                     if true { pc += 111 }
lbb_3558:
    jeq r2, 5, lbb_3594                             if r2 == (5 as i32 as i64 as u64) { pc += 35 }
    jeq r2, 6, lbb_3651                             if r2 == (6 as i32 as i64 as u64) { pc += 91 }
    ldxdw r1, [r10-0x110]                   
    jne r1, 0, lbb_3563                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3661                                     if true { pc += 98 }
lbb_3563:
    ldxdw r1, [r10-0xf8]                    
    ldxw r1, [r1+0x60]                      
    ldxdw r2, [r10-0x100]                   
    stxw [r2+0x308c], r1                    
    ja lbb_3669                                     if true { pc += 101 }
lbb_3568:
    jeq r2, 11, lbb_3602                            if r2 == (11 as i32 as i64 as u64) { pc += 33 }
    jeq r2, 12, lbb_3659                            if r2 == (12 as i32 as i64 as u64) { pc += 89 }
    ldxdw r1, [r10-0x110]                   
    jne r1, 0, lbb_3573                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3661                                     if true { pc += 88 }
lbb_3573:
    ldxdw r1, [r10-0xf8]                    
    ldxw r1, [r1+0x60]                      
    ldxdw r2, [r10-0x100]                   
    stxw [r2+0x3098], r1                    
    ja lbb_3669                                     if true { pc += 91 }
lbb_3578:
    jne r3, 0, lbb_3580                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3661                                     if true { pc += 81 }
lbb_3580:
    ldxdw r1, [r10-0x100]                   
    add64 r1, 8248                                  r1 += 8248   ///  r1 = r1.wrapping_add(8248 as i32 as i64 as u64)
    ldxdw r2, [r10-0xf8]                    
    add64 r2, 96                                    r2 += 96   ///  r2 = r2.wrapping_add(96 as i32 as i64 as u64)
    mov64 r3, 3072                                  r3 = 3072 as i32 as i64 as u64
    ja lbb_3617                                     if true { pc += 31 }
lbb_3586:
    ldxdw r1, [r10-0x110]                   
    jne r1, 0, lbb_3589                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3661                                     if true { pc += 72 }
lbb_3589:
    ldxdw r1, [r10-0xf8]                    
    ldxw r1, [r1+0x60]                      
    ldxdw r2, [r10-0x100]                   
    stxw [r2+0x30b4], r1                    
    ja lbb_3669                                     if true { pc += 75 }
lbb_3594:
    ldxdw r1, [r10-0x110]                   
    jne r1, 0, lbb_3597                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3661                                     if true { pc += 64 }
lbb_3597:
    ldxdw r1, [r10-0xf8]                    
    ldxw r1, [r1+0x60]                      
    ldxdw r2, [r10-0x100]                   
    stxw [r2+0x3054], r1                    
    ja lbb_3669                                     if true { pc += 67 }
lbb_3602:
    ldxdw r1, [r10-0x110]                   
    jne r1, 0, lbb_3605                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3661                                     if true { pc += 56 }
lbb_3605:
    ldxdw r1, [r10-0xf8]                    
    ldxw r1, [r1+0x60]                      
    ldxdw r2, [r10-0x100]                   
    stxw [r2+0x3090], r1                    
    ja lbb_3669                                     if true { pc += 59 }
lbb_3610:
    jne r0, 0, lbb_3612                             if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3661                                     if true { pc += 49 }
lbb_3612:
    ldxdw r1, [r10-0x100]                   
    add64 r1, 56                                    r1 += 56   ///  r1 = r1.wrapping_add(56 as i32 as i64 as u64)
    ldxdw r2, [r10-0xf8]                    
    add64 r2, 96                                    r2 += 96   ///  r2 = r2.wrapping_add(96 as i32 as i64 as u64)
    mov64 r3, 664                                   r3 = 664 as i32 as i64 as u64
lbb_3617:
    call function_23156                     
    ja lbb_3669                                     if true { pc += 50 }
lbb_3619:
    ldxdw r1, [r10-0x118]                   
    jne r1, 0, lbb_3622                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3661                                     if true { pc += 39 }
lbb_3622:
    ldxdw r7, [r10-0xf8]                    
    mov64 r2, r7                                    r2 = r7
    add64 r2, 96                                    r2 += 96   ///  r2 = r2.wrapping_add(96 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -224                                  r1 += -224   ///  r1 = r1.wrapping_add(-224 as i32 as i64 as u64)
    mov64 r3, 128                                   r3 = 128 as i32 as i64 as u64
    call function_23152                     
    ldxdw r1, [r7+0x68]                     
    ldxdw r2, [r7+0x70]                     
    ldxw r3, [r7+0x78]                      
    ldxdw r4, [r7+0x60]                     
    ldxdw r7, [r10-0x100]                   
    stxdw [r7+0x3038], r4                   
    stxw [r7+0x3050], r3                    
    stxdw [r7+0x3048], r2                   
    stxdw [r7+0x3040], r1                   
    mov64 r1, r7                                    r1 = r7
    add64 r1, 12380                                 r1 += 12380   ///  r1 = r1.wrapping_add(12380 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -196                                  r2 += -196   ///  r2 = r2.wrapping_add(-196 as i32 as i64 as u64)
    mov64 r3, 44                                    r3 = 44 as i32 as i64 as u64
    call function_23152                     
    ldxdw r1, [r10-0x98]                    
    stxdw [r7+0x309c], r1                   
    ldxdw r1, [r10-0x90]                    
    stxdw [r7+0x30a4], r1                   
    ldxdw r1, [r10-0x88]                    
    stxdw [r7+0x30ac], r1                   
    ja lbb_3669                                     if true { pc += 18 }
lbb_3651:
    ldxdw r1, [r10-0x110]                   
    jne r1, 0, lbb_3654                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3661                                     if true { pc += 7 }
lbb_3654:
    ldxdw r1, [r10-0xf8]                    
    ldxw r1, [r1+0x60]                      
    ldxdw r2, [r10-0x100]                   
    stxw [r2+0x3088], r1                    
    ja lbb_3669                                     if true { pc += 10 }
lbb_3659:
    ldxdw r1, [r10-0x110]                   
    jne r1, 0, lbb_3665                             if r1 != (0 as i32 as i64 as u64) { pc += 4 }
lbb_3661:
    ldxdw r1, [r10-0xf8]                    
    stxb [r1+0x0], r4                       
    mov64 r1, 43                                    r1 = 43 as i32 as i64 as u64
    ja lbb_3457                                     if true { pc += -208 }
lbb_3665:
    ldxdw r1, [r10-0xf8]                    
    ldxw r1, [r1+0x60]                      
    ldxdw r2, [r10-0x100]                   
    stxw [r2+0x3094], r1                    
lbb_3669:
    ldxdw r2, [r10-0xf8]                    
    ldxb r1, [r2+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxb [r2+0x0], r1                       
    ldxdw r1, [r10-0xf0]                    
    ldxdw r1, [r1+0x28]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x100033368 --> b"\xe4\xd1\xd98\xec\xff\xe2<\x8c\x94\xb1\xc3\xa2\xc8\x8bz\xd8\xbdn\xff\x8e\…        r2 load str located at 4295177064
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 45                                    r1 = 45 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_3685                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3457                                     if true { pc += -228 }
lbb_3685:
    ldxdw r4, [r10-0xf0]                    
    mov64 r1, r4                                    r1 = r4
    add64 r1, 24                                    r1 += 24   ///  r1 = r1.wrapping_add(24 as i32 as i64 as u64)
    lddw r2, 0x694781f0f75491d                      r2 load str located at 474135935579408669
    stxdw [r10-0x20], r2                    
    lddw r2, 0x46db918eff6ebdd8                     r2 load str located at 5105834645911420376
    stxdw [r10-0x28], r2                    
    lddw r2, 0x7a8bc8a2c3b1948c                     r2 load str located at 8830372095783441548
    stxdw [r10-0x30], r2                    
    lddw r2, 0x3ce2ffec38d9d1e4                     r2 load str located at 4387350377043448292
    stxdw [r10-0x38], r2                    
    ldxdw r3, [r10-0xf8]                    
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r2, [r4+0x20]                     
    stxdw [r10-0xc0], r3                    
    ldxdw r3, [r10-0x108]                   
    stxdw [r10-0xe0], r3                    
    mov64 r3, r10                                   r3 = r10
    add64 r3, -224                                  r3 += -224   ///  r3 = r3.wrapping_add(-224 as i32 as i64 as u64)
    stxdw [r10-0x48], r3                    
    lddw r3, 0x100033548 --> b"\x04programs/solfi-v2-program/src/instructions/update"        r3 load str located at 4295177544
    stxdw [r10-0x58], r3                    
    mov64 r3, r10                                   r3 = r10
    add64 r3, -56                                   r3 += -56   ///  r3 = r3.wrapping_add(-56 as i32 as i64 as u64)
    stxdw [r10-0x60], r3                    
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xd0], r2                    
    sth [r10-0xb8], 1                       
    sth [r10-0xc8], 0                       
    sth [r10-0xd8], 257                     
    stdw [r10-0x40], 3                      
    stdw [r10-0x50], 1                      
    stxdw [r10-0x8], r1                     
    mov64 r1, r4                                    r1 = r4
    add64 r1, 32                                    r1 += 32   ///  r1 = r1.wrapping_add(32 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stxdw [r10-0x18], r4                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -24                                   r2 += -24   ///  r2 = r2.wrapping_add(-24 as i32 as i64 as u64)
    call function_891                       
    mov64 r9, r0                                    r9 = r0
    mov64 r2, r9                                    r2 = r9
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jne r2, 26, lbb_3457                            if r2 != (26 as i32 as i64 as u64) { pc += -280 }
    ldxb r1, [r8+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r8+0x0], r1                       
    mov64 r9, 26                                    r9 = 26 as i32 as i64 as u64
    ja lbb_3338                                     if true { pc += -404 }

function_3742:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r8, 10                                    r8 = 10 as i32 as i64 as u64
    jne r3, 3, lbb_3802                             if r3 != (3 as i32 as i64 as u64) { pc += 56 }
    stxdw [r10-0x20], r5                    
    stxdw [r10-0x28], r4                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -40                                   r2 += -40   ///  r2 = r2.wrapping_add(-40 as i32 as i64 as u64)
    call function_531                       
    ldxdw r0, [r10-0x10]                    
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    ldxdw r2, [r10-0x18]                    
    jeq r2, r1, lbb_3761                            if r2 == r1 { pc += 3 }
    ldxdw r1, [r10-0x20]                    
    jeq r1, 0, lbb_3773                             if r1 == (0 as i32 as i64 as u64) { pc += 13 }
    call function_70                        
lbb_3761:
    mov64 r8, 2                                     r8 = 2 as i32 as i64 as u64
    mov64 r2, r0                                    r2 = r0
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    mov64 r3, r2                                    r3 = r2
    add64 r3, -2                                    r3 += -2   ///  r3 = r3.wrapping_add(-2 as i32 as i64 as u64)
    jlt r3, 2, lbb_3802                             if r3 < (2 as i32 as i64 as u64) { pc += 35 }
    jeq r2, 0, lbb_3802                             if r2 == (0 as i32 as i64 as u64) { pc += 34 }
    ldxdw r1, [r0-0x1]                      
    ldxdw r2, [r0+0x7]                      
    ldxdw r2, [r2+0x0]                      
    callx r2                                
lbb_3772:
    ja lbb_3802                                     if true { pc += 29 }
lbb_3773:
    stxdw [r10-0x38], r0                    
    ldxdw r9, [r10-0x8]                     
    mov64 r3, r7                                    r3 = r7
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_6494                      
    ldxw r8, [r10-0x30]                     
    jeq r8, 26, lbb_3784                            if r8 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3801                                     if true { pc += 17 }
lbb_3784:
    ldxdw r8, [r7+0x10]                     
    mov64 r1, r8                                    r1 = r8
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033328 --> b"\x06\x8747\xb4\xfb\xc9\xf4\x047\xdd<\xe6?\x9c\xfe\x8c$\x84\x03U\x93\xa1'\…        r2 load str located at 4295177000
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r4, r8                                    r4 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, 38                                    r1 = 38 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_3798                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3800                                     if true { pc += 2 }
lbb_3798:
    ldxdw r2, [r4+0x50]                     
    jeq r2, 1048576, lbb_3805                       if r2 == (1048576 as i32 as i64 as u64) { pc += 5 }
lbb_3800:
    ja lbb_3802                                     if true { pc += 1 }
lbb_3801:
    ldxw r1, [r10-0x2c]                     
lbb_3802:
    stxw [r6+0x4], r1                       
    stxw [r6+0x0], r8                       
    exit                                    
lbb_3805:
    mov64 r8, 11                                    r8 = 11 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxb r2, [r4+0x0]                       
    mov64 r3, r2                                    r3 = r2
    and64 r3, 15                                    r3 &= 15   ///  r3 = r3.and(15)
    jne r3, 0, lbb_3800                             if r3 != (0 as i32 as i64 as u64) { pc += -11 }
    or64 r2, 8                                      r2 |= 8   ///  r2 = r2.or(8)
    stxb [r4+0x0], r2                       
    stxdw [r10-0x40], r4                    
    mov64 r2, r4                                    r2 = r4
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r3, 1048576                               r3 = 1048576 as i32 as i64 as u64
    call function_13476                     
    ldxw r1, [r10-0x18]                     
    jne r1, 0, lbb_3874                             if r1 != (0 as i32 as i64 as u64) { pc += 52 }
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r7+0x0]                      
    mov64 r7, r1                                    r7 = r1
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, 12                                    r1 = 12 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_3835                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3876                                     if true { pc += 41 }
lbb_3835:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_13489                     
    ldxw r1, [r10-0x18]                     
    jne r1, 0, lbb_3874                             if r1 != (0 as i32 as i64 as u64) { pc += 33 }
    ldxdw r5, [r10-0x38]                    
    jeq r9, 0, lbb_3868                             if r9 == (0 as i32 as i64 as u64) { pc += 25 }
    ldxdw r2, [r10-0x10]                    
    lsh64 r9, 3                                     r9 <<= 3   ///  r9 = r9.wrapping_shl(3)
    ldxdw r1, [r2+0x0]                      
    mov64 r3, r2                                    r3 = r2
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
lbb_3848:
    jlt r1, 511, lbb_3850                           if r1 < (511 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3881                                     if true { pc += 31 }
lbb_3850:
    lsh64 r1, 3                                     r1 <<= 3   ///  r1 = r1.wrapping_shl(3)
    mov64 r4, r3                                    r4 = r3
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    ldxdw r1, [r5+0x0]                      
    stxdw [r4+0x0], r1                      
    ldxdw r1, [r2+0x0]                      
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_3860                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_3860:
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jne r4, 0, lbb_3885                             if r4 != (0 as i32 as i64 as u64) { pc += 23 }
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    mod64 r1, 511                                   r1 %= 511   ///  r1 = r1 % (511 as u64)
    stxdw [r2+0x0], r1                      
    add64 r9, -8                                    r9 += -8   ///  r9 = r9.wrapping_add(-8 as i32 as i64 as u64)
    jeq r9, 0, lbb_3868                             if r9 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3848                                     if true { pc += -20 }
lbb_3868:
    ldxdw r2, [r10-0x40]                    
    ldxb r1, [r2+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r2+0x0], r1                       
    mov64 r8, 26                                    r8 = 26 as i32 as i64 as u64
    ja lbb_3772                                     if true { pc += -102 }
lbb_3874:
    ldxw r1, [r10-0x10]                     
    ldxw r8, [r10-0x14]                     
lbb_3876:
    ldxdw r3, [r10-0x40]                    
    ldxb r2, [r3+0x0]                       
    and64 r2, 247                                   r2 &= 247   ///  r2 = r2.and(247)
    stxb [r3+0x0], r2                       
    ja lbb_3802                                     if true { pc += -79 }
lbb_3881:
    mov64 r2, 511                                   r2 = 511 as i32 as i64 as u64
    lddw r3, 0x100034838 --> b"\x00\x00\x00\x00I5\x03\x00C\x00\x00\x00\x00\x00\x00\x00(\x00\x00\x00\x09\…        r3 load str located at 4295182392
    call function_20679                     
lbb_3885:
    lddw r1, 0x100034850 --> b"\x00\x00\x00\x00I5\x03\x00C\x00\x00\x00\x00\x00\x00\x00)\x00\x00\x00 \x00…        r1 load str located at 4295182416
    call function_22712                     

function_3888:
    mov64 r8, r2                                    r8 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r1, 10                                    r1 = 10 as i32 as i64 as u64
    jne r3, 4, lbb_3950                             if r3 != (4 as i32 as i64 as u64) { pc += 58 }
    mov64 r3, r8                                    r3 = r8
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    call function_6494                      
    ldxw r1, [r10-0x50]                     
    jne r1, 26, lbb_3949                            if r1 != (26 as i32 as i64 as u64) { pc += 49 }
    ldxdw r9, [r8+0x10]                     
    mov64 r1, r9                                    r1 = r9
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033328 --> b"\x06\x8747\xb4\xfb\xc9\xf4\x047\xdd<\xe6?\x9c\xfe\x8c$\x84\x03U\x93\xa1'\…        r2 load str located at 4295177000
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 4                                     r2 = 4 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_3913                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3950                                     if true { pc += 37 }
lbb_3913:
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxb r3, [r9+0x0]                       
    mov64 r4, r3                                    r4 = r3
    and64 r4, 15                                    r4 &= 15   ///  r4 = r4.and(15)
    jne r4, 0, lbb_3950                             if r4 != (0 as i32 as i64 as u64) { pc += 31 }
    or64 r3, 8                                      r3 |= 8   ///  r3 = r3.or(8)
    stxb [r9+0x0], r3                       
    ldxdw r3, [r9+0x50]                     
    mov64 r2, r9                                    r2 = r9
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    call function_15574                     
    ldxw r1, [r10-0x28]                     
    jne r1, 0, lbb_3953                             if r1 != (0 as i32 as i64 as u64) { pc += 24 }
    ldxdw r7, [r10-0x20]                    
    ldxdw r8, [r8+0x18]                     
    mov64 r1, r8                                    r1 = r8
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033328 --> b"\x06\x8747\xb4\xfb\xc9\xf4\x047\xdd<\xe6?\x9c\xfe\x8c$\x84\x03U\x93\xa1'\…        r2 load str located at 4295177000
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 38                                    r2 = 38 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_3943                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_3945                                     if true { pc += 2 }
lbb_3943:
    ldxdw r3, [r8+0x50]                     
    jeq r3, 1048576, lbb_3956                       if r3 == (1048576 as i32 as i64 as u64) { pc += 11 }
lbb_3945:
    ldxb r3, [r9+0x0]                       
    and64 r3, 247                                   r3 &= 247   ///  r3 = r3.and(247)
    stxb [r9+0x0], r3                       
    ja lbb_3950                                     if true { pc += 1 }
lbb_3949:
    ldxw r2, [r10-0x4c]                     
lbb_3950:
    stxw [r6+0x4], r2                       
    stxw [r6+0x0], r1                       
    exit                                    
lbb_3953:
    ldxw r2, [r10-0x20]                     
    ldxw r1, [r10-0x24]                     
    ja lbb_3945                                     if true { pc += -11 }
lbb_3956:
    ldxb r4, [r7+0x1]                       
    mov64 r2, r7                                    r2 = r7
    add64 r2, 56                                    r2 += 56   ///  r2 = r2.wrapping_add(56 as i32 as i64 as u64)
    mov64 r3, r7                                    r3 = r7
    add64 r3, 88                                    r3 += 88   ///  r3 = r3.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    call function_17371                     
    ldxdw r1, [r10-0x10]                    
    stxdw [r10-0x30], r1                    
    ldxdw r1, [r10-0x18]                    
    stxdw [r10-0x38], r1                    
    ldxdw r1, [r10-0x20]                    
    stxdw [r10-0x40], r1                    
    ldxdw r1, [r10-0x28]                    
    stxdw [r10-0x48], r1                    
    ldxb r1, [r10-0x8]                      
    stxdw [r10-0x60], r1                    
    mov64 r2, r9                                    r2 = r9
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    stxdw [r10-0x68], r2                    
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 4                                     r2 = 4 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_3945                             if r0 != (0 as i32 as i64 as u64) { pc += -41 }
    ldxb r3, [r7+0x0]                       
    ldxdw r4, [r10-0x60]                    
    jne r4, r3, lbb_3945                            if r4 != r3 { pc += -44 }
    ldxdw r1, [r8+0x8]                      
    ldxdw r2, [r8+0x10]                     
    ldxdw r3, [r8+0x18]                     
    ldxdw r4, [r8+0x20]                     
    stxdw [r7+0x118], r4                    
    stxdw [r7+0x110], r3                    
    stxdw [r7+0x108], r2                    
    stxdw [r7+0x100], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    ldxdw r3, [r10-0x68]                    
    call function_15520                     
    ldxw r1, [r10-0x58]                     
    jne r1, 26, lbb_4009                            if r1 != (26 as i32 as i64 as u64) { pc += 5 }
    ldxb r1, [r9+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r9+0x0], r1                       
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ja lbb_3950                                     if true { pc += -59 }
lbb_4009:
    ldxw r2, [r10-0x54]                     
    ja lbb_3945                                     if true { pc += -66 }

function_4011:
    mov64 r9, r4                                    r9 = r4
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r8, 10                                    r8 = 10 as i32 as i64 as u64
    jne r3, 7, lbb_4215                             if r3 != (7 as i32 as i64 as u64) { pc += 199 }
    stxdw [r10-0x8a0], r5                   
    mov64 r3, r7                                    r3 = r7
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2192                                 r1 += -2192   ///  r1 = r1.wrapping_add(-2192 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_6494                      
    ldxw r8, [r10-0x890]                    
    jne r8, 26, lbb_4214                            if r8 != (26 as i32 as i64 as u64) { pc += 189 }
    ldxdw r1, [r7+0x10]                     
    stxdw [r10-0x8a8], r1                   
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033328 --> b"\x06\x8747\xb4\xfb\xc9\xf4\x047\xdd<\xe6?\x9c\xfe\x8c$\x84\x03U\x93\xa1'\…        r2 load str located at 4295177000
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, 4                                     r1 = 4 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_4038                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4215                                     if true { pc += 177 }
lbb_4038:
    ldxdw r1, [r7+0x18]                     
    stxdw [r10-0x8b0], r1                   
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033368 --> b"\xe4\xd1\xd98\xec\xff\xe2<\x8c\x94\xb1\xc3\xa2\xc8\x8bz\xd8\xbdn\xff\x8e\…        r2 load str located at 4295177064
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 44                                    r1 = 44 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_4215                             if r0 != (0 as i32 as i64 as u64) { pc += 166 }
    ldxdw r2, [r10-0x8b0]                   
    ldxdw r2, [r2+0x50]                     
    jne r2, 1032, lbb_4215                          if r2 != (1032 as i32 as i64 as u64) { pc += 163 }
    mov64 r8, 11                                    r8 = 11 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0x8a8]                   
    ldxb r2, [r2+0x0]                       
    mov64 r3, r2                                    r3 = r2
    and64 r3, 15                                    r3 &= 15   ///  r3 = r3.and(15)
    jne r3, 0, lbb_4215                             if r3 != (0 as i32 as i64 as u64) { pc += 156 }
    ldxdw r1, [r10-0x8b0]                   
    or64 r2, 8                                      r2 |= 8   ///  r2 = r2.or(8)
    ldxdw r8, [r10-0x8a8]                   
    stxb [r8+0x0], r2                       
    ldxdw r3, [r8+0x50]                     
    mov64 r2, r8                                    r2 = r8
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2152                                 r1 += -2152   ///  r1 = r1.wrapping_add(-2152 as i32 as i64 as u64)
    call function_15574                     
    ldxw r1, [r10-0x868]                    
    jne r1, 0, lbb_4218                             if r1 != (0 as i32 as i64 as u64) { pc += 147 }
    ldxdw r3, [r10-0x860]                   
    ldxb r4, [r3+0x1]                       
    mov64 r2, r3                                    r2 = r3
    add64 r2, 56                                    r2 += 56   ///  r2 = r2.wrapping_add(56 as i32 as i64 as u64)
    stxdw [r10-0x8b8], r3                   
    add64 r3, 88                                    r3 += 88   ///  r3 = r3.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2152                                 r1 += -2152   ///  r1 = r1.wrapping_add(-2152 as i32 as i64 as u64)
    call function_17371                     
    ldxdw r1, [r10-0x850]                   
    stxdw [r10-0x870], r1                   
    ldxdw r1, [r10-0x858]                   
    stxdw [r10-0x878], r1                   
    ldxdw r1, [r10-0x860]                   
    stxdw [r10-0x880], r1                   
    ldxdw r1, [r10-0x868]                   
    stxdw [r10-0x888], r1                   
    mov64 r2, r8                                    r2 = r8
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2184                                 r1 += -2184   ///  r1 = r1.wrapping_add(-2184 as i32 as i64 as u64)
    stxdw [r10-0x8c0], r2                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, 4                                     r1 = 4 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_4101                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4220                                     if true { pc += 119 }
lbb_4101:
    ldxdw r1, [r10-0x8b8]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2152                                 r1 += -2152   ///  r1 = r1.wrapping_add(-2152 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    ldxdw r3, [r10-0x8a0]                   
    call function_354                       
    ldxb r1, [r10-0x868]                    
    jne r1, 0, lbb_4225                             if r1 != (0 as i32 as i64 as u64) { pc += 116 }
    mov64 r1, 41                                    r1 = 41 as i32 as i64 as u64
    ldxdw r9, [r10-0x8b8]                   
    ldxb r2, [r9+0x2]                       
    jeq r2, 0, lbb_4114                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4220                                     if true { pc += 106 }
lbb_4114:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2152                                 r1 += -2152   ///  r1 = r1.wrapping_add(-2152 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    ldxdw r3, [r10-0x8b0]                   
    call function_4232                      
    ldxw r1, [r10-0x868]                    
    jne r1, 0, lbb_4218                             if r1 != (0 as i32 as i64 as u64) { pc += 97 }
    ldxw r1, [r10-0x860]                    
    stxdw [r10-0x8a0], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1116                                 r1 += -1116   ///  r1 = r1.wrapping_add(-1116 as i32 as i64 as u64)
    stxdw [r10-0x8c8], r1                   
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2140                                 r2 += -2140   ///  r2 = r2.wrapping_add(-2140 as i32 as i64 as u64)
    mov64 r3, 1020                                  r3 = 1020 as i32 as i64 as u64
    call function_23152                     
    ldxdw r1, [r10-0x8a0]                   
    stxw [r9+0x2c0], r1                     
    mov64 r1, r9                                    r1 = r9
    add64 r1, 708                                   r1 += 708   ///  r1 = r1.wrapping_add(708 as i32 as i64 as u64)
    ldxdw r2, [r10-0x8c8]                   
    mov64 r3, 1020                                  r3 = 1020 as i32 as i64 as u64
    call function_23152                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2200                                 r1 += -2200   ///  r1 = r1.wrapping_add(-2200 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    ldxdw r3, [r10-0x8c0]                   
    call function_15520                     
    ldxw r2, [r10-0x898]                    
    jeq r2, 26, lbb_4145                            if r2 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4229                                     if true { pc += 84 }
lbb_4145:
    ldxdw r1, [r7+0x28]                     
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x100033368 --> b"\xe4\xd1\xd98\xec\xff\xe2<\x8c\x94\xb1\xc3\xa2\xc8\x8bz\xd8\xbdn\xff\x8e\…        r2 load str located at 4295177064
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 45                                    r1 = 45 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_4156                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4220                                     if true { pc += 64 }
lbb_4156:
    mov64 r1, r7                                    r1 = r7
    add64 r1, 24                                    r1 += 24   ///  r1 = r1.wrapping_add(24 as i32 as i64 as u64)
    lddw r2, 0x694781f0f75491d                      r2 load str located at 474135935579408669
    stxdw [r10-0x20], r2                    
    lddw r2, 0x46db918eff6ebdd8                     r2 load str located at 5105834645911420376
    stxdw [r10-0x28], r2                    
    lddw r2, 0x7a8bc8a2c3b1948c                     r2 load str located at 8830372095783441548
    stxdw [r10-0x30], r2                    
    lddw r2, 0x3ce2ffec38d9d1e4                     r2 load str located at 4387350377043448292
    stxdw [r10-0x38], r2                    
    ldxdw r4, [r10-0x8b0]                   
    add64 r4, 8                                     r4 += 8   ///  r4 = r4.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r2, [r7+0x0]                      
    ldxdw r3, [r7+0x20]                     
    stxdw [r10-0x848], r4                   
    mov64 r4, r10                                   r4 = r10
    add64 r4, -2152                                 r4 += -2152   ///  r4 = r4.wrapping_add(-2152 as i32 as i64 as u64)
    stxdw [r10-0x48], r4                    
    lddw r4, 0x100033548 --> b"\x04programs/solfi-v2-program/src/instructions/update"        r4 load str located at 4295177544
    stxdw [r10-0x58], r4                    
    mov64 r4, r10                                   r4 = r10
    add64 r4, -56                                   r4 += -56   ///  r4 = r4.wrapping_add(-56 as i32 as i64 as u64)
    stxdw [r10-0x60], r4                    
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x858], r3                   
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x868], r2                   
    sth [r10-0x840], 1                      
    sth [r10-0x850], 0                      
    sth [r10-0x860], 257                    
    stdw [r10-0x40], 3                      
    stdw [r10-0x50], 1                      
    stxdw [r10-0x8], r1                     
    mov64 r1, r7                                    r1 = r7
    add64 r1, 32                                    r1 += 32   ///  r1 = r1.wrapping_add(32 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stxdw [r10-0x18], r7                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -24                                   r2 += -24   ///  r2 = r2.wrapping_add(-24 as i32 as i64 as u64)
    call function_891                       
    mov64 r8, r0                                    r8 = r0
    mov64 r2, r8                                    r2 = r8
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jne r2, 26, lbb_4220                            if r2 != (26 as i32 as i64 as u64) { pc += 12 }
    ldxdw r2, [r10-0x8a8]                   
    ldxb r1, [r2+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r2+0x0], r1                       
    mov64 r8, 26                                    r8 = 26 as i32 as i64 as u64
    ja lbb_4215                                     if true { pc += 1 }
lbb_4214:
    ldxw r1, [r10-0x88c]                    
lbb_4215:
    stxw [r6+0x4], r1                       
    stxw [r6+0x0], r8                       
    exit                                    
lbb_4218:
    ldxw r1, [r10-0x860]                    
    ldxw r8, [r10-0x864]                    
lbb_4220:
    ldxdw r3, [r10-0x8a8]                   
    ldxb r2, [r3+0x0]                       
    and64 r2, 247                                   r2 &= 247   ///  r2 = r2.and(247)
    stxb [r3+0x0], r2                       
    ja lbb_4215                                     if true { pc += -10 }
lbb_4225:
    ldxdw r1, [r10-0x860]                   
    call function_174                       
    mov64 r8, 2                                     r8 = 2 as i32 as i64 as u64
    ja lbb_4220                                     if true { pc += -9 }
lbb_4229:
    ldxw r1, [r10-0x894]                    
    mov64 r8, r2                                    r8 = r2
    ja lbb_4220                                     if true { pc += -12 }

function_4232:
    mov64 r6, r3                                    r6 = r3
    mov64 r7, r1                                    r7 = r1
    add64 r2, 704                                   r2 += 704   ///  r2 = r2.wrapping_add(704 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1024                                 r1 += -1024   ///  r1 = r1.wrapping_add(-1024 as i32 as i64 as u64)
    call function_15981                     
    ldxw r1, [r10-0x400]                    
    jne r1, 0, lbb_4257                             if r1 != (0 as i32 as i64 as u64) { pc += 17 }
    ldxb r1, [r6+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 8                                     r2 &= 8   ///  r2 = r2.and(8)
    jne r2, 0, lbb_4260                             if r2 != (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    jeq r2, 7, lbb_4260                             if r2 == (7 as i32 as i64 as u64) { pc += 13 }
    ldxdw r8, [r10-0x3f8]                   
    mov64 r2, r1                                    r2 = r1
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    stxb [r6+0x0], r2                       
    ldxdw r2, [r6+0x50]                     
    jgt r2, 7, lbb_4265                             if r2 > (7 as i32 as i64 as u64) { pc += 12 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    lddw r3, 0x100034868 --> b"\x00\x00\x00\x00\x8c5\x03\x00B\x00\x00\x00\x00\x00\x00\x00b\x00\x00\x00B\…        r3 load str located at 4295182440
    call function_22021                     
lbb_4257:
    stw [r7+0x8], 36                        
    stdw [r7+0x0], 1                        
    ja lbb_4305                                     if true { pc += 45 }
lbb_4260:
    lddw r1, 0xb00000001                            r1 load str located at 47244640257
    stxdw [r7+0x0], r1                      
    stw [r7+0x8], 0                         
    ja lbb_4305                                     if true { pc += 40 }
lbb_4265:
    ldxb r3, [r6+0x58]                      
    jne r3, 1, lbb_4292                             if r3 != (1 as i32 as i64 as u64) { pc += 25 }
    jne r2, 1032, lbb_4295                          if r2 != (1032 as i32 as i64 as u64) { pc += 27 }
    mov64 r2, r6                                    r2 = r6
    add64 r2, 96                                    r2 += 96   ///  r2 = r2.wrapping_add(96 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    and64 r3, 7                                     r3 &= 7   ///  r3 = r3.and(7)
    jeq r3, 0, lbb_4274                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4295                                     if true { pc += 21 }
lbb_4274:
    mov64 r9, r10                                   r9 = r10
    add64 r9, -1024                                 r9 += -1024   ///  r9 = r9.wrapping_add(-1024 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    mov64 r3, 1024                                  r3 = 1024 as i32 as i64 as u64
    call function_23152                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1032                                 r1 += -1032   ///  r1 = r1.wrapping_add(-1032 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, r8                                    r3 = r8
    call function_17062                     
    ldxw r1, [r10-0x408]                    
    jeq r1, 26, lbb_4287                            if r1 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4298                                     if true { pc += 11 }
lbb_4287:
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1024                                 r2 += -1024   ///  r2 = r2.wrapping_add(-1024 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_15897                     
    ja lbb_4302                                     if true { pc += 10 }
lbb_4292:
    stw [r7+0x8], 44                        
    stdw [r7+0x0], 1                        
    ja lbb_4304                                     if true { pc += 9 }
lbb_4295:
    stw [r7+0x8], 36                        
    stdw [r7+0x0], 1                        
    ja lbb_4304                                     if true { pc += 6 }
lbb_4298:
    ldxw r2, [r10-0x404]                    
    stxw [r7+0x4], r1                       
    stxw [r7+0x8], r2                       
    stw [r7+0x0], 1                         
lbb_4302:
    ldxb r1, [r6+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
lbb_4304:
    stxb [r6+0x0], r1                       
lbb_4305:
    exit                                    

function_4306:
    mov64 r8, r2                                    r8 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r1, 10                                    r1 = 10 as i32 as i64 as u64
    jne r3, 4, lbb_4368                             if r3 != (4 as i32 as i64 as u64) { pc += 58 }
    mov64 r3, r8                                    r3 = r8
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -216                                  r1 += -216   ///  r1 = r1.wrapping_add(-216 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    call function_6494                      
    ldxw r1, [r10-0xd8]                     
    jne r1, 26, lbb_4367                            if r1 != (26 as i32 as i64 as u64) { pc += 49 }
    ldxdw r9, [r8+0x10]                     
    mov64 r1, r9                                    r1 = r9
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033328 --> b"\x06\x8747\xb4\xfb\xc9\xf4\x047\xdd<\xe6?\x9c\xfe\x8c$\x84\x03U\x93\xa1'\…        r2 load str located at 4295177000
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 4                                     r2 = 4 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_4331                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4368                                     if true { pc += 37 }
lbb_4331:
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxb r3, [r9+0x0]                       
    mov64 r4, r3                                    r4 = r3
    and64 r4, 15                                    r4 &= 15   ///  r4 = r4.and(15)
    jne r4, 0, lbb_4368                             if r4 != (0 as i32 as i64 as u64) { pc += 31 }
    or64 r3, 8                                      r3 |= 8   ///  r3 = r3.or(8)
    stxb [r9+0x0], r3                       
    ldxdw r3, [r9+0x50]                     
    mov64 r2, r9                                    r2 = r9
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    call function_15574                     
    ldxw r1, [r10-0xd0]                     
    jne r1, 0, lbb_4371                             if r1 != (0 as i32 as i64 as u64) { pc += 24 }
    ldxdw r7, [r10-0xc8]                    
    ldxdw r8, [r8+0x18]                     
    mov64 r1, r8                                    r1 = r8
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033428 --> b"\xc1\xa4\x86\xeb\x03\xafw\xa7B\x17\xc6\x96"\xcf\x9e5\x0ea\xbbBK?_\x11?\x9…        r2 load str located at 4295177256
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 29                                    r2 = 29 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_4361                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4363                                     if true { pc += 2 }
lbb_4361:
    ldxdw r3, [r8+0x50]                     
    jeq r3, 168, lbb_4374                           if r3 == (168 as i32 as i64 as u64) { pc += 11 }
lbb_4363:
    ldxb r3, [r9+0x0]                       
    and64 r3, 247                                   r3 &= 247   ///  r3 = r3.and(247)
    stxb [r9+0x0], r3                       
    ja lbb_4368                                     if true { pc += 1 }
lbb_4367:
    ldxw r2, [r10-0xd4]                     
lbb_4368:
    stxw [r6+0x4], r2                       
    stxw [r6+0x0], r1                       
    exit                                    
lbb_4371:
    ldxw r2, [r10-0xc8]                     
    ldxw r1, [r10-0xcc]                     
    ja lbb_4363                                     if true { pc += -11 }
lbb_4374:
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxb r3, [r8+0x0]                       
    mov64 r4, r3                                    r4 = r3
    and64 r4, 8                                     r4 &= 8   ///  r4 = r4.and(8)
    jne r4, 0, lbb_4363                             if r4 != (0 as i32 as i64 as u64) { pc += -17 }
    mov64 r4, r3                                    r4 = r3
    and64 r4, 7                                     r4 &= 7   ///  r4 = r4.and(7)
    jeq r4, 7, lbb_4363                             if r4 == (7 as i32 as i64 as u64) { pc += -20 }
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    stxb [r8+0x0], r3                       
    mov64 r2, r8                                    r2 = r8
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r3, 168                                   r3 = 168 as i32 as i64 as u64
    call function_18741                     
    ldxb r1, [r10-0xd0]                     
    jne r1, 0, lbb_4454                             if r1 != (0 as i32 as i64 as u64) { pc += 61 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    ldxdw r3, [r10-0xc0]                    
    jeq r3, 0, lbb_4456                             if r3 == (0 as i32 as i64 as u64) { pc += 59 }
    ldxb r4, [r7+0x1]                       
    mov64 r2, r7                                    r2 = r7
    add64 r2, 56                                    r2 += 56   ///  r2 = r2.wrapping_add(56 as i32 as i64 as u64)
    mov64 r3, r7                                    r3 = r7
    add64 r3, 88                                    r3 += 88   ///  r3 = r3.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    call function_17371                     
    ldxdw r1, [r10-0xb8]                    
    stxdw [r10-0x8], r1                     
    ldxdw r1, [r10-0xc0]                    
    stxdw [r10-0x10], r1                    
    ldxdw r1, [r10-0xc8]                    
    stxdw [r10-0x18], r1                    
    ldxdw r1, [r10-0xd0]                    
    stxdw [r10-0x20], r1                    
    ldxb r1, [r10-0xb0]                     
    stxdw [r10-0xe8], r1                    
    mov64 r2, r9                                    r2 = r9
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0xf0], r2                    
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 4                                     r2 = 4 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_4428                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4456                                     if true { pc += 28 }
lbb_4428:
    ldxb r3, [r7+0x0]                       
    ldxdw r4, [r10-0xe8]                    
    jne r4, r3, lbb_4456                            if r4 != r3 { pc += 25 }
    ldxdw r1, [r8+0x8]                      
    ldxdw r2, [r8+0x10]                     
    ldxdw r3, [r8+0x18]                     
    ldxdw r4, [r8+0x20]                     
    stxdw [r7+0x30], r4                     
    stxdw [r7+0x28], r3                     
    stxdw [r7+0x20], r2                     
    stxdw [r7+0x18], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -224                                  r1 += -224   ///  r1 = r1.wrapping_add(-224 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    ldxdw r3, [r10-0xf0]                    
    call function_15520                     
    ldxw r1, [r10-0xe0]                     
    jne r1, 26, lbb_4460                            if r1 != (26 as i32 as i64 as u64) { pc += 14 }
    ldxb r1, [r8+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxb [r8+0x0], r1                       
    ldxb r1, [r9+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r9+0x0], r1                       
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ja lbb_4368                                     if true { pc += -86 }
lbb_4454:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 39                                    r2 = 39 as i32 as i64 as u64
lbb_4456:
    ldxb r3, [r8+0x0]                       
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    stxb [r8+0x0], r3                       
    ja lbb_4363                                     if true { pc += -97 }
lbb_4460:
    ldxw r2, [r10-0xdc]                     
    ja lbb_4456                                     if true { pc += -6 }

function_4462:
    mov64 r6, r1                                    r6 = r1
    mov64 r7, 10                                    r7 = 10 as i32 as i64 as u64
    jne r3, 12, lbb_4585                            if r3 != (12 as i32 as i64 as u64) { pc += 120 }
    stxdw [r10-0x110], r4                   
    stxdw [r10-0x108], r5                   
    mov64 r8, r2                                    r8 = r2
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    stxdw [r10-0x100], r2                   
    mov64 r3, r8                                    r3 = r8
    call function_6494                      
    ldxw r7, [r10-0xd0]                     
    jne r7, 26, lbb_4577                            if r7 != (26 as i32 as i64 as u64) { pc += 101 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ldxdw r8, [r8+0x0]                      
    ldxb r1, [r8+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 8                                     r2 &= 8   ///  r2 = r2.and(8)
    mov64 r7, 11                                    r7 = 11 as i32 as i64 as u64
    jne r2, 0, lbb_4585                             if r2 != (0 as i32 as i64 as u64) { pc += 102 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    jeq r2, 7, lbb_4585                             if r2 == (7 as i32 as i64 as u64) { pc += 99 }
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxb [r8+0x0], r1                       
    ldxdw r3, [r8+0x50]                     
    stxdw [r10-0x118], r8                   
    add64 r8, 88                                    r8 += 88   ///  r8 = r8.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    call function_10296                     
    ldxw r1, [r10-0x60]                     
    jne r1, 0, lbb_4579                             if r1 != (0 as i32 as i64 as u64) { pc += 82 }
    ldxdw r1, [r10-0x58]                    
    ldxdw r2, [r10-0x100]                   
    ldxdw r2, [r2+0x10]                     
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r9, 53                                    r9 = 53 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_4509                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4581                                     if true { pc += 72 }
lbb_4509:
    ldxdw r2, [r10-0x118]                   
    ldxb r1, [r2+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxb [r2+0x0], r1                       
    ldxdw r1, [r10-0x100]                   
    ldxdw r1, [r1+0x18]                     
    stxdw [r10-0x120], r1                   
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033328 --> b"\x06\x8747\xb4\xfb\xc9\xf4\x047\xdd<\xe6?\x9c\xfe\x8c$\x84\x03U\x93\xa1'\…        r2 load str located at 4295177000
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r9, 4                                     r9 = 4 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_4585                             if r0 != (0 as i32 as i64 as u64) { pc += 60 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x120]                   
    ldxb r1, [r1+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    mov64 r7, 11                                    r7 = 11 as i32 as i64 as u64
    jne r2, 0, lbb_4585                             if r2 != (0 as i32 as i64 as u64) { pc += 53 }
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    ldxdw r9, [r10-0x120]                   
    stxb [r9+0x0], r1                       
    ldxdw r3, [r9+0x50]                     
    mov64 r2, r9                                    r2 = r9
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    call function_15585                     
    ldxw r1, [r10-0x60]                     
    jne r1, 0, lbb_4588                             if r1 != (0 as i32 as i64 as u64) { pc += 45 }
    ldxdw r1, [r10-0x58]                    
    stxdw [r10-0x128], r1                   
    ldxb r4, [r1+0x1]                       
    ldxdw r7, [r10-0x100]                   
    ldxdw r3, [r7+0x48]                     
    ldxdw r2, [r7+0x40]                     
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x138], r2                   
    stxdw [r10-0x140], r3                   
    call function_17371                     
    ldxdw r1, [r10-0x48]                    
    stxdw [r10-0xb0], r1                    
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0xb8], r1                    
    ldxdw r1, [r10-0x58]                    
    stxdw [r10-0xc0], r1                    
    ldxdw r1, [r10-0x60]                    
    stxdw [r10-0xc8], r1                    
    mov64 r2, r9                                    r2 = r9
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -200                                  r1 += -200   ///  r1 = r1.wrapping_add(-200 as i32 as i64 as u64)
    stxdw [r10-0x130], r2                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r9, 4                                     r9 = 4 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_4594                             if r0 == (0 as i32 as i64 as u64) { pc += 19 }
lbb_4575:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ja lbb_4590                                     if true { pc += 13 }
lbb_4577:
    ldxw r9, [r10-0xcc]                     
    ja lbb_4585                                     if true { pc += 6 }
lbb_4579:
    ldxw r9, [r10-0x58]                     
    ldxw r7, [r10-0x5c]                     
lbb_4581:
    ldxdw r2, [r10-0x118]                   
    ldxb r1, [r2+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
lbb_4584:
    stxb [r2+0x0], r1                       
lbb_4585:
    stxw [r6+0x4], r9                       
    stxw [r6+0x0], r7                       
    exit                                    
lbb_4588:
    ldxw r9, [r10-0x58]                     
    ldxw r7, [r10-0x5c]                     
lbb_4590:
    ldxdw r2, [r10-0x120]                   
lbb_4591:
    ldxb r1, [r2+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    ja lbb_4584                                     if true { pc += -10 }
lbb_4594:
    ldxdw r2, [r7+0x20]                     
    ldxdw r1, [r10-0x128]                   
    add64 r1, 120                                   r1 += 120   ///  r1 = r1.wrapping_add(120 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r9, 7                                     r9 = 7 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_4575                             if r0 != (0 as i32 as i64 as u64) { pc += -29 }
    ldxdw r2, [r7+0x28]                     
    ldxdw r1, [r10-0x128]                   
    add64 r1, 152                                   r1 += 152   ///  r1 = r1.wrapping_add(152 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r9, 8                                     r9 = 8 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_4575                             if r0 != (0 as i32 as i64 as u64) { pc += -39 }
    ldxdw r1, [r10-0x128]                   
    ldxdw r1, [r10-0x118]                   
    ldxb r1, [r1+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    jne r2, 0, lbb_4633                             if r2 != (0 as i32 as i64 as u64) { pc += 13 }
    or64 r1, 8                                      r1 |= 8   ///  r1 = r1.or(8)
    ldxdw r2, [r10-0x118]                   
    stxb [r2+0x0], r1                       
    ldxdw r3, [r2+0x50]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    call function_10307                     
    ldxw r1, [r10-0x60]                     
    jeq r1, 0, lbb_4636                             if r1 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxw r9, [r10-0x58]                     
    ldxw r7, [r10-0x5c]                     
    ja lbb_4783                                     if true { pc += 150 }
lbb_4633:
    mov64 r7, 11                                    r7 = 11 as i32 as i64 as u64
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ja lbb_4590                                     if true { pc += -46 }
lbb_4636:
    ldxdw r1, [r10-0x100]                   
    mov64 r8, r1                                    r8 = r1
    add64 r8, 16                                    r8 += 16   ///  r8 = r8.wrapping_add(16 as i32 as i64 as u64)
    mov64 r4, r1                                    r4 = r1
    add64 r4, 24                                    r4 += 24   ///  r4 = r4.wrapping_add(24 as i32 as i64 as u64)
    mov64 r2, r1                                    r2 = r1
    add64 r2, 64                                    r2 += 64   ///  r2 = r2.wrapping_add(64 as i32 as i64 as u64)
    mov64 r3, r1                                    r3 = r1
    add64 r3, 32                                    r3 += 32   ///  r3 = r3.wrapping_add(32 as i32 as i64 as u64)
    ldxdw r5, [r10-0x58]                    
    stxdw [r10-0x158], r5                   
    stxdw [r10-0xff8], r8                   
    stxdw [r10-0x148], r4                   
    stxdw [r10-0x1000], r4                  
    mov64 r9, r1                                    r9 = r1
    add64 r9, 80                                    r9 += 80   ///  r9 = r9.wrapping_add(80 as i32 as i64 as u64)
    stxdw [r10-0xff0], r9                   
    stdw [r10-0xfe8], 1                     
    mov64 r4, r1                                    r4 = r1
    add64 r4, 48                                    r4 += 48   ///  r4 = r4.wrapping_add(48 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -216                                  r1 += -216   ///  r1 = r1.wrapping_add(-216 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    stxdw [r10-0x168], r2                   
    stxdw [r10-0x160], r3                   
    stxdw [r10-0x150], r4                   
    call function_5754                      
    ldxw r7, [r10-0xd8]                     
    jeq r7, 26, lbb_4666                            if r7 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4780                                     if true { pc += 114 }
lbb_4666:
    ldxdw r4, [r10-0x100]                   
    mov64 r2, r4                                    r2 = r4
    add64 r2, 72                                    r2 += 72   ///  r2 = r2.wrapping_add(72 as i32 as i64 as u64)
    mov64 r3, r4                                    r3 = r4
    add64 r3, 40                                    r3 += 40   ///  r3 = r3.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xff8], r8                   
    ldxdw r1, [r10-0x148]                   
    stxdw [r10-0x1000], r1                  
    mov64 r8, r4                                    r8 = r4
    add64 r8, 88                                    r8 += 88   ///  r8 = r8.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xff0], r8                   
    stdw [r10-0xfe8], 0                     
    add64 r4, 56                                    r4 += 56   ///  r4 = r4.wrapping_add(56 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -224                                  r1 += -224   ///  r1 = r1.wrapping_add(-224 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    stxdw [r10-0x178], r2                   
    stxdw [r10-0x170], r3                   
    stxdw [r10-0x100], r4                   
    call function_5754                      
    ldxw r7, [r10-0xe0]                     
    jeq r7, 26, lbb_4689                            if r7 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4782                                     if true { pc += 93 }
lbb_4689:
    ldxdw r4, [r9+0x0]                      
    add64 r4, 8                                     r4 += 8   ///  r4 = r4.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r7, [r10-0x128]                   
    mov64 r3, r7                                    r3 = r7
    add64 r3, 56                                    r3 += 56   ///  r3 = r3.wrapping_add(56 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    ldxdw r9, [r10-0x158]                   
    mov64 r2, r9                                    r2 = r9
    stxdw [r10-0x180], r4                   
    call function_17416                     
    ldxdw r1, [r10-0x48]                    
    stxdw [r10-0x90], r1                    
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0x98], r1                    
    ldxdw r1, [r10-0x58]                    
    stxdw [r10-0xa0], r1                    
    ldxdw r1, [r10-0x60]                    
    stxdw [r10-0xa8], r1                    
    mov64 r3, r7                                    r3 = r7
    add64 r3, 88                                    r3 += 88   ///  r3 = r3.wrapping_add(88 as i32 as i64 as u64)
    ldxdw r4, [r8+0x0]                      
    add64 r4, 8                                     r4 += 8   ///  r4 = r4.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r8, r4                                    r8 = r4
    call function_17416                     
    ldxdw r1, [r10-0x48]                    
    stxdw [r10-0x70], r1                    
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0x78], r1                    
    ldxdw r1, [r10-0x58]                    
    stxdw [r10-0x80], r1                    
    ldxdw r1, [r10-0x60]                    
    stxdw [r10-0x88], r1                    
    ldxdw r1, [r10-0x150]                   
    ldxdw r1, [r1+0x0]                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -168                                  r2 += -168   ///  r2 = r2.wrapping_add(-168 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r9, 11                                    r9 = 11 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_4738                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4783                                     if true { pc += 45 }
lbb_4738:
    ldxdw r1, [r10-0x100]                   
    ldxdw r1, [r1+0x0]                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -136                                  r2 += -136   ///  r2 = r2.wrapping_add(-136 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_4783                             if r0 != (0 as i32 as i64 as u64) { pc += 35 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -232                                  r1 += -232   ///  r1 = r1.wrapping_add(-232 as i32 as i64 as u64)
    ldxdw r2, [r10-0x128]                   
    ldxdw r3, [r10-0x130]                   
    call function_15520                     
    ldxw r7, [r10-0xe8]                     
    jne r7, 26, lbb_4788                            if r7 != (26 as i32 as i64 as u64) { pc += 33 }
    ldxdw r2, [r10-0x128]                   
    ldxb r1, [r2+0xf9]                      
    stxdw [r10-0x130], r1                   
    ldxb r7, [r2+0xf8]                      
    ldxb r1, [r2+0x1]                       
    stxb [r10-0x62], r1                     
    ldxb r1, [r2+0x0]                       
    stxb [r10-0x61], r1                     
    ldxdw r2, [r10-0x120]                   
    ldxb r1, [r2+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r2+0x0], r1                       
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    ldxdw r2, [r10-0x110]                   
    ldxdw r3, [r10-0x108]                   
    call function_446                       
    ldxdw r1, [r10-0x60]                    
    jeq r1, 0, lbb_4790                             if r1 == (0 as i32 as i64 as u64) { pc += 16 }
    ldxdw r2, [r10-0x58]                    
    call function_160                       
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r9, 33                                    r9 = 33 as i32 as i64 as u64
lbb_4778:
    ldxdw r2, [r10-0x118]                   
    ja lbb_4591                                     if true { pc += -189 }
lbb_4780:
    ldxw r9, [r10-0xd4]                     
    ja lbb_4783                                     if true { pc += 1 }
lbb_4782:
    ldxw r9, [r10-0xdc]                     
lbb_4783:
    ldxdw r2, [r10-0x118]                   
    ldxb r1, [r2+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r2+0x0], r1                       
    ja lbb_4590                                     if true { pc += -198 }
lbb_4788:
    ldxw r9, [r10-0xe4]                     
    ja lbb_4783                                     if true { pc += -7 }
lbb_4790:
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0x108], r1                   
    ldxdw r9, [r10-0x58]                    
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, r9                                    r2 = r9
    call function_160                       
    mov64 r1, r10                                   r1 = r10
    add64 r1, -97                                   r1 += -97   ///  r1 = r1.wrapping_add(-97 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -98                                   r1 += -98   ///  r1 = r1.wrapping_add(-98 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    ldxdw r1, [r10-0x140]                   
    stxdw [r10-0x40], r1                    
    ldxdw r1, [r10-0x138]                   
    stxdw [r10-0x50], r1                    
    lddw r1, 0x1000334fa --> b"marketconfigprograms/solfi-v2-program/src/instruct"        r1 load str located at 4295177466
    stxdw [r10-0x60], r1                    
    stdw [r10-0x18], 1                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x38], 32                     
    stdw [r10-0x48], 32                     
    stdw [r10-0x58], 6                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x8], 5                       
    jeq r9, 0, lbb_4840                             if r9 == (0 as i32 as i64 as u64) { pc += 21 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0xfe0], r1                   
    stxdw [r10-0xfe8], r7                   
    stxdw [r10-0xff0], r9                   
    ldxdw r1, [r10-0x180]                   
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0x168]                   
    stxdw [r10-0x1000], r1                  
    stdw [r10-0xfd8], 1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -240                                  r1 += -240   ///  r1 = r1.wrapping_add(-240 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0x160]                   
    ldxdw r3, [r10-0x150]                   
    ldxdw r4, [r10-0x148]                   
    call function_5273                      
    ldxw r9, [r10-0xec]                     
    ldxw r7, [r10-0xf0]                     
    jeq r7, 26, lbb_4840                            if r7 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4778                                     if true { pc += -62 }
lbb_4840:
    ldxdw r1, [r10-0x108]                   
    jeq r1, 0, lbb_4864                             if r1 == (0 as i32 as i64 as u64) { pc += 22 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0xfe0], r1                   
    ldxdw r1, [r10-0x130]                   
    stxdw [r10-0xfe8], r1                   
    ldxdw r1, [r10-0x108]                   
    stxdw [r10-0xff0], r1                   
    stxdw [r10-0xff8], r8                   
    ldxdw r1, [r10-0x178]                   
    stxdw [r10-0x1000], r1                  
    stdw [r10-0xfd8], 1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -248                                  r1 += -248   ///  r1 = r1.wrapping_add(-248 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0x170]                   
    ldxdw r3, [r10-0x100]                   
    ldxdw r4, [r10-0x148]                   
    call function_5273                      
    ldxw r9, [r10-0xf4]                     
    ldxw r7, [r10-0xf8]                     
    jeq r7, 26, lbb_4864                            if r7 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_4778                                     if true { pc += -86 }
lbb_4864:
    ldxdw r2, [r10-0x118]                   
    ldxb r1, [r2+0x0]                       
    and64 r1, 247                                   r1 &= 247   ///  r1 = r1.and(247)
    stxb [r2+0x0], r1                       
    mov64 r7, 26                                    r7 = 26 as i32 as i64 as u64
    ja lbb_4585                                     if true { pc += -285 }

function_4870:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxdw r7, [r5-0xff8]                    
    ldxb r0, [r7+0x0]                       
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r0, 6, lbb_5266                             if r0 != (6 as i32 as i64 as u64) { pc += 391 }
    ldxb r0, [r7+0x1]                       
    jne r0, 221, lbb_5266                           if r0 != (221 as i32 as i64 as u64) { pc += 389 }
    ldxb r0, [r7+0x2]                       
    jne r0, 246, lbb_5266                           if r0 != (246 as i32 as i64 as u64) { pc += 387 }
    ldxb r0, [r7+0x3]                       
    jne r0, 225, lbb_5266                           if r0 != (225 as i32 as i64 as u64) { pc += 385 }
    ldxdw r0, [r5-0xff0]                    
    ldxb r9, [r7+0x4]                       
    jeq r9, 238, lbb_4957                           if r9 == (238 as i32 as i64 as u64) { pc += 73 }
    jne r9, 215, lbb_5266                           if r9 != (215 as i32 as i64 as u64) { pc += 381 }
    ldxb r5, [r7+0x5]                       
    jne r5, 101, lbb_5266                           if r5 != (101 as i32 as i64 as u64) { pc += 379 }
    ldxb r5, [r7+0x6]                       
    jne r5, 161, lbb_5266                           if r5 != (161 as i32 as i64 as u64) { pc += 377 }
    ldxb r5, [r7+0x7]                       
    jne r5, 147, lbb_5266                           if r5 != (147 as i32 as i64 as u64) { pc += 375 }
    ldxb r5, [r7+0x8]                       
    jne r5, 217, lbb_5266                           if r5 != (217 as i32 as i64 as u64) { pc += 373 }
    ldxb r5, [r7+0x9]                       
    jne r5, 203, lbb_5266                           if r5 != (203 as i32 as i64 as u64) { pc += 371 }
    ldxb r5, [r7+0xa]                       
    jne r5, 225, lbb_5266                           if r5 != (225 as i32 as i64 as u64) { pc += 369 }
    ldxb r5, [r7+0xb]                       
    jne r5, 70, lbb_5266                            if r5 != (70 as i32 as i64 as u64) { pc += 367 }
    ldxb r5, [r7+0xc]                       
    jne r5, 206, lbb_5266                           if r5 != (206 as i32 as i64 as u64) { pc += 365 }
    ldxb r5, [r7+0xd]                       
    jne r5, 235, lbb_5266                           if r5 != (235 as i32 as i64 as u64) { pc += 363 }
    ldxb r5, [r7+0xe]                       
    jne r5, 121, lbb_5266                           if r5 != (121 as i32 as i64 as u64) { pc += 361 }
    ldxb r5, [r7+0xf]                       
    jne r5, 172, lbb_5266                           if r5 != (172 as i32 as i64 as u64) { pc += 359 }
    ldxb r5, [r7+0x10]                      
    jne r5, 28, lbb_5266                            if r5 != (28 as i32 as i64 as u64) { pc += 357 }
    ldxb r5, [r7+0x11]                      
    jne r5, 180, lbb_5266                           if r5 != (180 as i32 as i64 as u64) { pc += 355 }
    ldxb r5, [r7+0x12]                      
    jne r5, 133, lbb_5266                           if r5 != (133 as i32 as i64 as u64) { pc += 353 }
    ldxb r5, [r7+0x13]                      
    jne r5, 237, lbb_5266                           if r5 != (237 as i32 as i64 as u64) { pc += 351 }
    ldxb r5, [r7+0x14]                      
    jne r5, 95, lbb_5266                            if r5 != (95 as i32 as i64 as u64) { pc += 349 }
    ldxb r5, [r7+0x15]                      
    jne r5, 91, lbb_5266                            if r5 != (91 as i32 as i64 as u64) { pc += 347 }
    ldxb r5, [r7+0x16]                      
    jne r5, 55, lbb_5266                            if r5 != (55 as i32 as i64 as u64) { pc += 345 }
    ldxb r5, [r7+0x17]                      
    jne r5, 145, lbb_5266                           if r5 != (145 as i32 as i64 as u64) { pc += 343 }
    ldxb r5, [r7+0x18]                      
    jne r5, 58, lbb_5266                            if r5 != (58 as i32 as i64 as u64) { pc += 341 }
    ldxb r5, [r7+0x19]                      
    jne r5, 140, lbb_5266                           if r5 != (140 as i32 as i64 as u64) { pc += 339 }
    ldxb r5, [r7+0x1a]                      
    jne r5, 245, lbb_5266                           if r5 != (245 as i32 as i64 as u64) { pc += 337 }
    ldxb r5, [r7+0x1b]                      
    jne r5, 133, lbb_5266                           if r5 != (133 as i32 as i64 as u64) { pc += 335 }
    ldxb r5, [r7+0x1c]                      
    jne r5, 126, lbb_5266                           if r5 != (126 as i32 as i64 as u64) { pc += 333 }
    ldxb r5, [r7+0x1d]                      
    jne r5, 255, lbb_5266                           if r5 != (255 as i32 as i64 as u64) { pc += 331 }
    ldxb r5, [r7+0x1e]                      
    jne r5, 0, lbb_5266                             if r5 != (0 as i32 as i64 as u64) { pc += 329 }
    ldxb r5, [r7+0x1f]                      
    jeq r5, 169, lbb_4940                           if r5 == (169 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5266                                     if true { pc += 326 }
lbb_4940:
    stxdw [r10-0xb8], r0                    
    stxdw [r10-0xc0], r4                    
    stxdw [r10-0xc8], r3                    
    stxdw [r10-0xd0], r2                    
    mov64 r3, r10                                   r3 = r10
    add64 r3, -248                                  r3 += -248   ///  r3 = r3.wrapping_add(-248 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -208                                  r2 += -208   ///  r2 = r2.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r3                                    r1 = r3
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_8192                      
    mov64 r1, r6                                    r1 = r6
    ldxw r8, [r10-0xf4]                     
    ldxw r6, [r10-0xf8]                     
    ja lbb_5266                                     if true { pc += 309 }
lbb_4957:
    ldxb r9, [r7+0x5]                       
    jne r9, 117, lbb_5266                           if r9 != (117 as i32 as i64 as u64) { pc += 307 }
    ldxb r8, [r7+0x6]                       
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 143, lbb_5266                           if r9 != (143 as i32 as i64 as u64) { pc += 303 }
    ldxb r8, [r7+0x7]                       
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 222, lbb_5266                           if r9 != (222 as i32 as i64 as u64) { pc += 299 }
    ldxb r8, [r7+0x8]                       
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 24, lbb_5266                            if r9 != (24 as i32 as i64 as u64) { pc += 295 }
    ldxb r8, [r7+0x9]                       
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 66, lbb_5266                            if r9 != (66 as i32 as i64 as u64) { pc += 291 }
    ldxb r8, [r7+0xa]                       
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 93, lbb_5266                            if r9 != (93 as i32 as i64 as u64) { pc += 287 }
    ldxb r8, [r7+0xb]                       
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 188, lbb_5266                           if r9 != (188 as i32 as i64 as u64) { pc += 283 }
    ldxb r8, [r7+0xc]                       
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 228, lbb_5266                           if r9 != (228 as i32 as i64 as u64) { pc += 279 }
    ldxb r8, [r7+0xd]                       
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 108, lbb_5266                           if r9 != (108 as i32 as i64 as u64) { pc += 275 }
    ldxb r8, [r7+0xe]                       
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 205, lbb_5266                           if r9 != (205 as i32 as i64 as u64) { pc += 271 }
    ldxb r8, [r7+0xf]                       
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 218, lbb_5266                           if r9 != (218 as i32 as i64 as u64) { pc += 267 }
    ldxb r8, [r7+0x10]                      
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 182, lbb_5266                           if r9 != (182 as i32 as i64 as u64) { pc += 263 }
    ldxb r8, [r7+0x11]                      
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 26, lbb_5266                            if r9 != (26 as i32 as i64 as u64) { pc += 259 }
    ldxb r8, [r7+0x12]                      
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 252, lbb_5266                           if r9 != (252 as i32 as i64 as u64) { pc += 255 }
    ldxb r8, [r7+0x13]                      
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 77, lbb_5266                            if r9 != (77 as i32 as i64 as u64) { pc += 251 }
    ldxb r8, [r7+0x14]                      
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 131, lbb_5266                           if r9 != (131 as i32 as i64 as u64) { pc += 247 }
    ldxb r8, [r7+0x15]                      
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 185, lbb_5266                           if r9 != (185 as i32 as i64 as u64) { pc += 243 }
    ldxb r8, [r7+0x16]                      
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 13, lbb_5266                            if r9 != (13 as i32 as i64 as u64) { pc += 239 }
    ldxb r8, [r7+0x17]                      
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 39, lbb_5266                            if r9 != (39 as i32 as i64 as u64) { pc += 235 }
    ldxb r8, [r7+0x18]                      
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 254, lbb_5266                           if r9 != (254 as i32 as i64 as u64) { pc += 231 }
    ldxb r8, [r7+0x19]                      
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 189, lbb_5266                           if r9 != (189 as i32 as i64 as u64) { pc += 227 }
    ldxb r8, [r7+0x1a]                      
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 249, lbb_5266                           if r9 != (249 as i32 as i64 as u64) { pc += 223 }
    ldxb r8, [r7+0x1b]                      
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 40, lbb_5266                            if r9 != (40 as i32 as i64 as u64) { pc += 219 }
    ldxb r8, [r7+0x1c]                      
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 216, lbb_5266                           if r9 != (216 as i32 as i64 as u64) { pc += 215 }
    ldxb r8, [r7+0x1d]                      
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 161, lbb_5266                           if r9 != (161 as i32 as i64 as u64) { pc += 211 }
    ldxb r8, [r7+0x1e]                      
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r9, 139, lbb_5266                           if r9 != (139 as i32 as i64 as u64) { pc += 207 }
    ldxb r7, [r7+0x1f]                      
    jeq r7, 252, lbb_5062                           if r7 == (252 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5266                                     if true { pc += 204 }
lbb_5062:
    stxdw [r10-0x100], r1                   
    ldxdw r1, [r5-0xfe8]                    
    ldxdw r8, [r5-0x1000]                   
    stxdw [r10-0x108], r2                   
    ldxdw r2, [r2+0x0]                      
    ldxdw r5, [r2+0x8]                      
    ldxdw r7, [r2+0x10]                     
    ldxdw r9, [r2+0x18]                     
    ldxdw r2, [r2+0x20]                     
    stxdw [r10-0x68], r2                    
    stxdw [r10-0x70], r9                    
    stxdw [r10-0x78], r7                    
    stxdw [r10-0x80], r5                    
    stxdw [r10-0x110], r8                   
    ldxdw r2, [r8+0x0]                      
    ldxdw r5, [r2+0x8]                      
    ldxdw r7, [r2+0x10]                     
    ldxdw r9, [r2+0x18]                     
    ldxdw r2, [r2+0x20]                     
    stxdw [r10-0x48], r2                    
    stxdw [r10-0x50], r9                    
    stxdw [r10-0x58], r7                    
    stxdw [r10-0x60], r5                    
    mov64 r7, r3                                    r7 = r3
    ldxdw r2, [r3+0x0]                      
    ldxdw r3, [r2+0x8]                      
    ldxdw r5, [r2+0x10]                     
    ldxdw r9, [r2+0x18]                     
    ldxdw r2, [r2+0x20]                     
    stxdw [r10-0x28], r2                    
    stxdw [r10-0x30], r9                    
    stxdw [r10-0x38], r5                    
    stxdw [r10-0x40], r3                    
    mov64 r9, r4                                    r9 = r4
    ldxdw r2, [r4+0x0]                      
    ldxdw r3, [r2+0x8]                      
    ldxdw r4, [r2+0x10]                     
    ldxdw r5, [r2+0x18]                     
    ldxdw r2, [r2+0x20]                     
    stxdw [r10-0x8], r2                     
    stxdw [r10-0x10], r5                    
    stxdw [r10-0x18], r4                    
    stxdw [r10-0x20], r3                    
    stxdw [r10-0xfe0], r0                   
    stxdw [r10-0xfd8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    stdw [r10-0xfe8], 0                     
    stdw [r10-0xff0], 8                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -128                                  r3 += -128   ///  r3 = r3.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -96                                   r4 += -96   ///  r4 = r4.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    lddw r2, 0x100033488 --> b"\x06\xdd\xf6\xe1\xeeu\x8f\xde\x18B]\xbc\xe4l\xcd\xda\xb6\x1a\xfcM\x83\xb9…        r2 load str located at 4295177352
    call function_7280                      
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    ldxdw r2, [r10-0xd0]                    
    jeq r2, r1, lbb_5264                            if r2 == r1 { pc += 135 }
    ldxdw r1, [r10-0xa0]                    
    stxdw [r10-0xf0], r1                    
    ldxdw r1, [r10-0x98]                    
    stxdw [r10-0xe8], r1                    
    ldxdw r1, [r10-0x90]                    
    stxdw [r10-0xe0], r1                    
    ldxdw r1, [r10-0x88]                    
    stxdw [r10-0xd8], r1                    
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r6, [r10-0xa8]                    
    ldxdw r3, [r10-0xb0]                    
    stxdw [r10-0x118], r3                   
    ldxdw r3, [r10-0xc0]                    
    ldxdw r4, [r10-0x108]                   
    mov64 r5, r7                                    r5 = r7
    jeq r3, 0, lbb_5234                             if r3 == (0 as i32 as i64 as u64) { pc += 88 }
    mov64 r2, r3                                    r2 = r3
    mul64 r2, 34                                    r2 *= 34   ///  r2 = r2.wrapping_mul(34 as u64)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    lddw r4, 0x3c3c3c3c3c3c3c3                      r4 load str located at 271275648142787523
    jgt r3, r4, lbb_5269                            if r3 > r4 { pc += 117 }
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r8, 0x300008000                            r8 load str located at 12884934656
    jeq r1, 0, lbb_5159                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, r1                                    r8 = r1
lbb_5159:
    mov64 r7, r8                                    r7 = r8
    sub64 r7, r2                                    r7 -= r2   ///  r7 = r7.wrapping_sub(r2)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jgt r7, r8, lbb_5166                            if r7 > r8 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_5166:
    jne r0, 0, lbb_5168                             if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r7                                    r4 = r7
lbb_5168:
    lddw r0, 0x300000008                            r0 load str located at 12884901896
    jlt r4, r0, lbb_5269                            if r4 < r0 { pc += 98 }
    ldxdw r1, [r10-0xc8]                    
    lddw r0, 0x300000000                            r0 load str located at 12884901888
    stxdw [r0+0x0], r4                      
    add64 r1, 33                                    r1 += 33   ///  r1 = r1.wrapping_add(33 as i32 as i64 as u64)
    mov64 r7, r4                                    r7 = r4
lbb_5177:
    ldxb r0, [r1-0x1]                       
    ldxb r8, [r1+0x0]                       
    stxb [r7+0x0], r8                       
    ldxdw r8, [r1-0x21]                     
    stxdw [r7+0x1], r8                      
    ldxdw r8, [r1-0x19]                     
    stxdw [r7+0x9], r8                      
    ldxdw r8, [r1-0x11]                     
    stxdw [r7+0x11], r8                     
    ldxdw r8, [r1-0x9]                      
    stxdw [r7+0x19], r8                     
    stxb [r7+0x21], r0                      
    add64 r1, 34                                    r1 += 34   ///  r1 = r1.wrapping_add(34 as i32 as i64 as u64)
    add64 r7, 34                                    r7 += 34   ///  r7 = r7.wrapping_add(34 as i32 as i64 as u64)
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    jeq r3, 0, lbb_5194                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5177                                     if true { pc += -17 }
lbb_5194:
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r7, 0x300008000                            r7 load str located at 12884934656
    jeq r1, 0, lbb_5201                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, r1                                    r7 = r1
lbb_5201:
    div64 r2, 34                                    r2 /= 34   ///  r2 = r2 / (34 as u64)
    mov64 r3, r2                                    r3 = r2
    lsh64 r3, 4                                     r3 <<= 4   ///  r3 = r3.wrapping_shl(4)
    mov64 r8, r7                                    r8 = r7
    sub64 r8, r3                                    r8 -= r3   ///  r8 = r8.wrapping_sub(r3)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jgt r8, r7, lbb_5210                            if r8 > r7 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_5210:
    jne r0, 0, lbb_5212                             if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r8                                    r1 = r8
lbb_5212:
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    lddw r0, 0x300000008                            r0 load str located at 12884901896
    jlt r1, r0, lbb_5270                            if r1 < r0 { pc += 54 }
    lddw r3, 0x300000000                            r3 load str located at 12884901888
    stxdw [r3+0x0], r1                      
    mov64 r3, r1                                    r3 = r1
    add64 r3, 9                                     r3 += 9   ///  r3 = r3.wrapping_add(9 as i32 as i64 as u64)
    mov64 r7, r2                                    r7 = r2
lbb_5222:
    ldxb r0, [r4+0x0]                       
    ldxb r8, [r4+0x21]                      
    stxb [r3+0x0], r8                       
    stxb [r3-0x1], r0                       
    mov64 r0, r4                                    r0 = r4
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r3-0x9], r0                      
    add64 r4, 34                                    r4 += 34   ///  r4 = r4.wrapping_add(34 as i32 as i64 as u64)
    add64 r3, 16                                    r3 += 16   ///  r3 = r3.wrapping_add(16 as i32 as i64 as u64)
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    jeq r7, 0, lbb_5234                             if r7 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5222                                     if true { pc += -12 }
lbb_5234:
    ldxdw r3, [r10-0xd8]                    
    stxdw [r10-0x28], r3                    
    ldxdw r3, [r10-0xe0]                    
    stxdw [r10-0x30], r3                    
    ldxdw r3, [r10-0xe8]                    
    stxdw [r10-0x38], r3                    
    ldxdw r3, [r10-0xf0]                    
    stxdw [r10-0x40], r3                    
    stxdw [r10-0xb0], r2                    
    stxdw [r10-0xb8], r1                    
    stxdw [r10-0xc0], r6                    
    ldxdw r1, [r10-0x118]                   
    stxdw [r10-0xc8], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    stxdw [r10-0xd0], r1                    
    stxdw [r10-0x8], r9                     
    stxdw [r10-0x10], r5                    
    ldxdw r1, [r10-0x110]                   
    stxdw [r10-0x18], r1                    
    ldxdw r1, [r10-0x108]                   
    stxdw [r10-0x20], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -32                                   r2 += -32   ///  r2 = r2.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_663                       
    mov64 r6, r0                                    r6 = r0
lbb_5264:
    mov64 r8, 30                                    r8 = 30 as i32 as i64 as u64
    ldxdw r1, [r10-0x100]                   
lbb_5266:
    stxw [r1+0x4], r8                       
    stxw [r1+0x0], r6                       
    exit                                    
lbb_5269:
    call function_20091                     
lbb_5270:
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, r3                                    r2 = r3
    call function_20091                     

function_5273:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r9, [r5-0xff8]                    
    ldxb r0, [r9+0x0]                       
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r0, 6, lbb_5676                             if r0 != (6 as i32 as i64 as u64) { pc += 398 }
    ldxb r0, [r9+0x1]                       
    jne r0, 221, lbb_5676                           if r0 != (221 as i32 as i64 as u64) { pc += 396 }
    ldxb r0, [r9+0x2]                       
    jne r0, 246, lbb_5676                           if r0 != (246 as i32 as i64 as u64) { pc += 394 }
    ldxb r0, [r9+0x3]                       
    jne r0, 225, lbb_5676                           if r0 != (225 as i32 as i64 as u64) { pc += 392 }
    ldxdw r0, [r5-0xfd8]                    
    stxdw [r10-0x100], r0                   
    ldxdw r0, [r5-0xfe0]                    
    stxdw [r10-0x108], r0                   
    ldxdw r0, [r5-0xff0]                    
    ldxb r7, [r9+0x4]                       
    jeq r7, 238, lbb_5364                           if r7 == (238 as i32 as i64 as u64) { pc += 73 }
    jne r7, 215, lbb_5676                           if r7 != (215 as i32 as i64 as u64) { pc += 384 }
    ldxb r5, [r9+0x5]                       
    jne r5, 101, lbb_5676                           if r5 != (101 as i32 as i64 as u64) { pc += 382 }
    ldxb r5, [r9+0x6]                       
    jne r5, 161, lbb_5676                           if r5 != (161 as i32 as i64 as u64) { pc += 380 }
    ldxb r5, [r9+0x7]                       
    jne r5, 147, lbb_5676                           if r5 != (147 as i32 as i64 as u64) { pc += 378 }
    ldxb r5, [r9+0x8]                       
    jne r5, 217, lbb_5676                           if r5 != (217 as i32 as i64 as u64) { pc += 376 }
    ldxb r5, [r9+0x9]                       
    jne r5, 203, lbb_5676                           if r5 != (203 as i32 as i64 as u64) { pc += 374 }
    ldxb r5, [r9+0xa]                       
    jne r5, 225, lbb_5676                           if r5 != (225 as i32 as i64 as u64) { pc += 372 }
    ldxb r5, [r9+0xb]                       
    jne r5, 70, lbb_5676                            if r5 != (70 as i32 as i64 as u64) { pc += 370 }
    ldxb r5, [r9+0xc]                       
    jne r5, 206, lbb_5676                           if r5 != (206 as i32 as i64 as u64) { pc += 368 }
    ldxb r5, [r9+0xd]                       
    jne r5, 235, lbb_5676                           if r5 != (235 as i32 as i64 as u64) { pc += 366 }
    ldxb r5, [r9+0xe]                       
    jne r5, 121, lbb_5676                           if r5 != (121 as i32 as i64 as u64) { pc += 364 }
    ldxb r5, [r9+0xf]                       
    jne r5, 172, lbb_5676                           if r5 != (172 as i32 as i64 as u64) { pc += 362 }
    ldxb r5, [r9+0x10]                      
    jne r5, 28, lbb_5676                            if r5 != (28 as i32 as i64 as u64) { pc += 360 }
    ldxb r5, [r9+0x11]                      
    jne r5, 180, lbb_5676                           if r5 != (180 as i32 as i64 as u64) { pc += 358 }
    ldxb r5, [r9+0x12]                      
    jne r5, 133, lbb_5676                           if r5 != (133 as i32 as i64 as u64) { pc += 356 }
    ldxb r5, [r9+0x13]                      
    jne r5, 237, lbb_5676                           if r5 != (237 as i32 as i64 as u64) { pc += 354 }
    ldxb r5, [r9+0x14]                      
    jne r5, 95, lbb_5676                            if r5 != (95 as i32 as i64 as u64) { pc += 352 }
    ldxb r5, [r9+0x15]                      
    jne r5, 91, lbb_5676                            if r5 != (91 as i32 as i64 as u64) { pc += 350 }
    ldxb r5, [r9+0x16]                      
    jne r5, 55, lbb_5676                            if r5 != (55 as i32 as i64 as u64) { pc += 348 }
    ldxb r5, [r9+0x17]                      
    jne r5, 145, lbb_5676                           if r5 != (145 as i32 as i64 as u64) { pc += 346 }
    ldxb r5, [r9+0x18]                      
    jne r5, 58, lbb_5676                            if r5 != (58 as i32 as i64 as u64) { pc += 344 }
    ldxb r5, [r9+0x19]                      
    jne r5, 140, lbb_5676                           if r5 != (140 as i32 as i64 as u64) { pc += 342 }
    ldxb r5, [r9+0x1a]                      
    jne r5, 245, lbb_5676                           if r5 != (245 as i32 as i64 as u64) { pc += 340 }
    ldxb r5, [r9+0x1b]                      
    jne r5, 133, lbb_5676                           if r5 != (133 as i32 as i64 as u64) { pc += 338 }
    ldxb r5, [r9+0x1c]                      
    jne r5, 126, lbb_5676                           if r5 != (126 as i32 as i64 as u64) { pc += 336 }
    ldxb r5, [r9+0x1d]                      
    jne r5, 255, lbb_5676                           if r5 != (255 as i32 as i64 as u64) { pc += 334 }
    ldxb r5, [r9+0x1e]                      
    jne r5, 0, lbb_5676                             if r5 != (0 as i32 as i64 as u64) { pc += 332 }
    ldxb r5, [r9+0x1f]                      
    jeq r5, 169, lbb_5347                           if r5 == (169 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5676                                     if true { pc += 329 }
lbb_5347:
    stxdw [r10-0xb8], r0                    
    stxdw [r10-0xc0], r4                    
    stxdw [r10-0xc8], r3                    
    stxdw [r10-0xd0], r2                    
    mov64 r3, r10                                   r3 = r10
    add64 r3, -248                                  r3 += -248   ///  r3 = r3.wrapping_add(-248 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -208                                  r2 += -208   ///  r2 = r2.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r3                                    r1 = r3
    ldxdw r3, [r10-0x108]                   
    ldxdw r4, [r10-0x100]                   
    call function_8192                      
    mov64 r1, r6                                    r1 = r6
    ldxw r6, [r10-0xf4]                     
    ldxw r8, [r10-0xf8]                     
    ja lbb_5676                                     if true { pc += 312 }
lbb_5364:
    ldxb r7, [r9+0x5]                       
    jne r7, 117, lbb_5676                           if r7 != (117 as i32 as i64 as u64) { pc += 310 }
    ldxb r6, [r9+0x6]                       
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 143, lbb_5676                           if r7 != (143 as i32 as i64 as u64) { pc += 306 }
    ldxb r6, [r9+0x7]                       
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 222, lbb_5676                           if r7 != (222 as i32 as i64 as u64) { pc += 302 }
    ldxb r6, [r9+0x8]                       
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 24, lbb_5676                            if r7 != (24 as i32 as i64 as u64) { pc += 298 }
    ldxb r6, [r9+0x9]                       
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 66, lbb_5676                            if r7 != (66 as i32 as i64 as u64) { pc += 294 }
    ldxb r6, [r9+0xa]                       
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 93, lbb_5676                            if r7 != (93 as i32 as i64 as u64) { pc += 290 }
    ldxb r6, [r9+0xb]                       
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 188, lbb_5676                           if r7 != (188 as i32 as i64 as u64) { pc += 286 }
    ldxb r6, [r9+0xc]                       
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 228, lbb_5676                           if r7 != (228 as i32 as i64 as u64) { pc += 282 }
    ldxb r6, [r9+0xd]                       
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 108, lbb_5676                           if r7 != (108 as i32 as i64 as u64) { pc += 278 }
    ldxb r6, [r9+0xe]                       
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 205, lbb_5676                           if r7 != (205 as i32 as i64 as u64) { pc += 274 }
    ldxb r6, [r9+0xf]                       
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 218, lbb_5676                           if r7 != (218 as i32 as i64 as u64) { pc += 270 }
    ldxb r6, [r9+0x10]                      
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 182, lbb_5676                           if r7 != (182 as i32 as i64 as u64) { pc += 266 }
    ldxb r6, [r9+0x11]                      
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 26, lbb_5676                            if r7 != (26 as i32 as i64 as u64) { pc += 262 }
    ldxb r6, [r9+0x12]                      
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 252, lbb_5676                           if r7 != (252 as i32 as i64 as u64) { pc += 258 }
    ldxb r6, [r9+0x13]                      
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 77, lbb_5676                            if r7 != (77 as i32 as i64 as u64) { pc += 254 }
    ldxb r6, [r9+0x14]                      
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 131, lbb_5676                           if r7 != (131 as i32 as i64 as u64) { pc += 250 }
    ldxb r6, [r9+0x15]                      
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 185, lbb_5676                           if r7 != (185 as i32 as i64 as u64) { pc += 246 }
    ldxb r6, [r9+0x16]                      
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 13, lbb_5676                            if r7 != (13 as i32 as i64 as u64) { pc += 242 }
    ldxb r6, [r9+0x17]                      
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 39, lbb_5676                            if r7 != (39 as i32 as i64 as u64) { pc += 238 }
    ldxb r6, [r9+0x18]                      
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 254, lbb_5676                           if r7 != (254 as i32 as i64 as u64) { pc += 234 }
    ldxb r6, [r9+0x19]                      
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 189, lbb_5676                           if r7 != (189 as i32 as i64 as u64) { pc += 230 }
    ldxb r6, [r9+0x1a]                      
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 249, lbb_5676                           if r7 != (249 as i32 as i64 as u64) { pc += 226 }
    ldxb r6, [r9+0x1b]                      
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 40, lbb_5676                            if r7 != (40 as i32 as i64 as u64) { pc += 222 }
    ldxb r6, [r9+0x1c]                      
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 216, lbb_5676                           if r7 != (216 as i32 as i64 as u64) { pc += 218 }
    ldxb r6, [r9+0x1d]                      
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 161, lbb_5676                           if r7 != (161 as i32 as i64 as u64) { pc += 214 }
    ldxb r6, [r9+0x1e]                      
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 139, lbb_5676                           if r7 != (139 as i32 as i64 as u64) { pc += 210 }
    ldxb r6, [r9+0x1f]                      
    mov64 r7, r6                                    r7 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jeq r7, 252, lbb_5471                           if r7 == (252 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5676                                     if true { pc += 205 }
lbb_5471:
    stxdw [r10-0x110], r1                   
    ldxdw r1, [r5-0xfe8]                    
    ldxdw r9, [r5-0x1000]                   
    stxdw [r10-0x118], r2                   
    ldxdw r2, [r2+0x0]                      
    ldxdw r5, [r2+0x8]                      
    ldxdw r6, [r2+0x10]                     
    ldxdw r7, [r2+0x18]                     
    ldxdw r2, [r2+0x20]                     
    stxdw [r10-0x68], r2                    
    stxdw [r10-0x70], r7                    
    stxdw [r10-0x78], r6                    
    stxdw [r10-0x80], r5                    
    stxdw [r10-0x120], r9                   
    ldxdw r2, [r9+0x0]                      
    ldxdw r5, [r2+0x8]                      
    ldxdw r6, [r2+0x10]                     
    ldxdw r7, [r2+0x18]                     
    ldxdw r2, [r2+0x20]                     
    stxdw [r10-0x48], r2                    
    stxdw [r10-0x50], r7                    
    stxdw [r10-0x58], r6                    
    stxdw [r10-0x60], r5                    
    mov64 r6, r3                                    r6 = r3
    ldxdw r2, [r3+0x0]                      
    ldxdw r3, [r2+0x8]                      
    ldxdw r5, [r2+0x10]                     
    ldxdw r7, [r2+0x18]                     
    ldxdw r2, [r2+0x20]                     
    stxdw [r10-0x28], r2                    
    stxdw [r10-0x30], r7                    
    stxdw [r10-0x38], r5                    
    stxdw [r10-0x40], r3                    
    mov64 r7, r4                                    r7 = r4
    ldxdw r2, [r4+0x0]                      
    ldxdw r3, [r2+0x8]                      
    ldxdw r4, [r2+0x10]                     
    ldxdw r5, [r2+0x18]                     
    ldxdw r2, [r2+0x20]                     
    stxdw [r10-0x8], r2                     
    stxdw [r10-0x10], r5                    
    stxdw [r10-0x18], r4                    
    stxdw [r10-0x20], r3                    
    stxdw [r10-0xfe0], r0                   
    stxdw [r10-0xfd8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    stdw [r10-0xfe8], 0                     
    stdw [r10-0xff0], 8                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -128                                  r3 += -128   ///  r3 = r3.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -96                                   r4 += -96   ///  r4 = r4.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    lddw r2, 0x100033488 --> b"\x06\xdd\xf6\xe1\xeeu\x8f\xde\x18B]\xbc\xe4l\xcd\xda\xb6\x1a\xfcM\x83\xb9…        r2 load str located at 4295177352
    call function_7280                      
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    ldxdw r2, [r10-0xd0]                    
    jeq r2, r1, lbb_5674                            if r2 == r1 { pc += 136 }
    ldxdw r1, [r10-0xa0]                    
    stxdw [r10-0xf0], r1                    
    ldxdw r1, [r10-0x98]                    
    stxdw [r10-0xe8], r1                    
    ldxdw r1, [r10-0x90]                    
    stxdw [r10-0xe0], r1                    
    ldxdw r1, [r10-0x88]                    
    stxdw [r10-0xd8], r1                    
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r8, [r10-0xa8]                    
    ldxdw r3, [r10-0xb0]                    
    stxdw [r10-0x128], r3                   
    ldxdw r3, [r10-0xc0]                    
    ldxdw r4, [r10-0x118]                   
    mov64 r5, r6                                    r5 = r6
    mov64 r4, r7                                    r4 = r7
    jeq r3, 0, lbb_5644                             if r3 == (0 as i32 as i64 as u64) { pc += 88 }
    mov64 r2, r3                                    r2 = r3
    mul64 r2, 34                                    r2 *= 34   ///  r2 = r2.wrapping_mul(34 as u64)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    lddw r0, 0x3c3c3c3c3c3c3c3                      r0 load str located at 271275648142787523
    jgt r3, r0, lbb_5679                            if r3 > r0 { pc += 117 }
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r7, 0x300008000                            r7 load str located at 12884934656
    jeq r1, 0, lbb_5569                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, r1                                    r7 = r1
lbb_5569:
    mov64 r6, r7                                    r6 = r7
    sub64 r6, r2                                    r6 -= r2   ///  r6 = r6.wrapping_sub(r2)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jgt r6, r7, lbb_5576                            if r6 > r7 { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_5576:
    jne r9, 0, lbb_5578                             if r9 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, r6                                    r0 = r6
lbb_5578:
    lddw r6, 0x300000008                            r6 load str located at 12884901896
    jlt r0, r6, lbb_5679                            if r0 < r6 { pc += 98 }
    ldxdw r1, [r10-0xc8]                    
    lddw r6, 0x300000000                            r6 load str located at 12884901888
    stxdw [r6+0x0], r0                      
    add64 r1, 33                                    r1 += 33   ///  r1 = r1.wrapping_add(33 as i32 as i64 as u64)
    mov64 r6, r0                                    r6 = r0
lbb_5587:
    ldxb r7, [r1-0x1]                       
    ldxb r9, [r1+0x0]                       
    stxb [r6+0x0], r9                       
    ldxdw r9, [r1-0x21]                     
    stxdw [r6+0x1], r9                      
    ldxdw r9, [r1-0x19]                     
    stxdw [r6+0x9], r9                      
    ldxdw r9, [r1-0x11]                     
    stxdw [r6+0x11], r9                     
    ldxdw r9, [r1-0x9]                      
    stxdw [r6+0x19], r9                     
    stxb [r6+0x21], r7                      
    add64 r1, 34                                    r1 += 34   ///  r1 = r1.wrapping_add(34 as i32 as i64 as u64)
    add64 r6, 34                                    r6 += 34   ///  r6 = r6.wrapping_add(34 as i32 as i64 as u64)
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    jeq r3, 0, lbb_5604                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5587                                     if true { pc += -17 }
lbb_5604:
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r6, 0x300008000                            r6 load str located at 12884934656
    jeq r1, 0, lbb_5611                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r1                                    r6 = r1
lbb_5611:
    div64 r2, 34                                    r2 /= 34   ///  r2 = r2 / (34 as u64)
    mov64 r3, r2                                    r3 = r2
    lsh64 r3, 4                                     r3 <<= 4   ///  r3 = r3.wrapping_shl(4)
    mov64 r7, r6                                    r7 = r6
    sub64 r7, r3                                    r7 -= r3   ///  r7 = r7.wrapping_sub(r3)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jgt r7, r6, lbb_5620                            if r7 > r6 { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_5620:
    jne r9, 0, lbb_5622                             if r9 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r7                                    r1 = r7
lbb_5622:
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    lddw r6, 0x300000008                            r6 load str located at 12884901896
    jlt r1, r6, lbb_5680                            if r1 < r6 { pc += 54 }
    lddw r3, 0x300000000                            r3 load str located at 12884901888
    stxdw [r3+0x0], r1                      
    mov64 r3, r1                                    r3 = r1
    add64 r3, 9                                     r3 += 9   ///  r3 = r3.wrapping_add(9 as i32 as i64 as u64)
    mov64 r6, r2                                    r6 = r2
lbb_5632:
    ldxb r7, [r0+0x0]                       
    ldxb r9, [r0+0x21]                      
    stxb [r3+0x0], r9                       
    stxb [r3-0x1], r7                       
    mov64 r7, r0                                    r7 = r0
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r3-0x9], r7                      
    add64 r0, 34                                    r0 += 34   ///  r0 = r0.wrapping_add(34 as i32 as i64 as u64)
    add64 r3, 16                                    r3 += 16   ///  r3 = r3.wrapping_add(16 as i32 as i64 as u64)
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    jeq r6, 0, lbb_5644                             if r6 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5632                                     if true { pc += -12 }
lbb_5644:
    ldxdw r3, [r10-0xd8]                    
    stxdw [r10-0x28], r3                    
    ldxdw r3, [r10-0xe0]                    
    stxdw [r10-0x30], r3                    
    ldxdw r3, [r10-0xe8]                    
    stxdw [r10-0x38], r3                    
    ldxdw r3, [r10-0xf0]                    
    stxdw [r10-0x40], r3                    
    stxdw [r10-0xb0], r2                    
    stxdw [r10-0xb8], r1                    
    stxdw [r10-0xc0], r8                    
    ldxdw r1, [r10-0x128]                   
    stxdw [r10-0xc8], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    stxdw [r10-0xd0], r1                    
    stxdw [r10-0x8], r4                     
    stxdw [r10-0x10], r5                    
    ldxdw r1, [r10-0x120]                   
    stxdw [r10-0x18], r1                    
    ldxdw r1, [r10-0x118]                   
    stxdw [r10-0x20], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -32                                   r2 += -32   ///  r2 = r2.wrapping_add(-32 as i32 as i64 as u64)
    ldxdw r3, [r10-0x108]                   
    ldxdw r4, [r10-0x100]                   
    call function_663                       
    mov64 r8, r0                                    r8 = r0
lbb_5674:
    mov64 r6, 30                                    r6 = 30 as i32 as i64 as u64
    ldxdw r1, [r10-0x110]                   
lbb_5676:
    stxw [r1+0x4], r6                       
    stxw [r1+0x0], r8                       
    exit                                    
lbb_5679:
    call function_20091                     
lbb_5680:
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, r3                                    r2 = r3
    call function_20091                     

function_5683:
    ldxdw r7, [r1+0x0]                      
    mov64 r6, r7                                    r6 = r7
    add64 r6, 40                                    r6 += 40   ///  r6 = r6.wrapping_add(40 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    lddw r2, 0x1000333c8 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295177160
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_5709                             if r0 == (0 as i32 as i64 as u64) { pc += 15 }
    mov64 r1, r6                                    r1 = r6
    lddw r2, 0x100033488 --> b"\x06\xdd\xf6\xe1\xeeu\x8f\xde\x18B]\xbc\xe4l\xcd\xda\xb6\x1a\xfcM\x83\xb9…        r2 load str located at 4295177352
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, r0                                    r1 = r0
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r0, 35                                    r0 = 35 as i32 as i64 as u64
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jeq r1, 0, lbb_5723                             if r1 == (0 as i32 as i64 as u64) { pc += 18 }
lbb_5705:
    and64 r0, 255                                   r0 &= 255   ///  r0 = r0.and(255)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    exit                                    
lbb_5709:
    mov64 r2, 3                                     r2 = 3 as i32 as i64 as u64
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r1, [r7+0x50]                     
    jne r1, 82, lbb_5705                            if r1 != (82 as i32 as i64 as u64) { pc += -8 }
    mov64 r2, 11                                    r2 = 11 as i32 as i64 as u64
    ldxb r1, [r7+0x0]                       
    mov64 r3, r1                                    r3 = r1
    and64 r3, 8                                     r3 &= 8   ///  r3 = r3.and(8)
    jne r3, 0, lbb_5705                             if r3 != (0 as i32 as i64 as u64) { pc += -13 }
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jeq r1, 7, lbb_5705                             if r1 == (7 as i32 as i64 as u64) { pc += -15 }
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    ldxb r0, [r7+0x84]                      
    ja lbb_5705                                     if true { pc += -18 }
lbb_5723:
    mov64 r2, 11                                    r2 = 11 as i32 as i64 as u64
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxb r1, [r7+0x0]                       
    mov64 r3, r1                                    r3 = r1
    and64 r3, 8                                     r3 &= 8   ///  r3 = r3.and(8)
    jne r3, 0, lbb_5705                             if r3 != (0 as i32 as i64 as u64) { pc += -24 }
    mov64 r3, r1                                    r3 = r1
    and64 r3, 7                                     r3 &= 7   ///  r3 = r3.and(7)
    jeq r3, 7, lbb_5705                             if r3 == (7 as i32 as i64 as u64) { pc += -27 }
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxb [r7+0x0], r1                       
    ldxdw r3, [r7+0x50]                     
    mov64 r2, r7                                    r2 = r7
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    call function_7510                      
    ldxw r1, [r10-0x58]                     
    jeq r1, 2, lbb_5748                             if r1 == (2 as i32 as i64 as u64) { pc += 6 }
    ldxb r1, [r7+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r0, [r10-0x28]                     
    stxb [r7+0x0], r1                       
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    ja lbb_5705                                     if true { pc += -43 }
lbb_5748:
    ldxb r1, [r7+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxb [r7+0x0], r1                       
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r0, 31                                    r0 = 31 as i32 as i64 as u64
    ja lbb_5705                                     if true { pc += -49 }

function_5754:
    mov64 r7, r5                                    r7 = r5
    stxdw [r10-0x1a0], r4                   
    mov64 r8, r3                                    r8 = r3
    mov64 r6, r2                                    r6 = r2
    stxdw [r10-0x190], r1                   
    ldxdw r1, [r7-0xff0]                    
    ldxdw r9, [r1+0x0]                      
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    lddw r2, 0x1000333c8 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295177160
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    ldxdw r1, [r7-0xfe8]                    
    stxdw [r10-0x198], r1                   
    ldxdw r1, [r7-0xff8]                    
    stxdw [r10-0x1b0], r1                   
    ldxdw r1, [r7-0x1000]                   
    stxdw [r10-0x1a8], r1                   
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_5786                             if r0 == (0 as i32 as i64 as u64) { pc += 10 }
    mov64 r1, r9                                    r1 = r9
    lddw r2, 0x100033488 --> b"\x06\xdd\xf6\xe1\xeeu\x8f\xde\x18B]\xbc\xe4l\xcd\xda\xb6\x1a\xfcM\x83\xb9…        r2 load str located at 4295177352
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jne r0, 0, lbb_5830                             if r0 != (0 as i32 as i64 as u64) { pc += 44 }
lbb_5786:
    ldxdw r6, [r6+0x0]                      
    mov64 r1, r6                                    r1 = r6
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r1, 35                                    r1 = 35 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_5830                             if r0 != (0 as i32 as i64 as u64) { pc += 33 }
    ldxdw r8, [r8+0x0]                      
    mov64 r7, r8                                    r7 = r8
    add64 r7, 40                                    r7 += 40   ///  r7 = r7.wrapping_add(40 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_5812                             if r0 == (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 7                                     r1 = 7 as i32 as i64 as u64
    ldxdw r2, [r10-0x198]                   
    jne r2, 0, lbb_5829                             if r2 != (0 as i32 as i64 as u64) { pc += 19 }
lbb_5810:
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    ja lbb_5829                                     if true { pc += 17 }
lbb_5812:
    stxdw [r10-0x1c0], r8                   
    stxdw [r10-0x1b8], r6                   
    ldxdw r1, [r10-0x1a0]                   
    ldxdw r8, [r1+0x0]                      
    mov64 r6, r8                                    r6 = r8
    add64 r6, 40                                    r6 += 40   ///  r6 = r6.wrapping_add(40 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_5834                             if r0 == (0 as i32 as i64 as u64) { pc += 9 }
    mov64 r1, 9                                     r1 = 9 as i32 as i64 as u64
    ldxdw r2, [r10-0x198]                   
    jne r2, 0, lbb_5829                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_5828:
    mov64 r1, 10                                    r1 = 10 as i32 as i64 as u64
lbb_5829:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_5830:
    ldxdw r2, [r10-0x190]                   
    stxw [r2+0x4], r1                       
    stxw [r2+0x0], r7                       
    exit                                    
lbb_5834:
    mov64 r9, r8                                    r9 = r8
    mov64 r1, r7                                    r1 = r7
    lddw r2, 0x1000333c8 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295177160
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_5899                             if r0 == (0 as i32 as i64 as u64) { pc += 56 }
    mov64 r1, r7                                    r1 = r7
    lddw r2, 0x100033488 --> b"\x06\xdd\xf6\xe1\xeeu\x8f\xde\x18B]\xbc\xe4l\xcd\xda\xb6\x1a\xfcM\x83\xb9…        r2 load str located at 4295177352
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    ldxdw r8, [r10-0x1b8]                   
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jeq r0, 0, lbb_5855                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5830                                     if true { pc += -25 }
lbb_5855:
    mov64 r4, 11                                    r4 = 11 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r7, [r10-0x1c0]                   
    ldxb r2, [r7+0x0]                       
    mov64 r3, r2                                    r3 = r2
    and64 r3, 8                                     r3 &= 8   ///  r3 = r3.and(8)
    jne r3, 0, lbb_5989                             if r3 != (0 as i32 as i64 as u64) { pc += 127 }
    mov64 r3, r2                                    r3 = r2
    and64 r3, 7                                     r3 &= 7   ///  r3 = r3.and(7)
    jeq r3, 7, lbb_5989                             if r3 == (7 as i32 as i64 as u64) { pc += 124 }
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    stxb [r7+0x0], r2                       
    ldxdw r3, [r7+0x50]                     
    mov64 r2, r7                                    r2 = r7
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    call function_7599                      
    ldxw r1, [r10-0x28]                     
    jeq r1, 2, lbb_5984                             if r1 == (2 as i32 as i64 as u64) { pc += 109 }
    ldxdw r1, [r10-0xa5]                    
    stxdw [r10-0xc8], r1                    
    ldxb r1, [r10-0xae]                     
    stxb [r10-0x146], r1                    
    ldxh r1, [r10-0xb0]                     
    stxh [r10-0x148], r1                    
    ldxdw r1, [r10-0x9d]                    
    stxdw [r10-0xc0], r1                    
    ldxdw r1, [r10-0x98]                    
    stxdw [r10-0xbb], r1                    
    ldxdw r1, [r10-0x90]                    
    stxdw [r10-0xe8], r1                    
    ldxdw r1, [r10-0x88]                    
    stxdw [r10-0xe0], r1                    
    ldxdw r1, [r10-0x80]                    
    stxdw [r10-0xd8], r1                    
    ldxdw r1, [r10-0x78]                    
    stxdw [r10-0xd0], r1                    
    ldxb r1, [r7+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    ldxw r2, [r10-0xad]                     
    ldxw r3, [r10-0xa9]                     
    stxb [r7+0x0], r1                       
    ja lbb_5937                                     if true { pc += 38 }
lbb_5899:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    ldxdw r2, [r10-0x1c0]                   
    call function_0                         
    ldxw r1, [r10-0xa4]                     
    ldxw r7, [r10-0xa8]                     
    ldxdw r5, [r10-0xb0]                    
    ldxdw r8, [r10-0x1b8]                   
    jeq r5, 0, lbb_5830                             if r5 == (0 as i32 as i64 as u64) { pc += -78 }
    ldxdw r4, [r10-0xa0]                    
    ldxb r2, [r5+0x2]                       
    stxb [r10-0x146], r2                    
    ldxh r2, [r5+0x0]                       
    stxh [r10-0x148], r2                    
    ldxw r3, [r5+0x7]                       
    ldxw r2, [r5+0x3]                       
    ldxdw r0, [r5+0x18]                     
    stxdw [r10-0xbb], r0                    
    ldxdw r0, [r5+0x13]                     
    stxdw [r10-0xc0], r0                    
    ldxdw r0, [r5+0xb]                      
    stxdw [r10-0xc8], r0                    
    ldxdw r0, [r5+0x38]                     
    stxdw [r10-0xd0], r0                    
    ldxdw r0, [r5+0x30]                     
    stxdw [r10-0xd8], r0                    
    ldxdw r0, [r5+0x28]                     
    stxdw [r10-0xe0], r0                    
    ldxdw r5, [r5+0x20]                     
    stxdw [r10-0xe8], r5                    
    and64 r4, 7                                     r4 &= 7   ///  r4 = r4.and(7)
    mov64 r5, -1                                    r5 = -1 as i32 as i64 as u64
    lsh64 r5, r4                                    r5 <<= r4   ///  r5 = r5.wrapping_shl(r4 as u32)
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    or64 r1, r7                                     r1 |= r7   ///  r1 = r1.or(r7)
    ldxb r4, [r1+0x0]                       
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    stxb [r1+0x0], r4                       
lbb_5937:
    ldxdw r1, [r10-0xbb]                    
    stxdw [r10-0x170], r1                   
    ldxdw r1, [r10-0xc0]                    
    stxdw [r10-0x175], r1                   
    ldxdw r1, [r10-0xc8]                    
    stxdw [r10-0x17d], r1                   
    ldxh r1, [r10-0x148]                    
    stxh [r10-0x188], r1                    
    ldxb r1, [r10-0x146]                    
    stxb [r10-0x186], r1                    
    ldxdw r1, [r10-0xd0]                    
    stxdw [r10-0x150], r1                   
    ldxdw r1, [r10-0xd8]                    
    stxdw [r10-0x158], r1                   
    ldxdw r1, [r10-0xe0]                    
    stxdw [r10-0x160], r1                   
    ldxdw r1, [r10-0xe8]                    
    stxdw [r10-0x168], r1                   
    stxw [r10-0x181], r3                    
    stxw [r10-0x185], r2                    
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -392                                  r1 += -392   ///  r1 = r1.wrapping_add(-392 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_5980                             if r0 != (0 as i32 as i64 as u64) { pc += 14 }
    ldxdw r1, [r10-0x1a8]                   
    ldxdw r2, [r1+0x0]                      
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -360                                  r1 += -360   ///  r1 = r1.wrapping_add(-360 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_5991                             if r0 == (0 as i32 as i64 as u64) { pc += 15 }
    mov64 r1, 7                                     r1 = 7 as i32 as i64 as u64
    ldxdw r2, [r10-0x198]                   
    jne r2, 0, lbb_5829                             if r2 != (0 as i32 as i64 as u64) { pc += -150 }
    ja lbb_5983                                     if true { pc += 3 }
lbb_5980:
    mov64 r1, 7                                     r1 = 7 as i32 as i64 as u64
    ldxdw r2, [r10-0x198]                   
    jne r2, 0, lbb_5829                             if r2 != (0 as i32 as i64 as u64) { pc += -154 }
lbb_5983:
    ja lbb_5810                                     if true { pc += -174 }
lbb_5984:
    ldxb r1, [r7+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxb [r7+0x0], r1                       
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
lbb_5989:
    mov64 r7, r4                                    r7 = r4
    ja lbb_5830                                     if true { pc += -161 }
lbb_5991:
    mov64 r1, r6                                    r1 = r6
    lddw r2, 0x1000333c8 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295177160
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_6053                             if r0 == (0 as i32 as i64 as u64) { pc += 54 }
    mov64 r1, r6                                    r1 = r6
    lddw r2, 0x100033488 --> b"\x06\xdd\xf6\xe1\xeeu\x8f\xde\x18B]\xbc\xe4l\xcd\xda\xb6\x1a\xfcM\x83\xb9…        r2 load str located at 4295177352
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_6010                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_5830                                     if true { pc += -180 }
lbb_6010:
    mov64 r7, 11                                    r7 = 11 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxb r2, [r9+0x0]                       
    mov64 r3, r2                                    r3 = r2
    and64 r3, 8                                     r3 &= 8   ///  r3 = r3.and(8)
    jne r3, 0, lbb_5830                             if r3 != (0 as i32 as i64 as u64) { pc += -186 }
    mov64 r3, r2                                    r3 = r2
    and64 r3, 7                                     r3 &= 7   ///  r3 = r3.and(7)
    jeq r3, 7, lbb_5830                             if r3 == (7 as i32 as i64 as u64) { pc += -189 }
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    stxb [r9+0x0], r2                       
    ldxdw r3, [r9+0x50]                     
    mov64 r2, r9                                    r2 = r9
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    call function_7599                      
    ldxw r1, [r10-0x28]                     
    jeq r1, 2, lbb_6134                             if r1 == (2 as i32 as i64 as u64) { pc += 105 }
    ldxdw r1, [r10-0xa5]                    
    stxdw [r10-0x100], r1                   
    ldxb r1, [r10-0xae]                     
    stxb [r10-0x122], r1                    
    ldxh r1, [r10-0xb0]                     
    stxh [r10-0x124], r1                    
    ldxdw r1, [r10-0x9d]                    
    stxdw [r10-0xf8], r1                    
    ldxdw r1, [r10-0x98]                    
    stxdw [r10-0xf3], r1                    
    ldxdw r1, [r10-0x90]                    
    stxdw [r10-0x120], r1                   
    ldxdw r1, [r10-0x88]                    
    stxdw [r10-0x118], r1                   
    ldxdw r1, [r10-0x80]                    
    stxdw [r10-0x110], r1                   
    ldxdw r1, [r10-0x78]                    
    stxdw [r10-0x108], r1                   
    ldxb r1, [r9+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    ldxw r2, [r10-0xad]                     
    ldxw r3, [r10-0xa9]                     
    stxb [r9+0x0], r1                       
    ja lbb_6090                                     if true { pc += 37 }
lbb_6053:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    call function_0                         
    ldxw r1, [r10-0xa4]                     
    ldxw r7, [r10-0xa8]                     
    ldxdw r5, [r10-0xb0]                    
    jeq r5, 0, lbb_5830                             if r5 == (0 as i32 as i64 as u64) { pc += -231 }
    ldxdw r4, [r10-0xa0]                    
    ldxb r2, [r5+0x2]                       
    stxb [r10-0x122], r2                    
    ldxh r2, [r5+0x0]                       
    stxh [r10-0x124], r2                    
    ldxw r3, [r5+0x7]                       
    ldxw r2, [r5+0x3]                       
    ldxdw r0, [r5+0x18]                     
    stxdw [r10-0xf3], r0                    
    ldxdw r0, [r5+0x13]                     
    stxdw [r10-0xf8], r0                    
    ldxdw r0, [r5+0xb]                      
    stxdw [r10-0x100], r0                   
    ldxdw r0, [r5+0x38]                     
    stxdw [r10-0x108], r0                   
    ldxdw r0, [r5+0x30]                     
    stxdw [r10-0x110], r0                   
    ldxdw r0, [r5+0x28]                     
    stxdw [r10-0x118], r0                   
    ldxdw r5, [r5+0x20]                     
    stxdw [r10-0x120], r5                   
    and64 r4, 7                                     r4 &= 7   ///  r4 = r4.and(7)
    mov64 r5, -1                                    r5 = -1 as i32 as i64 as u64
    lsh64 r5, r4                                    r5 <<= r4   ///  r5 = r5.wrapping_shl(r4 as u32)
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    or64 r1, r7                                     r1 |= r7   ///  r1 = r1.or(r7)
    ldxb r4, [r1+0x0]                       
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    stxb [r1+0x0], r4                       
lbb_6090:
    ldxdw r1, [r10-0xf3]                    
    stxdw [r10-0x12c], r1                   
    ldxdw r1, [r10-0xf8]                    
    stxdw [r10-0x131], r1                   
    ldxdw r1, [r10-0x100]                   
    stxdw [r10-0x139], r1                   
    ldxh r1, [r10-0x124]                    
    stxh [r10-0x144], r1                    
    ldxb r1, [r10-0x122]                    
    stxb [r10-0x142], r1                    
    ldxdw r1, [r10-0x108]                   
    stxdw [r10-0x98], r1                    
    ldxdw r1, [r10-0x110]                   
    stxdw [r10-0xa0], r1                    
    ldxdw r1, [r10-0x118]                   
    stxdw [r10-0xa8], r1                    
    ldxdw r1, [r10-0x120]                   
    stxdw [r10-0xb0], r1                    
    stxw [r10-0x13d], r3                    
    stxw [r10-0x141], r2                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -324                                  r1 += -324   ///  r1 = r1.wrapping_add(-324 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_6130                             if r0 != (0 as i32 as i64 as u64) { pc += 12 }
    ldxdw r1, [r10-0x1b0]                   
    ldxdw r2, [r1+0x0]                      
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_6130                             if r0 != (0 as i32 as i64 as u64) { pc += 2 }
    mov64 r7, 26                                    r7 = 26 as i32 as i64 as u64
    ja lbb_5830                                     if true { pc += -300 }
lbb_6130:
    mov64 r1, 9                                     r1 = 9 as i32 as i64 as u64
    ldxdw r2, [r10-0x198]                   
    jne r2, 0, lbb_5829                             if r2 != (0 as i32 as i64 as u64) { pc += -304 }
    ja lbb_5828                                     if true { pc += -306 }
lbb_6134:
    ldxb r1, [r9+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxb [r9+0x0], r1                       
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    ja lbb_5830                                     if true { pc += -310 }

function_6140:
    mov64 r6, r1                                    r6 = r1
    ldxdw r8, [r2+0x0]                      
    mov64 r7, r8                                    r7 = r8
    add64 r7, 40                                    r7 += 40   ///  r7 = r7.wrapping_add(40 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    lddw r2, 0x1000333c8 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295177160
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_6163                             if r0 == (0 as i32 as i64 as u64) { pc += 11 }
    mov64 r1, r7                                    r1 = r7
    lddw r2, 0x100033488 --> b"\x06\xdd\xf6\xe1\xeeu\x8f\xde\x18B]\xbc\xe4l\xcd\xda\xb6\x1a\xfcM\x83\xb9…        r2 load str located at 4295177352
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_6186                             if r0 == (0 as i32 as i64 as u64) { pc += 26 }
    stw [r6+0x8], 11                        
    stdw [r6+0x0], 1                        
    ja lbb_6221                                     if true { pc += 58 }
lbb_6163:
    mov64 r1, 3                                     r1 = 3 as i32 as i64 as u64
    ldxdw r2, [r8+0x50]                     
    jne r2, 165, lbb_6182                           if r2 != (165 as i32 as i64 as u64) { pc += 16 }
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    ldxb r2, [r8+0x0]                       
    mov64 r3, r2                                    r3 = r2
    and64 r3, 8                                     r3 &= 8   ///  r3 = r3.and(8)
    jne r3, 0, lbb_6182                             if r3 != (0 as i32 as i64 as u64) { pc += 11 }
    mov64 r3, r2                                    r3 = r2
    and64 r3, 7                                     r3 &= 7   ///  r3 = r3.and(7)
    jeq r3, 7, lbb_6182                             if r3 == (7 as i32 as i64 as u64) { pc += 8 }
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    stxb [r8+0x0], r2                       
    mov64 r1, r8                                    r1 = r8
    add64 r1, 88                                    r1 += 88   ///  r1 = r1.wrapping_add(88 as i32 as i64 as u64)
    call function_8237                      
    stxdw [r6+0x8], r0                      
    stw [r6+0x0], 0                         
    ja lbb_6206                                     if true { pc += 24 }
lbb_6182:
    stxw [r6+0x4], r1                       
    stw [r6+0x8], 0                         
    stw [r6+0x0], 1                         
    ja lbb_6221                                     if true { pc += 35 }
lbb_6186:
    ldxb r1, [r8+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 8                                     r2 &= 8   ///  r2 = r2.and(8)
    jne r2, 0, lbb_6210                             if r2 != (0 as i32 as i64 as u64) { pc += 20 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    jeq r2, 7, lbb_6210                             if r2 == (7 as i32 as i64 as u64) { pc += 17 }
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxb [r8+0x0], r1                       
    ldxdw r3, [r8+0x50]                     
    mov64 r2, r8                                    r2 = r8
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    call function_7599                      
    ldxw r1, [r10-0x28]                     
    jeq r1, 2, lbb_6204                             if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6215                                     if true { pc += 11 }
lbb_6204:
    stw [r6+0x8], 11                        
    stdw [r6+0x0], 1                        
lbb_6206:
    ldxb r1, [r8+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxb [r8+0x0], r1                       
    ja lbb_6221                                     if true { pc += 11 }
lbb_6210:
    lddw r1, 0xb00000001                            r1 load str located at 47244640257
    stxdw [r6+0x0], r1                      
    stw [r6+0x8], 0                         
    ja lbb_6221                                     if true { pc += 6 }
lbb_6215:
    ldxb r1, [r8+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    ldxdw r2, [r10-0x70]                    
    stxb [r8+0x0], r1                       
    stxdw [r6+0x8], r2                      
    stw [r6+0x0], 0                         
lbb_6221:
    exit                                    

function_6222:
    mov64 r9, r5                                    r9 = r5
    mov64 r7, r4                                    r7 = r4
    mov64 r8, r3                                    r8 = r3
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x100033328 --> b"\x06\x8747\xb4\xfb\xc9\xf4\x047\xdd<\xe6?\x9c\xfe\x8c$\x84\x03U\x93\xa1'\…        r2 load str located at 4295177000
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 6                                     r1 = 6 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_6400                             if r0 != (0 as i32 as i64 as u64) { pc += 165 }
    ldxdw r5, [r9-0xff8]                    
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jeq r5, 0, lbb_6400                             if r5 == (0 as i32 as i64 as u64) { pc += 161 }
    ldxdw r4, [r9-0x1000]                   
    add64 r5, -1                                    r5 += -1   ///  r5 = r5.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r3, [r4+0x0]                       
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    jsgt r3, 6, lbb_6260                            if (r3 as i64) > (6 as i32 as i64) { pc += 16 }
    jsgt r3, 2, lbb_6276                            if (r3 as i64) > (2 as i32 as i64) { pc += 31 }
    jeq r3, 0, lbb_6353                             if r3 == (0 as i32 as i64 as u64) { pc += 107 }
    jeq r3, 1, lbb_6375                             if r3 == (1 as i32 as i64 as u64) { pc += 128 }
    jeq r3, 2, lbb_6249                             if r3 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6400                                     if true { pc += 151 }
lbb_6249:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    call function_1067                      
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ldxw r3, [r10-0x60]                     
    jeq r3, 26, lbb_6400                            if r3 == (26 as i32 as i64 as u64) { pc += 143 }
    ldxw r2, [r10-0x5c]                     
    mov64 r1, r3                                    r1 = r3
    ja lbb_6400                                     if true { pc += 140 }
lbb_6260:
    jsgt r3, 9, lbb_6291                            if (r3 as i64) > (9 as i32 as i64) { pc += 30 }
    jeq r3, 7, lbb_6364                             if r3 == (7 as i32 as i64 as u64) { pc += 102 }
    jeq r3, 8, lbb_6386                             if r3 == (8 as i32 as i64 as u64) { pc += 123 }
    jeq r3, 9, lbb_6265                             if r3 == (9 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6400                                     if true { pc += 135 }
lbb_6265:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    call function_3274                      
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ldxw r3, [r10-0x28]                     
    jeq r3, 26, lbb_6400                            if r3 == (26 as i32 as i64 as u64) { pc += 127 }
    ldxw r2, [r10-0x24]                     
    mov64 r1, r3                                    r1 = r3
    ja lbb_6400                                     if true { pc += 124 }
lbb_6276:
    jsgt r3, 4, lbb_6306                            if (r3 as i64) > (4 as i32 as i64) { pc += 29 }
    jeq r3, 3, lbb_6320                             if r3 == (3 as i32 as i64 as u64) { pc += 42 }
    jeq r3, 4, lbb_6280                             if r3 == (4 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6400                                     if true { pc += 120 }
lbb_6280:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    call function_4462                      
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ldxw r3, [r10-0x50]                     
    jeq r3, 26, lbb_6400                            if r3 == (26 as i32 as i64 as u64) { pc += 112 }
    ldxw r2, [r10-0x4c]                     
    mov64 r1, r3                                    r1 = r3
    ja lbb_6400                                     if true { pc += 109 }
lbb_6291:
    jsgt r3, 11, lbb_6398                           if (r3 as i64) > (11 as i32 as i64) { pc += 106 }
    jeq r3, 10, lbb_6331                            if r3 == (10 as i32 as i64 as u64) { pc += 38 }
    jeq r3, 11, lbb_6295                            if r3 == (11 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6400                                     if true { pc += 105 }
lbb_6295:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    call function_3888                      
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ldxw r3, [r10-0x18]                     
    jeq r3, 26, lbb_6400                            if r3 == (26 as i32 as i64 as u64) { pc += 97 }
    ldxw r2, [r10-0x14]                     
    mov64 r1, r3                                    r1 = r3
    ja lbb_6400                                     if true { pc += 94 }
lbb_6306:
    jeq r3, 5, lbb_6342                             if r3 == (5 as i32 as i64 as u64) { pc += 35 }
    jeq r3, 6, lbb_6309                             if r3 == (6 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6400                                     if true { pc += 91 }
lbb_6309:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    call function_1620                      
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ldxw r3, [r10-0x40]                     
    jeq r3, 26, lbb_6400                            if r3 == (26 as i32 as i64 as u64) { pc += 83 }
    ldxw r2, [r10-0x3c]                     
    mov64 r1, r3                                    r1 = r3
    ja lbb_6400                                     if true { pc += 80 }
lbb_6320:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    call function_1732                      
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ldxw r3, [r10-0x58]                     
    jeq r3, 26, lbb_6400                            if r3 == (26 as i32 as i64 as u64) { pc += 72 }
    ldxw r2, [r10-0x54]                     
    mov64 r1, r3                                    r1 = r3
    ja lbb_6400                                     if true { pc += 69 }
lbb_6331:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    call function_4306                      
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ldxw r3, [r10-0x20]                     
    jeq r3, 26, lbb_6400                            if r3 == (26 as i32 as i64 as u64) { pc += 61 }
    ldxw r2, [r10-0x1c]                     
    mov64 r1, r3                                    r1 = r3
    ja lbb_6400                                     if true { pc += 58 }
lbb_6342:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    call function_4011                      
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ldxw r3, [r10-0x48]                     
    jeq r3, 26, lbb_6400                            if r3 == (26 as i32 as i64 as u64) { pc += 50 }
    ldxw r2, [r10-0x44]                     
    mov64 r1, r3                                    r1 = r3
    ja lbb_6400                                     if true { pc += 47 }
lbb_6353:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    call function_1968                      
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ldxw r3, [r10-0x70]                     
    jeq r3, 26, lbb_6400                            if r3 == (26 as i32 as i64 as u64) { pc += 39 }
    ldxw r2, [r10-0x6c]                     
    mov64 r1, r3                                    r1 = r3
    ja lbb_6400                                     if true { pc += 36 }
lbb_6364:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -56                                   r1 += -56   ///  r1 = r1.wrapping_add(-56 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    call function_2314                      
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ldxw r3, [r10-0x38]                     
    jeq r3, 26, lbb_6400                            if r3 == (26 as i32 as i64 as u64) { pc += 28 }
    ldxw r2, [r10-0x34]                     
    mov64 r1, r3                                    r1 = r3
    ja lbb_6400                                     if true { pc += 25 }
lbb_6375:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    call function_1676                      
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ldxw r3, [r10-0x68]                     
    jeq r3, 26, lbb_6400                            if r3 == (26 as i32 as i64 as u64) { pc += 17 }
    ldxw r2, [r10-0x64]                     
    mov64 r1, r3                                    r1 = r3
    ja lbb_6400                                     if true { pc += 14 }
lbb_6386:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    mov64 r4, r5                                    r4 = r5
    call function_2154                      
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ldxw r3, [r10-0x30]                     
    jeq r3, 26, lbb_6400                            if r3 == (26 as i32 as i64 as u64) { pc += 5 }
    ldxw r2, [r10-0x2c]                     
    mov64 r1, r3                                    r1 = r3
    ja lbb_6400                                     if true { pc += 2 }
lbb_6398:
    jeq r3, 12, lbb_6403                            if r3 == (12 as i32 as i64 as u64) { pc += 4 }
    jeq r3, 13, lbb_6414                            if r3 == (13 as i32 as i64 as u64) { pc += 14 }
lbb_6400:
    stxw [r6+0x4], r2                       
    stxw [r6+0x0], r1                       
    exit                                    
lbb_6403:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    call function_3742                      
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ldxw r3, [r10-0x10]                     
    jeq r3, 26, lbb_6400                            if r3 == (26 as i32 as i64 as u64) { pc += -11 }
    ldxw r2, [r10-0xc]                      
    mov64 r1, r3                                    r1 = r3
    ja lbb_6400                                     if true { pc += -14 }
lbb_6414:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -8                                    r1 += -8   ///  r1 = r1.wrapping_add(-8 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    call function_3128                      
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ldxw r3, [r10-0x8]                      
    jeq r3, 26, lbb_6400                            if r3 == (26 as i32 as i64 as u64) { pc += -22 }
    ldxw r2, [r10-0x4]                      
    mov64 r1, r3                                    r1 = r3
    ja lbb_6400                                     if true { pc += -25 }

function_6425:
    mov64 r6, r1                                    r6 = r1
    mov64 r9, 7                                     r9 = 7 as i32 as i64 as u64
    ldxdw r7, [r2+0x0]                      
    ldxb r1, [r7+0x1]                       
    jeq r1, 0, lbb_6442                             if r1 == (0 as i32 as i64 as u64) { pc += 12 }
    ldxdw r8, [r3+0x0]                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033328 --> b"\x06\x8747\xb4\xfb\xc9\xf4\x047\xdd<\xe6?\x9c\xfe\x8c$\x84\x03U\x93\xa1'\…        r2 load str located at 4295177000
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r4, 3                                     r4 = 3 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_6445                             if r0 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_6442:
    stxw [r6+0x4], r4                       
    stxw [r6+0x0], r9                       
    exit                                    
lbb_6445:
    mov64 r2, r8                                    r2 = r8
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    lddw r1, 0x100033448 --> b"\xa4\xa7M\x03\xd49\xb1\xf5\xd9\xdd0\x98\xcf\xfe\x87\xf0\xfb\x9c\xeey\xea\…        r1 load str located at 4295177288
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r4, 3                                     r4 = 3 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_6442                             if r0 != (0 as i32 as i64 as u64) { pc += -13 }
    mov64 r9, 11                                    r9 = 11 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxb r1, [r8+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 8                                     r2 &= 8   ///  r2 = r2.and(8)
    jne r2, 0, lbb_6442                             if r2 != (0 as i32 as i64 as u64) { pc += -19 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    jeq r2, 7, lbb_6442                             if r2 == (7 as i32 as i64 as u64) { pc += -22 }
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxb [r8+0x0], r1                       
    ldxdw r3, [r8+0x50]                     
    stxdw [r10-0x18], r8                    
    add64 r8, 88                                    r8 += 88   ///  r8 = r8.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    call function_10296                     
    ldxw r1, [r10-0x10]                     
    jne r1, 0, lbb_6487                             if r1 != (0 as i32 as i64 as u64) { pc += 12 }
    ldxdw r1, [r10-0x8]                     
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r9, 26                                    r9 = 26 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_6489                             if r0 == (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r4, 12                                    r4 = 12 as i32 as i64 as u64
    ja lbb_6489                                     if true { pc += 2 }
lbb_6487:
    ldxw r4, [r10-0x8]                      
    ldxw r9, [r10-0xc]                      
lbb_6489:
    ldxdw r2, [r10-0x18]                    
    ldxb r1, [r2+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxb [r2+0x0], r1                       
    ja lbb_6442                                     if true { pc += -52 }

function_6494:
    mov64 r6, r1                                    r6 = r1
    mov64 r8, 7                                     r8 = 7 as i32 as i64 as u64
    ldxdw r1, [r2+0x0]                      
    stxdw [r10-0x18], r1                    
    ldxb r1, [r1+0x1]                       
    jeq r1, 0, lbb_6512                             if r1 == (0 as i32 as i64 as u64) { pc += 12 }
    ldxdw r7, [r3+0x0]                      
    mov64 r1, r7                                    r1 = r7
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    lddw r2, 0x100033328 --> b"\x06\x8747\xb4\xfb\xc9\xf4\x047\xdd<\xe6?\x9c\xfe\x8c$\x84\x03U\x93\xa1'\…        r2 load str located at 4295177000
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r9, 3                                     r9 = 3 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_6515                             if r0 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_6512:
    stxw [r6+0x4], r9                       
    stxw [r6+0x0], r8                       
    exit                                    
lbb_6515:
    mov64 r2, r7                                    r2 = r7
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    lddw r1, 0x100033448 --> b"\xa4\xa7M\x03\xd49\xb1\xf5\xd9\xdd0\x98\xcf\xfe\x87\xf0\xfb\x9c\xeey\xea\…        r1 load str located at 4295177288
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_6512                             if r0 != (0 as i32 as i64 as u64) { pc += -12 }
    mov64 r8, 11                                    r8 = 11 as i32 as i64 as u64
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ldxb r1, [r7+0x0]                       
    mov64 r2, r1                                    r2 = r1
    and64 r2, 8                                     r2 &= 8   ///  r2 = r2.and(8)
    jne r2, 0, lbb_6512                             if r2 != (0 as i32 as i64 as u64) { pc += -18 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    jeq r2, 7, lbb_6512                             if r2 == (7 as i32 as i64 as u64) { pc += -21 }
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxb [r7+0x0], r1                       
    ldxdw r3, [r7+0x50]                     
    stxdw [r10-0x20], r7                    
    add64 r7, 88                                    r7 += 88   ///  r7 = r7.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_10296                     
    ldxw r1, [r10-0x10]                     
    jne r1, 0, lbb_6566                             if r1 != (0 as i32 as i64 as u64) { pc += 22 }
    ldxdw r1, [r10-0x8]                     
    ldxdw r7, [r10-0x18]                    
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x18], r1                    
    add64 r1, 32                                    r1 += 32   ///  r1 = r1.wrapping_add(32 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r8, 26                                    r8 = 26 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_6568                             if r0 == (0 as i32 as i64 as u64) { pc += 12 }
    ldxdw r1, [r10-0x18]                    
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_6568                             if r0 == (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r9, 13                                    r9 = 13 as i32 as i64 as u64
    ja lbb_6568                                     if true { pc += 2 }
lbb_6566:
    ldxw r9, [r10-0x8]                      
    ldxw r8, [r10-0xc]                      
lbb_6568:
    ldxdw r2, [r10-0x20]                    
    ldxb r1, [r2+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxb [r2+0x0], r1                       
    ja lbb_6512                                     if true { pc += -61 }

entrypoint:
    mov64 r2, r1                                    r2 = r1
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxdw r1, [r2+0x0]                      
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_6622                             if r1 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r4, r1                                    r4 = r1
    jlt r1, 128, lbb_6582                           if r1 < (128 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 128                                   r4 = 128 as i32 as i64 as u64
lbb_6582:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r0, r10                                   r0 = r10
    add64 r0, -1024                                 r0 += -1024   ///  r0 = r0.wrapping_add(-1024 as i32 as i64 as u64)
    ja lbb_6597                                     if true { pc += 11 }
lbb_6586:
    stb [r7+0x0], 0                         
    ldxdw r8, [r7+0x50]                     
    add64 r3, r8                                    r3 += r8   ///  r3 = r3.wrapping_add(r8)
    add64 r3, 10335                                 r3 += 10335   ///  r3 = r3.wrapping_add(10335 as i32 as i64 as u64)
    and64 r3, -8                                    r3 &= -8   ///  r3 = r3.and(-8)
lbb_6591:
    add64 r5, 1                                     r5 += 1   ///  r5 = r5.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r0+0x0], r7                      
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    jlt r5, r4, lbb_6597                            if r5 < r4 { pc += 1 }
    ja lbb_6607                                     if true { pc += 10 }
lbb_6597:
    mov64 r7, r2                                    r7 = r2
    add64 r7, r3                                    r7 += r3   ///  r7 = r7.wrapping_add(r3)
    ldxb r8, [r7+0x0]                       
    jeq r8, 255, lbb_6586                           if r8 == (255 as i32 as i64 as u64) { pc += -15 }
    lsh64 r8, 3                                     r8 <<= 3   ///  r8 = r8.wrapping_shl(3)
    mov64 r7, r10                                   r7 = r10
    add64 r7, -1024                                 r7 += -1024   ///  r7 = r7.wrapping_add(-1024 as i32 as i64 as u64)
    add64 r7, r8                                    r7 += r8   ///  r7 = r7.wrapping_add(r8)
    ldxdw r7, [r7+0x0]                      
    ja lbb_6591                                     if true { pc += -16 }
lbb_6607:
    jlt r1, 129, lbb_6622                           if r1 < (129 as i32 as i64 as u64) { pc += 14 }
    mov64 r5, r4                                    r5 = r4
lbb_6609:
    mov64 r0, r2                                    r0 = r2
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    ldxb r7, [r0+0x0]                       
    jeq r7, 255, lbb_6617                           if r7 == (255 as i32 as i64 as u64) { pc += 4 }
lbb_6613:
    add64 r5, 1                                     r5 += 1   ///  r5 = r5.wrapping_add(1 as i32 as i64 as u64)
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    jlt r5, r1, lbb_6609                            if r5 < r1 { pc += -7 }
    ja lbb_6622                                     if true { pc += 5 }
lbb_6617:
    ldxdw r0, [r0+0x50]                     
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
    add64 r3, 10335                                 r3 += 10335   ///  r3 = r3.wrapping_add(10335 as i32 as i64 as u64)
    and64 r3, -8                                    r3 &= -8   ///  r3 = r3.and(-8)
    ja lbb_6613                                     if true { pc += -9 }
lbb_6622:
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    ldxdw r1, [r2+0x0]                      
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x1000], r2                  
    stxdw [r10-0xff8], r1                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1032                                 r1 += -1032   ///  r1 = r1.wrapping_add(-1032 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -1024                                 r3 += -1024   ///  r3 = r3.wrapping_add(-1024 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    call function_6222                      
    ldxw r1, [r10-0x408]                    
    jeq r1, 26, lbb_6740                            if r1 == (26 as i32 as i64 as u64) { pc += 104 }
    jsgt r1, 12, lbb_6646                           if (r1 as i64) > (12 as i32 as i64) { pc += 9 }
    jsgt r1, 5, lbb_6653                            if (r1 as i64) > (5 as i32 as i64) { pc += 15 }
    jsgt r1, 2, lbb_6675                            if (r1 as i64) > (2 as i32 as i64) { pc += 36 }
    jeq r1, 0, lbb_6699                             if r1 == (0 as i32 as i64 as u64) { pc += 59 }
    lddw r6, 0x200000000                            r6 load str located at 8589934592
    jeq r1, 1, lbb_6740                             if r1 == (1 as i32 as i64 as u64) { pc += 97 }
    lddw r6, 0x300000000                            r6 load str located at 12884901888
    ja lbb_6740                                     if true { pc += 94 }
lbb_6646:
    jsgt r1, 18, lbb_6659                           if (r1 as i64) > (18 as i32 as i64) { pc += 12 }
    jsgt r1, 15, lbb_6680                           if (r1 as i64) > (15 as i32 as i64) { pc += 32 }
    jeq r1, 13, lbb_6705                            if r1 == (13 as i32 as i64 as u64) { pc += 56 }
    jeq r1, 14, lbb_6732                            if r1 == (14 as i32 as i64 as u64) { pc += 82 }
    lddw r6, 0x1000000000                           r6 load str located at 68719476736
    ja lbb_6740                                     if true { pc += 87 }
lbb_6653:
    jsgt r1, 8, lbb_6665                            if (r1 as i64) > (8 as i32 as i64) { pc += 11 }
    jeq r1, 6, lbb_6693                             if r1 == (6 as i32 as i64 as u64) { pc += 38 }
    jeq r1, 7, lbb_6714                             if r1 == (7 as i32 as i64 as u64) { pc += 58 }
    lddw r6, 0x900000000                            r6 load str located at 38654705664
    ja lbb_6740                                     if true { pc += 81 }
lbb_6659:
    jsgt r1, 21, lbb_6670                           if (r1 as i64) > (21 as i32 as i64) { pc += 10 }
    jeq r1, 19, lbb_6696                            if r1 == (19 as i32 as i64 as u64) { pc += 35 }
    jeq r1, 20, lbb_6717                            if r1 == (20 as i32 as i64 as u64) { pc += 55 }
    lddw r6, 0x1600000000                           r6 load str located at 94489280512
    ja lbb_6740                                     if true { pc += 75 }
lbb_6665:
    jsgt r1, 10, lbb_6685                           if (r1 as i64) > (10 as i32 as i64) { pc += 19 }
    jeq r1, 9, lbb_6720                             if r1 == (9 as i32 as i64 as u64) { pc += 53 }
    lddw r6, 0xb00000000                            r6 load str located at 47244640256
    ja lbb_6740                                     if true { pc += 70 }
lbb_6670:
    jsgt r1, 23, lbb_6689                           if (r1 as i64) > (23 as i32 as i64) { pc += 18 }
    jeq r1, 22, lbb_6723                            if r1 == (22 as i32 as i64 as u64) { pc += 51 }
    lddw r6, 0x1800000000                           r6 load str located at 103079215104
    ja lbb_6740                                     if true { pc += 65 }
lbb_6675:
    jeq r1, 3, lbb_6708                             if r1 == (3 as i32 as i64 as u64) { pc += 32 }
    jeq r1, 4, lbb_6735                             if r1 == (4 as i32 as i64 as u64) { pc += 58 }
    lddw r6, 0x600000000                            r6 load str located at 25769803776
    ja lbb_6740                                     if true { pc += 60 }
lbb_6680:
    jeq r1, 16, lbb_6711                            if r1 == (16 as i32 as i64 as u64) { pc += 30 }
    jeq r1, 17, lbb_6738                            if r1 == (17 as i32 as i64 as u64) { pc += 56 }
    lddw r6, 0x1300000000                           r6 load str located at 81604378624
    ja lbb_6740                                     if true { pc += 55 }
lbb_6685:
    jeq r1, 11, lbb_6726                            if r1 == (11 as i32 as i64 as u64) { pc += 40 }
    lddw r6, 0xd00000000                            r6 load str located at 55834574848
    ja lbb_6740                                     if true { pc += 51 }
lbb_6689:
    jeq r1, 24, lbb_6729                            if r1 == (24 as i32 as i64 as u64) { pc += 39 }
    lddw r6, 0x1a00000000                           r6 load str located at 111669149696
    ja lbb_6740                                     if true { pc += 47 }
lbb_6693:
    lddw r6, 0x700000000                            r6 load str located at 30064771072
    ja lbb_6740                                     if true { pc += 44 }
lbb_6696:
    lddw r6, 0x1400000000                           r6 load str located at 85899345920
    ja lbb_6740                                     if true { pc += 41 }
lbb_6699:
    ldxw r1, [r10-0x404]                    
    lddw r6, 0x100000000                            r6 load str located at 4294967296
    jeq r1, 0, lbb_6740                             if r1 == (0 as i32 as i64 as u64) { pc += 37 }
    mov64 r6, r1                                    r6 = r1
    ja lbb_6740                                     if true { pc += 35 }
lbb_6705:
    lddw r6, 0xe00000000                            r6 load str located at 60129542144
    ja lbb_6740                                     if true { pc += 32 }
lbb_6708:
    lddw r6, 0x400000000                            r6 load str located at 17179869184
    ja lbb_6740                                     if true { pc += 29 }
lbb_6711:
    lddw r6, 0x1100000000                           r6 load str located at 73014444032
    ja lbb_6740                                     if true { pc += 26 }
lbb_6714:
    lddw r6, 0x800000000                            r6 load str located at 34359738368
    ja lbb_6740                                     if true { pc += 23 }
lbb_6717:
    lddw r6, 0x1500000000                           r6 load str located at 90194313216
    ja lbb_6740                                     if true { pc += 20 }
lbb_6720:
    lddw r6, 0xa00000000                            r6 load str located at 42949672960
    ja lbb_6740                                     if true { pc += 17 }
lbb_6723:
    lddw r6, 0x1700000000                           r6 load str located at 98784247808
    ja lbb_6740                                     if true { pc += 14 }
lbb_6726:
    lddw r6, 0xc00000000                            r6 load str located at 51539607552
    ja lbb_6740                                     if true { pc += 11 }
lbb_6729:
    lddw r6, 0x1900000000                           r6 load str located at 107374182400
    ja lbb_6740                                     if true { pc += 8 }
lbb_6732:
    lddw r6, 0xf00000000                            r6 load str located at 64424509440
    ja lbb_6740                                     if true { pc += 5 }
lbb_6735:
    lddw r6, 0x500000000                            r6 load str located at 21474836480
    ja lbb_6740                                     if true { pc += 2 }
lbb_6738:
    lddw r6, 0x1200000000                           r6 load str located at 77309411328
lbb_6740:
    mov64 r0, r6                                    r0 = r6
    exit                                    

function_6742:
    lddw r3, 0x300000000                            r3 load str located at 12884901888
    ldxdw r3, [r3+0x0]                      
    lddw r4, 0x300008000                            r4 load str located at 12884934656
    jeq r3, 0, lbb_6749                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r3                                    r4 = r3
lbb_6749:
    mov64 r3, r4                                    r3 = r4
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jgt r3, r4, lbb_6755                            if r3 > r4 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_6755:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jne r5, 0, lbb_6758                             if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r3                                    r1 = r3
lbb_6758:
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    lddw r2, 0x300000008                            r2 load str located at 12884901896
    jlt r1, r2, lbb_6767                            if r1 < r2 { pc += 4 }
    lddw r2, 0x300000000                            r2 load str located at 12884901888
    stxdw [r2+0x0], r1                      
    mov64 r0, r1                                    r0 = r1
lbb_6767:
    exit                                    

function_6768:
    exit                                    

function_6769:
    mov64 r5, r2                                    r5 = r2
    mov64 r2, r1                                    r2 = r1
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    ldxdw r1, [r1+0x0]                      
    lddw r6, 0x300008000                            r6 load str located at 12884934656
    jeq r1, 0, lbb_6778                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r1                                    r6 = r1
lbb_6778:
    mov64 r1, r6                                    r1 = r6
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jgt r1, r6, lbb_6784                            if r1 > r6 { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_6784:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r7, 0, lbb_6787                             if r7 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r1                                    r6 = r1
lbb_6787:
    neg64 r3                                        r3 = -r3   ///  r3 = (r3 as i64).wrapping_neg() as u64
    and64 r6, r3                                    r6 &= r3   ///  r6 = r6.and(r3)
    lddw r1, 0x300000008                            r1 load str located at 12884901896
    jlt r6, r1, lbb_6801                            if r6 < r1 { pc += 9 }
    lddw r1, 0x300000000                            r1 load str located at 12884901888
    stxdw [r1+0x0], r6                      
    jlt r5, r4, lbb_6797                            if r5 < r4 { pc += 1 }
    mov64 r5, r4                                    r5 = r4
lbb_6797:
    mov64 r1, r6                                    r1 = r6
    mov64 r3, r5                                    r3 = r5
    call function_23152                     
    mov64 r0, r6                                    r0 = r6
lbb_6801:
    exit                                    

custom_panic:
    ldxdw r1, [r1+0x18]                     
    ldxdw r2, [r1+0x8]                      
    ldxdw r1, [r1+0x0]                      
    syscall [invalid]                       
    lddw r1, 0x1000335ce --> b"** PANICKED **"        r1 load str located at 4295177678
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    syscall [invalid]                       
    exit                                    

function_6811:
    call function_19937                     
    exit                                    

function_6813:
    mov64 r7, r3                                    r7 = r3
    mov64 r8, r2                                    r8 = r2
    mov64 r6, r1                                    r6 = r1
    jeq r8, 0, lbb_6824                             if r8 == (0 as i32 as i64 as u64) { pc += 7 }
    ldxdw r1, [r4+0x8]                      
    jeq r1, 0, lbb_6840                             if r1 == (0 as i32 as i64 as u64) { pc += 21 }
    ldxdw r2, [r4+0x10]                     
    jne r2, 0, lbb_6826                             if r2 != (0 as i32 as i64 as u64) { pc += 5 }
    jne r7, 0, lbb_6842                             if r7 != (0 as i32 as i64 as u64) { pc += 20 }
lbb_6822:
    mov64 r0, r8                                    r0 = r8
    ja lbb_6845                                     if true { pc += 21 }
lbb_6824:
    stdw [r6+0x8], 0                        
    ja lbb_6838                                     if true { pc += 12 }
lbb_6826:
    ldxdw r1, [r4+0x0]                      
    mov64 r3, r8                                    r3 = r8
    mov64 r4, r7                                    r4 = r7
    call function_6769                      
    mov64 r1, r6                                    r1 = r6
    add64 r1, 16                                    r1 += 16   ///  r1 = r1.wrapping_add(16 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jeq r0, 0, lbb_6836                             if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6850                                     if true { pc += 14 }
lbb_6836:
    stxdw [r2+0x0], r8                      
    stxdw [r1+0x0], r7                      
lbb_6838:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ja lbb_6853                                     if true { pc += 13 }
lbb_6840:
    jne r7, 0, lbb_6842                             if r7 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_6822                                     if true { pc += -20 }
lbb_6842:
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r8                                    r2 = r8
    call function_6742                      
lbb_6845:
    mov64 r1, r6                                    r1 = r6
    add64 r1, 16                                    r1 += 16   ///  r1 = r1.wrapping_add(16 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jeq r0, 0, lbb_6836                             if r0 == (0 as i32 as i64 as u64) { pc += -14 }
lbb_6850:
    stxdw [r2+0x0], r0                      
    stxdw [r1+0x0], r7                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_6853:
    stxdw [r6+0x0], r1                      
    exit                                    

function_6855:
    mov64 r6, r1                                    r6 = r1
    mov64 r4, r2                                    r4 = r2
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r4, r2, lbb_6862                            if r4 < r2 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_6862:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_6891                             if r3 != (0 as i32 as i64 as u64) { pc += 27 }
    ldxdw r1, [r6+0x0]                      
    mov64 r7, r1                                    r7 = r1
    lsh64 r7, 1                                     r7 <<= 1   ///  r7 = r7.wrapping_shl(1)
    jgt r7, r4, lbb_6869                            if r7 > r4 { pc += 1 }
    mov64 r7, r4                                    r7 = r4
lbb_6869:
    jgt r7, 8, lbb_6871                             if r7 > (8 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 8                                     r7 = 8 as i32 as i64 as u64
lbb_6871:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r2, r7                                    r2 = r7
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    rsh64 r2, 63                                    r2 >>= 63   ///  r2 = r2.wrapping_shr(63)
    jeq r1, 0, lbb_6880                             if r1 == (0 as i32 as i64 as u64) { pc += 4 }
    ldxdw r3, [r6+0x8]                      
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r3                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_6880:
    stxdw [r10-0x10], r3                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -24                                   r4 += -24   ///  r4 = r4.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r3, r7                                    r3 = r7
    call function_6813                      
    ldxdw r1, [r10-0x30]                    
    jeq r1, 0, lbb_6892                             if r1 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r2, [r10-0x20]                    
    ldxdw r1, [r10-0x28]                    
lbb_6891:
    call function_20091                     
lbb_6892:
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x0], r7                      
    stxdw [r6+0x8], r1                      
    exit                                    

function_6896:
    mov64 r6, r1                                    r6 = r1
    ldxdw r4, [r6+0x0]                      
    mov64 r3, r4                                    r3 = r4
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r3, 0, lbb_6904                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_6904:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_6935                             if r5 != (0 as i32 as i64 as u64) { pc += 29 }
    mov64 r7, r4                                    r7 = r4
    lsh64 r7, 1                                     r7 <<= 1   ///  r7 = r7.wrapping_shl(1)
    jgt r7, r3, lbb_6910                            if r7 > r3 { pc += 1 }
    mov64 r7, r3                                    r7 = r3
lbb_6910:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x3c3c3c3c3c3c3c4                      r3 load str located at 271275648142787524
    jlt r7, r3, lbb_6915                            if r7 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_6915:
    jgt r7, 4, lbb_6917                             if r7 > (4 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 4                                     r7 = 4 as i32 as i64 as u64
lbb_6917:
    mov64 r3, r7                                    r3 = r7
    mul64 r3, 34                                    r3 *= 34   ///  r3 = r3.wrapping_mul(34 as u64)
    jeq r4, 0, lbb_6925                             if r4 == (0 as i32 as i64 as u64) { pc += 5 }
    ldxdw r1, [r6+0x8]                      
    mul64 r4, 34                                    r4 *= 34   ///  r4 = r4.wrapping_mul(34 as u64)
    stxdw [r10-0x8], r4                     
    stxdw [r10-0x18], r1                    
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_6925:
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -24                                   r4 += -24   ///  r4 = r4.wrapping_add(-24 as i32 as i64 as u64)
    call function_6813                      
    ldxdw r1, [r10-0x30]                    
    jeq r1, 0, lbb_6936                             if r1 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r2, [r10-0x20]                    
    ldxdw r1, [r10-0x28]                    
lbb_6935:
    call function_20091                     
lbb_6936:
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x0], r7                      
    stxdw [r6+0x8], r1                      
    exit                                    

function_6940:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r1, 80                                    r1 = 80 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_6742                      
    jeq r0, 0, lbb_7277                             if r0 == (0 as i32 as i64 as u64) { pc += 331 }
    stxdw [r10-0x10], r0                    
    stdw [r10-0x18], 80                     
    ldxw r1, [r7+0x0]                       
    jsgt r1, 21, lbb_6956                           if (r1 as i64) > (21 as i32 as i64) { pc += 6 }
    jsgt r1, 10, lbb_6971                           if (r1 as i64) > (10 as i32 as i64) { pc += 20 }
    jsgt r1, 4, lbb_6981                            if (r1 as i64) > (4 as i32 as i64) { pc += 29 }
    jsgt r1, 1, lbb_7028                            if (r1 as i64) > (1 as i32 as i64) { pc += 75 }
    jeq r1, 0, lbb_7192                             if r1 == (0 as i32 as i64 as u64) { pc += 238 }
    stb [r0+0x0], 1                         
    ja lbb_7246                                     if true { pc += 290 }
lbb_6956:
    jsgt r1, 32, lbb_6965                           if (r1 as i64) > (32 as i32 as i64) { pc += 8 }
    jsgt r1, 26, lbb_6994                           if (r1 as i64) > (26 as i32 as i64) { pc += 36 }
    jsgt r1, 23, lbb_7076                           if (r1 as i64) > (23 as i32 as i64) { pc += 117 }
    jeq r1, 22, lbb_7233                            if r1 == (22 as i32 as i64 as u64) { pc += 273 }
    ldxdw r1, [r7+0x8]                      
    stxdw [r0+0x1], r1                      
    stb [r0+0x0], 23                        
    mov64 r8, 9                                     r8 = 9 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += 282 }
lbb_6965:
    jsgt r1, 38, lbb_7023                           if (r1 as i64) > (38 as i32 as i64) { pc += 57 }
    jsgt r1, 35, lbb_7084                           if (r1 as i64) > (35 as i32 as i64) { pc += 117 }
    jeq r1, 33, lbb_7219                            if r1 == (33 as i32 as i64 as u64) { pc += 251 }
    jeq r1, 34, lbb_7239                            if r1 == (34 as i32 as i64 as u64) { pc += 270 }
    stb [r0+0x0], 35                        
    ja lbb_7107                                     if true { pc += 136 }
lbb_6971:
    jsgt r1, 15, lbb_6989                           if (r1 as i64) > (15 as i32 as i64) { pc += 17 }
    jsgt r1, 12, lbb_7035                           if (r1 as i64) > (12 as i32 as i64) { pc += 62 }
    jeq r1, 11, lbb_7160                            if r1 == (11 as i32 as i64 as u64) { pc += 186 }
    ldxdw r1, [r7+0x8]                      
    ldxb r2, [r7+0x10]                      
    stxb [r0+0x9], r2                       
    stxdw [r0+0x1], r1                      
    stb [r0+0x0], 12                        
    mov64 r8, 10                                    r8 = 10 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += 266 }
lbb_6981:
    jsgt r1, 7, lbb_7044                            if (r1 as i64) > (7 as i32 as i64) { pc += 62 }
    jeq r1, 5, lbb_7104                             if r1 == (5 as i32 as i64 as u64) { pc += 121 }
    jeq r1, 6, lbb_7166                             if r1 == (6 as i32 as i64 as u64) { pc += 182 }
    ldxdw r1, [r7+0x8]                      
    stxdw [r0+0x1], r1                      
    stb [r0+0x0], 7                         
    mov64 r8, 9                                     r8 = 9 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += 258 }
lbb_6989:
    jsgt r1, 18, lbb_7048                           if (r1 as i64) > (18 as i32 as i64) { pc += 58 }
    jeq r1, 16, lbb_7106                            if r1 == (16 as i32 as i64 as u64) { pc += 115 }
    jeq r1, 17, lbb_7174                            if r1 == (17 as i32 as i64 as u64) { pc += 182 }
    stb [r0+0x0], 18                        
    ja lbb_7107                                     if true { pc += 113 }
lbb_6994:
    jsgt r1, 29, lbb_7080                           if (r1 as i64) > (29 as i32 as i64) { pc += 85 }
    jeq r1, 27, lbb_7162                            if r1 == (27 as i32 as i64 as u64) { pc += 166 }
    jeq r1, 28, lbb_7235                            if r1 == (28 as i32 as i64 as u64) { pc += 238 }
    stb [r0+0x0], 29                        
    stdw [r10-0x8], 1                       
    ldxdw r8, [r7+0x18]                     
    jeq r8, 0, lbb_7248                             if r8 == (0 as i32 as i64 as u64) { pc += 247 }
    ldxdw r7, [r7+0x10]                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lsh64 r8, 1                                     r8 <<= 1   ///  r8 = r8.wrapping_shl(1)
    ja lbb_7013                                     if true { pc += 8 }
lbb_7005:
    add64 r7, 2                                     r7 += 2   ///  r7 = r7.wrapping_add(2 as i32 as i64 as u64)
    ldxdw r1, [r10-0x10]                    
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    stxh [r1+0x0], r9                       
    add64 r2, 2                                     r2 += 2   ///  r2 = r2.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r10-0x8], r2                     
    add64 r8, -2                                    r8 += -2   ///  r8 = r8.wrapping_add(-2 as i32 as i64 as u64)
    jeq r8, 0, lbb_7248                             if r8 == (0 as i32 as i64 as u64) { pc += 235 }
lbb_7013:
    ldxdw r1, [r10-0x18]                    
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    ldxh r9, [r7+0x0]                       
    jgt r1, 1, lbb_7005                             if r1 > (1 as i32 as i64 as u64) { pc += -12 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_6855                      
    ldxdw r2, [r10-0x8]                     
    ja lbb_7005                                     if true { pc += -18 }
lbb_7023:
    jsgt r1, 41, lbb_7088                           if (r1 as i64) > (41 as i32 as i64) { pc += 64 }
    jeq r1, 39, lbb_7221                            if r1 == (39 as i32 as i64 as u64) { pc += 196 }
    jeq r1, 40, lbb_7241                            if r1 == (40 as i32 as i64 as u64) { pc += 215 }
    stb [r0+0x0], 41                        
    ja lbb_7246                                     if true { pc += 218 }
lbb_7028:
    jeq r1, 2, lbb_7092                             if r1 == (2 as i32 as i64 as u64) { pc += 63 }
    jeq r1, 3, lbb_7148                             if r1 == (3 as i32 as i64 as u64) { pc += 118 }
    ldxdw r1, [r7+0x8]                      
    stxdw [r0+0x1], r1                      
    stb [r0+0x0], 4                         
    mov64 r8, 9                                     r8 = 9 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += 212 }
lbb_7035:
    jeq r1, 13, lbb_7097                            if r1 == (13 as i32 as i64 as u64) { pc += 61 }
    jeq r1, 14, lbb_7153                            if r1 == (14 as i32 as i64 as u64) { pc += 116 }
    ldxdw r1, [r7+0x8]                      
    ldxb r2, [r7+0x10]                      
    stxb [r0+0x9], r2                       
    stxdw [r0+0x1], r1                      
    stb [r0+0x0], 15                        
    mov64 r8, 10                                    r8 = 10 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += 203 }
lbb_7044:
    jeq r1, 8, lbb_7117                             if r1 == (8 as i32 as i64 as u64) { pc += 72 }
    jeq r1, 9, lbb_7176                             if r1 == (9 as i32 as i64 as u64) { pc += 130 }
    stb [r0+0x0], 10                        
    ja lbb_7246                                     if true { pc += 198 }
lbb_7048:
    jeq r1, 19, lbb_7122                            if r1 == (19 as i32 as i64 as u64) { pc += 73 }
    jeq r1, 20, lbb_7178                            if r1 == (20 as i32 as i64 as u64) { pc += 128 }
    stb [r0+0x0], 21                        
    stdw [r10-0x8], 1                       
    ldxdw r8, [r7+0x18]                     
    jeq r8, 0, lbb_7248                             if r8 == (0 as i32 as i64 as u64) { pc += 194 }
    ldxdw r7, [r7+0x10]                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lsh64 r8, 1                                     r8 <<= 1   ///  r8 = r8.wrapping_shl(1)
    ja lbb_7066                                     if true { pc += 8 }
lbb_7058:
    add64 r7, 2                                     r7 += 2   ///  r7 = r7.wrapping_add(2 as i32 as i64 as u64)
    ldxdw r1, [r10-0x10]                    
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    stxh [r1+0x0], r9                       
    add64 r2, 2                                     r2 += 2   ///  r2 = r2.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r10-0x8], r2                     
    add64 r8, -2                                    r8 += -2   ///  r8 = r8.wrapping_add(-2 as i32 as i64 as u64)
    jeq r8, 0, lbb_7248                             if r8 == (0 as i32 as i64 as u64) { pc += 182 }
lbb_7066:
    ldxdw r1, [r10-0x18]                    
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    ldxh r9, [r7+0x0]                       
    jgt r1, 1, lbb_7058                             if r1 > (1 as i32 as i64 as u64) { pc += -12 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_6855                      
    ldxdw r2, [r10-0x8]                     
    ja lbb_7058                                     if true { pc += -18 }
lbb_7076:
    jeq r1, 24, lbb_7127                            if r1 == (24 as i32 as i64 as u64) { pc += 50 }
    jeq r1, 25, lbb_7227                            if r1 == (25 as i32 as i64 as u64) { pc += 149 }
    stb [r0+0x0], 26                        
    ja lbb_7246                                     if true { pc += 166 }
lbb_7080:
    jeq r1, 30, lbb_7164                            if r1 == (30 as i32 as i64 as u64) { pc += 83 }
    jeq r1, 31, lbb_7237                            if r1 == (31 as i32 as i64 as u64) { pc += 155 }
    stb [r0+0x0], 32                        
    ja lbb_7246                                     if true { pc += 162 }
lbb_7084:
    jeq r1, 36, lbb_7223                            if r1 == (36 as i32 as i64 as u64) { pc += 138 }
    jeq r1, 37, lbb_7243                            if r1 == (37 as i32 as i64 as u64) { pc += 157 }
    stb [r0+0x0], 38                        
    ja lbb_7246                                     if true { pc += 158 }
lbb_7088:
    jeq r1, 42, lbb_7225                            if r1 == (42 as i32 as i64 as u64) { pc += 136 }
    jeq r1, 43, lbb_7245                            if r1 == (43 as i32 as i64 as u64) { pc += 155 }
    stb [r0+0x0], 44                        
    ja lbb_7246                                     if true { pc += 154 }
lbb_7092:
    ldxb r1, [r7+0x8]                       
    stxb [r0+0x1], r1                       
    stb [r0+0x0], 2                         
    mov64 r8, 2                                     r8 = 2 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += 150 }
lbb_7097:
    ldxdw r1, [r7+0x8]                      
    ldxb r2, [r7+0x10]                      
    stxb [r0+0x9], r2                       
    stxdw [r0+0x1], r1                      
    stb [r0+0x0], 13                        
    mov64 r8, 10                                    r8 = 10 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += 143 }
lbb_7104:
    stb [r0+0x0], 5                         
    ja lbb_7246                                     if true { pc += 140 }
lbb_7106:
    stb [r0+0x0], 16                        
lbb_7107:
    ldxdw r1, [r7+0x8]                      
    stxdw [r0+0x1], r1                      
    ldxdw r1, [r7+0x10]                     
    stxdw [r0+0x9], r1                      
    ldxdw r1, [r7+0x18]                     
    stxdw [r0+0x11], r1                     
    ldxdw r1, [r7+0x20]                     
    stxdw [r0+0x19], r1                     
    mov64 r8, 33                                    r8 = 33 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += 130 }
lbb_7117:
    ldxdw r1, [r7+0x8]                      
    stxdw [r0+0x1], r1                      
    stb [r0+0x0], 8                         
    mov64 r8, 9                                     r8 = 9 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += 125 }
lbb_7122:
    ldxb r1, [r7+0x8]                       
    stxb [r0+0x1], r1                       
    stb [r0+0x0], 19                        
    mov64 r8, 2                                     r8 = 2 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += 120 }
lbb_7127:
    stb [r0+0x0], 24                        
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    stdw [r10-0x8], 1                       
    ldxdw r2, [r7+0x8]                      
    ldxdw r7, [r7+0x10]                     
    jlt r7, 80, lbb_7142                            if r7 < (80 as i32 as i64 as u64) { pc += 9 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r8, r2                                    r8 = r2
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r3, r7                                    r3 = r7
    call function_6855                      
    mov64 r2, r8                                    r2 = r8
    ldxdw r0, [r10-0x10]                    
    ldxdw r8, [r10-0x8]                     
lbb_7142:
    add64 r0, r8                                    r0 += r8   ///  r0 = r0.wrapping_add(r8)
    mov64 r1, r0                                    r1 = r0
    mov64 r3, r7                                    r3 = r7
    call function_23152                     
    add64 r8, r7                                    r8 += r7   ///  r8 = r8.wrapping_add(r7)
    ja lbb_7247                                     if true { pc += 99 }
lbb_7148:
    ldxdw r1, [r7+0x8]                      
    stxdw [r0+0x1], r1                      
    stb [r0+0x0], 3                         
    mov64 r8, 9                                     r8 = 9 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += 94 }
lbb_7153:
    ldxdw r1, [r7+0x8]                      
    ldxb r2, [r7+0x10]                      
    stxb [r0+0x9], r2                       
    stxdw [r0+0x1], r1                      
    stb [r0+0x0], 14                        
    mov64 r8, 10                                    r8 = 10 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += 87 }
lbb_7160:
    stb [r0+0x0], 11                        
    ja lbb_7246                                     if true { pc += 84 }
lbb_7162:
    stb [r0+0x0], 27                        
    ja lbb_7246                                     if true { pc += 82 }
lbb_7164:
    stb [r0+0x0], 30                        
    ja lbb_7246                                     if true { pc += 80 }
lbb_7166:
    stb [r0+0x0], 6                         
    ldxb r1, [r7+0x8]                       
    stxb [r0+0x1], r1                       
    ldxw r1, [r7+0xc]                       
    jne r1, 0, lbb_7255                             if r1 != (0 as i32 as i64 as u64) { pc += 84 }
    stb [r0+0x2], 0                         
    mov64 r8, 3                                     r8 = 3 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += 73 }
lbb_7174:
    stb [r0+0x0], 17                        
    ja lbb_7246                                     if true { pc += 70 }
lbb_7176:
    stb [r0+0x0], 9                         
    ja lbb_7246                                     if true { pc += 68 }
lbb_7178:
    ldxb r1, [r7+0x8]                       
    stxb [r0+0x1], r1                       
    stb [r0+0x0], 20                        
    ldxdw r1, [r7+0x21]                     
    stxdw [r0+0x1a], r1                     
    ldxdw r1, [r7+0x19]                     
    stxdw [r0+0x12], r1                     
    ldxdw r1, [r7+0x11]                     
    stxdw [r0+0xa], r1                      
    ldxdw r1, [r7+0x9]                      
    stxdw [r0+0x2], r1                      
    ldxw r1, [r7+0x2c]                      
    jne r1, 0, lbb_7208                             if r1 != (0 as i32 as i64 as u64) { pc += 17 }
    ja lbb_7205                                     if true { pc += 13 }
lbb_7192:
    ldxb r1, [r7+0x8]                       
    stxb [r0+0x1], r1                       
    stb [r0+0x0], 0                         
    ldxdw r1, [r7+0x21]                     
    stxdw [r0+0x1a], r1                     
    ldxdw r1, [r7+0x19]                     
    stxdw [r0+0x12], r1                     
    ldxdw r1, [r7+0x11]                     
    stxdw [r0+0xa], r1                      
    ldxdw r1, [r7+0x9]                      
    stxdw [r0+0x2], r1                      
    ldxw r1, [r7+0x2c]                      
    jne r1, 0, lbb_7208                             if r1 != (0 as i32 as i64 as u64) { pc += 3 }
lbb_7205:
    stb [r0+0x22], 0                        
    mov64 r8, 35                                    r8 = 35 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += 39 }
lbb_7208:
    stb [r0+0x22], 1                        
    ldxdw r1, [r7+0x30]                     
    stxdw [r0+0x23], r1                     
    ldxdw r1, [r7+0x38]                     
    stxdw [r0+0x2b], r1                     
    ldxdw r1, [r7+0x40]                     
    stxdw [r0+0x33], r1                     
    ldxdw r1, [r7+0x48]                     
    stxdw [r0+0x3b], r1                     
    mov64 r8, 67                                    r8 = 67 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += 28 }
lbb_7219:
    stb [r0+0x0], 33                        
    ja lbb_7246                                     if true { pc += 25 }
lbb_7221:
    stb [r0+0x0], 39                        
    ja lbb_7246                                     if true { pc += 23 }
lbb_7223:
    stb [r0+0x0], 36                        
    ja lbb_7246                                     if true { pc += 21 }
lbb_7225:
    stb [r0+0x0], 42                        
    ja lbb_7246                                     if true { pc += 19 }
lbb_7227:
    stb [r0+0x0], 25                        
    ldxw r1, [r7+0x8]                       
    jne r1, 0, lbb_7266                             if r1 != (0 as i32 as i64 as u64) { pc += 36 }
    stb [r0+0x1], 0                         
    mov64 r8, 2                                     r8 = 2 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += 14 }
lbb_7233:
    stb [r0+0x0], 22                        
    ja lbb_7246                                     if true { pc += 11 }
lbb_7235:
    stb [r0+0x0], 28                        
    ja lbb_7246                                     if true { pc += 9 }
lbb_7237:
    stb [r0+0x0], 31                        
    ja lbb_7246                                     if true { pc += 7 }
lbb_7239:
    stb [r0+0x0], 34                        
    ja lbb_7246                                     if true { pc += 5 }
lbb_7241:
    stb [r0+0x0], 40                        
    ja lbb_7246                                     if true { pc += 3 }
lbb_7243:
    stb [r0+0x0], 37                        
    ja lbb_7246                                     if true { pc += 1 }
lbb_7245:
    stb [r0+0x0], 43                        
lbb_7246:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
lbb_7247:
    stxdw [r10-0x8], r8                     
lbb_7248:
    ldxdw r1, [r10-0x8]                     
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x10]                    
    stxdw [r6+0x8], r1                      
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x0], r1                      
    exit                                    
lbb_7255:
    stb [r0+0x2], 1                         
    ldxdw r1, [r7+0x10]                     
    stxdw [r0+0x3], r1                      
    ldxdw r1, [r7+0x18]                     
    stxdw [r0+0xb], r1                      
    ldxdw r1, [r7+0x20]                     
    stxdw [r0+0x13], r1                     
    ldxdw r1, [r7+0x28]                     
    stxdw [r0+0x1b], r1                     
    mov64 r8, 35                                    r8 = 35 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += -19 }
lbb_7266:
    stb [r0+0x1], 1                         
    ldxdw r1, [r7+0xc]                      
    stxdw [r0+0x2], r1                      
    ldxdw r1, [r7+0x14]                     
    stxdw [r0+0xa], r1                      
    ldxdw r1, [r7+0x1c]                     
    stxdw [r0+0x12], r1                     
    ldxdw r1, [r7+0x24]                     
    stxdw [r0+0x1a], r1                     
    mov64 r8, 34                                    r8 = 34 as i32 as i64 as u64
    ja lbb_7247                                     if true { pc += -30 }
lbb_7277:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 80                                    r2 = 80 as i32 as i64 as u64
    call function_20091                     

function_7280:
    mov64 r6, r5                                    r6 = r5
    stxdw [r10-0xa8], r4                    
    stxdw [r10-0xb0], r3                    
    mov64 r9, r2                                    r9 = r2
    stxdw [r10-0x98], r1                    
    mov64 r1, r9                                    r1 = r9
    lddw r2, 0x100033488 --> b"\x06\xdd\xf6\xe1\xeeu\x8f\xde\x18B]\xbc\xe4l\xcd\xda\xb6\x1a\xfcM\x83\xb9…        r2 load str located at 4295177352
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    ldxdw r3, [r6-0xfd8]                    
    ldxdw r2, [r6-0xfe0]                    
    ldxdw r7, [r6-0xfe8]                    
    ldxdw r8, [r6-0xff0]                    
    ldxdw r1, [r6-0xff8]                    
    stxdw [r10-0xa0], r1                    
    ldxdw r1, [r6-0x1000]                   
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    stxdw [r10-0x90], r1                    
    jeq r0, 0, lbb_7314                             if r0 == (0 as i32 as i64 as u64) { pc += 13 }
    mov64 r1, r9                                    r1 = r9
    stxdw [r10-0x88], r2                    
    lddw r2, 0x1000333c8 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295177160
    mov64 r6, r3                                    r6 = r3
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    ldxdw r1, [r10-0x90]                    
    mov64 r3, r6                                    r3 = r6
    ldxdw r2, [r10-0x88]                    
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_7351                             if r0 != (0 as i32 as i64 as u64) { pc += 37 }
lbb_7314:
    stxdw [r10-0x88], r9                    
    stxb [r10-0x40], r3                     
    stxdw [r10-0x48], r2                    
    stw [r10-0x50], 12                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -80                                   r2 += -80   ///  r2 = r2.wrapping_add(-80 as i32 as i64 as u64)
    call function_6940                      
    mov64 r6, r7                                    r6 = r7
    add64 r6, 4                                     r6 += 4   ///  r6 = r6.wrapping_add(4 as i32 as i64 as u64)
    jeq r6, 0, lbb_7359                             if r6 == (0 as i32 as i64 as u64) { pc += 33 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xb8], r1                    
    mov64 r9, r6                                    r9 = r6
    mul64 r9, 34                                    r9 *= 34   ///  r9 = r9.wrapping_mul(34 as u64)
    lddw r1, 0x3c3c3c3c3c3c3c3                      r1 load str located at 271275648142787523
    jgt r6, r1, lbb_7507                            if r6 > r1 { pc += 174 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0xb8], r1                    
    mov64 r1, r9                                    r1 = r9
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_6742                      
    jeq r0, 0, lbb_7507                             if r0 == (0 as i32 as i64 as u64) { pc += 168 }
    stxdw [r10-0x60], r0                    
    stxdw [r10-0x68], r6                    
    ldxdw r2, [r10-0xb0]                    
    ldxdw r1, [r2+0x18]                     
    stxdw [r10-0x38], r1                    
    ldxdw r1, [r2+0x10]                     
    stxdw [r10-0x40], r1                    
    ldxdw r1, [r2+0x8]                      
    stxdw [r10-0x48], r1                    
    ldxdw r1, [r2+0x0]                      
    stxdw [r10-0x50], r1                    
    ja lbb_7376                                     if true { pc += 25 }
lbb_7351:
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    ldxdw r2, [r10-0x98]                    
    stxdw [r2+0x0], r1                      
    lddw r1, 0x8000000000000006                     r1 load str located at -9223372036854775802
    stxdw [r2+0x8], r1                      
    ja lbb_7506                                     if true { pc += 147 }
lbb_7359:
    stdw [r10-0x58], 0                      
    stdw [r10-0x60], 1                      
    stdw [r10-0x68], 0                      
    ldxdw r2, [r10-0xb0]                    
    ldxdw r1, [r2+0x18]                     
    stxdw [r10-0x38], r1                    
    ldxdw r1, [r2+0x10]                     
    stxdw [r10-0x40], r1                    
    ldxdw r1, [r2+0x8]                      
    stxdw [r10-0x48], r1                    
    ldxdw r1, [r2+0x0]                      
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    call function_6896                      
    ldxdw r6, [r10-0x68]                    
    ldxdw r0, [r10-0x60]                    
lbb_7376:
    ldxdw r1, [r10-0x38]                    
    stxdw [r0+0x18], r1                     
    ldxdw r1, [r10-0x40]                    
    stxdw [r0+0x10], r1                     
    ldxdw r1, [r10-0x48]                    
    stxdw [r0+0x8], r1                      
    ldxdw r1, [r10-0x50]                    
    stxdw [r0+0x0], r1                      
    sth [r0+0x20], 256                      
    stdw [r10-0x58], 1                      
    jne r6, 1, lbb_7392                             if r6 != (1 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    call function_6896                      
    ldxdw r6, [r10-0x68]                    
    ldxdw r0, [r10-0x60]                    
lbb_7392:
    ldxdw r2, [r10-0xa8]                    
    ldxdw r1, [r2+0x18]                     
    stxdw [r0+0x3a], r1                     
    ldxdw r1, [r2+0x10]                     
    stxdw [r0+0x32], r1                     
    ldxdw r1, [r2+0x8]                      
    stxdw [r0+0x2a], r1                     
    ldxdw r1, [r2+0x0]                      
    stxdw [r0+0x22], r1                     
    sth [r0+0x42], 0                        
    stdw [r10-0x58], 2                      
    jne r6, 2, lbb_7409                             if r6 != (2 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    call function_6896                      
    ldxdw r6, [r10-0x68]                    
    ldxdw r0, [r10-0x60]                    
lbb_7409:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0x88]                    
    jeq r7, 0, lbb_7413                             if r7 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_7413:
    ldxdw r2, [r10-0x90]                    
    ldxdw r1, [r2+0x18]                     
    stxdw [r0+0x5c], r1                     
    ldxdw r1, [r2+0x10]                     
    stxdw [r0+0x54], r1                     
    ldxdw r1, [r2+0x8]                      
    stxdw [r0+0x4c], r1                     
    ldxdw r1, [r2+0x0]                      
    stxdw [r0+0x44], r1                     
    sth [r0+0x64], 256                      
    stdw [r10-0x58], 3                      
    jne r6, 3, lbb_7430                             if r6 != (3 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    call function_6896                      
    ldxdw r3, [r10-0x88]                    
    ldxdw r0, [r10-0x60]                    
lbb_7430:
    ldxdw r2, [r10-0xa0]                    
    ldxdw r1, [r2+0x18]                     
    stxdw [r0+0x7e], r1                     
    ldxdw r1, [r2+0x10]                     
    stxdw [r0+0x76], r1                     
    ldxdw r1, [r2+0x8]                      
    stxdw [r0+0x6e], r1                     
    ldxdw r1, [r2+0x0]                      
    stxdw [r0+0x66], r1                     
    stxb [r0+0x86], r9                      
    stb [r0+0x87], 0                        
    stdw [r10-0x58], 4                      
    jeq r7, 0, lbb_7481                             if r7 == (0 as i32 as i64 as u64) { pc += 38 }
    mov64 r6, 4                                     r6 = 4 as i32 as i64 as u64
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    lsh64 r7, 3                                     r7 <<= 3   ///  r7 = r7.wrapping_shl(3)
    ja lbb_7464                                     if true { pc += 17 }
lbb_7447:
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r0                                    r1 = r0
    add64 r1, r9                                    r1 += r9   ///  r1 = r1.wrapping_add(r9)
    ldxdw r2, [r10-0x38]                    
    stxdw [r1+0xa0], r2                     
    ldxdw r2, [r10-0x40]                    
    stxdw [r1+0x98], r2                     
    ldxdw r2, [r10-0x48]                    
    stxdw [r1+0x90], r2                     
    ldxdw r2, [r10-0x50]                    
    stxdw [r1+0x88], r2                     
    sth [r1+0xa8], 1                        
    add64 r9, 34                                    r9 += 34   ///  r9 = r9.wrapping_add(34 as i32 as i64 as u64)
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x58], r6                    
    add64 r7, -8                                    r7 += -8   ///  r7 = r7.wrapping_add(-8 as i32 as i64 as u64)
    jeq r7, 0, lbb_7481                             if r7 == (0 as i32 as i64 as u64) { pc += 17 }
lbb_7464:
    ldxdw r1, [r8+0x0]                      
    ldxdw r2, [r1+0x18]                     
    stxdw [r10-0x38], r2                    
    ldxdw r2, [r1+0x10]                     
    stxdw [r10-0x40], r2                    
    ldxdw r2, [r1+0x8]                      
    stxdw [r10-0x48], r2                    
    ldxdw r1, [r1+0x0]                      
    stxdw [r10-0x50], r1                    
    ldxdw r1, [r10-0x68]                    
    jne r6, r1, lbb_7447                            if r6 != r1 { pc += -28 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    call function_6896                      
    ldxdw r3, [r10-0x88]                    
    ldxdw r0, [r10-0x60]                    
    ja lbb_7447                                     if true { pc += -34 }
lbb_7481:
    ldxdw r1, [r3+0x18]                     
    stxdw [r10-0x8], r1                     
    ldxdw r1, [r3+0x10]                     
    stxdw [r10-0x10], r1                    
    ldxdw r1, [r3+0x8]                      
    stxdw [r10-0x18], r1                    
    ldxdw r1, [r3+0x0]                      
    stxdw [r10-0x20], r1                    
    ldxdw r1, [r10-0x68]                    
    stxdw [r10-0x50], r1                    
    ldxdw r1, [r10-0x60]                    
    stxdw [r10-0x48], r1                    
    ldxdw r1, [r10-0x58]                    
    stxdw [r10-0x40], r1                    
    ldxdw r1, [r10-0x80]                    
    stxdw [r10-0x38], r1                    
    ldxdw r1, [r10-0x78]                    
    stxdw [r10-0x30], r1                    
    ldxdw r1, [r10-0x70]                    
    stxdw [r10-0x28], r1                    
    mov64 r2, r10                                   r2 = r10
    add64 r2, -80                                   r2 += -80   ///  r2 = r2.wrapping_add(-80 as i32 as i64 as u64)
    ldxdw r1, [r10-0x98]                    
    mov64 r3, 80                                    r3 = 80 as i32 as i64 as u64
    call function_23152                     
lbb_7506:
    exit                                    
lbb_7507:
    ldxdw r1, [r10-0xb8]                    
    mov64 r2, r9                                    r2 = r9
    call function_20091                     

function_7510:
    jgt r3, 81, lbb_7516                            if r3 > (81 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 82                                    r1 = 82 as i32 as i64 as u64
    mov64 r2, r3                                    r2 = r3
    lddw r3, 0x100034880 --> b"\x00\x00\x00\x00\xdc5\x03\x00\x0c\x00\x00\x00\x00\x00\x00\x006\x00\x00\x0…        r3 load str located at 4295182464
    call function_22021                     
lbb_7516:
    ldxb r3, [r2+0x0]                       
    jeq r3, 1, lbb_7528                             if r3 == (1 as i32 as i64 as u64) { pc += 10 }
    jne r3, 0, lbb_7548                             if r3 != (0 as i32 as i64 as u64) { pc += 29 }
    ldxb r3, [r2+0x1]                       
    jne r3, 0, lbb_7548                             if r3 != (0 as i32 as i64 as u64) { pc += 27 }
    ldxb r3, [r2+0x2]                       
    jne r3, 0, lbb_7548                             if r3 != (0 as i32 as i64 as u64) { pc += 25 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxb r3, [r2+0x3]                       
    mov64 r5, r3                                    r5 = r3
    jeq r5, 0, lbb_7542                             if r5 == (0 as i32 as i64 as u64) { pc += 15 }
    ja lbb_7548                                     if true { pc += 20 }
lbb_7528:
    ldxb r3, [r2+0x1]                       
    jne r3, 0, lbb_7548                             if r3 != (0 as i32 as i64 as u64) { pc += 18 }
    ldxb r3, [r2+0x2]                       
    jne r3, 0, lbb_7548                             if r3 != (0 as i32 as i64 as u64) { pc += 16 }
    ldxb r3, [r2+0x3]                       
    jne r3, 0, lbb_7548                             if r3 != (0 as i32 as i64 as u64) { pc += 14 }
    ldxdw r3, [r2+0x4]                      
    ldxdw r4, [r2+0x1c]                     
    stxdw [r10-0x20], r4                    
    ldxdw r4, [r2+0x14]                     
    stxdw [r10-0x28], r4                    
    ldxdw r4, [r2+0xc]                      
    stxdw [r10-0x30], r4                    
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
lbb_7542:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxb r0, [r2+0x2c]                      
    ldxdw r5, [r2+0x24]                     
    ldxb r7, [r2+0x2d]                      
    jeq r7, 0, lbb_7554                             if r7 == (0 as i32 as i64 as u64) { pc += 7 }
    jeq r7, 1, lbb_7553                             if r7 == (1 as i32 as i64 as u64) { pc += 5 }
lbb_7548:
    lddw r2, 0x8000000000000003                     r2 load str located at -9223372036854775805
    stxdw [r1+0x8], r2                      
    stw [r1+0x0], 2                         
lbb_7552:
    exit                                    
lbb_7553:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
lbb_7554:
    ldxb r7, [r2+0x2e]                      
    jeq r7, 1, lbb_7565                             if r7 == (1 as i32 as i64 as u64) { pc += 9 }
    jne r7, 0, lbb_7548                             if r7 != (0 as i32 as i64 as u64) { pc += -9 }
    ldxb r7, [r2+0x2f]                      
    jne r7, 0, lbb_7548                             if r7 != (0 as i32 as i64 as u64) { pc += -11 }
    ldxb r7, [r2+0x30]                      
    jne r7, 0, lbb_7548                             if r7 != (0 as i32 as i64 as u64) { pc += -13 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxb r2, [r2+0x31]                      
    jeq r2, 0, lbb_7579                             if r2 == (0 as i32 as i64 as u64) { pc += 15 }
    ja lbb_7548                                     if true { pc += -17 }
lbb_7565:
    ldxb r7, [r2+0x2f]                      
    jne r7, 0, lbb_7548                             if r7 != (0 as i32 as i64 as u64) { pc += -19 }
    ldxb r7, [r2+0x30]                      
    jne r7, 0, lbb_7548                             if r7 != (0 as i32 as i64 as u64) { pc += -21 }
    ldxb r7, [r2+0x31]                      
    jne r7, 0, lbb_7548                             if r7 != (0 as i32 as i64 as u64) { pc += -23 }
    ldxdw r7, [r2+0x32]                     
    ldxdw r8, [r2+0x4a]                     
    stxdw [r10-0x8], r8                     
    ldxdw r8, [r2+0x42]                     
    stxdw [r10-0x10], r8                    
    ldxdw r2, [r2+0x3a]                     
    stxdw [r10-0x18], r2                    
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
lbb_7579:
    ldxdw r2, [r10-0x20]                    
    stxdw [r1+0x1c], r2                     
    ldxdw r2, [r10-0x28]                    
    stxdw [r1+0x14], r2                     
    ldxdw r2, [r10-0x30]                    
    stxdw [r1+0xc], r2                      
    stxdw [r1+0x38], r7                     
    stxw [r1+0x34], r8                      
    stxb [r1+0x31], r6                      
    stxb [r1+0x30], r0                      
    stxdw [r1+0x28], r5                     
    stxdw [r1+0x4], r3                      
    stxw [r1+0x0], r4                       
    ldxdw r2, [r10-0x18]                    
    stxdw [r1+0x40], r2                     
    ldxdw r2, [r10-0x10]                    
    stxdw [r1+0x48], r2                     
    ldxdw r2, [r10-0x8]                     
    stxdw [r1+0x50], r2                     
    ja lbb_7552                                     if true { pc += -47 }

function_7599:
    jgt r3, 164, lbb_7605                           if r3 > (164 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 165                                   r1 = 165 as i32 as i64 as u64
    mov64 r2, r3                                    r2 = r3
    lddw r3, 0x100034898 --> b"\x00\x00\x00\x00\xdc5\x03\x00\x0c\x00\x00\x00\x00\x00\x00\x00\x97\x00\x00…        r3 load str located at 4295182488
    call function_22021                     
lbb_7605:
    ldxdw r7, [r2+0x40]                     
    ldxb r3, [r2+0x48]                      
    jeq r3, 1, lbb_7632                             if r3 == (1 as i32 as i64 as u64) { pc += 24 }
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += 67 }
    ldxb r3, [r2+0x49]                      
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += 65 }
    ldxb r3, [r2+0x4a]                      
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += 63 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ldxb r3, [r2+0x4b]                      
    jeq r3, 0, lbb_7617                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7676                                     if true { pc += 59 }
lbb_7617:
    ldxb r8, [r2+0x6c]                      
    jlt r8, 3, lbb_7620                             if r8 < (3 as i32 as i64 as u64) { pc += 1 }
    ja lbb_7648                                     if true { pc += 28 }
lbb_7620:
    ldxb r3, [r2+0x6d]                      
    jeq r3, 1, lbb_7655                             if r3 == (1 as i32 as i64 as u64) { pc += 33 }
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += 53 }
    ldxb r3, [r2+0x6e]                      
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += 51 }
    ldxb r3, [r2+0x6f]                      
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += 49 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxb r3, [r2+0x70]                      
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r3, 0, lbb_7663                             if r3 == (0 as i32 as i64 as u64) { pc += 32 }
    ja lbb_7676                                     if true { pc += 44 }
lbb_7632:
    ldxb r3, [r2+0x49]                      
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += 42 }
    ldxb r3, [r2+0x4a]                      
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += 40 }
    ldxb r3, [r2+0x4b]                      
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += 38 }
    ldxdw r4, [r2+0x4c]                     
    ldxdw r3, [r2+0x64]                     
    stxdw [r10-0x20], r3                    
    ldxdw r3, [r2+0x5c]                     
    stxdw [r10-0x28], r3                    
    ldxdw r3, [r2+0x54]                     
    stxdw [r10-0x30], r3                    
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    ldxb r8, [r2+0x6c]                      
    jlt r8, 3, lbb_7620                             if r8 < (3 as i32 as i64 as u64) { pc += -28 }
lbb_7648:
    lddw r2, 0x8000000000000003                     r2 load str located at -9223372036854775805
    stxdw [r1+0x0], r2                      
    stb [r1+0xf], 0                         
    sth [r1+0xd], 0                         
    stw [r1+0x9], 0                         
    ja lbb_7679                                     if true { pc += 24 }
lbb_7655:
    ldxb r3, [r2+0x6e]                      
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += 19 }
    ldxb r3, [r2+0x6f]                      
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += 17 }
    ldxb r3, [r2+0x70]                      
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += 15 }
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    ldxdw r0, [r2+0x71]                     
lbb_7663:
    ldxdw r3, [r2+0x79]                     
    stxdw [r10-0x78], r3                    
    ldxb r3, [r2+0x81]                      
    jeq r3, 1, lbb_7681                             if r3 == (1 as i32 as i64 as u64) { pc += 14 }
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += 8 }
    ldxb r3, [r2+0x82]                      
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += 6 }
    ldxb r3, [r2+0x83]                      
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += 4 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    stxdw [r10-0x80], r3                    
    ldxb r3, [r2+0x84]                      
    jeq r3, 0, lbb_7697                             if r3 == (0 as i32 as i64 as u64) { pc += 21 }
lbb_7676:
    lddw r2, 0x8000000000000003                     r2 load str located at -9223372036854775805
    stxdw [r1+0x0], r2                      
lbb_7679:
    stw [r1+0x88], 2                        
lbb_7680:
    exit                                    
lbb_7681:
    ldxb r3, [r2+0x82]                      
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += -7 }
    ldxb r3, [r2+0x83]                      
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += -9 }
    ldxb r3, [r2+0x84]                      
    jne r3, 0, lbb_7676                             if r3 != (0 as i32 as i64 as u64) { pc += -11 }
    ldxdw r3, [r2+0x85]                     
    stxdw [r10-0xa0], r3                    
    ldxdw r3, [r2+0x9d]                     
    stxdw [r10-0x8], r3                     
    ldxdw r3, [r2+0x95]                     
    stxdw [r10-0x10], r3                    
    ldxdw r3, [r2+0x8d]                     
    stxdw [r10-0x18], r3                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    stxdw [r10-0x80], r3                    
lbb_7697:
    mov64 r3, r2                                    r3 = r2
    add64 r3, 32                                    r3 += 32   ///  r3 = r3.wrapping_add(32 as i32 as i64 as u64)
    stxdw [r10-0x88], r4                    
    ldxdw r4, [r2+0x18]                     
    stxdw [r10-0x58], r4                    
    ldxdw r4, [r2+0x10]                     
    stxdw [r10-0x60], r4                    
    ldxdw r4, [r2+0x8]                      
    stxdw [r10-0x68], r4                    
    ldxdw r2, [r2+0x0]                      
    stxdw [r10-0x70], r2                    
    ldxdw r2, [r3+0x0]                      
    stxdw [r10-0x50], r2                    
    ldxdw r2, [r3+0x8]                      
    stxdw [r10-0x48], r2                    
    ldxdw r2, [r3+0x10]                     
    stxdw [r10-0x40], r2                    
    ldxdw r2, [r3+0x18]                     
    stxdw [r10-0x38], r2                    
    ldxdw r2, [r10-0x20]                    
    stxdw [r1+0x64], r2                     
    ldxdw r2, [r10-0x28]                    
    stxdw [r1+0x5c], r2                     
    ldxdw r2, [r10-0x30]                    
    stxdw [r1+0x54], r2                     
    mov64 r2, r10                                   r2 = r10
    add64 r2, -112                                  r2 += -112   ///  r2 = r2.wrapping_add(-112 as i32 as i64 as u64)
    mov64 r6, r1                                    r6 = r1
    mov64 r3, 64                                    r3 = 64 as i32 as i64 as u64
    stxdw [r10-0x90], r5                    
    stxdw [r10-0x98], r0                    
    call function_23152                     
    ldxdw r1, [r10-0xa0]                    
    stxdw [r6+0x8c], r1                     
    ldxdw r1, [r10-0x80]                    
    stxw [r6+0x88], r1                      
    ldxdw r1, [r10-0x78]                    
    stxdw [r6+0x80], r1                     
    ldxdw r1, [r10-0x98]                    
    stxdw [r6+0x78], r1                     
    ldxdw r1, [r10-0x90]                    
    stxw [r6+0x70], r1                      
    stxb [r6+0x6c], r8                      
    ldxdw r1, [r10-0x88]                    
    stxdw [r6+0x4c], r1                     
    stxw [r6+0x48], r9                      
    stxdw [r6+0x40], r7                     
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x94], r1                     
    ldxdw r1, [r10-0x10]                    
    stxdw [r6+0x9c], r1                     
    ldxdw r1, [r10-0x8]                     
    stxdw [r6+0xa4], r1                     
    ja lbb_7680                                     if true { pc += -71 }

function_7751:
    mov64 r9, 10                                    r9 = 10 as i32 as i64 as u64
    ldxdw r0, [r2+0x20]                     
    jlt r0, 2, lbb_7774                             if r0 < (2 as i32 as i64 as u64) { pc += 20 }
    stxdw [r10-0xc8], r0                    
    stxdw [r10-0xa0], r1                    
    stxdw [r10-0xb0], r4                    
    stxdw [r10-0xc0], r5                    
    stxdw [r10-0xb8], r2                    
    ldxdw r8, [r2+0x18]                     
    ldxdw r2, [r8+0x0]                      
    stxdw [r10-0xa8], r3                    
    ldxdw r1, [r3+0x0]                      
    ldxdw r6, [r1+0x0]                      
    mov64 r7, r6                                    r7 = r6
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_7776                             if r0 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_7773:
    ldxdw r1, [r10-0xa0]                    
lbb_7774:
    stxw [r1+0x0], r9                       
    exit                                    
lbb_7776:
    stxdw [r10-0xd0], r8                    
    ldxb r2, [r8+0x8]                       
    mov64 r1, -120                                  r1 = -120 as i32 as i64 as u64
    jeq r2, 0, lbb_7781                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
lbb_7781:
    ldxb r2, [r6+0x0]                       
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r9, 11                                    r9 = 11 as i32 as i64 as u64
    jne r1, 0, lbb_7773                             if r1 != (0 as i32 as i64 as u64) { pc += -13 }
    ldxb r1, [r6+0x1]                       
    ldxb r2, [r6+0x2]                       
    ldxb r4, [r6+0x3]                       
    ldxdw r3, [r6+0x50]                     
    mov64 r5, r6                                    r5 = r6
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r5                    
    mov64 r5, r6                                    r5 = r6
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r5                    
    stxdw [r10-0x88], r3                    
    add64 r6, 72                                    r6 += 72   ///  r6 = r6.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r6                    
    stxdw [r10-0x98], r7                    
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_7804                             if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_7804:
    stxb [r10-0x66], r3                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0xa8]                    
    ldxdw r5, [r10-0xd0]                    
    jne r2, 0, lbb_7810                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_7810:
    stxb [r10-0x67], r4                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_7814                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_7814:
    stxb [r10-0x68], r2                     
    stdw [r10-0x70], 0                      
    ldxdw r2, [r5+0x10]                     
    ldxdw r1, [r3+0x8]                      
    ldxdw r8, [r1+0x0]                      
    mov64 r7, r8                                    r7 = r8
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    mov64 r6, r5                                    r6 = r5
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_7773                             if r0 != (0 as i32 as i64 as u64) { pc += -55 }
    ldxb r2, [r6+0x18]                      
    mov64 r1, -120                                  r1 = -120 as i32 as i64 as u64
    jeq r2, 0, lbb_7832                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
lbb_7832:
    ldxb r2, [r8+0x0]                       
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r9, 11                                    r9 = 11 as i32 as i64 as u64
    jne r1, 0, lbb_7773                             if r1 != (0 as i32 as i64 as u64) { pc += -64 }
    ldxb r3, [r8+0x1]                       
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_7842                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_7842:
    ldxb r6, [r8+0x2]                       
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ldxdw r5, [r10-0xc0]                    
    ldxdw r4, [r10-0xb0]                    
    ldxdw r3, [r10-0xb8]                    
    jne r6, 0, lbb_7849                             if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_7849:
    ldxb r6, [r8+0x3]                       
    jne r6, 0, lbb_7852                             if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_7852:
    ldxdw r6, [r8+0x50]                     
    mov64 r9, r8                                    r9 = r8
    add64 r9, 40                                    r9 += 40   ///  r9 = r9.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r9                    
    mov64 r9, r8                                    r9 = r8
    add64 r9, 88                                    r9 += 88   ///  r9 = r9.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r9                    
    stxdw [r10-0x50], r6                    
    add64 r8, 72                                    r8 += 72   ///  r8 = r8.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r8                    
    stxdw [r10-0x60], r7                    
    stxb [r10-0x2e], r2                     
    stxb [r10-0x2f], r0                     
    stxb [r10-0x30], r1                     
    stdw [r10-0x38], 0                      
    ldxdw r1, [r3+0x0]                      
    ldxdw r2, [r3+0x8]                      
    ldxdw r3, [r3+0x10]                     
    stxdw [r10-0x8], r3                     
    stxdw [r10-0x10], r2                    
    ldxdw r2, [r10-0xc8]                    
    stxdw [r10-0x18], r2                    
    ldxdw r2, [r10-0xd0]                    
    stxdw [r10-0x20], r2                    
    stxdw [r10-0x28], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -152                                  r2 += -152   ///  r2 = r2.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    syscall [invalid]                       
    mov64 r9, 26                                    r9 = 26 as i32 as i64 as u64
    ja lbb_7773                                     if true { pc += -112 }

function_7885:
    mov64 r5, r4                                    r5 = r4
    mov64 r4, r3                                    r4 = r3
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r2+0x0]                      
    ldxdw r3, [r1+0x0]                      
    mov64 r1, r3                                    r1 = r3
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x98], r1                    
    sth [r10-0x90], 257                     
    stb [r10-0x81], 0                       
    sth [r10-0x83], 0                       
    stb [r10-0x84], 1                       
    ldxdw r2, [r2+0x8]                      
    ldxdw r0, [r2+0x18]                     
    stxdw [r10-0x68], r0                    
    ldxdw r0, [r2+0x10]                     
    stxdw [r10-0x70], r0                    
    ldxdw r0, [r2+0x8]                      
    stxdw [r10-0x78], r0                    
    ldxdw r2, [r2+0x0]                      
    stxdw [r10-0x80], r2                    
    ldxb r2, [r3+0x0]                       
    jne r2, 0, lbb_7954                             if r2 != (0 as i32 as i64 as u64) { pc += 46 }
    ldxb r2, [r3+0x1]                       
    ldxb r0, [r3+0x2]                       
    ldxb r7, [r3+0x3]                       
    ldxdw r8, [r3+0x50]                     
    mov64 r9, r3                                    r9 = r3
    add64 r9, 40                                    r9 += 40   ///  r9 = r9.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r9                    
    mov64 r9, r3                                    r9 = r3
    add64 r9, 88                                    r9 += 88   ///  r9 = r9.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r9                    
    stxdw [r10-0x50], r8                    
    add64 r3, 72                                    r3 += 72   ///  r3 = r3.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r3                    
    stxdw [r10-0x60], r1                    
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r7, 0, lbb_7925                             if r7 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_7925:
    stxb [r10-0x2e], r1                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_7929                             if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_7929:
    stxb [r10-0x2f], r1                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_7933                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_7933:
    stxb [r10-0x30], r1                     
    stdw [r10-0x38], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -132                                  r1 += -132   ///  r1 = r1.wrapping_add(-132 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -152                                  r1 += -152   ///  r1 = r1.wrapping_add(-152 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100033408 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295177224
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 36                      
    stdw [r10-0x18], 1                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -96                                   r2 += -96   ///  r2 = r2.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    syscall [invalid]                       
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ja lbb_7955                                     if true { pc += 1 }
lbb_7954:
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
lbb_7955:
    stxw [r6+0x0], r1                       
    exit                                    

function_7957:
    mov64 r5, r4                                    r5 = r4
    mov64 r4, r3                                    r4 = r3
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r2+0x8]                      
    ldxdw r0, [r1+0x0]                      
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r3, [r2+0x0]                      
    ldxdw r7, [r3+0x0]                      
    stxdw [r10-0x80], r0                    
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x90], r7                    
    sth [r10-0x78], 257                     
    sth [r10-0x88], 257                     
    stw [r10-0x6c], 0                       
    ldxdw r0, [r2+0x18]                     
    stxdw [r10-0x68], r0                    
    ldxdw r0, [r2+0x20]                     
    stxdw [r10-0x60], r0                    
    ldxdw r2, [r2+0x10]                     
    ldxdw r0, [r2+0x18]                     
    stxdw [r10-0x40], r0                    
    ldxdw r0, [r2+0x10]                     
    stxdw [r10-0x48], r0                    
    ldxdw r0, [r2+0x8]                      
    stxdw [r10-0x50], r0                    
    ldxdw r2, [r2+0x0]                      
    stxdw [r10-0x58], r2                    
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    stxdw [r10-0x20], r2                    
    mov64 r2, r10                                   r2 = r10
    add64 r2, -108                                  r2 += -108   ///  r2 = r2.wrapping_add(-108 as i32 as i64 as u64)
    stxdw [r10-0x30], r2                    
    lddw r2, 0x100033408 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r2 load str located at 4295177224
    stxdw [r10-0x38], r2                    
    stdw [r10-0x18], 2                      
    stdw [r10-0x28], 52                     
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x10], r3                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -152                                  r1 += -152   ///  r1 = r1.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -56                                   r2 += -56   ///  r2 = r2.wrapping_add(-56 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -16                                   r3 += -16   ///  r3 = r3.wrapping_add(-16 as i32 as i64 as u64)
    call function_7751                      
    ldxdw r1, [r10-0x98]                    
    stxdw [r6+0x0], r1                      
    exit                                    

function_8007:
    mov64 r9, 10                                    r9 = 10 as i32 as i64 as u64
    ldxdw r0, [r2+0x20]                     
    jlt r0, 3, lbb_8030                             if r0 < (3 as i32 as i64 as u64) { pc += 20 }
    stxdw [r10-0x100], r0                   
    stxdw [r10-0xd8], r1                    
    stxdw [r10-0xe8], r4                    
    stxdw [r10-0xf8], r5                    
    stxdw [r10-0xf0], r2                    
    ldxdw r8, [r2+0x18]                     
    ldxdw r2, [r8+0x0]                      
    stxdw [r10-0xe0], r3                    
    ldxdw r1, [r3+0x0]                      
    ldxdw r6, [r1+0x0]                      
    mov64 r7, r6                                    r7 = r6
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_8032                             if r0 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_8029:
    ldxdw r1, [r10-0xd8]                    
lbb_8030:
    stxw [r1+0x0], r9                       
    exit                                    
lbb_8032:
    stxdw [r10-0x108], r8                   
    ldxb r2, [r8+0x8]                       
    mov64 r1, -120                                  r1 = -120 as i32 as i64 as u64
    jeq r2, 0, lbb_8037                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
lbb_8037:
    ldxb r2, [r6+0x0]                       
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r9, 11                                    r9 = 11 as i32 as i64 as u64
    jne r1, 0, lbb_8029                             if r1 != (0 as i32 as i64 as u64) { pc += -13 }
    ldxb r1, [r6+0x1]                       
    ldxb r2, [r6+0x2]                       
    ldxb r4, [r6+0x3]                       
    ldxdw r3, [r6+0x50]                     
    mov64 r5, r6                                    r5 = r6
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xb0], r5                    
    mov64 r5, r6                                    r5 = r6
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xb8], r5                    
    stxdw [r10-0xc0], r3                    
    add64 r6, 72                                    r6 += 72   ///  r6 = r6.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0xc8], r6                    
    stxdw [r10-0xd0], r7                    
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_8060                             if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_8060:
    stxb [r10-0x9e], r3                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0xe0]                    
    ldxdw r5, [r10-0x108]                   
    jne r2, 0, lbb_8066                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_8066:
    stxb [r10-0x9f], r4                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_8070                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_8070:
    stxb [r10-0xa0], r2                     
    stdw [r10-0xa8], 0                      
    ldxdw r2, [r5+0x10]                     
    ldxdw r1, [r3+0x8]                      
    ldxdw r6, [r1+0x0]                      
    mov64 r7, r6                                    r7 = r6
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    mov64 r8, r5                                    r8 = r5
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_8029                             if r0 != (0 as i32 as i64 as u64) { pc += -55 }
    ldxb r2, [r8+0x18]                      
    mov64 r1, -120                                  r1 = -120 as i32 as i64 as u64
    jeq r2, 0, lbb_8088                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
lbb_8088:
    ldxb r2, [r6+0x0]                       
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r9, 11                                    r9 = 11 as i32 as i64 as u64
    jne r1, 0, lbb_8029                             if r1 != (0 as i32 as i64 as u64) { pc += -64 }
    ldxb r1, [r6+0x1]                       
    ldxb r2, [r6+0x2]                       
    ldxb r4, [r6+0x3]                       
    ldxdw r3, [r6+0x50]                     
    mov64 r5, r6                                    r5 = r6
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r5                    
    mov64 r5, r6                                    r5 = r6
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r5                    
    stxdw [r10-0x88], r3                    
    add64 r6, 72                                    r6 += 72   ///  r6 = r6.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r6                    
    stxdw [r10-0x98], r7                    
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_8111                             if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_8111:
    stxb [r10-0x66], r3                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0xe0]                    
    ldxdw r5, [r10-0x108]                   
    jne r2, 0, lbb_8117                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_8117:
    stxb [r10-0x67], r4                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_8121                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_8121:
    stxb [r10-0x68], r2                     
    stdw [r10-0x70], 0                      
    ldxdw r2, [r5+0x20]                     
    ldxdw r1, [r3+0x10]                     
    ldxdw r8, [r1+0x0]                      
    mov64 r7, r8                                    r7 = r8
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    mov64 r6, r5                                    r6 = r5
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_8029                             if r0 != (0 as i32 as i64 as u64) { pc += -106 }
    ldxb r2, [r6+0x28]                      
    mov64 r1, -120                                  r1 = -120 as i32 as i64 as u64
    jeq r2, 0, lbb_8139                             if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
lbb_8139:
    ldxb r2, [r8+0x0]                       
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r9, 11                                    r9 = 11 as i32 as i64 as u64
    jne r1, 0, lbb_8029                             if r1 != (0 as i32 as i64 as u64) { pc += -115 }
    ldxb r3, [r8+0x1]                       
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_8149                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_8149:
    ldxb r6, [r8+0x2]                       
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ldxdw r5, [r10-0xf8]                    
    ldxdw r4, [r10-0xe8]                    
    ldxdw r3, [r10-0xf0]                    
    jne r6, 0, lbb_8156                             if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_8156:
    ldxb r6, [r8+0x3]                       
    jne r6, 0, lbb_8159                             if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_8159:
    ldxdw r6, [r8+0x50]                     
    mov64 r9, r8                                    r9 = r8
    add64 r9, 40                                    r9 += 40   ///  r9 = r9.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r9                    
    mov64 r9, r8                                    r9 = r8
    add64 r9, 88                                    r9 += 88   ///  r9 = r9.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r9                    
    stxdw [r10-0x50], r6                    
    add64 r8, 72                                    r8 += 72   ///  r8 = r8.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r8                    
    stxdw [r10-0x60], r7                    
    stxb [r10-0x2e], r2                     
    stxb [r10-0x2f], r0                     
    stxb [r10-0x30], r1                     
    stdw [r10-0x38], 0                      
    ldxdw r1, [r3+0x0]                      
    ldxdw r2, [r3+0x8]                      
    ldxdw r3, [r3+0x10]                     
    stxdw [r10-0x8], r3                     
    stxdw [r10-0x10], r2                    
    ldxdw r2, [r10-0x100]                   
    stxdw [r10-0x18], r2                    
    ldxdw r2, [r10-0x108]                   
    stxdw [r10-0x20], r2                    
    stxdw [r10-0x28], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -208                                  r2 += -208   ///  r2 = r2.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r3, 3                                     r3 = 3 as i32 as i64 as u64
    syscall [invalid]                       
    mov64 r9, 26                                    r9 = 26 as i32 as i64 as u64
    ja lbb_8029                                     if true { pc += -163 }

function_8192:
    mov64 r5, r4                                    r5 = r4
    mov64 r4, r3                                    r4 = r3
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r2+0x10]                     
    ldxdw r7, [r1+0x0]                      
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r3, [r2+0x0]                      
    ldxdw r8, [r3+0x0]                      
    ldxdw r0, [r2+0x8]                      
    ldxdw r9, [r0+0x0]                      
    stxdw [r10-0x60], r7                    
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x70], r9                    
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x80], r8                    
    sth [r10-0x58], 256                     
    sth [r10-0x68], 1                       
    sth [r10-0x78], 1                       
    stb [r10-0x49], 3                       
    ldxdw r2, [r2+0x18]                     
    stxdw [r10-0x48], r2                    
    mov64 r2, r10                                   r2 = r10
    add64 r2, -128                                  r2 += -128   ///  r2 = r2.wrapping_add(-128 as i32 as i64 as u64)
    stxdw [r10-0x28], r2                    
    mov64 r2, r10                                   r2 = r10
    add64 r2, -73                                   r2 += -73   ///  r2 = r2.wrapping_add(-73 as i32 as i64 as u64)
    stxdw [r10-0x38], r2                    
    lddw r2, 0x1000333c8 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r2 load str located at 4295177160
    stxdw [r10-0x40], r2                    
    stdw [r10-0x20], 3                      
    stdw [r10-0x30], 9                      
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x10], r0                    
    stxdw [r10-0x18], r3                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -136                                  r1 += -136   ///  r1 = r1.wrapping_add(-136 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -24                                   r3 += -24   ///  r3 = r3.wrapping_add(-24 as i32 as i64 as u64)
    call function_8007                      
    ldxdw r1, [r10-0x88]                    
    stxdw [r6+0x0], r1                      
    exit                                    

function_8237:
    ldxdw r0, [r1+0x40]                     
    exit                                    

function_8239:
    mov64 r6, r3                                    r6 = r3
    mov64 r7, r1                                    r7 = r1
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jeq r6, 0, lbb_8255                             if r6 == (0 as i32 as i64 as u64) { pc += 12 }
    stxdw [r10-0x8], r7                     
    mov64 r7, r2                                    r7 = r2
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jslt r6, 0, lbb_8274                            if (r6 as i64) < (0 as i32 as i64) { pc += 27 }
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_6742                      
    mov64 r8, r0                                    r8 = r0
    jeq r8, 0, lbb_8274                             if r8 == (0 as i32 as i64 as u64) { pc += 21 }
    mov64 r2, r7                                    r2 = r7
    ldxdw r7, [r10-0x8]                     
lbb_8255:
    mov64 r1, r8                                    r1 = r8
    mov64 r3, r6                                    r3 = r6
    call function_23152                     
    mov64 r1, 24                                    r1 = 24 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6742                      
    jne r0, 0, lbb_8265                             if r0 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    call function_20094                     
lbb_8265:
    stxdw [r0+0x8], r8                      
    stxdw [r0+0x10], r6                     
    stxdw [r0+0x0], r6                      
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r0                                    r2 = r0
    lddw r3, 0x1000348d0 --> b"\x00\x00\x00\x00\x00\x05\x01\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r3 load str located at 4295182544
    call function_19910                     
    exit                                    
lbb_8274:
    mov64 r1, r9                                    r1 = r9
    mov64 r2, r6                                    r2 = r6
    call function_20091                     

function_8277:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, 24                                    r1 = 24 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6742                      
    jne r0, 0, lbb_8285                             if r0 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    call function_20094                     
lbb_8285:
    ldxdw r1, [r6+0x10]                     
    stxdw [r0+0x10], r1                     
    ldxdw r1, [r6+0x8]                      
    stxdw [r0+0x8], r1                      
    ldxdw r1, [r6+0x0]                      
    stxdw [r0+0x0], r1                      
    mov64 r1, 20                                    r1 = 20 as i32 as i64 as u64
    mov64 r2, r0                                    r2 = r0
    lddw r3, 0x1000348d0 --> b"\x00\x00\x00\x00\x00\x05\x01\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r3 load str located at 4295182544
    call function_19910                     
    exit                                    

function_8297:
    ldxw r3, [r2+0x34]                      
    mov64 r4, r3                                    r4 = r3
    and64 r4, 16                                    r4 &= 16   ///  r4 = r4.and(16)
    jne r4, 0, lbb_8306                             if r4 != (0 as i32 as i64 as u64) { pc += 5 }
    and64 r3, 32                                    r3 &= 32   ///  r3 = r3.and(32)
    jeq r3, 0, lbb_8304                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8308                                     if true { pc += 4 }
lbb_8304:
    call function_22975                     
    ja lbb_8309                                     if true { pc += 3 }
lbb_8306:
    call function_22723                     
    ja lbb_8309                                     if true { pc += 1 }
lbb_8308:
    call function_22767                     
lbb_8309:
    exit                                    

function_8310:
    ldxdw r2, [r1+0x10]                     
    jeq r2, 0, lbb_8315                             if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r1, [r1+0x18]                     
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6768                      
lbb_8315:
    exit                                    

function_8316:
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_8321                             if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r1, [r1+0x8]                      
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6768                      
lbb_8321:
    exit                                    

function_8322:
    exit                                    

function_8323:
    mov64 r7, r1                                    r7 = r1
    ldxdw r1, [r7+0x8]                      
    stxdw [r10-0x8], r1                     
    ldxdw r8, [r7+0x10]                     
    jeq r8, 0, lbb_8336                             if r8 == (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r9, [r10-0x8]                     
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    lddw r6, 0x8000000000000000                     r6 load str located at -9223372036854775808
    ja lbb_8343                                     if true { pc += 10 }
lbb_8333:
    add64 r9, 40                                    r9 += 40   ///  r9 = r9.wrapping_add(40 as i32 as i64 as u64)
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    jne r8, 0, lbb_8343                             if r8 != (0 as i32 as i64 as u64) { pc += 7 }
lbb_8336:
    ldxdw r2, [r7+0x0]                      
    jeq r2, 0, lbb_8361                             if r2 == (0 as i32 as i64 as u64) { pc += 23 }
    mul64 r2, 40                                    r2 *= 40   ///  r2 = r2.wrapping_mul(40 as u64)
    ldxdw r1, [r10-0x8]                     
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_8361                                     if true { pc += 18 }
lbb_8343:
    ldxdw r2, [r9-0x8]                      
    mov64 r4, r2                                    r4 = r2
    xor64 r4, r6                                    r4 ^= r6   ///  r4 = r4.xor(r6)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 31, lbb_8350                            if r4 != (31 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_8350:
    jlt r4, 34, lbb_8352                            if r4 < (34 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_8352:
    jeq r2, 0, lbb_8333                             if r2 == (0 as i32 as i64 as u64) { pc += -20 }
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_8333                             if r1 != (0 as i32 as i64 as u64) { pc += -23 }
    ldxdw r1, [r9+0x0]                      
    lsh64 r2, 4                                     r2 <<= 4   ///  r2 = r2.wrapping_shl(4)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_8333                                     if true { pc += -28 }
lbb_8361:
    exit                                    

function_8362:
    lddw r2, 0x10003366f --> b"description() is deprecated; use DisplayFor portab"        r2 load str located at 4295177839
    stxdw [r1+0x0], r2                      
    stdw [r1+0x8], 40                       
    exit                                    

function_8367:
    stdw [r1+0x0], 0                        
    exit                                    

function_8369:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_19155                     
    ldxdw r1, [r10-0x8]                     
    ldxdw r3, [r1+0x30]                     
    ldxdw r2, [r10-0x10]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    callx r3                                
    ldxdw r1, [r10-0x20]                    
    ldxdw r2, [r10-0x18]                    
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_8384:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_19155                     
    ldxdw r1, [r10-0x8]                     
    ldxdw r3, [r1+0x30]                     
    ldxdw r2, [r10-0x10]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    callx r3                                
    ldxdw r1, [r10-0x20]                    
    ldxdw r2, [r10-0x18]                    
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_8399:
    exit                                    

function_8400:
    lddw r2, 0xb55131cab6a0bc46                     r2 load str located at -5381465333013889978
    stxdw [r1+0x8], r2                      
    lddw r2, 0x67640bae5de3f2b1                     r2 load str located at 7450092527105077937
    stxdw [r1+0x0], r2                      
    exit                                    

function_8407:
    lddw r2, 0x1b204ea073ccb1b7                     r2 load str located at 1954648689323323831
    stxdw [r1+0x8], r2                      
    lddw r2, 0xe61d2cabdb5e3d                       r2 load str located at 64771322342497853
    stxdw [r1+0x0], r2                      
    exit                                    

function_8414:
    lddw r2, 0x88861a174f9fc4a1                     r2 load str located at -8609164950249683807
    stxdw [r1+0x8], r2                      
    lddw r2, 0x8adcec2ea02f91f2                     r2 load str located at -8440611916599029262
    stxdw [r1+0x0], r2                      
    exit                                    

function_8421:
    lddw r2, 0x5bb5e6a9fa88d8f9                     r2 load str located at 6608441645963204857
    stxdw [r1+0x8], r2                      
    lddw r2, 0x9df6d35319584c1                      r2 load str located at 711407341380404417
    stxdw [r1+0x0], r2                      
    exit                                    

function_8428:
    lddw r2, 0x75d6fb826124db4f                     r2 load str located at 8491250684847774543
    stxdw [r1+0x8], r2                      
    lddw r2, 0x97144fa85f6cdcb0                     r2 load str located at -7560330289874150224
    stxdw [r1+0x0], r2                      
    exit                                    

function_8435:
    mov64 r6, r1                                    r6 = r1
    ldxdw r3, [r2+0x8]                      
    jeq r3, 0, lbb_8451                             if r3 == (0 as i32 as i64 as u64) { pc += 13 }
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    ldxdw r4, [r2+0x0]                      
    ldxb r1, [r4+0x0]                       
    stxdw [r2+0x8], r3                      
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r2+0x0], r4                      
    stxb [r10-0x59], r1                     
    jeq r1, 0, lbb_8457                             if r1 == (0 as i32 as i64 as u64) { pc += 11 }
    jeq r1, 1, lbb_8448                             if r1 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8460                                     if true { pc += 12 }
lbb_8448:
    stb [r6+0x1], 1                         
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_8485                                     if true { pc += 34 }
lbb_8451:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    stxdw [r6+0x8], r0                      
    stb [r6+0x0], 1                         
    ja lbb_8486                                     if true { pc += 29 }
lbb_8457:
    stb [r6+0x1], 0                         
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_8485                                     if true { pc += 25 }
lbb_8460:
    lddw r1, 0x100034958 --> b"\x00\x00\x00\x00\xd36\x03\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295182680
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x10002cf18 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295151384
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_20113                     
    mov64 r1, r7                                    r1 = r7
    call function_8277                      
    stxdw [r6+0x8], r0                      
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_8485:
    stxb [r6+0x0], r1                       
lbb_8486:
    exit                                    

function_8487:
    ldxdw r4, [r2+0x18]                     
    ldxdw r3, [r2+0x8]                      
    jeq r3, 1, lbb_8497                             if r3 == (1 as i32 as i64 as u64) { pc += 7 }
    jne r3, 0, lbb_8495                             if r3 != (0 as i32 as i64 as u64) { pc += 4 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jeq r4, 0, lbb_8517                             if r4 == (0 as i32 as i64 as u64) { pc += 22 }
lbb_8495:
    call function_20113                     
    ja lbb_8525                                     if true { pc += 28 }
lbb_8497:
    jeq r4, 0, lbb_8499                             if r4 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8495                                     if true { pc += -4 }
lbb_8499:
    ldxdw r2, [r2+0x0]                      
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r3, [r2+0x0]                      
    ldxdw r8, [r2+0x8]                      
    jeq r8, 0, lbb_8517                             if r8 == (0 as i32 as i64 as u64) { pc += 12 }
    stxdw [r10-0x8], r3                     
    mov64 r9, r1                                    r9 = r1
    jslt r8, 0, lbb_8526                            if (r8 as i64) < (0 as i32 as i64) { pc += 18 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r1, r8                                    r1 = r8
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_6742                      
    mov64 r6, r0                                    r6 = r0
    jeq r6, 0, lbb_8526                             if r6 == (0 as i32 as i64 as u64) { pc += 12 }
    mov64 r7, r8                                    r7 = r8
    mov64 r1, r9                                    r1 = r9
    ldxdw r3, [r10-0x8]                     
lbb_8517:
    mov64 r8, r1                                    r8 = r1
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r3                                    r2 = r3
    mov64 r3, r7                                    r3 = r7
    call function_23152                     
    stxdw [r8+0x8], r6                      
    stxdw [r8+0x10], r7                     
    stxdw [r8+0x0], r7                      
lbb_8525:
    exit                                    
lbb_8526:
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r8                                    r2 = r8
    call function_20091                     

function_8529:
    mov64 r7, r3                                    r7 = r3
    mov64 r8, r2                                    r8 = r2
    mov64 r6, r1                                    r6 = r1
    jeq r8, 0, lbb_8543                             if r8 == (0 as i32 as i64 as u64) { pc += 10 }
    ldxdw r1, [r4+0x8]                      
    jeq r1, 0, lbb_8552                             if r1 == (0 as i32 as i64 as u64) { pc += 17 }
    ldxdw r2, [r4+0x10]                     
    jne r2, 0, lbb_8546                             if r2 != (0 as i32 as i64 as u64) { pc += 9 }
    jeq r7, 0, lbb_8561                             if r7 == (0 as i32 as i64 as u64) { pc += 23 }
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r8                                    r2 = r8
    call function_6742                      
    jeq r0, 0, lbb_8557                             if r0 == (0 as i32 as i64 as u64) { pc += 15 }
    ja lbb_8563                                     if true { pc += 20 }
lbb_8543:
    stdw [r6+0x8], 0                        
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ja lbb_8566                                     if true { pc += 20 }
lbb_8546:
    ldxdw r1, [r4+0x0]                      
    mov64 r3, r8                                    r3 = r8
    mov64 r4, r7                                    r4 = r7
    call function_6769                      
    jeq r0, 0, lbb_8557                             if r0 == (0 as i32 as i64 as u64) { pc += 6 }
    ja lbb_8563                                     if true { pc += 11 }
lbb_8552:
    jeq r7, 0, lbb_8561                             if r7 == (0 as i32 as i64 as u64) { pc += 8 }
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r8                                    r2 = r8
    call function_6742                      
    jne r0, 0, lbb_8563                             if r0 != (0 as i32 as i64 as u64) { pc += 6 }
lbb_8557:
    stxdw [r6+0x10], r7                     
    stxdw [r6+0x8], r8                      
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ja lbb_8566                                     if true { pc += 5 }
lbb_8561:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r0, r8                                    r0 = r8
lbb_8563:
    stxdw [r6+0x10], r7                     
    stxdw [r6+0x8], r0                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_8566:
    stxdw [r6+0x0], r1                      
    exit                                    

function_8568:
    mov64 r6, r1                                    r6 = r1
    ldxdw r4, [r6+0x0]                      
    mov64 r3, r4                                    r3 = r4
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r3, 0, lbb_8576                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_8576:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_8608                             if r5 != (0 as i32 as i64 as u64) { pc += 30 }
    mov64 r7, r4                                    r7 = r4
    lsh64 r7, 1                                     r7 <<= 1   ///  r7 = r7.wrapping_shl(1)
    jgt r7, r3, lbb_8582                            if r7 > r3 { pc += 1 }
    mov64 r7, r3                                    r7 = r3
lbb_8582:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x333333333333334                      r3 load str located at 230584300921369396
    jlt r7, r3, lbb_8587                            if r7 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_8587:
    jgt r7, 4, lbb_8589                             if r7 > (4 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 4                                     r7 = 4 as i32 as i64 as u64
lbb_8589:
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r3, r7                                    r3 = r7
    mul64 r3, 40                                    r3 *= 40   ///  r3 = r3.wrapping_mul(40 as u64)
    jeq r4, 0, lbb_8598                             if r4 == (0 as i32 as i64 as u64) { pc += 5 }
    ldxdw r1, [r6+0x8]                      
    mul64 r4, 40                                    r4 *= 40   ///  r4 = r4.wrapping_mul(40 as u64)
    stxdw [r10-0x8], r4                     
    stxdw [r10-0x18], r1                    
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
lbb_8598:
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -24                                   r4 += -24   ///  r4 = r4.wrapping_add(-24 as i32 as i64 as u64)
    call function_8529                      
    ldxdw r1, [r10-0x30]                    
    jeq r1, 0, lbb_8609                             if r1 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r2, [r10-0x20]                    
    ldxdw r1, [r10-0x28]                    
lbb_8608:
    call function_20091                     
lbb_8609:
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x0], r7                      
    stxdw [r6+0x8], r1                      
    exit                                    

function_8613:
    mov64 r6, r1                                    r6 = r1
    ldxdw r4, [r6+0x0]                      
    mov64 r3, r4                                    r3 = r4
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r3, 0, lbb_8621                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_8621:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_8653                             if r5 != (0 as i32 as i64 as u64) { pc += 30 }
    mov64 r7, r4                                    r7 = r4
    lsh64 r7, 1                                     r7 <<= 1   ///  r7 = r7.wrapping_shl(1)
    jgt r7, r3, lbb_8627                            if r7 > r3 { pc += 1 }
    mov64 r7, r3                                    r7 = r3
lbb_8627:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x800000000000000                      r3 load str located at 576460752303423488
    jlt r7, r3, lbb_8632                            if r7 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_8632:
    jgt r7, 4, lbb_8634                             if r7 > (4 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 4                                     r7 = 4 as i32 as i64 as u64
lbb_8634:
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r3, r7                                    r3 = r7
    lsh64 r3, 4                                     r3 <<= 4   ///  r3 = r3.wrapping_shl(4)
    jeq r4, 0, lbb_8643                             if r4 == (0 as i32 as i64 as u64) { pc += 5 }
    ldxdw r1, [r6+0x8]                      
    lsh64 r4, 4                                     r4 <<= 4   ///  r4 = r4.wrapping_shl(4)
    stxdw [r10-0x8], r4                     
    stxdw [r10-0x18], r1                    
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
lbb_8643:
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -24                                   r4 += -24   ///  r4 = r4.wrapping_add(-24 as i32 as i64 as u64)
    call function_8529                      
    ldxdw r1, [r10-0x30]                    
    jeq r1, 0, lbb_8654                             if r1 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r2, [r10-0x20]                    
    ldxdw r1, [r10-0x28]                    
lbb_8653:
    call function_20091                     
lbb_8654:
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x0], r7                      
    stxdw [r6+0x8], r1                      
    exit                                    

function_8658:
    mov64 r6, r1                                    r6 = r1
    ldxdw r4, [r6+0x0]                      
    mov64 r3, r4                                    r3 = r4
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r3, 0, lbb_8666                             if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_8666:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_8698                             if r5 != (0 as i32 as i64 as u64) { pc += 30 }
    mov64 r1, r4                                    r1 = r4
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    mov64 r7, r1                                    r7 = r1
    jgt r1, r3, lbb_8673                            if r1 > r3 { pc += 1 }
    mov64 r7, r3                                    r7 = r3
lbb_8673:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x4000000000000000                     r3 load str located at 4611686018427387904
    jlt r7, r3, lbb_8679                            if r7 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_8679:
    jgt r7, 4, lbb_8681                             if r7 > (4 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 4                                     r7 = 4 as i32 as i64 as u64
lbb_8681:
    mov64 r3, r7                                    r3 = r7
    lsh64 r3, 1                                     r3 <<= 1   ///  r3 = r3.wrapping_shl(1)
    jeq r4, 0, lbb_8688                             if r4 == (0 as i32 as i64 as u64) { pc += 4 }
    ldxdw r4, [r6+0x8]                      
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r4                    
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_8688:
    stxdw [r10-0x10], r5                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -24                                   r4 += -24   ///  r4 = r4.wrapping_add(-24 as i32 as i64 as u64)
    call function_8529                      
    ldxdw r1, [r10-0x30]                    
    jeq r1, 0, lbb_8699                             if r1 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r2, [r10-0x20]                    
    ldxdw r1, [r10-0x28]                    
lbb_8698:
    call function_20091                     
lbb_8699:
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x0], r7                      
    stxdw [r6+0x8], r1                      
    exit                                    

function_8703:
    mov64 r6, r1                                    r6 = r1
    jeq r3, 0, lbb_8754                             if r3 == (0 as i32 as i64 as u64) { pc += 49 }
    ldxb r0, [r2+0x0]                       
    stxb [r10-0x59], r0                     
    jsgt r0, 1, lbb_8717                            if (r0 as i64) > (1 as i32 as i64) { pc += 9 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r7, r3                                    r7 = r3
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    jeq r0, 0, lbb_8762                             if r0 == (0 as i32 as i64 as u64) { pc += 50 }
    jeq r0, 1, lbb_8714                             if r0 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8720                                     if true { pc += 6 }
lbb_8714:
    jlt r3, 5, lbb_8754                             if r3 < (5 as i32 as i64 as u64) { pc += 39 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ja lbb_8759                                     if true { pc += 42 }
lbb_8717:
    jeq r0, 2, lbb_8744                             if r0 == (2 as i32 as i64 as u64) { pc += 26 }
    jeq r0, 3, lbb_8747                             if r0 == (3 as i32 as i64 as u64) { pc += 28 }
    jeq r0, 4, lbb_8752                             if r0 == (4 as i32 as i64 as u64) { pc += 32 }
lbb_8720:
    lddw r1, 0x100034cb0 --> b"\x00\x00\x00\x00\xb48\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295183536
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100010468 --> b"a#4\x00\x00\x00\x00\x00\xbf4\x00\x00\x00\x00\x00\x00W\x04\x00\x00\x10\x00…        r1 load str located at 4295033960
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_20113                     
    mov64 r1, r7                                    r1 = r7
    call function_8277                      
    ja lbb_8772                                     if true { pc += 28 }
lbb_8744:
    jlt r3, 5, lbb_8754                             if r3 < (5 as i32 as i64 as u64) { pc += 9 }
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    ja lbb_8759                                     if true { pc += 12 }
lbb_8747:
    jlt r3, 9, lbb_8754                             if r3 < (9 as i32 as i64 as u64) { pc += 6 }
    mov64 r1, 3                                     r1 = 3 as i32 as i64 as u64
    ldxdw r5, [r2+0x1]                      
    add64 r3, -9                                    r3 += -9   ///  r3 = r3.wrapping_add(-9 as i32 as i64 as u64)
    ja lbb_8761                                     if true { pc += 9 }
lbb_8752:
    jlt r3, 5, lbb_8754                             if r3 < (5 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8758                                     if true { pc += 4 }
lbb_8754:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    ja lbb_8772                                     if true { pc += 14 }
lbb_8758:
    mov64 r1, 4                                     r1 = 4 as i32 as i64 as u64
lbb_8759:
    ldxw r4, [r2+0x1]                       
    add64 r3, -5                                    r3 += -5   ///  r3 = r3.wrapping_add(-5 as i32 as i64 as u64)
lbb_8761:
    mov64 r7, r3                                    r7 = r3
lbb_8762:
    jne r7, 0, lbb_8767                             if r7 != (0 as i32 as i64 as u64) { pc += 4 }
    stxdw [r6+0x8], r5                      
    stxw [r6+0x4], r4                       
    stxw [r6+0x0], r1                       
    ja lbb_8774                                     if true { pc += 7 }
lbb_8767:
    mov64 r1, 21                                    r1 = 21 as i32 as i64 as u64
    lddw r2, 0x1000336f0 --> b"Not all bytes read"        r2 load str located at 4295177968
    mov64 r3, 18                                    r3 = 18 as i32 as i64 as u64
    call function_8239                      
lbb_8772:
    stxdw [r6+0x8], r0                      
    stw [r6+0x0], 5                         
lbb_8774:
    exit                                    

function_8775:
    lddw r3, 0x100034988 --> b"\x00\x00\x00\x000\x05\x01\x00\x10\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00…        r3 load str located at 4295182728
    stxdw [r1+0x8], r3                      
    add64 r2, 16                                    r2 += 16   ///  r2 = r2.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r1+0x0], r2                      
    exit                                    

function_8781:
    lddw r3, 0x100034a00 --> b"\x00\x00\x00\x00\x00\x05\x01\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r3 load str located at 4295182848
    stxdw [r1+0x8], r3                      
    add64 r2, 16                                    r2 += 16   ///  r2 = r2.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r1+0x0], r2                      
    exit                                    

function_8787:
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    exit                                    

function_8791:
    ldxdw r2, [r1+0x10]                     
    jeq r2, 0, lbb_8799                             if r2 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r3, [r1+0x18]                     
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r3                                    r1 = r3
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6768                      
    mov64 r1, r6                                    r1 = r6
lbb_8799:
    mov64 r2, 40                                    r2 = 40 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    exit                                    

function_8803:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    exit                                    

function_8805:
    lddw r3, 0x100034a78 --> b"\x00\x00\x00\x00\xd0\x04\x01\x00(\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00…        r3 load str located at 4295182968
    stxdw [r1+0x8], r3                      
    stxdw [r1+0x0], r2                      
    exit                                    

function_8810:
    lddw r3, 0x100034af0 --> b"\x00\x00\x00\x000\x05\x01\x00 \x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x0…        r3 load str located at 4295183088
    stxdw [r1+0x8], r3                      
    stxdw [r1+0x0], r2                      
    exit                                    

function_8815:
    mov64 r0, r1                                    r0 = r1
    add64 r0, 16                                    r0 += 16   ///  r0 = r0.wrapping_add(16 as i32 as i64 as u64)
    lddw r1, 0xc9756ff18f43d98c                     r1 load str located at -3930112016529499764
    jeq r3, r1, lbb_8821                            if r3 == r1 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_8821:
    lddw r1, 0xb9251a33c36c6868                     r1 load str located at -5105645792930273176
    jeq r2, r1, lbb_8825                            if r2 == r1 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_8825:
    exit                                    

function_8826:
    mov64 r0, r1                                    r0 = r1
    add64 r0, 16                                    r0 += 16   ///  r0 = r0.wrapping_add(16 as i32 as i64 as u64)
    lddw r1, 0x63eb502cd6cb5d6d                     r1 load str located at 7199936582794304877
    jeq r3, r1, lbb_8832                            if r3 == r1 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_8832:
    lddw r1, 0xb98b1b7157a64178                     r1 load str located at -5076933981314334344
    jeq r2, r1, lbb_8836                            if r2 == r1 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_8836:
    exit                                    

function_8837:
    mov64 r2, 40                                    r2 = 40 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    exit                                    

function_8841:
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    exit                                    

function_8845:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r7+0x20]                     
    stxdw [r10-0x8], r1                     
    ldxdw r1, [r7+0x18]                     
    stxdw [r10-0x10], r1                    
    ldxdw r1, [r7+0x10]                     
    stxdw [r10-0x18], r1                    
    mov64 r1, 24                                    r1 = 24 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6742                      
    mov64 r8, r0                                    r8 = r0
    jne r8, 0, lbb_8861                             if r8 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    call function_20094                     
lbb_8861:
    ldxdw r1, [r10-0x8]                     
    stxdw [r8+0x10], r1                     
    ldxdw r1, [r10-0x10]                    
    stxdw [r8+0x8], r1                      
    ldxdw r1, [r10-0x18]                    
    stxdw [r8+0x0], r1                      
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 40                                    r2 = 40 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    lddw r1, 0x100034a00 --> b"\x00\x00\x00\x00\x00\x05\x01\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r1 load str located at 4295182848
    stxdw [r6+0x8], r1                      
    stxdw [r6+0x0], r8                      
    exit                                    

function_8876:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r7+0x18]                     
    stxdw [r10-0x8], r1                     
    ldxdw r9, [r7+0x10]                     
    mov64 r1, 16                                    r1 = 16 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6742                      
    mov64 r8, r0                                    r8 = r0
    jne r8, 0, lbb_8889                             if r8 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 16                                    r2 = 16 as i32 as i64 as u64
    call function_20094                     
lbb_8889:
    ldxdw r1, [r10-0x8]                     
    stxdw [r8+0x8], r1                      
    stxdw [r8+0x0], r9                      
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    lddw r1, 0x100034988 --> b"\x00\x00\x00\x000\x05\x01\x00\x10\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00…        r1 load str located at 4295182728
    stxdw [r6+0x8], r1                      
    stxdw [r6+0x0], r8                      
    exit                                    

function_8901:
    mov64 r6, r3                                    r6 = r3
    mov64 r8, r2                                    r8 = r2
    mov64 r7, r1                                    r7 = r1
    mov64 r1, 32                                    r1 = 32 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6742                      
    jne r0, 0, lbb_8911                             if r0 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    call function_20094                     
lbb_8911:
    stxdw [r0+0x18], r8                     
    stxdw [r0+0x10], r7                     
    stxb [r0+0x8], r6                       
    lddw r1, 0x100034b48 --> b"\x00\x00\x00\x00\xb8\x13\x01\x00\x00\x00\x00\x00X\x13\x01\x00\x00\x00\x00…        r1 load str located at 4295183176
    stxdw [r0+0x0], r1                      
    exit                                    

function_8918:
    mov64 r6, r2                                    r6 = r2
    ldxdw r2, [r1+0x10]                     
    stxdw [r10-0x8], r2                     
    ldxdw r2, [r1+0x8]                      
    stxdw [r10-0x10], r2                    
    ldxdw r1, [r1+0x0]                      
    stxdw [r10-0x18], r1                    
    mov64 r1, 40                                    r1 = 40 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6742                      
    jne r0, 0, lbb_8932                             if r0 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 40                                    r2 = 40 as i32 as i64 as u64
    call function_20094                     
lbb_8932:
    stxb [r0+0x8], r6                       
    lddw r1, 0x100034b80 --> b"\x00\x00\x00\x00\xd8\x13\x01\x00\x00\x00\x00\x00\x88\x13\x01\x00\x00\x00\…        r1 load str located at 4295183232
    stxdw [r0+0x0], r1                      
    ldxdw r1, [r10-0x1f]                    
    stxdw [r0+0x9], r1                      
    ldxdw r1, [r10-0x17]                    
    stxdw [r0+0x11], r1                     
    ldxdw r1, [r10-0xf]                     
    stxdw [r0+0x19], r1                     
    ldxdw r1, [r10-0x8]                     
    stxdw [r0+0x20], r1                     
    exit                                    

function_8945:
    mov64 r2, r1                                    r2 = r1
    ldxdw r1, [r2+0x18]                     
    ldxdw r3, [r2+0x8]                      
    jeq r3, 1, lbb_8962                             if r3 == (1 as i32 as i64 as u64) { pc += 13 }
    jne r3, 0, lbb_8953                             if r3 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_8967                             if r1 == (0 as i32 as i64 as u64) { pc += 14 }
lbb_8953:
    mov64 r6, r10                                   r6 = r10
    add64 r6, -24                                   r6 += -24   ///  r6 = r6.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    call function_20113                     
    call function_19908                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r0                                    r2 = r0
    call function_8918                      
    ja lbb_8972                                     if true { pc += 10 }
lbb_8962:
    jeq r1, 0, lbb_8964                             if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_8953                                     if true { pc += -11 }
lbb_8964:
    ldxdw r1, [r2+0x0]                      
    ldxdw r6, [r1+0x8]                      
    ldxdw r7, [r1+0x0]                      
lbb_8967:
    call function_19908                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r0                                    r3 = r0
    call function_8901                      
lbb_8972:
    exit                                    

function_8973:
    call function_19166                     
    exit                                    

function_8975:
    call function_19166                     
    exit                                    

function_8977:
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r2+0x8]                      
    jlt r1, 4, lbb_10000                            if r1 < (4 as i32 as i64 as u64) { pc += 1020 }
    add64 r1, -4                                    r1 += -4   ///  r1 = r1.wrapping_add(-4 as i32 as i64 as u64)
    ldxdw r3, [r2+0x0]                      
    ldxw r4, [r3+0x0]                       
    stxdw [r2+0x8], r1                      
    add64 r3, 4                                     r3 += 4   ///  r3 = r3.wrapping_add(4 as i32 as i64 as u64)
    stxdw [r2+0x0], r3                      
    jeq r4, 0, lbb_10008                            if r4 == (0 as i32 as i64 as u64) { pc += 1021 }
    stxdw [r10-0x108], r2                   
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    stxdw [r10-0xc8], r1                    
    mov64 r7, r4                                    r7 = r4
    stxdw [r10-0x100], r4                   
    jlt r4, 102, lbb_8995                           if r4 < (102 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 102                                   r7 = 102 as i32 as i64 as u64
lbb_8995:
    stxdw [r10-0x110], r6                   
    mov64 r6, r7                                    r6 = r7
    mul64 r6, 40                                    r6 *= 40   ///  r6 = r6.wrapping_mul(40 as u64)
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6742                      
    jeq r0, 0, lbb_10187                            if r0 == (0 as i32 as i64 as u64) { pc += 1185 }
    stxdw [r10-0xa8], r0                    
    stxdw [r10-0xb0], r7                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    stdw [r10-0xa0], 0                      
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r5, [r10-0x108]                   
    ldxdw r3, [r10-0x100]                   
    ja lbb_9035                                     if true { pc += 25 }
lbb_9010:
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    mul64 r1, 40                                    r1 *= 40   ///  r1 = r1.wrapping_mul(40 as u64)
    ldxdw r2, [r10-0xa8]                    
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxdw r1, [r10-0xf0]                    
    stxdw [r2+0x20], r1                     
    ldxdw r1, [r10-0xe0]                    
    stxdw [r2+0x18], r1                     
    mov64 r1, r9                                    r1 = r9
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    stxh [r2+0x16], r1                      
    stxw [r2+0x12], r9                      
    ldxdw r1, [r10-0xd8]                    
    stxb [r2+0x11], r1                      
    stxb [r2+0x10], r7                      
    stxdw [r2+0x8], r0                      
    stxdw [r2+0x0], r8                      
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0xa0], r6                    
    mov64 r1, r4                                    r1 = r4
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jlt r1, r3, lbb_9035                            if r1 < r3 { pc += 1 }
    ja lbb_10104                                    if true { pc += 1069 }
lbb_9035:
    ldxdw r8, [r5+0x8]                      
    jeq r8, 0, lbb_10019                            if r8 == (0 as i32 as i64 as u64) { pc += 982 }
    ldxdw r9, [r5+0x0]                      
    ldxb r2, [r9+0x0]                       
    mov64 r1, r9                                    r1 = r9
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    stxb [r10-0x91], r2                     
    jsgt r2, 16, lbb_9072                           if (r2 as i64) > (16 as i32 as i64) { pc += 25 }
    jsgt r2, 7, lbb_9095                            if (r2 as i64) > (7 as i32 as i64) { pc += 47 }
    jsgt r2, 3, lbb_9181                            if (r2 as i64) > (3 as i32 as i64) { pc += 132 }
    jsgt r2, 1, lbb_9255                            if (r2 as i64) > (1 as i32 as i64) { pc += 205 }
    jeq r2, 0, lbb_9392                             if r2 == (0 as i32 as i64 as u64) { pc += 341 }
    jeq r2, 1, lbb_9053                             if r2 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9789                                     if true { pc += 736 }
lbb_9053:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 958 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 950 }
    ldxb r2, [r9+0x9]                       
    mov64 r1, r9                                    r1 = r9
    add64 r1, 10                                    r1 += 10   ///  r1 = r1.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -10                                   r1 += -10   ///  r1 = r1.wrapping_add(-10 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    stxb [r10-0x10], r2                     
    jlt r2, 2, lbb_9712                             if r2 < (2 as i32 as i64 as u64) { pc += 641 }
    ja lbb_10132                                    if true { pc += 1060 }
lbb_9072:
    jsgt r2, 24, lbb_9117                           if (r2 as i64) > (24 as i32 as i64) { pc += 44 }
    jsgt r2, 20, lbb_9211                           if (r2 as i64) > (20 as i32 as i64) { pc += 137 }
    jsgt r2, 18, lbb_9275                           if (r2 as i64) > (18 as i32 as i64) { pc += 200 }
    jeq r2, 17, lbb_9413                            if r2 == (17 as i32 as i64 as u64) { pc += 337 }
    jeq r2, 18, lbb_9078                            if r2 == (18 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9789                                     if true { pc += 711 }
lbb_9078:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 933 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 925 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000012                     r8 load str located at -9223372036854775790
    ja lbb_9765                                     if true { pc += 670 }
lbb_9095:
    jsgt r2, 11, lbb_9139                           if (r2 as i64) > (11 as i32 as i64) { pc += 43 }
    jsgt r2, 9, lbb_9352                            if (r2 as i64) > (9 as i32 as i64) { pc += 255 }
    jeq r2, 8, lbb_9564                             if r2 == (8 as i32 as i64 as u64) { pc += 466 }
    jeq r2, 9, lbb_9100                             if r2 == (9 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9789                                     if true { pc += 689 }
lbb_9100:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 911 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 903 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000009                     r8 load str located at -9223372036854775799
    ja lbb_9765                                     if true { pc += 648 }
lbb_9117:
    jsgt r2, 28, lbb_9160                           if (r2 as i64) > (28 as i32 as i64) { pc += 42 }
    jsgt r2, 26, lbb_9372                           if (r2 as i64) > (26 as i32 as i64) { pc += 253 }
    jeq r2, 25, lbb_9581                            if r2 == (25 as i32 as i64 as u64) { pc += 461 }
    jeq r2, 26, lbb_9122                            if r2 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9789                                     if true { pc += 667 }
lbb_9122:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 889 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 881 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x800000000000001a                     r8 load str located at -9223372036854775782
    ja lbb_9765                                     if true { pc += 626 }
lbb_9139:
    jsgt r2, 13, lbb_9234                           if (r2 as i64) > (13 as i32 as i64) { pc += 94 }
    jeq r2, 12, lbb_9661                            if r2 == (12 as i32 as i64 as u64) { pc += 520 }
    jeq r2, 13, lbb_9143                            if r2 == (13 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9789                                     if true { pc += 646 }
lbb_9143:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 868 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 860 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x800000000000000d                     r8 load str located at -9223372036854775795
    ja lbb_9765                                     if true { pc += 605 }
lbb_9160:
    jsgt r2, 30, lbb_9786                           if (r2 as i64) > (30 as i32 as i64) { pc += 625 }
    jeq r2, 29, lbb_9678                            if r2 == (29 as i32 as i64 as u64) { pc += 516 }
    jeq r2, 30, lbb_9164                            if r2 == (30 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9789                                     if true { pc += 625 }
lbb_9164:
    jlt r8, 9, lbb_10012                            if r8 < (9 as i32 as i64 as u64) { pc += 847 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 839 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x800000000000001e                     r8 load str located at -9223372036854775778
    ja lbb_9765                                     if true { pc += 584 }
lbb_9181:
    jsgt r2, 5, lbb_9295                            if (r2 as i64) > (5 as i32 as i64) { pc += 113 }
    jeq r2, 4, lbb_9430                             if r2 == (4 as i32 as i64 as u64) { pc += 247 }
    jeq r2, 5, lbb_9185                             if r2 == (5 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9789                                     if true { pc += 604 }
lbb_9185:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 826 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 818 }
    ldxb r7, [r9+0x9]                       
    mov64 r1, r9                                    r1 = r9
    add64 r1, 10                                    r1 += 10   ///  r1 = r1.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -10                                   r1 += -10   ///  r1 = r1.wrapping_add(-10 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 810 }
    add64 r8, -11                                   r8 += -11   ///  r8 = r8.wrapping_add(-11 as i32 as i64 as u64)
    ldxb r1, [r9+0xa]                       
    stxdw [r10-0xd8], r1                    
    stxdw [r5+0x8], r8                      
    add64 r9, 11                                    r9 += 11   ///  r9 = r9.wrapping_add(11 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000005                     r8 load str located at -9223372036854775803
    ja lbb_9606                                     if true { pc += 395 }
lbb_9211:
    jsgt r2, 22, lbb_9330                           if (r2 as i64) > (22 as i32 as i64) { pc += 118 }
    jeq r2, 21, lbb_9456                            if r2 == (21 as i32 as i64 as u64) { pc += 243 }
    jeq r2, 22, lbb_9215                            if r2 == (22 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9789                                     if true { pc += 574 }
lbb_9215:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 796 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 788 }
    ldxb r2, [r9+0x9]                       
    mov64 r1, r9                                    r1 = r9
    add64 r1, 10                                    r1 += 10   ///  r1 = r1.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -10                                   r1 += -10   ///  r1 = r1.wrapping_add(-10 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    stxb [r10-0x10], r2                     
    jlt r2, 2, lbb_9714                             if r2 < (2 as i32 as i64 as u64) { pc += 481 }
    ja lbb_10132                                    if true { pc += 898 }
lbb_9234:
    jeq r2, 14, lbb_9644                            if r2 == (14 as i32 as i64 as u64) { pc += 409 }
    jeq r2, 15, lbb_9695                            if r2 == (15 as i32 as i64 as u64) { pc += 459 }
    jeq r2, 16, lbb_9238                            if r2 == (16 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9789                                     if true { pc += 551 }
lbb_9238:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 773 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 765 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000010                     r8 load str located at -9223372036854775792
    ja lbb_9765                                     if true { pc += 510 }
lbb_9255:
    jeq r2, 2, lbb_9473                             if r2 == (2 as i32 as i64 as u64) { pc += 217 }
    jeq r2, 3, lbb_9258                             if r2 == (3 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9789                                     if true { pc += 531 }
lbb_9258:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 753 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 745 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000003                     r8 load str located at -9223372036854775805
    ja lbb_9765                                     if true { pc += 490 }
lbb_9275:
    jeq r2, 19, lbb_9490                            if r2 == (19 as i32 as i64 as u64) { pc += 214 }
    jeq r2, 20, lbb_9278                            if r2 == (20 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9789                                     if true { pc += 511 }
lbb_9278:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 733 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 725 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000014                     r8 load str located at -9223372036854775788
    ja lbb_9765                                     if true { pc += 470 }
lbb_9295:
    jeq r2, 6, lbb_9507                             if r2 == (6 as i32 as i64 as u64) { pc += 211 }
    jeq r2, 7, lbb_9298                             if r2 == (7 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9789                                     if true { pc += 491 }
lbb_9298:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 713 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 705 }
    ldxb r2, [r9+0x9]                       
    mov64 r1, r9                                    r1 = r9
    add64 r1, 10                                    r1 += 10   ///  r1 = r1.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -10                                   r1 += -10   ///  r1 = r1.wrapping_add(-10 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    stxb [r10-0x10], r2                     
    jeq r2, 0, lbb_9319                             if r2 == (0 as i32 as i64 as u64) { pc += 2 }
    jne r2, 1, lbb_10167                            if r2 != (1 as i32 as i64 as u64) { pc += 849 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
lbb_9319:
    jne r1, 0, lbb_9321                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_10012                                    if true { pc += 691 }
lbb_9321:
    stxdw [r10-0xd8], r7                    
    add64 r8, -11                                   r8 += -11   ///  r8 = r8.wrapping_add(-11 as i32 as i64 as u64)
    ldxb r7, [r9+0xa]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 11                                    r9 += 11   ///  r9 = r9.wrapping_add(11 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000007                     r8 load str located at -9223372036854775801
    ja lbb_9765                                     if true { pc += 435 }
lbb_9330:
    jeq r2, 23, lbb_9533                            if r2 == (23 as i32 as i64 as u64) { pc += 202 }
    jeq r2, 24, lbb_9333                            if r2 == (24 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9789                                     if true { pc += 456 }
lbb_9333:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 678 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 670 }
    ldxb r2, [r9+0x9]                       
    mov64 r1, r9                                    r1 = r9
    add64 r1, 10                                    r1 += 10   ///  r1 = r1.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -10                                   r1 += -10   ///  r1 = r1.wrapping_add(-10 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    stxb [r10-0x10], r2                     
    jlt r2, 2, lbb_9716                             if r2 < (2 as i32 as i64 as u64) { pc += 365 }
    ja lbb_10132                                    if true { pc += 780 }
lbb_9352:
    jeq r2, 10, lbb_9608                            if r2 == (10 as i32 as i64 as u64) { pc += 255 }
    jeq r2, 11, lbb_9355                            if r2 == (11 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9789                                     if true { pc += 434 }
lbb_9355:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 656 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 648 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x800000000000000b                     r8 load str located at -9223372036854775797
    ja lbb_9765                                     if true { pc += 393 }
lbb_9372:
    jeq r2, 27, lbb_9627                            if r2 == (27 as i32 as i64 as u64) { pc += 254 }
    jeq r2, 28, lbb_9375                            if r2 == (28 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9789                                     if true { pc += 414 }
lbb_9375:
    jlt r8, 9, lbb_10012                            if r8 < (9 as i32 as i64 as u64) { pc += 636 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 628 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x800000000000001c                     r8 load str located at -9223372036854775780
    ja lbb_9765                                     if true { pc += 373 }
lbb_9392:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 619 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 611 }
    ldxb r2, [r9+0x9]                       
    mov64 r1, r9                                    r1 = r9
    add64 r1, 10                                    r1 += 10   ///  r1 = r1.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -10                                   r1 += -10   ///  r1 = r1.wrapping_add(-10 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    stxb [r10-0x10], r2                     
    jlt r2, 2, lbb_9411                             if r2 < (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_10132                                    if true { pc += 721 }
lbb_9411:
    jne r1, 0, lbb_9747                             if r1 != (0 as i32 as i64 as u64) { pc += 335 }
    ja lbb_10012                                    if true { pc += 599 }
lbb_9413:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 598 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 590 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000011                     r8 load str located at -9223372036854775791
    ja lbb_9765                                     if true { pc += 335 }
lbb_9430:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 581 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 573 }
    ldxb r7, [r9+0x9]                       
    mov64 r1, r9                                    r1 = r9
    add64 r1, 10                                    r1 += 10   ///  r1 = r1.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -10                                   r1 += -10   ///  r1 = r1.wrapping_add(-10 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 565 }
    add64 r8, -11                                   r8 += -11   ///  r8 = r8.wrapping_add(-11 as i32 as i64 as u64)
    ldxb r1, [r9+0xa]                       
    stxdw [r10-0xd8], r1                    
    stxdw [r5+0x8], r8                      
    add64 r9, 11                                    r9 += 11   ///  r9 = r9.wrapping_add(11 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000004                     r8 load str located at -9223372036854775804
    ja lbb_9606                                     if true { pc += 150 }
lbb_9456:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 555 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 547 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000015                     r8 load str located at -9223372036854775787
    ja lbb_9765                                     if true { pc += 292 }
lbb_9473:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 538 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 530 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000002                     r8 load str located at -9223372036854775806
    ja lbb_9765                                     if true { pc += 275 }
lbb_9490:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 521 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 513 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000013                     r8 load str located at -9223372036854775789
    ja lbb_9765                                     if true { pc += 258 }
lbb_9507:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 504 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 496 }
    ldxb r7, [r9+0x9]                       
    mov64 r1, r9                                    r1 = r9
    add64 r1, 10                                    r1 += 10   ///  r1 = r1.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -10                                   r1 += -10   ///  r1 = r1.wrapping_add(-10 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 488 }
    add64 r8, -11                                   r8 += -11   ///  r8 = r8.wrapping_add(-11 as i32 as i64 as u64)
    ldxb r1, [r9+0xa]                       
    stxdw [r10-0xd8], r1                    
    stxdw [r5+0x8], r8                      
    add64 r9, 11                                    r9 += 11   ///  r9 = r9.wrapping_add(11 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000006                     r8 load str located at -9223372036854775802
    ja lbb_9606                                     if true { pc += 73 }
lbb_9533:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 478 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 470 }
    ldxdw r7, [r9+0x9]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 17                                    r1 += 17   ///  r1 = r1.wrapping_add(17 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -17                                   r1 += -17   ///  r1 = r1.wrapping_add(-17 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 462 }
    add64 r8, -18                                   r8 += -18   ///  r8 = r8.wrapping_add(-18 as i32 as i64 as u64)
    ldxb r1, [r9+0x11]                      
    stxdw [r10-0xe0], r1                    
    stxdw [r5+0x8], r8                      
    add64 r9, 18                                    r9 += 18   ///  r9 = r9.wrapping_add(18 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000017                     r8 load str located at -9223372036854775785
    mov64 r9, r7                                    r9 = r7
    rsh64 r9, 16                                    r9 >>= 16   ///  r9 = r9.wrapping_shr(16)
    mov64 r1, r7                                    r1 = r7
    rsh64 r1, 8                                     r1 >>= 8   ///  r1 = r1.wrapping_shr(8)
    stxdw [r10-0xd8], r1                    
    ja lbb_9765                                     if true { pc += 201 }
lbb_9564:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 447 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 439 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000008                     r8 load str located at -9223372036854775800
    ja lbb_9765                                     if true { pc += 184 }
lbb_9581:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 430 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 422 }
    ldxb r7, [r9+0x9]                       
    mov64 r1, r9                                    r1 = r9
    add64 r1, 10                                    r1 += 10   ///  r1 = r1.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -10                                   r1 += -10   ///  r1 = r1.wrapping_add(-10 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 414 }
    add64 r8, -11                                   r8 += -11   ///  r8 = r8.wrapping_add(-11 as i32 as i64 as u64)
    ldxb r1, [r9+0xa]                       
    stxdw [r10-0xd8], r1                    
    stxdw [r5+0x8], r8                      
    add64 r9, 11                                    r9 += 11   ///  r9 = r9.wrapping_add(11 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000019                     r8 load str located at -9223372036854775783
lbb_9606:
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ja lbb_9765                                     if true { pc += 157 }
lbb_9608:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 403 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 395 }
    ldxb r2, [r9+0x9]                       
    mov64 r1, r9                                    r1 = r9
    add64 r1, 10                                    r1 += 10   ///  r1 = r1.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -10                                   r1 += -10   ///  r1 = r1.wrapping_add(-10 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    stxb [r10-0x10], r2                     
    jlt r2, 2, lbb_9718                             if r2 < (2 as i32 as i64 as u64) { pc += 92 }
    ja lbb_10132                                    if true { pc += 505 }
lbb_9627:
    jlt r8, 9, lbb_10012                            if r8 < (9 as i32 as i64 as u64) { pc += 384 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 376 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x800000000000001b                     r8 load str located at -9223372036854775781
    ja lbb_9765                                     if true { pc += 121 }
lbb_9644:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 367 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 359 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x800000000000000e                     r8 load str located at -9223372036854775794
    ja lbb_9765                                     if true { pc += 104 }
lbb_9661:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 350 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 342 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x800000000000000c                     r8 load str located at -9223372036854775796
    ja lbb_9765                                     if true { pc += 87 }
lbb_9678:
    jlt r8, 9, lbb_10012                            if r8 < (9 as i32 as i64 as u64) { pc += 333 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 325 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x800000000000001d                     r8 load str located at -9223372036854775779
    ja lbb_9765                                     if true { pc += 70 }
lbb_9695:
    jlt r1, 8, lbb_10012                            if r1 < (8 as i32 as i64 as u64) { pc += 316 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 308 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x800000000000000f                     r8 load str located at -9223372036854775793
    ja lbb_9765                                     if true { pc += 53 }
lbb_9712:
    jne r1, 0, lbb_9720                             if r1 != (0 as i32 as i64 as u64) { pc += 7 }
    ja lbb_10012                                    if true { pc += 298 }
lbb_9714:
    jne r1, 0, lbb_9729                             if r1 != (0 as i32 as i64 as u64) { pc += 14 }
    ja lbb_10012                                    if true { pc += 296 }
lbb_9716:
    jne r1, 0, lbb_9738                             if r1 != (0 as i32 as i64 as u64) { pc += 21 }
    ja lbb_10012                                    if true { pc += 294 }
lbb_9718:
    jne r1, 0, lbb_9756                             if r1 != (0 as i32 as i64 as u64) { pc += 37 }
    ja lbb_10012                                    if true { pc += 292 }
lbb_9720:
    stxdw [r10-0xd8], r2                    
    add64 r8, -11                                   r8 += -11   ///  r8 = r8.wrapping_add(-11 as i32 as i64 as u64)
    ldxb r7, [r9+0xa]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 11                                    r9 += 11   ///  r9 = r9.wrapping_add(11 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000001                     r8 load str located at -9223372036854775807
    ja lbb_9765                                     if true { pc += 36 }
lbb_9729:
    stxdw [r10-0xd8], r2                    
    add64 r8, -11                                   r8 += -11   ///  r8 = r8.wrapping_add(-11 as i32 as i64 as u64)
    ldxb r7, [r9+0xa]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 11                                    r9 += 11   ///  r9 = r9.wrapping_add(11 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000016                     r8 load str located at -9223372036854775786
    ja lbb_9765                                     if true { pc += 27 }
lbb_9738:
    stxdw [r10-0xd8], r2                    
    add64 r8, -11                                   r8 += -11   ///  r8 = r8.wrapping_add(-11 as i32 as i64 as u64)
    ldxb r7, [r9+0xa]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 11                                    r9 += 11   ///  r9 = r9.wrapping_add(11 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000018                     r8 load str located at -9223372036854775784
    ja lbb_9765                                     if true { pc += 18 }
lbb_9747:
    stxdw [r10-0xd8], r2                    
    add64 r8, -11                                   r8 += -11   ///  r8 = r8.wrapping_add(-11 as i32 as i64 as u64)
    ldxb r7, [r9+0xa]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 11                                    r9 += 11   ///  r9 = r9.wrapping_add(11 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000000                     r8 load str located at -9223372036854775808
    ja lbb_9765                                     if true { pc += 9 }
lbb_9756:
    stxdw [r10-0xd8], r2                    
    add64 r8, -11                                   r8 += -11   ///  r8 = r8.wrapping_add(-11 as i32 as i64 as u64)
    ldxb r7, [r9+0xa]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 11                                    r9 += 11   ///  r9 = r9.wrapping_add(11 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x800000000000000a                     r8 load str located at -9223372036854775798
    ja lbb_9765                                     if true { pc += 0 }
lbb_9765:
    ldxdw r1, [r10-0xb0]                    
    jne r6, r1, lbb_9010                            if r6 != r1 { pc += -757 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    stxdw [r10-0xe8], r8                    
    stxdw [r10-0xf8], r9                    
    mov64 r9, r4                                    r9 = r4
    stxdw [r10-0xd0], r7                    
    mov64 r7, r0                                    r7 = r0
    mov64 r8, r6                                    r8 = r6
    mov64 r6, r5                                    r6 = r5
    call function_8568                      
    mov64 r5, r6                                    r5 = r6
    mov64 r6, r8                                    r6 = r8
    mov64 r0, r7                                    r0 = r7
    ldxdw r7, [r10-0xd0]                    
    mov64 r4, r9                                    r4 = r9
    ldxdw r9, [r10-0xf8]                    
    ldxdw r8, [r10-0xe8]                    
    ldxdw r3, [r10-0x100]                   
    ja lbb_9010                                     if true { pc += -776 }
lbb_9786:
    jeq r2, 31, lbb_9807                            if r2 == (31 as i32 as i64 as u64) { pc += 20 }
    jeq r2, 32, lbb_9936                            if r2 == (32 as i32 as i64 as u64) { pc += 148 }
    jeq r2, 33, lbb_9919                            if r2 == (33 as i32 as i64 as u64) { pc += 130 }
lbb_9789:
    lddw r1, 0x100034cb0 --> b"\x00\x00\x00\x00\xb48\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295183536
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100010468 --> b"a#4\x00\x00\x00\x00\x00\xbf4\x00\x00\x00\x00\x00\x00W\x04\x00\x00\x10\x00…        r1 load str located at 4295033960
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -145                                  r1 += -145   ///  r1 = r1.wrapping_add(-145 as i32 as i64 as u64)
    stxdw [r10-0x58], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r6, r10                                   r6 = r10
    add64 r6, -144                                  r6 += -144   ///  r6 = r6.wrapping_add(-144 as i32 as i64 as u64)
    ja lbb_10149                                    if true { pc += 342 }
lbb_9807:
    jlt r8, 5, lbb_10012                            if r8 < (5 as i32 as i64 as u64) { pc += 204 }
    add64 r8, -5                                    r8 += -5   ///  r8 = r8.wrapping_add(-5 as i32 as i64 as u64)
    ldxw r1, [r9+0x1]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 5                                     r9 += 5   ///  r9 = r9.wrapping_add(5 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    jeq r1, 0, lbb_9953                             if r1 == (0 as i32 as i64 as u64) { pc += 139 }
    stxdw [r10-0xf0], r4                    
    mov64 r7, r1                                    r7 = r1
    stxdw [r10-0xe0], r1                    
    jlt r1, 256, lbb_9819                           if r1 < (256 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 256                                   r7 = 256 as i32 as i64 as u64
lbb_9819:
    mov64 r6, r7                                    r6 = r7
    lsh64 r6, 4                                     r6 <<= 4   ///  r6 = r6.wrapping_shl(4)
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6742                      
    jeq r0, 0, lbb_10187                            if r0 == (0 as i32 as i64 as u64) { pc += 362 }
    stxdw [r10-0x70], r0                    
    stxdw [r10-0x78], r7                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    stdw [r10-0x68], 0                      
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0xe0]                    
    ja lbb_9843                                     if true { pc += 11 }
lbb_9832:
    ldxdw r2, [r10-0xd0]                    
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    mov64 r1, r0                                    r1 = r0
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    stxdw [r1+0x8], r3                      
    stxdw [r1+0x0], r7                      
    mov64 r7, r2                                    r7 = r2
    stxdw [r10-0x68], r7                    
    add64 r6, 16                                    r6 += 16   ///  r6 = r6.wrapping_add(16 as i32 as i64 as u64)
    jlt r7, r4, lbb_9843                            if r7 < r4 { pc += 1 }
    ja lbb_9959                                     if true { pc += 116 }
lbb_9843:
    jeq r8, 0, lbb_10119                            if r8 == (0 as i32 as i64 as u64) { pc += 275 }
    stxdw [r10-0xd0], r7                    
    mov64 r1, r8                                    r1 = r8
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r7, [r9+0x0]                       
    ldxdw r3, [r10-0x108]                   
    stxdw [r3+0x8], r1                      
    mov64 r2, r9                                    r2 = r9
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r3+0x0], r2                      
    stxb [r10-0x59], r7                     
    jsgt r7, 2, lbb_9865                            if (r7 as i64) > (2 as i32 as i64) { pc += 10 }
    mov64 r9, r2                                    r9 = r2
    mov64 r8, r1                                    r8 = r1
    jeq r7, 0, lbb_9906                             if r7 == (0 as i32 as i64 as u64) { pc += 48 }
    jeq r7, 1, lbb_9863                             if r7 == (1 as i32 as i64 as u64) { pc += 4 }
    jeq r7, 2, lbb_9861                             if r7 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_9868                                     if true { pc += 7 }
lbb_9861:
    mov64 r7, 2                                     r7 = 2 as i32 as i64 as u64
    ja lbb_9904                                     if true { pc += 41 }
lbb_9863:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ja lbb_9904                                     if true { pc += 39 }
lbb_9865:
    jeq r7, 3, lbb_9894                             if r7 == (3 as i32 as i64 as u64) { pc += 28 }
    jeq r7, 4, lbb_9903                             if r7 == (4 as i32 as i64 as u64) { pc += 36 }
    jeq r7, 5, lbb_9892                             if r7 == (5 as i32 as i64 as u64) { pc += 24 }
lbb_9868:
    lddw r1, 0x100034cb0 --> b"\x00\x00\x00\x00\xb48\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295183536
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100010468 --> b"a#4\x00\x00\x00\x00\x00\xbf4\x00\x00\x00\x00\x00\x00W\x04\x00\x00\x10\x00…        r1 load str located at 4295033960
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r6, r10                                   r6 = r10
    add64 r6, -88                                   r6 += -88   ///  r6 = r6.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    call function_20113                     
    mov64 r1, r6                                    r1 = r6
    call function_8277                      
    ja lbb_10122                                    if true { pc += 230 }
lbb_9892:
    mov64 r7, 5                                     r7 = 5 as i32 as i64 as u64
    ja lbb_9904                                     if true { pc += 10 }
lbb_9894:
    jlt r8, 9, lbb_10119                            if r8 < (9 as i32 as i64 as u64) { pc += 224 }
    add64 r8, -9                                    r8 += -9   ///  r8 = r8.wrapping_add(-9 as i32 as i64 as u64)
    ldxdw r3, [r9+0x1]                      
    ldxdw r1, [r10-0x108]                   
    stxdw [r1+0x8], r8                      
    add64 r9, 9                                     r9 += 9   ///  r9 = r9.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r1+0x0], r9                      
    mov64 r7, 3                                     r7 = 3 as i32 as i64 as u64
    ja lbb_9906                                     if true { pc += 3 }
lbb_9903:
    mov64 r7, 4                                     r7 = 4 as i32 as i64 as u64
lbb_9904:
    mov64 r9, r2                                    r9 = r2
    mov64 r8, r1                                    r8 = r1
lbb_9906:
    ldxdw r1, [r10-0x78]                    
    ldxdw r2, [r10-0xd0]                    
    jne r2, r1, lbb_9832                            if r2 != r1 { pc += -77 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -120                                  r1 += -120   ///  r1 = r1.wrapping_add(-120 as i32 as i64 as u64)
    stxdw [r10-0xd8], r9                    
    mov64 r9, r3                                    r9 = r3
    call function_8613                      
    mov64 r3, r9                                    r3 = r9
    ldxdw r9, [r10-0xd8]                    
    ldxdw r4, [r10-0xe0]                    
    ldxdw r0, [r10-0x70]                    
    ja lbb_9832                                     if true { pc += -87 }
lbb_9919:
    jlt r8, 9, lbb_10012                            if r8 < (9 as i32 as i64 as u64) { pc += 92 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 84 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000021                     r8 load str located at -9223372036854775775
    ja lbb_9765                                     if true { pc += -171 }
lbb_9936:
    jlt r8, 9, lbb_10012                            if r8 < (9 as i32 as i64 as u64) { pc += 75 }
    ldxdw r0, [r9+0x1]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10012                            if r1 == (0 as i32 as i64 as u64) { pc += 67 }
    add64 r8, -10                                   r8 += -10   ///  r8 = r8.wrapping_add(-10 as i32 as i64 as u64)
    ldxb r7, [r9+0x9]                       
    stxdw [r5+0x8], r8                      
    add64 r9, 10                                    r9 += 10   ///  r9 = r9.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    lddw r8, 0x8000000000000020                     r8 load str located at -9223372036854775776
    ja lbb_9765                                     if true { pc += -188 }
lbb_9953:
    mov64 r0, 8                                     r0 = 8 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xe8], r1                    
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jgt r8, 7, lbb_9973                             if r8 > (7 as i32 as i64 as u64) { pc += 15 }
    ja lbb_10012                                    if true { pc += 53 }
lbb_9959:
    ldxdw r0, [r10-0x70]                    
    ldxdw r9, [r10-0x78]                    
    mov64 r8, r0                                    r8 = r0
    ldxdw r6, [r10-0x110]                   
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    jeq r9, r1, lbb_10017                           if r9 == r1 { pc += 51 }
    ldxdw r5, [r10-0x108]                   
    ldxdw r8, [r5+0x8]                      
    jlt r8, 8, lbb_10176                            if r8 < (8 as i32 as i64 as u64) { pc += 207 }
    stxdw [r10-0xe8], r9                    
    ldxdw r9, [r5+0x0]                      
    ldxdw r3, [r10-0x100]                   
    ldxdw r4, [r10-0xf0]                    
lbb_9973:
    ldxdw r2, [r9+0x0]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r5+0x0], r1                      
    mov64 r1, r8                                    r1 = r8
    add64 r1, -8                                    r1 += -8   ///  r1 = r1.wrapping_add(-8 as i32 as i64 as u64)
    stxdw [r5+0x8], r1                      
    jeq r1, 0, lbb_10156                            if r1 == (0 as i32 as i64 as u64) { pc += 175 }
    stxdw [r10-0xe0], r2                    
    add64 r8, -9                                    r8 += -9   ///  r8 = r8.wrapping_add(-9 as i32 as i64 as u64)
    ldxb r1, [r9+0x8]                       
    stxdw [r10-0xf0], r1                    
    stxdw [r5+0x8], r8                      
    add64 r9, 9                                     r9 += 9   ///  r9 = r9.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r5+0x0], r9                      
    ldxdw r6, [r10-0x110]                   
    lddw r1, 0x8000000000000022                     r1 load str located at -9223372036854775774
    ldxdw r8, [r10-0xe8]                    
    jeq r8, r1, lbb_10023                           if r8 == r1 { pc += 30 }
    ldxdw r6, [r10-0xa0]                    
    mov64 r1, r7                                    r1 = r7
    rsh64 r1, 8                                     r1 >>= 8   ///  r1 = r1.wrapping_shr(8)
    stxdw [r10-0xd8], r1                    
    mov64 r9, r7                                    r9 = r7
    rsh64 r9, 16                                    r9 >>= 16   ///  r9 = r9.wrapping_shr(16)
    ja lbb_9765                                     if true { pc += -235 }
lbb_10000:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    stxdw [r6+0x8], r0                      
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    stxdw [r6+0x0], r1                      
    ja lbb_10066                                    if true { pc += 58 }
lbb_10008:
    stdw [r6+0x10], 0                       
    stdw [r6+0x8], 8                        
    stdw [r6+0x0], 0                        
    ja lbb_10066                                    if true { pc += 54 }
lbb_10012:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
lbb_10015:
    mov64 r8, r0                                    r8 = r0
    ldxdw r6, [r10-0x110]                   
lbb_10017:
    mov64 r0, r8                                    r0 = r8
    ja lbb_10023                                    if true { pc += 4 }
lbb_10019:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    ldxdw r6, [r10-0x110]                   
lbb_10023:
    lddw r7, 0x8000000000000000                     r7 load str located at -9223372036854775808
    stxdw [r6+0x0], r7                      
    stxdw [r6+0x8], r0                      
    ldxdw r6, [r10-0xa8]                    
    ldxdw r8, [r10-0xa0]                    
    jeq r8, 0, lbb_10036                            if r8 == (0 as i32 as i64 as u64) { pc += 6 }
    mov64 r9, r6                                    r9 = r6
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_10043                                    if true { pc += 10 }
lbb_10033:
    add64 r9, 40                                    r9 += 40   ///  r9 = r9.wrapping_add(40 as i32 as i64 as u64)
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    jne r8, 0, lbb_10043                            if r8 != (0 as i32 as i64 as u64) { pc += 7 }
lbb_10036:
    ldxdw r2, [r10-0xb0]                    
    jeq r2, 0, lbb_10061                            if r2 == (0 as i32 as i64 as u64) { pc += 23 }
    mul64 r2, 40                                    r2 *= 40   ///  r2 = r2.wrapping_mul(40 as u64)
    mov64 r1, r6                                    r1 = r6
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_10061                                    if true { pc += 18 }
lbb_10043:
    ldxdw r2, [r9-0x8]                      
    mov64 r4, r2                                    r4 = r2
    xor64 r4, r7                                    r4 ^= r7   ///  r4 = r4.xor(r7)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 31, lbb_10050                           if r4 != (31 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_10050:
    jlt r4, 34, lbb_10052                           if r4 < (34 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_10052:
    jeq r2, 0, lbb_10033                            if r2 == (0 as i32 as i64 as u64) { pc += -20 }
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_10033                            if r1 != (0 as i32 as i64 as u64) { pc += -23 }
    ldxdw r1, [r9+0x0]                      
    lsh64 r2, 4                                     r2 <<= 4   ///  r2 = r2.wrapping_shl(4)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_10033                                    if true { pc += -28 }
lbb_10061:
    ldxdw r6, [r10-0xc8]                    
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    jeq r6, r1, lbb_10066                           if r6 == r1 { pc += 1 }
    ja lbb_10067                                    if true { pc += 1 }
lbb_10066:
    exit                                    
lbb_10067:
    ldxdw r1, [r10-0xc0]                    
    stxdw [r10-0xd0], r1                    
    ldxdw r8, [r10-0xb8]                    
    jeq r8, 0, lbb_10079                            if r8 == (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r9, [r10-0xd0]                    
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    lddw r7, 0x8000000000000000                     r7 load str located at -9223372036854775808
    ja lbb_10086                                    if true { pc += 10 }
lbb_10076:
    add64 r9, 40                                    r9 += 40   ///  r9 = r9.wrapping_add(40 as i32 as i64 as u64)
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    jne r8, 0, lbb_10086                            if r8 != (0 as i32 as i64 as u64) { pc += 7 }
lbb_10079:
    jeq r6, 0, lbb_10066                            if r6 == (0 as i32 as i64 as u64) { pc += -14 }
    mul64 r6, 40                                    r6 *= 40   ///  r6 = r6.wrapping_mul(40 as u64)
    ldxdw r1, [r10-0xd0]                    
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_10066                                    if true { pc += -20 }
lbb_10086:
    ldxdw r2, [r9-0x8]                      
    mov64 r4, r2                                    r4 = r2
    xor64 r4, r7                                    r4 ^= r7   ///  r4 = r4.xor(r7)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 31, lbb_10093                           if r4 != (31 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_10093:
    jlt r4, 34, lbb_10095                           if r4 < (34 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_10095:
    jeq r2, 0, lbb_10076                            if r2 == (0 as i32 as i64 as u64) { pc += -20 }
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_10076                            if r1 != (0 as i32 as i64 as u64) { pc += -23 }
    ldxdw r1, [r9+0x0]                      
    lsh64 r2, 4                                     r2 <<= 4   ///  r2 = r2.wrapping_shl(4)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_10076                                    if true { pc += -28 }
lbb_10104:
    ldxdw r1, [r10-0xa0]                    
    ldxdw r2, [r10-0x110]                   
    stxdw [r2+0x10], r1                     
    ldxdw r1, [r10-0xa8]                    
    stxdw [r2+0x8], r1                      
    ldxdw r1, [r10-0xb0]                    
    stxdw [r2+0x0], r1                      
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    ldxdw r2, [r10-0xc8]                    
    jeq r2, r1, lbb_10066                           if r2 == r1 { pc += -49 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -200                                  r1 += -200   ///  r1 = r1.wrapping_add(-200 as i32 as i64 as u64)
    call function_8323                      
    ja lbb_10066                                    if true { pc += -53 }
lbb_10119:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
lbb_10122:
    mov64 r8, r0                                    r8 = r0
    ldxdw r2, [r10-0x78]                    
    ldxdw r6, [r10-0x110]                   
    jeq r2, 0, lbb_10017                            if r2 == (0 as i32 as i64 as u64) { pc += -109 }
    lsh64 r2, 4                                     r2 <<= 4   ///  r2 = r2.wrapping_shl(4)
    ldxdw r1, [r10-0x70]                    
lbb_10128:
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    mov64 r0, r8                                    r0 = r8
    ja lbb_10023                                    if true { pc += -109 }
lbb_10132:
    lddw r1, 0x100034958 --> b"\x00\x00\x00\x00\xd36\x03\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295182680
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -120                                  r1 += -120   ///  r1 = r1.wrapping_add(-120 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x10002cf18 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295151384
lbb_10140:
    stxdw [r10-0x70], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x78], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r6, r10                                   r6 = r10
    add64 r6, -88                                   r6 += -88   ///  r6 = r6.wrapping_add(-88 as i32 as i64 as u64)
lbb_10149:
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    call function_20113                     
    mov64 r1, r6                                    r1 = r6
    call function_8277                      
    ja lbb_10015                                    if true { pc += -141 }
lbb_10156:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    mov64 r6, r0                                    r6 = r0
    call function_19705                     
    mov64 r8, r0                                    r8 = r0
    mov64 r1, r6                                    r1 = r6
    ldxdw r6, [r10-0x110]                   
    ldxdw r2, [r10-0xe8]                    
    jeq r2, 0, lbb_10017                            if r2 == (0 as i32 as i64 as u64) { pc += -148 }
    lsh64 r2, 4                                     r2 <<= 4   ///  r2 = r2.wrapping_shl(4)
    ja lbb_10128                                    if true { pc += -39 }
lbb_10167:
    lddw r1, 0x100034cb0 --> b"\x00\x00\x00\x00\xb48\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295183536
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -120                                  r1 += -120   ///  r1 = r1.wrapping_add(-120 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100010468 --> b"a#4\x00\x00\x00\x00\x00\xbf4\x00\x00\x00\x00\x00\x00W\x04\x00\x00\x10\x00…        r1 load str located at 4295033960
    ja lbb_10140                                    if true { pc += -36 }
lbb_10176:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    mov64 r6, r0                                    r6 = r0
    call function_19705                     
    mov64 r8, r0                                    r8 = r0
    mov64 r1, r6                                    r1 = r6
    ldxdw r6, [r10-0x110]                   
    jeq r9, 0, lbb_10017                            if r9 == (0 as i32 as i64 as u64) { pc += -167 }
    lsh64 r9, 4                                     r9 <<= 4   ///  r9 = r9.wrapping_shl(4)
    mov64 r2, r9                                    r2 = r9
    ja lbb_10128                                    if true { pc += -59 }
lbb_10187:
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, r6                                    r2 = r6
    call function_20091                     
    mov64 r6, r2                                    r6 = r2
    mov64 r2, r1                                    r2 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_19155                     
    ldxdw r1, [r10-0x8]                     
    ldxdw r3, [r1+0x20]                     
    ldxdw r1, [r10-0x10]                    
    mov64 r2, r6                                    r2 = r6
    callx r3                                
    exit                                    

function_10201:
    mov64 r6, r2                                    r6 = r2
    mov64 r2, r1                                    r2 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_19155                     
    ldxdw r1, [r10-0x8]                     
    ldxdw r3, [r1+0x20]                     
    ldxdw r1, [r10-0x10]                    
    mov64 r2, r6                                    r2 = r6
    callx r3                                
    exit                                    

function_10212:
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r1+0x10]                     
    ldxdw r1, [r1+0x8]                      
    call function_21608                     
    exit                                    

function_10217:
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r1+0x8]                      
    ldxdw r1, [r1+0x0]                      
    call function_21608                     
    exit                                    

function_10222:
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r1+0x10]                     
    ldxdw r1, [r1+0x8]                      
    call function_21842                     
    exit                                    

function_10227:
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r1+0x8]                      
    ldxdw r1, [r1+0x0]                      
    call function_21842                     
    exit                                    

function_10232:
    mov64 r6, r1                                    r6 = r1
    ldxdw r3, [r2+0x8]                      
    jeq r3, 0, lbb_10257                            if r3 == (0 as i32 as i64 as u64) { pc += 22 }
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    ldxdw r4, [r2+0x0]                      
    ldxb r1, [r4+0x0]                       
    stxdw [r2+0x8], r3                      
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r2+0x0], r4                      
    stxb [r10-0x59], r1                     
    jeq r1, 0, lbb_10265                            if r1 == (0 as i32 as i64 as u64) { pc += 22 }
    jeq r1, 1, lbb_10245                            if r1 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_10268                                    if true { pc += 23 }
lbb_10245:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    call function_17795                     
    ldxdw r0, [r10-0x38]                    
    ldxdw r1, [r10-0x40]                    
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    jeq r1, r2, lbb_10260                           if r1 == r2 { pc += 7 }
    ldxdw r2, [r10-0x30]                    
    stxdw [r6+0x10], r2                     
    stxdw [r6+0x8], r0                      
    ja lbb_10294                                    if true { pc += 37 }
lbb_10257:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
lbb_10260:
    lddw r1, 0x8000000000000001                     r1 load str located at -9223372036854775807
    stxdw [r6+0x0], r1                      
    stxdw [r6+0x8], r0                      
    ja lbb_10295                                    if true { pc += 30 }
lbb_10265:
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    ja lbb_10294                                    if true { pc += 26 }
lbb_10268:
    lddw r1, 0x100034bb8 --> b"\x00\x00\x00\x00\x027\x03\x00\x1f\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295183288
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x10002cf18 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295151384
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 2                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_20113                     
    mov64 r1, r7                                    r1 = r7
    call function_8277                      
    stxdw [r6+0x8], r0                      
    lddw r1, 0x8000000000000001                     r1 load str located at -9223372036854775807
lbb_10294:
    stxdw [r6+0x0], r1                      
lbb_10295:
    exit                                    

function_10296:
    jne r3, 352, lbb_10303                          if r3 != (352 as i32 as i64 as u64) { pc += 6 }
    mov64 r3, r2                                    r3 = r2
    and64 r3, 7                                     r3 &= 7   ///  r3 = r3.and(7)
    jne r3, 0, lbb_10303                            if r3 != (0 as i32 as i64 as u64) { pc += 3 }
    stxdw [r1+0x8], r2                      
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ja lbb_10305                                    if true { pc += 2 }
lbb_10303:
    stw [r1+0x4], 3                         
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_10305:
    stxw [r1+0x0], r2                       
    exit                                    

function_10307:
    jne r3, 352, lbb_10314                          if r3 != (352 as i32 as i64 as u64) { pc += 6 }
    mov64 r3, r2                                    r3 = r2
    and64 r3, 7                                     r3 &= 7   ///  r3 = r3.and(7)
    jne r3, 0, lbb_10314                            if r3 != (0 as i32 as i64 as u64) { pc += 3 }
    stxdw [r1+0x8], r2                      
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ja lbb_10316                                    if true { pc += 2 }
lbb_10314:
    stw [r1+0x4], 3                         
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_10316:
    stxw [r1+0x0], r2                       
    exit                                    

function_10318:
    mov64 r7, r5                                    r7 = r5
    stxdw [r10-0x570], r4                   
    stxdw [r10-0x578], r3                   
    mov64 r8, r1                                    r8 = r1
    stdw [r10-0x98], 0                      
    ldxdw r6, [r2+0x0]                      
    mov64 r1, r6                                    r1 = r6
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0x100033348 --> b"\x06\xa7\xd5\x17\x18{\xd1f5\xda\xd4\x04U\xfd\xc2\xc0\xc1$\xc6\x8f!Vu\xa5\…        r2 load str located at 4295177032
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 16                                    r1 = 16 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jne r0, 0, lbb_10431                            if r0 != (0 as i32 as i64 as u64) { pc += 97 }
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    ldxb r2, [r6+0x0]                       
    mov64 r3, r2                                    r3 = r2
    and64 r3, 8                                     r3 &= 8   ///  r3 = r3.and(8)
    jne r3, 0, lbb_10431                            if r3 != (0 as i32 as i64 as u64) { pc += 92 }
    mov64 r3, r2                                    r3 = r2
    and64 r3, 7                                     r3 &= 7   ///  r3 = r3.and(7)
    jeq r3, 7, lbb_10431                            if r3 == (7 as i32 as i64 as u64) { pc += 89 }
    stxdw [r10-0x5e8], r8                   
    ldxdw r1, [r7-0xfe0]                    
    stxdw [r10-0x608], r1                   
    ldxdw r1, [r7-0xfe8]                    
    stxdw [r10-0x618], r1                   
    ldxdw r1, [r7-0xff0]                    
    stxdw [r10-0x630], r1                   
    ldxdw r1, [r7-0xff8]                    
    stxdw [r10-0x610], r1                   
    ldxdw r1, [r7-0x1000]                   
    stxdw [r10-0x600], r1                   
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    stxb [r6+0x0], r2                       
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x5f8], r1                   
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x4b0], r1                   
    ldxh r1, [r6+0x58]                      
    stxdw [r10-0x4c8], r1                   
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x508], r1                   
    stxdw [r10-0x500], r1                   
    stxdw [r10-0x4f8], r1                   
    stxdw [r10-0x4f0], r1                   
    stxdw [r10-0x4e8], r1                   
    stxdw [r10-0x4a0], r1                   
    stxdw [r10-0x548], r1                   
    stxdw [r10-0x5b8], r1                   
    stxdw [r10-0x538], r1                   
    stxdw [r10-0x530], r1                   
    stxdw [r10-0x540], r1                   
    stxdw [r10-0x510], r1                   
    stxdw [r10-0x520], r1                   
    stxdw [r10-0x518], r1                   
    stxdw [r10-0x550], r1                   
    stxdw [r10-0x4a8], r1                   
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    stxdw [r10-0x4c0], r3                   
    stxdw [r10-0x4b8], r3                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    stxdw [r10-0x5d8], r3                   
    stxdw [r10-0x528], r3                   
    stxdw [r10-0x568], r3                   
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    stxdw [r10-0x558], r3                   
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r7, [r10-0x4c8]                   
    stxdw [r10-0x5f0], r2                   
    jeq r7, 0, lbb_13200                            if r7 == (0 as i32 as i64 as u64) { pc += 2803 }
    mov64 r1, r2                                    r1 = r2
    add64 r1, 90                                    r1 += 90   ///  r1 = r1.wrapping_add(90 as i32 as i64 as u64)
    stxdw [r10-0x5e0], r1                   
    mov64 r7, r2                                    r7 = r2
    add64 r7, 88                                    r7 += 88   ///  r7 = r7.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x4b0], r1                   
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x628], r1                   
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x568], r1                   
    stxdw [r10-0x528], r1                   
    stxdw [r10-0x5d8], r1                   
    stxdw [r10-0x4d0], r1                   
    stxdw [r10-0x4b8], r1                   
    stxdw [r10-0x4c0], r1                   
    stxdw [r10-0x5c0], r1                   
    stxdw [r10-0x5c8], r1                   
    stxdw [r10-0x4a8], r1                   
    stxdw [r10-0x550], r1                   
    stxdw [r10-0x518], r1                   
    stxdw [r10-0x520], r1                   
    stxdw [r10-0x510], r1                   
    stxdw [r10-0x540], r1                   
    stxdw [r10-0x530], r1                   
    stxdw [r10-0x538], r1                   
    stxdw [r10-0x5b8], r1                   
    stxdw [r10-0x548], r1                   
    stxdw [r10-0x4f0], r1                   
    stxdw [r10-0x4f8], r1                   
    stxdw [r10-0x500], r1                   
    stxdw [r10-0x5d0], r7                   
    ja lbb_10452                                    if true { pc += 21 }
lbb_10431:
    stxw [r8+0x4], r1                       
    stw [r8+0x8], 0                         
    stw [r8+0x0], 2                         
lbb_10434:
    exit                                    
lbb_10435:
    lddw r4, 0x52142a73b3f727ae                     r4 load str located at 5914398887073228718
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += 7 }
    lddw r4, 0x6859164a61d89e53                     r4 load str located at 7519065561596730963
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += 4 }
    lddw r4, 0x68fa3ea74c5f68ae                     r4 load str located at 7564427412383951022
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += 1 }
lbb_10444:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_10445:
    stxdw [r10-0x550], r1                   
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    ldxh r1, [r7+0x0]                       
    ldxdw r4, [r10-0x4d8]                   
    ldxdw r5, [r10-0x4c8]                   
    jlt r6, r1, lbb_10452                           if r6 < r1 { pc += 1 }
    ja lbb_13153                                    if true { pc += 2701 }
lbb_10452:
    stxdw [r10-0x4e8], r3                   
    stxdw [r10-0x560], r2                   
    stxdw [r10-0x5b0], r5                   
    ldxdw r1, [r10-0x4c0]                   
    stxdw [r10-0x5a8], r1                   
    ldxdw r1, [r10-0x4b8]                   
    stxdw [r10-0x5a0], r1                   
    stxdw [r10-0x598], r4                   
    ldxdw r1, [r10-0x4d0]                   
    stxdw [r10-0x590], r1                   
    stxdw [r10-0x508], r6                   
    mov64 r1, r6                                    r1 = r6
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    ldxdw r2, [r10-0x5e0]                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxh r1, [r2+0x0]                       
    add64 r7, r1                                    r7 += r1   ///  r7 = r7.wrapping_add(r1)
    ldxh r3, [r7+0x0]                       
    mov64 r1, r3                                    r1 = r3
    mul64 r1, 33                                    r1 *= 33   ///  r1 = r1.wrapping_mul(33 as u64)
    stxdw [r10-0x4d0], r7                   
    mov64 r2, r7                                    r2 = r7
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    stxdw [r10-0x580], r2                   
    ldxh r1, [r2+0x22]                      
    stxdw [r10-0x558], r1                   
    ldxdw r6, [r10-0x4a8]                   
    ldxdw r7, [r10-0x518]                   
    ldxdw r1, [r10-0x520]                   
    stxdw [r10-0x4d8], r3                   
    jeq r3, 0, lbb_10818                            if r3 == (0 as i32 as i64 as u64) { pc += 335 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, r0                                    r5 = r0
    mov64 r4, r8                                    r4 = r8
    stxdw [r10-0x4e0], r6                   
    stxdw [r10-0x4c8], r7                   
    stxdw [r10-0x4b8], r1                   
    ja lbb_10506                                    if true { pc += 16 }
lbb_10490:
    ldxdw r2, [r10-0x4b8]                   
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    ldxdw r3, [r10-0x4c0]                   
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    mov64 r0, r9                                    r0 = r9
    mov64 r5, r0                                    r5 = r0
    mov64 r8, r6                                    r8 = r6
    mov64 r4, r8                                    r4 = r8
    ldxdw r6, [r10-0x4a8]                   
    stxdw [r10-0x4e0], r6                   
    stxdw [r10-0x4c8], r7                   
    stxdw [r10-0x4b8], r1                   
    ldxdw r9, [r10-0x4a0]                   
    ldxdw r2, [r10-0x4d8]                   
    jlt r3, r2, lbb_10506                           if r3 < r2 { pc += 1 }
    ja lbb_10818                                    if true { pc += 312 }
lbb_10506:
    stxdw [r10-0x4c0], r3                   
    mul64 r3, 33                                    r3 *= 33   ///  r3 = r3.wrapping_mul(33 as u64)
    ldxdw r8, [r10-0x4d0]                   
    add64 r8, r3                                    r8 += r3   ///  r8 = r8.wrapping_add(r3)
    ldxb r0, [r8+0x2]                       
    mov64 r3, r0                                    r3 = r0
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    and64 r0, 2                                     r0 &= 2   ///  r0 = r0.and(2)
    jeq r0, 0, lbb_10516                            if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_10523                                    if true { pc += 7 }
lbb_10516:
    ldxdw r6, [r10-0x4b0]                   
    jeq r3, 0, lbb_10519                            if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_10530                                    if true { pc += 11 }
lbb_10519:
    ldxdw r1, [r10-0x4f0]                   
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x4f0], r1                   
    ja lbb_10537                                    if true { pc += 14 }
lbb_10523:
    ldxdw r6, [r10-0x4b0]                   
    jeq r3, 0, lbb_10526                            if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_10534                                    if true { pc += 8 }
lbb_10526:
    ldxdw r1, [r10-0x500]                   
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x500], r1                   
    ja lbb_10537                                    if true { pc += 7 }
lbb_10530:
    ldxdw r1, [r10-0x4e8]                   
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x4e8], r1                   
    ja lbb_10537                                    if true { pc += 3 }
lbb_10534:
    ldxdw r1, [r10-0x4f8]                   
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x4f8], r1                   
lbb_10537:
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    ldxb r0, [r8+0x3]                       
    xor64 r0, 168                                   r0 ^= 168   ///  r0 = r0.xor(168)
    ldxb r3, [r8+0x4]                       
    xor64 r3, 181                                   r3 ^= 181   ///  r3 = r3.xor(181)
    lsh64 r3, 8                                     r3 <<= 8   ///  r3 = r3.wrapping_shl(8)
    or64 r3, r0                                     r3 |= r0   ///  r3 = r3.or(r0)
    ldxb r0, [r8+0x5]                       
    xor64 r0, 169                                   r0 ^= 169   ///  r0 = r0.xor(169)
    lsh64 r0, 16                                    r0 <<= 16   ///  r0 = r0.wrapping_shl(16)
    or64 r3, r0                                     r3 |= r0   ///  r3 = r3.or(r0)
    ldxb r0, [r8+0x6]                       
    xor64 r0, 173                                   r0 ^= 173   ///  r0 = r0.xor(173)
    lsh64 r0, 24                                    r0 <<= 24   ///  r0 = r0.wrapping_shl(24)
    or64 r3, r0                                     r3 |= r0   ///  r3 = r3.or(r0)
    ldxb r0, [r8+0x7]                       
    xor64 r0, 112                                   r0 ^= 112   ///  r0 = r0.xor(112)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    or64 r3, r0                                     r3 |= r0   ///  r3 = r3.or(r0)
    ldxb r0, [r8+0x8]                       
    xor64 r0, 91                                    r0 ^= 91   ///  r0 = r0.xor(91)
    lsh64 r0, 40                                    r0 <<= 40   ///  r0 = r0.wrapping_shl(40)
    or64 r3, r0                                     r3 |= r0   ///  r3 = r3.or(r0)
    ldxb r0, [r8+0x9]                       
    xor64 r0, 91                                    r0 ^= 91   ///  r0 = r0.xor(91)
    lsh64 r0, 48                                    r0 <<= 48   ///  r0 = r0.wrapping_shl(48)
    or64 r3, r0                                     r3 |= r0   ///  r3 = r3.or(r0)
    ldxb r0, [r8+0xa]                       
    xor64 r0, 251                                   r0 ^= 251   ///  r0 = r0.xor(251)
    lsh64 r0, 56                                    r0 <<= 56   ///  r0 = r0.wrapping_shl(56)
    or64 r3, r0                                     r3 |= r0   ///  r3 = r3.or(r0)
    mov64 r0, r10                                   r0 = r10
    add64 r0, -1176                                 r0 += -1176   ///  r0 = r0.wrapping_add(-1176 as i32 as i64 as u64)
    mov64 r9, r6                                    r9 = r6
    lsh64 r9, 3                                     r9 <<= 3   ///  r9 = r9.wrapping_shl(3)
    add64 r8, 3                                     r8 += 3   ///  r8 = r8.wrapping_add(3 as i32 as i64 as u64)
lbb_10574:
    jeq r9, 0, lbb_10580                            if r9 == (0 as i32 as i64 as u64) { pc += 5 }
    add64 r9, -8                                    r9 += -8   ///  r9 = r9.wrapping_add(-8 as i32 as i64 as u64)
    ldxdw r1, [r0+0x0]                      
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
    jeq r1, r3, lbb_10589                           if r1 == r3 { pc += 10 }
    ja lbb_10574                                    if true { pc += -6 }
lbb_10580:
    jgt r6, 127, lbb_13149                          if r6 > (127 as i32 as i64 as u64) { pc += 2568 }
    lsh64 r6, 3                                     r6 <<= 3   ///  r6 = r6.wrapping_shl(3)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1176                                 r1 += -1176   ///  r1 = r1.wrapping_add(-1176 as i32 as i64 as u64)
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    stxdw [r1+0x0], r3                      
    ldxdw r6, [r10-0x98]                    
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x98], r6                    
lbb_10589:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    lddw r0, 0x6d7358b2b3b99387                     r0 load str located at 7886744896998577031
    jsgt r3, r0, lbb_10604                          if (r3 as i64) > (r0 as i64) { pc += 11 }
    mov64 r0, r2                                    r0 = r2
    lddw r2, 0xecbd84d20e56441f                     r2 load str located at -1387807072441711585
    jsgt r3, r2, lbb_10615                          if (r3 as i64) > (r2 as i64) { pc += 18 }
    lddw r2, 0x9d86aa2eabfe9e14                     r2 load str located at -7095797045444829676
    jeq r3, r2, lbb_10629                           if r3 == r2 { pc += 29 }
    lddw r2, 0xcb48e0d5f8d4b221                     r2 load str located at -3798539076079668703
    jeq r3, r2, lbb_10629                           if r3 == r2 { pc += 26 }
    ja lbb_10628                                    if true { pc += 24 }
lbb_10604:
    mov64 r0, r2                                    r0 = r2
    lddw r2, 0x7e00d060f4b22216                     r2 load str located at 9079485963619672598
    jsgt r3, r2, lbb_10622                          if (r3 as i64) > (r2 as i64) { pc += 14 }
    lddw r2, 0x6d7358b2b3b99388                     r2 load str located at 7886744896998577032
    jeq r3, r2, lbb_10629                           if r3 == r2 { pc += 18 }
    lddw r2, 0x7de1c42e48a4fb19                     r2 load str located at 9070746827567201049
    jeq r3, r2, lbb_10629                           if r3 == r2 { pc += 15 }
    ja lbb_10628                                    if true { pc += 13 }
lbb_10615:
    lddw r2, 0xecbd84d20e564420                     r2 load str located at -1387807072441711584
    jeq r3, r2, lbb_10629                           if r3 == r2 { pc += 11 }
    lddw r2, 0x301eaca12a453259                     r2 load str located at 3467398571320750681
    jeq r3, r2, lbb_10629                           if r3 == r2 { pc += 8 }
    ja lbb_10628                                    if true { pc += 6 }
lbb_10622:
    lddw r2, 0x7e00d060f4b22217                     r2 load str located at 9079485963619672599
    jeq r3, r2, lbb_10629                           if r3 == r2 { pc += 4 }
    lddw r2, 0x7ee095091cb5e7d0                     r2 load str located at 9142471109931034576
    jeq r3, r2, lbb_10629                           if r3 == r2 { pc += 1 }
lbb_10628:
    mov64 r9, r5                                    r9 = r5
lbb_10629:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    lddw r2, 0xd1a65331a3c5fee6                     r2 load str located at -3339890600982479130
    jsgt r3, r2, lbb_10646                          if (r3 as i64) > (r2 as i64) { pc += 13 }
    lddw r2, 0x9e6174cc7372135b                     r2 load str located at -7034212721516997797
    jsgt r3, r2, lbb_10659                          if (r3 as i64) > (r2 as i64) { pc += 23 }
    lddw r2, 0x8d9d93a901267e10                     r2 load str located at -8242269388940083696
    jeq r3, r2, lbb_10669                           if r3 == r2 { pc += 30 }
    lddw r2, 0x911b1f8fe94c222a                     r2 load str located at -7990758410883947990
    jeq r3, r2, lbb_10669                           if r3 == r2 { pc += 27 }
    lddw r2, 0x914737f86ea0b9bc                     r2 load str located at -7978346674716100164
    jeq r3, r2, lbb_10669                           if r3 == r2 { pc += 24 }
    ja lbb_10769                                    if true { pc += 123 }
lbb_10646:
    lddw r2, 0x7266a5881e4e0087                     r2 load str located at 8243458171990835335
    jsgt r3, r2, lbb_10760                          if (r3 as i64) > (r2 as i64) { pc += 111 }
    lddw r2, 0xd1a65331a3c5fee7                     r2 load str located at -3339890600982479129
    jeq r3, r2, lbb_10669                           if r3 == r2 { pc += 17 }
    lddw r2, 0xf03ca64824327c8a                     r2 load str located at -1135850177229063030
    jeq r3, r2, lbb_10669                           if r3 == r2 { pc += 14 }
    lddw r2, 0x469819caf0858bf9                     r2 load str located at 5086844138524347385
    jeq r3, r2, lbb_10669                           if r3 == r2 { pc += 11 }
    ja lbb_10769                                    if true { pc += 110 }
lbb_10659:
    lddw r2, 0x9e6174cc7372135c                     r2 load str located at -7034212721516997796
    jeq r3, r2, lbb_10669                           if r3 == r2 { pc += 7 }
    lddw r2, 0xb0c37334309b22e4                     r2 load str located at -5709593234584689948
    jeq r3, r2, lbb_10669                           if r3 == r2 { pc += 4 }
    lddw r2, 0xccc9a9f77d4aaf04                     r2 load str located at -3690231539229348092
    jeq r3, r2, lbb_10669                           if r3 == r2 { pc += 1 }
    ja lbb_10769                                    if true { pc += 100 }
lbb_10669:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    lddw r2, 0x1712a7fbeeea09a2                     r2 load str located at 1662575912940079522
    jsgt r3, r2, lbb_10683                          if (r3 as i64) > (r2 as i64) { pc += 10 }
    lddw r2, 0x93fbd23f67eb09a3                     r2 load str located at -7783396361211541085
    jeq r3, r2, lbb_10693                           if r3 == r2 { pc += 17 }
    lddw r2, 0xdbc4d917379b0dae                     r2 load str located at -2610723190227661394
    jeq r3, r2, lbb_10693                           if r3 == r2 { pc += 14 }
    lddw r2, 0xf6d7f9f178ea09a3                     r2 load str located at -659784004875449949
    jeq r3, r2, lbb_10693                           if r3 == r2 { pc += 11 }
    ja lbb_10747                                    if true { pc += 64 }
lbb_10683:
    lddw r2, 0x511aa8ef60eb09a2                     r2 load str located at 5844169212543306146
    jsgt r3, r2, lbb_10741                          if (r3 as i64) > (r2 as i64) { pc += 55 }
    lddw r2, 0x1712a7fbeeea09a3                     r2 load str located at 1662575912940079523
    jeq r3, r2, lbb_10693                           if r3 == r2 { pc += 4 }
    lddw r2, 0x1a0cb11ebdea09a3                     r2 load str located at 1877069890300021155
    jeq r3, r2, lbb_10693                           if r3 == r2 { pc += 1 }
    ja lbb_10747                                    if true { pc += 54 }
lbb_10693:
    stxdw [r10-0x4a0], r0                   
    stxdw [r10-0x4b0], r6                   
    stxdw [r10-0x4a8], r4                   
    mov64 r6, r5                                    r6 = r5
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    lddw r2, 0xb9e1103ad3c8a1b7                     r2 load str located at -5052739462085697097
    jsgt r3, r2, lbb_10711                          if (r3 as i64) > (r2 as i64) { pc += 10 }
    lddw r1, 0x8d9d05f25177a1a4                     r1 load str located at -8242425204711120476
    jeq r3, r1, lbb_10721                           if r3 == r1 { pc += 17 }
    lddw r1, 0x96f97bf7cb676485                     r1 load str located at -7567881394619718523
    jeq r3, r1, lbb_10721                           if r3 == r1 { pc += 14 }
    lddw r1, 0xa902723f26f39f01                     r1 load str located at -6268322115784302847
    jeq r3, r1, lbb_10721                           if r3 == r1 { pc += 11 }
    ja lbb_10720                                    if true { pc += 9 }
lbb_10711:
    lddw r1, 0xb9e1103ad3c8a1b8                     r1 load str located at -5052739462085697096
    jeq r3, r1, lbb_10721                           if r3 == r1 { pc += 7 }
    lddw r1, 0xefc035593538d02d                     r1 load str located at -1170877245855051731
    jeq r3, r1, lbb_10721                           if r3 == r1 { pc += 4 }
    lddw r1, 0xe4a9af003b8f0dc                      r1 load str located at 1029805820452860124
    jeq r3, r1, lbb_10721                           if r3 == r1 { pc += 1 }
lbb_10720:
    ldxdw r7, [r10-0x4c8]                   
lbb_10721:
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0x100033468 --> b"\x0a\xf1\xc3C!\x88\xca:cQ5\xa1:\x18\x95\x1a\xce\xbd)\xe6\xac-\xaeg\xff\xd…        r2 load str located at 4295177320
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    jslt r0, 0, lbb_10490                           if (r0 as i64) < (0 as i32 as i64) { pc += -240 }
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0x1000333e8 --> b"\x0a\xf1\xc3C!\x88\xca:cR\x0bS\xec\xba\xf3\x1b<\x17b.\x98\x82:\xaf\x1c\xc…        r2 load str located at 4295177192
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jslt r0, 1, lbb_10490                           if (r0 as i64) < (1 as i32 as i64) { pc += -249 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_10490                                    if true { pc += -251 }
lbb_10741:
    lddw r2, 0x511aa8ef60eb09a3                     r2 load str located at 5844169212543306147
    jeq r3, r2, lbb_10693                           if r3 == r2 { pc += -51 }
    lddw r2, 0x5743936a3eec09a3                     r2 load str located at 6288031589270817187
    jeq r3, r2, lbb_10693                           if r3 == r2 { pc += -54 }
lbb_10747:
    lddw r2, 0x3f7d6e8bcec09a2                      r2 load str located at 285933396451658146
    jsgt r3, r2, lbb_10779                          if (r3 as i64) > (r2 as i64) { pc += 29 }
    lddw r2, 0xee914c3eb8ec09a2                     r2 load str located at -1256138988785497694
    jsgt r3, r2, lbb_10797                          if (r3 as i64) > (r2 as i64) { pc += 44 }
    lddw r2, 0x9cc11cdbd7ef09a3                     r2 load str located at -7151403002741454429
    jeq r3, r2, lbb_10693                           if r3 == r2 { pc += -63 }
    lddw r2, 0xee20283577ef09a3                     r2 load str located at -1287985283317429853
    jeq r3, r2, lbb_10693                           if r3 == r2 { pc += -66 }
    ja lbb_10816                                    if true { pc += 56 }
lbb_10760:
    lddw r2, 0x7266a5881e4e0088                     r2 load str located at 8243458171990835336
    jeq r3, r2, lbb_10669                           if r3 == r2 { pc += -94 }
    lddw r2, 0x736ddc3c52a23532                     r2 load str located at 8317546238465684786
    jeq r3, r2, lbb_10669                           if r3 == r2 { pc += -97 }
    lddw r2, 0x739df54d45de4d4a                     r2 load str located at 8331084597938769226
    jeq r3, r2, lbb_10669                           if r3 == r2 { pc += -100 }
lbb_10769:
    lddw r2, 0x22d0fff7f3f93d58                     r2 load str located at 2508786432860568920
    jsgt r3, r2, lbb_10789                          if (r3 as i64) > (r2 as i64) { pc += 17 }
    lddw r2, 0x90c117bdf9293f4c                     r2 load str located at -8016099757046284468
    jeq r3, r2, lbb_10669                           if r3 == r2 { pc += -106 }
    lddw r2, 0xef804cb1b1327dcb                     r2 load str located at -1188865975560012341
    jeq r3, r2, lbb_10669                           if r3 == r2 { pc += -109 }
    ja lbb_10795                                    if true { pc += 16 }
lbb_10779:
    lddw r2, 0x1715e1d7caed09a2                     r2 load str located at 1663483954322016674
    jsgt r3, r2, lbb_10807                          if (r3 as i64) > (r2 as i64) { pc += 25 }
    lddw r2, 0x3f7d6e8bcec09a3                      r2 load str located at 285933396451658147
    jeq r3, r2, lbb_10693                           if r3 == r2 { pc += -92 }
    lddw r2, 0x11c3f2a1fcee09a3                     r2 load str located at 1280133496650467747
    jeq r3, r2, lbb_10693                           if r3 == r2 { pc += -95 }
    ja lbb_10816                                    if true { pc += 27 }
lbb_10789:
    lddw r2, 0x22d0fff7f3f93d59                     r2 load str located at 2508786432860568921
    jeq r3, r2, lbb_10669                           if r3 == r2 { pc += -123 }
    lddw r2, 0x4a916ec792390d92                     r2 load str located at 5373197633860472210
    jeq r3, r2, lbb_10669                           if r3 == r2 { pc += -126 }
lbb_10795:
    mov64 r5, r4                                    r5 = r4
    ja lbb_10669                                    if true { pc += -128 }
lbb_10797:
    lddw r2, 0xee914c3eb8ec09a3                     r2 load str located at -1256138988785497693
    jeq r3, r2, lbb_10693                           if r3 == r2 { pc += -107 }
    lddw r2, 0xf8a2a4e633ef09a3                     r2 load str located at -530680497455560285
    jeq r3, r2, lbb_10693                           if r3 == r2 { pc += -110 }
    lddw r2, 0x2aeb88eef09a3                        r2 load str located at 755057648667043
    jeq r3, r2, lbb_10693                           if r3 == r2 { pc += -113 }
    ja lbb_10816                                    if true { pc += 9 }
lbb_10807:
    lddw r2, 0x1715e1d7caed09a3                     r2 load str located at 1663483954322016675
    jeq r3, r2, lbb_10693                           if r3 == r2 { pc += -117 }
    lddw r2, 0x2fc48a6378ef09a3                     r2 load str located at 3442028175053228451
    jeq r3, r2, lbb_10693                           if r3 == r2 { pc += -120 }
    lddw r2, 0x5e9754b240ed09a3                     r2 load str located at 6816009685618723235
    jeq r3, r2, lbb_10693                           if r3 == r2 { pc += -123 }
lbb_10816:
    ldxdw r4, [r10-0x4e0]                   
    ja lbb_10693                                    if true { pc += -125 }
lbb_10818:
    stxdw [r10-0x520], r1                   
    stxdw [r10-0x518], r7                   
    stxdw [r10-0x4a8], r6                   
    stxdw [r10-0x588], r8                   
    stxdw [r10-0x4e0], r0                   
    stxdw [r10-0x4a0], r9                   
    ldxdw r2, [r10-0x580]                   
    mov64 r8, r2                                    r8 = r2
    add64 r8, 36                                    r8 += 36   ///  r8 = r8.wrapping_add(36 as i32 as i64 as u64)
    ldxb r1, [r2+0x2]                       
    xor64 r1, 168                                   r1 ^= 168   ///  r1 = r1.xor(168)
    ldxb r5, [r2+0x3]                       
    xor64 r5, 181                                   r5 ^= 181   ///  r5 = r5.xor(181)
    lsh64 r5, 8                                     r5 <<= 8   ///  r5 = r5.wrapping_shl(8)
    or64 r5, r1                                     r5 |= r1   ///  r5 = r5.or(r1)
    ldxb r1, [r2+0x4]                       
    xor64 r1, 169                                   r1 ^= 169   ///  r1 = r1.xor(169)
    lsh64 r1, 16                                    r1 <<= 16   ///  r1 = r1.wrapping_shl(16)
    or64 r5, r1                                     r5 |= r1   ///  r5 = r5.or(r1)
    ldxb r1, [r2+0x5]                       
    xor64 r1, 173                                   r1 ^= 173   ///  r1 = r1.xor(173)
    lsh64 r1, 24                                    r1 <<= 24   ///  r1 = r1.wrapping_shl(24)
    or64 r5, r1                                     r5 |= r1   ///  r5 = r5.or(r1)
    ldxb r1, [r2+0x6]                       
    xor64 r1, 112                                   r1 ^= 112   ///  r1 = r1.xor(112)
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    or64 r5, r1                                     r5 |= r1   ///  r5 = r5.or(r1)
    ldxb r1, [r2+0x7]                       
    xor64 r1, 91                                    r1 ^= 91   ///  r1 = r1.xor(91)
    lsh64 r1, 40                                    r1 <<= 40   ///  r1 = r1.wrapping_shl(40)
    or64 r5, r1                                     r5 |= r1   ///  r5 = r5.or(r1)
    ldxb r1, [r2+0x8]                       
    xor64 r1, 91                                    r1 ^= 91   ///  r1 = r1.xor(91)
    lsh64 r1, 48                                    r1 <<= 48   ///  r1 = r1.wrapping_shl(48)
    or64 r5, r1                                     r5 |= r1   ///  r5 = r5.or(r1)
    ldxb r1, [r2+0x9]                       
    xor64 r1, 251                                   r1 ^= 251   ///  r1 = r1.xor(251)
    lsh64 r1, 56                                    r1 <<= 56   ///  r1 = r1.wrapping_shl(56)
    or64 r5, r1                                     r5 |= r1   ///  r5 = r5.or(r1)
    lddw r1, 0x3a655514f07d921c                     r1 load str located at 4207862975270064668
    stxdw [r10-0x4b0], r5                   
    jsgt r5, r1, lbb_10963                          if (r5 as i64) > (r1 as i64) { pc += 102 }
    lddw r1, 0x959b6a82f67cccac                     r1 load str located at -7666416829954470740
    jeq r5, r1, lbb_10973                           if r5 == r1 { pc += 109 }
    lddw r1, 0xa12a3db13be376a2                     r1 load str located at -6833581653158037854
    jeq r5, r1, lbb_10973                           if r5 == r1 { pc += 106 }
    lddw r1, 0xc94c7a95c2efb3ab                     r1 load str located at -3941640790216821845
    jeq r5, r1, lbb_10871                           if r5 == r1 { pc += 1 }
    ja lbb_11916                                    if true { pc += 1045 }
lbb_10871:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    ldxdw r3, [r10-0x558]                   
    call function_8703                      
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxw r1, [r10-0x40]                     
    jeq r1, 2, lbb_10880                            if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    ldxdw r2, [r10-0x560]                   
lbb_10880:
    mov64 r3, r2                                    r3 = r2
    ldxw r2, [r10-0x3c]                     
    ldxdw r4, [r10-0x638]                   
    ldxdw r6, [r10-0x508]                   
    jeq r1, 2, lbb_10886                            if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, r4                                    r2 = r4
lbb_10886:
    stxdw [r10-0x560], r3                   
    stxdw [r10-0x638], r2                   
    jne r1, 5, lbb_10912                            if r1 != (5 as i32 as i64 as u64) { pc += 23 }
    ldxdw r7, [r10-0x38]                    
    mov64 r1, r7                                    r1 = r7
    and64 r1, 3                                     r1 &= 3   ///  r1 = r1.and(3)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -2                                    r2 += -2   ///  r2 = r2.wrapping_add(-2 as i32 as i64 as u64)
    jlt r2, 2, lbb_10912                            if r2 < (2 as i32 as i64 as u64) { pc += 17 }
    jeq r1, 0, lbb_10912                            if r1 == (0 as i32 as i64 as u64) { pc += 16 }
    ldxdw r9, [r7-0x1]                      
    ldxdw r6, [r7+0x7]                      
    ldxdw r2, [r6+0x0]                      
    mov64 r1, r9                                    r1 = r9
    callx r2                                
    ldxdw r2, [r6+0x8]                      
    jeq r2, 0, lbb_10906                            if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r3, [r6+0x10]                     
    mov64 r1, r9                                    r1 = r9
    call function_6768                      
lbb_10906:
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ldxdw r6, [r10-0x508]                   
lbb_10912:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    ldxdw r3, [r10-0x558]                   
    call function_8703                      
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxw r1, [r10-0x40]                     
    jeq r1, 3, lbb_10921                            if r1 == (3 as i32 as i64 as u64) { pc += 1 }
    ldxdw r2, [r10-0x628]                   
lbb_10921:
    stxdw [r10-0x628], r2                   
    ldxdw r7, [r10-0x38]                    
    mov64 r2, r7                                    r2 = r7
    ldxdw r5, [r10-0x4b0]                   
    ldxdw r0, [r10-0x530]                   
    ldxdw r3, [r10-0x538]                   
    ldxdw r4, [r10-0x540]                   
    ldxdw r8, [r10-0x548]                   
    jeq r1, 3, lbb_10931                            if r1 == (3 as i32 as i64 as u64) { pc += 1 }
    ldxdw r2, [r10-0x620]                   
lbb_10931:
    stxdw [r10-0x620], r2                   
    jne r1, 5, lbb_10960                            if r1 != (5 as i32 as i64 as u64) { pc += 27 }
    mov64 r1, r7                                    r1 = r7
    and64 r1, 3                                     r1 &= 3   ///  r1 = r1.and(3)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -2                                    r2 += -2   ///  r2 = r2.wrapping_add(-2 as i32 as i64 as u64)
    jlt r2, 2, lbb_10960                            if r2 < (2 as i32 as i64 as u64) { pc += 22 }
    jeq r1, 0, lbb_10960                            if r1 == (0 as i32 as i64 as u64) { pc += 21 }
    ldxdw r8, [r7-0x1]                      
    ldxdw r6, [r7+0x7]                      
    ldxdw r2, [r6+0x0]                      
    mov64 r1, r8                                    r1 = r8
    callx r2                                
    ldxdw r2, [r6+0x8]                      
    jeq r2, 0, lbb_10949                            if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r3, [r6+0x10]                     
    mov64 r1, r8                                    r1 = r8
    call function_6768                      
lbb_10949:
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ldxdw r6, [r10-0x508]                   
    ldxdw r5, [r10-0x4b0]                   
    ldxdw r0, [r10-0x530]                   
    ldxdw r3, [r10-0x538]                   
    ldxdw r4, [r10-0x540]                   
    ldxdw r8, [r10-0x548]                   
lbb_10960:
    ldxdw r2, [r10-0x510]                   
    ldxdw r7, [r10-0x5d0]                   
    ja lbb_12632                                    if true { pc += 1669 }
lbb_10963:
    lddw r1, 0x3a655514f07d921d                     r1 load str located at 4207862975270064669
    jeq r5, r1, lbb_10973                           if r5 == r1 { pc += 7 }
    lddw r1, 0x3cc6f006d17cccac                     r1 load str located at 4379451599739473068
    jeq r5, r1, lbb_10973                           if r5 == r1 { pc += 4 }
    lddw r1, 0x3e30e49d807cccac                     r1 load str located at 4481332994350304428
    jeq r5, r1, lbb_10973                           if r5 == r1 { pc += 1 }
    ja lbb_11916                                    if true { pc += 943 }
lbb_10973:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x5c8], r1                   
    stxdw [r10-0x5c0], r1                   
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x568], r1                   
    ldxdw r1, [r10-0x558]                   
    jlt r1, 8, lbb_11916                            if r1 < (8 as i32 as i64 as u64) { pc += 936 }
    ldxdw r6, [r8+0x0]                      
    lddw r1, 0x14afc431ccfa64bb                     r1 load str located at 1490625719854326971
    jeq r6, r1, lbb_11007                           if r6 == r1 { pc += 23 }
    lddw r1, 0x2aade37a97cb17e5                     r1 load str located at 3075364236236101605
    jne r6, r1, lbb_11047                           if r6 != r1 { pc += 60 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x5c0], r1                   
    ldxdw r1, [r10-0x4d8]                   
    jlt r1, 4, lbb_11091                            if r1 < (4 as i32 as i64 as u64) { pc += 100 }
    ldxdw r7, [r10-0x4d0]                   
    mov64 r1, r7                                    r1 = r7
    add64 r1, 69                                    r1 += 69   ///  r1 = r1.wrapping_add(69 as i32 as i64 as u64)
    add64 r7, 102                                   r7 += 102   ///  r7 = r7.wrapping_add(102 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    ldxdw r5, [r10-0x4b0]                   
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x5c0], r1                   
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jeq r0, 0, lbb_11091                            if r0 == (0 as i32 as i64 as u64) { pc += 87 }
    ldxdw r1, [r10-0x4d0]                   
    add64 r1, 36                                    r1 += 36   ///  r1 = r1.wrapping_add(36 as i32 as i64 as u64)
    ja lbb_11074                                    if true { pc += 67 }
lbb_11007:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x5c8], r1                   
    stxdw [r10-0x5c0], r1                   
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x568], r1                   
    ldxdw r1, [r10-0x4d8]                   
    jgt r1, 4, lbb_11015                            if r1 > (4 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11916                                    if true { pc += 901 }
lbb_11015:
    ldxdw r7, [r10-0x4d0]                   
    mov64 r1, r7                                    r1 = r7
    add64 r1, 36                                    r1 += 36   ///  r1 = r1.wrapping_add(36 as i32 as i64 as u64)
    add64 r7, 69                                    r7 += 69   ///  r7 = r7.wrapping_add(69 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    ldxdw r5, [r10-0x4b0]                   
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x5c0], r1                   
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x5c8], r1                   
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x568], r1                   
    jeq r0, 0, lbb_11916                            if r0 == (0 as i32 as i64 as u64) { pc += 884 }
    ldxdw r2, [r10-0x4d0]                   
    mov64 r1, r2                                    r1 = r2
    add64 r1, 102                                   r1 += 102   ///  r1 = r1.wrapping_add(102 as i32 as i64 as u64)
    add64 r2, 135                                   r2 += 135   ///  r2 = r2.wrapping_add(135 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    ldxdw r5, [r10-0x4b0]                   
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x568], r1                   
    jeq r0, 0, lbb_11916                            if r0 == (0 as i32 as i64 as u64) { pc += 872 }
    ldxdw r1, [r10-0x4d0]                   
    add64 r1, 3                                     r1 += 3   ///  r1 = r1.wrapping_add(3 as i32 as i64 as u64)
    ja lbb_11074                                    if true { pc += 27 }
lbb_11047:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0x4d8]                   
    jlt r2, 9, lbb_11083                            if r2 < (9 as i32 as i64 as u64) { pc += 33 }
    lddw r2, 0x819cd641339b20c1                     r2 load str located at -9107168770922962751
    jeq r6, r2, lbb_11054                           if r6 == r2 { pc += 1 }
    ja lbb_11083                                    if true { pc += 29 }
lbb_11054:
    ldxdw r2, [r10-0x4d0]                   
    mov64 r1, r2                                    r1 = r2
    add64 r1, 102                                   r1 += 102   ///  r1 = r1.wrapping_add(102 as i32 as i64 as u64)
    add64 r2, 168                                   r2 += 168   ///  r2 = r2.wrapping_add(168 as i32 as i64 as u64)
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    ldxdw r5, [r10-0x4b0]                   
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x5c0], r1                   
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x5c8], r1                   
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x568], r1                   
    jeq r0, 0, lbb_11916                            if r0 == (0 as i32 as i64 as u64) { pc += 846 }
    ldxdw r1, [r10-0x4d0]                   
    mov64 r7, r1                                    r7 = r1
    add64 r7, 267                                   r7 += 267   ///  r7 = r7.wrapping_add(267 as i32 as i64 as u64)
    add64 r1, 234                                   r1 += 234   ///  r1 = r1.wrapping_add(234 as i32 as i64 as u64)
lbb_11074:
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r0, 0, lbb_11082                            if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_11082:
    ldxdw r5, [r10-0x4b0]                   
lbb_11083:
    stxdw [r10-0x5c0], r1                   
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x5c8], r1                   
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x568], r1                   
    lddw r1, 0x2aade37a97cb17e5                     r1 load str located at 3075364236236101605
    jne r6, r1, lbb_11916                           if r6 != r1 { pc += 825 }
lbb_11091:
    ldxdw r2, [r10-0x558]                   
    mov64 r1, r2                                    r1 = r2
    and64 r1, 65532                                 r1 &= 65532   ///  r1 = r1.and(65532)
    jeq r1, 8, lbb_11108                            if r1 == (8 as i32 as i64 as u64) { pc += 13 }
    mov64 r3, r2                                    r3 = r2
    add64 r3, -12                                   r3 += -12   ///  r3 = r3.wrapping_add(-12 as i32 as i64 as u64)
    ldxdw r1, [r10-0x580]                   
    ldxw r2, [r1+0x2c]                      
    add64 r1, 48                                    r1 += 48   ///  r1 = r1.wrapping_add(48 as i32 as i64 as u64)
    stxdw [r10-0x4b8], r1                   
    stxdw [r10-0x90], r1                    
    jne r2, 0, lbb_11114                            if r2 != (0 as i32 as i64 as u64) { pc += 11 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    stxdw [r10-0x4b8], r1                   
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ja lbb_12218                                    if true { pc += 1110 }
lbb_11108:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    ldxdw r5, [r10-0x4b0]                   
    mov64 r9, r0                                    r9 = r0
    ja lbb_11886                                    if true { pc += 772 }
lbb_11114:
    mov64 r8, r3                                    r8 = r3
    mov64 r6, r2                                    r6 = r2
    stxdw [r10-0x4c0], r2                   
    jlt r2, 102, lbb_11119                          if r2 < (102 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 102                                   r6 = 102 as i32 as i64 as u64
lbb_11119:
    mov64 r7, r6                                    r7 = r6
    mul64 r7, 40                                    r7 *= 40   ///  r7 = r7.wrapping_mul(40 as u64)
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6742                      
    jeq r0, 0, lbb_13473                            if r0 == (0 as i32 as i64 as u64) { pc += 2348 }
    stxdw [r10-0x78], r0                    
    stxdw [r10-0x80], r6                    
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r4, 35                                    r4 = 35 as i32 as i64 as u64
    stdw [r10-0x70], 0                      
    ldxdw r5, [r10-0x4b0]                   
    ldxdw r1, [r10-0x4c0]                   
    mov64 r3, r8                                    r3 = r8
    ja lbb_11165                                    if true { pc += 31 }
lbb_11134:
    mov64 r1, r0                                    r1 = r0
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    stxb [r1-0x1], r9                       
    stxb [r1-0x2], r8                       
    stxb [r1-0x3], r7                       
    ldxdw r2, [r10-0x658]                   
    stxdw [r1-0xb], r2                      
    ldxdw r2, [r10-0x640]                   
    stxdw [r1-0x13], r2                     
    ldxdw r2, [r10-0x568]                   
    stxdw [r1-0x1b], r2                     
    ldxdw r2, [r10-0x648]                   
    stxw [r1-0x1f], r2                      
    ldxdw r2, [r10-0x660]                   
    stxb [r1-0x20], r2                      
    ldxdw r2, [r10-0x650]                   
    stxb [r1-0x21], r2                      
    ldxdw r2, [r10-0x5c8]                   
    stxb [r1-0x22], r2                      
    ldxdw r2, [r10-0x4d8]                   
    stxb [r1-0x23], r2                      
    ldxw r2, [r10-0x64]                     
    stxw [r1+0x0], r2                       
    ldxb r2, [r10-0x60]                     
    stxb [r1+0x4], r2                       
    stxdw [r10-0x70], r6                    
    add64 r4, 40                                    r4 += 40   ///  r4 = r4.wrapping_add(40 as i32 as i64 as u64)
    mov64 r7, r6                                    r7 = r6
    ldxdw r1, [r10-0x4c0]                   
    jlt r6, r1, lbb_11165                           if r6 < r1 { pc += 1 }
    ja lbb_12212                                    if true { pc += 1047 }
lbb_11165:
    jeq r3, 0, lbb_11747                            if r3 == (0 as i32 as i64 as u64) { pc += 581 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r2, r3                                    r2 = r3
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    ldxdw r1, [r10-0x4b8]                   
    ldxb r9, [r1+0x0]                       
    stxdw [r10-0x88], r2                    
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x90], r1                    
    stxb [r10-0x59], r9                     
    stxdw [r10-0x4c8], r0                   
    jsgt r9, 47, lbb_11194                          if (r9 as i64) > (47 as i32 as i64) { pc += 17 }
    jsgt r9, 23, lbb_11231                          if (r9 as i64) > (23 as i32 as i64) { pc += 53 }
    jsgt r9, 11, lbb_11255                          if (r9 as i64) > (11 as i32 as i64) { pc += 76 }
    jlt r9, 8, lbb_11703                            if r9 < (8 as i32 as i64 as u64) { pc += 523 }
    mov64 r1, r9                                    r1 = r9
    add64 r1, -9                                    r1 += -9   ///  r1 = r1.wrapping_add(-9 as i32 as i64 as u64)
    jlt r1, 3, lbb_11703                            if r1 < (3 as i32 as i64 as u64) { pc += 520 }
    jeq r9, 8, lbb_11185                            if r9 == (8 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11654                                    if true { pc += 469 }
lbb_11185:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_8435                      
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 504 }
    ja lbb_12844                                    if true { pc += 1650 }
lbb_11194:
    jsgt r9, 71, lbb_11202                          if (r9 as i64) > (71 as i32 as i64) { pc += 7 }
    jsgt r9, 59, lbb_11297                          if (r9 as i64) > (59 as i32 as i64) { pc += 101 }
    mov64 r1, r9                                    r1 = r9
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    jlt r1, 10, lbb_11703                           if r1 < (10 as i32 as i64 as u64) { pc += 504 }
    jeq r9, 58, lbb_11574                           if r9 == (58 as i32 as i64 as u64) { pc += 374 }
    jeq r9, 59, lbb_11703                           if r9 == (59 as i32 as i64 as u64) { pc += 502 }
    ja lbb_11654                                    if true { pc += 452 }
lbb_11202:
    jsgt r9, 84, lbb_11271                          if (r9 as i64) > (84 as i32 as i64) { pc += 68 }
    jsgt r9, 80, lbb_11391                          if (r9 as i64) > (80 as i32 as i64) { pc += 187 }
    mov64 r1, r9                                    r1 = r9
    add64 r1, -76                                   r1 += -76   ///  r1 = r1.wrapping_add(-76 as i32 as i64 as u64)
    jlt r1, 5, lbb_11703                            if r1 < (5 as i32 as i64 as u64) { pc += 496 }
    mov64 r1, r9                                    r1 = r9
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    jlt r1, 3, lbb_11703                            if r1 < (3 as i32 as i64 as u64) { pc += 493 }
    jeq r9, 75, lbb_11212                           if r9 == (75 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11654                                    if true { pc += 442 }
lbb_11212:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_17795                     
    ldxdw r1, [r10-0x38]                    
    stxdw [r10-0x640], r1                   
    ldxdw r6, [r10-0x40]                    
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    jeq r6, r1, lbb_13147                           if r6 == r1 { pc += 1923 }
    mov64 r2, r6                                    r2 = r6
    lddw r1, 0xffffffff00000000                     r1 load str located at -4294967296
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    ldxdw r1, [r10-0x30]                    
    stxdw [r10-0x658], r1                   
    ja lbb_11796                                    if true { pc += 565 }
lbb_11231:
    jsgt r9, 33, lbb_11248                          if (r9 as i64) > (33 as i32 as i64) { pc += 16 }
    jsgt r9, 28, lbb_11374                          if (r9 as i64) > (28 as i32 as i64) { pc += 141 }
    jsgt r9, 26, lbb_11466                          if (r9 as i64) > (26 as i32 as i64) { pc += 232 }
    mov64 r1, r9                                    r1 = r9
    add64 r1, -25                                   r1 += -25   ///  r1 = r1.wrapping_add(-25 as i32 as i64 as u64)
    jlt r1, 2, lbb_11703                            if r1 < (2 as i32 as i64 as u64) { pc += 466 }
    jeq r9, 24, lbb_11239                           if r9 == (24 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11654                                    if true { pc += 415 }
lbb_11239:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_17745                     
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 450 }
    ja lbb_12844                                    if true { pc += 1596 }
lbb_11248:
    jsgt r9, 40, lbb_11313                          if (r9 as i64) > (40 as i32 as i64) { pc += 64 }
    mov64 r1, r9                                    r1 = r9
    add64 r1, -34                                   r1 += -34   ///  r1 = r1.wrapping_add(-34 as i32 as i64 as u64)
    jlt r1, 5, lbb_11703                            if r1 < (5 as i32 as i64 as u64) { pc += 451 }
    jeq r9, 39, lbb_11592                           if r9 == (39 as i32 as i64 as u64) { pc += 339 }
    jeq r9, 40, lbb_11703                           if r9 == (40 as i32 as i64 as u64) { pc += 449 }
    ja lbb_11654                                    if true { pc += 399 }
lbb_11255:
    jsgt r9, 17, lbb_11359                          if (r9 as i64) > (17 as i32 as i64) { pc += 103 }
    jsgt r9, 14, lbb_11407                          if (r9 as i64) > (14 as i32 as i64) { pc += 150 }
    mov64 r1, r9                                    r1 = r9
    add64 r1, -13                                   r1 += -13   ///  r1 = r1.wrapping_add(-13 as i32 as i64 as u64)
    jlt r1, 2, lbb_11703                            if r1 < (2 as i32 as i64 as u64) { pc += 443 }
    jeq r9, 12, lbb_11262                           if r9 == (12 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11654                                    if true { pc += 392 }
lbb_11262:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_17745                     
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 427 }
    ja lbb_12844                                    if true { pc += 1573 }
lbb_11271:
    jsgt r9, 89, lbb_11650                          if (r9 as i64) > (89 as i32 as i64) { pc += 378 }
    jsgt r9, 86, lbb_11453                          if (r9 as i64) > (86 as i32 as i64) { pc += 180 }
    jeq r9, 85, lbb_11631                           if r9 == (85 as i32 as i64 as u64) { pc += 357 }
    jeq r9, 86, lbb_11276                           if r9 == (86 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11654                                    if true { pc += 378 }
lbb_11276:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_8435                      
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11285                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12844                                    if true { pc += 1559 }
lbb_11285:
    ldxdw r1, [r10-0x88]                    
    jeq r1, 0, lbb_11772                            if r1 == (0 as i32 as i64 as u64) { pc += 485 }
    ldxb r2, [r10-0x3f]                     
    stxdw [r10-0x650], r2                   
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    ldxdw r2, [r10-0x90]                    
    ldxb r3, [r2+0x0]                       
    stxdw [r10-0x5c8], r3                   
    stxdw [r10-0x88], r1                    
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x90], r2                    
    ja lbb_11699                                    if true { pc += 402 }
lbb_11297:
    jsgt r9, 63, lbb_11339                          if (r9 as i64) > (63 as i32 as i64) { pc += 41 }
    mov64 r1, r9                                    r1 = r9
    add64 r1, -62                                   r1 += -62   ///  r1 = r1.wrapping_add(-62 as i32 as i64 as u64)
    jlt r1, 2, lbb_11703                            if r1 < (2 as i32 as i64 as u64) { pc += 402 }
    jeq r9, 60, lbb_11504                           if r9 == (60 as i32 as i64 as u64) { pc += 202 }
    jeq r9, 61, lbb_11304                           if r9 == (61 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11654                                    if true { pc += 350 }
lbb_11304:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_8435                      
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 385 }
    ja lbb_12844                                    if true { pc += 1531 }
lbb_11313:
    jsgt r9, 43, lbb_11400                          if (r9 as i64) > (43 as i32 as i64) { pc += 86 }
    jeq r9, 41, lbb_11543                           if r9 == (41 as i32 as i64 as u64) { pc += 228 }
    jeq r9, 42, lbb_11513                           if r9 == (42 as i32 as i64 as u64) { pc += 197 }
    jeq r9, 43, lbb_11318                           if r9 == (43 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11654                                    if true { pc += 336 }
lbb_11318:
    jeq r2, 0, lbb_11747                            if r2 == (0 as i32 as i64 as u64) { pc += 428 }
    jeq r3, 2, lbb_11747                            if r3 == (2 as i32 as i64 as u64) { pc += 427 }
    mov64 r2, r3                                    r2 = r3
    add64 r2, -3                                    r2 += -3   ///  r2 = r2.wrapping_add(-3 as i32 as i64 as u64)
    jlt r2, 4, lbb_11747                            if r2 < (4 as i32 as i64 as u64) { pc += 424 }
    mov64 r2, r3                                    r2 = r3
    add64 r2, -7                                    r2 += -7   ///  r2 = r2.wrapping_add(-7 as i32 as i64 as u64)
    jlt r2, 4, lbb_11747                            if r2 < (4 as i32 as i64 as u64) { pc += 421 }
    ldxb r1, [r1+0x0]                       
    stxdw [r10-0x5c8], r1                   
    ldxdw r2, [r10-0x4b8]                   
    ldxb r1, [r2+0x2]                       
    stxdw [r10-0x650], r1                   
    ldxw r1, [r2+0x3]                       
    stxdw [r10-0x648], r1                   
    add64 r3, -11                                   r3 += -11   ///  r3 = r3.wrapping_add(-11 as i32 as i64 as u64)
    ldxw r6, [r2+0x7]                       
    stxdw [r10-0x88], r3                    
    add64 r2, 11                                    r2 += 11   ///  r2 = r2.wrapping_add(11 as i32 as i64 as u64)
    stxdw [r10-0x90], r2                    
    ja lbb_11649                                    if true { pc += 310 }
lbb_11339:
    stxdw [r10-0x4d8], r3                   
    mov64 r3, r9                                    r3 = r9
    add64 r3, -65                                   r3 += -65   ///  r3 = r3.wrapping_add(-65 as i32 as i64 as u64)
    jlt r3, 6, lbb_11703                            if r3 < (6 as i32 as i64 as u64) { pc += 360 }
    jeq r9, 64, lbb_11640                           if r9 == (64 as i32 as i64 as u64) { pc += 296 }
    jeq r9, 71, lbb_11346                           if r9 == (71 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11654                                    if true { pc += 308 }
lbb_11346:
    ldxdw r3, [r10-0x4d8]                   
    jeq r2, 0, lbb_11747                            if r2 == (0 as i32 as i64 as u64) { pc += 399 }
    jeq r3, 2, lbb_11747                            if r3 == (2 as i32 as i64 as u64) { pc += 398 }
    ldxb r1, [r1+0x0]                       
    stxdw [r10-0x5c8], r1                   
    add64 r3, -3                                    r3 += -3   ///  r3 = r3.wrapping_add(-3 as i32 as i64 as u64)
    ldxdw r1, [r10-0x4b8]                   
    ldxb r2, [r1+0x2]                       
    stxdw [r10-0x650], r2                   
    stxdw [r10-0x88], r3                    
    add64 r1, 3                                     r1 += 3   ///  r1 = r1.wrapping_add(3 as i32 as i64 as u64)
    stxdw [r10-0x90], r1                    
    ja lbb_11630                                    if true { pc += 271 }
lbb_11359:
    jsgt r9, 20, lbb_11440                          if (r9 as i64) > (20 as i32 as i64) { pc += 80 }
    mov64 r1, r9                                    r1 = r9
    add64 r1, -19                                   r1 += -19   ///  r1 = r1.wrapping_add(-19 as i32 as i64 as u64)
    jlt r1, 2, lbb_11703                            if r1 < (2 as i32 as i64 as u64) { pc += 340 }
    jeq r9, 18, lbb_11365                           if r9 == (18 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11654                                    if true { pc += 289 }
lbb_11365:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_8435                      
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 324 }
    ja lbb_12844                                    if true { pc += 1470 }
lbb_11374:
    stxdw [r10-0x4d8], r3                   
    mov64 r3, r9                                    r3 = r9
    add64 r3, -30                                   r3 += -30   ///  r3 = r3.wrapping_add(-30 as i32 as i64 as u64)
    jlt r3, 3, lbb_11703                            if r3 < (3 as i32 as i64 as u64) { pc += 325 }
    jeq r9, 29, lbb_11746                           if r9 == (29 as i32 as i64 as u64) { pc += 367 }
    jeq r9, 33, lbb_11381                           if r9 == (33 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11654                                    if true { pc += 273 }
lbb_11381:
    ldxdw r2, [r10-0x4d8]                   
    jlt r2, 5, lbb_11747                            if r2 < (5 as i32 as i64 as u64) { pc += 364 }
    add64 r2, -5                                    r2 += -5   ///  r2 = r2.wrapping_add(-5 as i32 as i64 as u64)
    ldxdw r3, [r10-0x4b8]                   
    ldxw r1, [r3+0x1]                       
    stxdw [r10-0x648], r1                   
    stxdw [r10-0x88], r2                    
    add64 r3, 5                                     r3 += 5   ///  r3 = r3.wrapping_add(5 as i32 as i64 as u64)
    stxdw [r10-0x90], r3                    
    ja lbb_11551                                    if true { pc += 160 }
lbb_11391:
    mov64 r0, r3                                    r0 = r3
    mov64 r1, r9                                    r1 = r9
    add64 r1, -83                                   r1 += -83   ///  r1 = r1.wrapping_add(-83 as i32 as i64 as u64)
    jlt r1, 2, lbb_11703                            if r1 < (2 as i32 as i64 as u64) { pc += 308 }
    jeq r9, 81, lbb_11552                           if r9 == (81 as i32 as i64 as u64) { pc += 156 }
    jeq r9, 82, lbb_11398                           if r9 == (82 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11654                                    if true { pc += 256 }
lbb_11398:
    jgt r2, 7, lbb_11554                            if r2 > (7 as i32 as i64 as u64) { pc += 155 }
    ja lbb_11747                                    if true { pc += 347 }
lbb_11400:
    jsgt r9, 45, lbb_11478                          if (r9 as i64) > (45 as i32 as i64) { pc += 77 }
    jeq r9, 44, lbb_11619                           if r9 == (44 as i32 as i64 as u64) { pc += 217 }
    jeq r9, 45, lbb_11404                           if r9 == (45 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11654                                    if true { pc += 250 }
lbb_11404:
    jeq r2, 0, lbb_11747                            if r2 == (0 as i32 as i64 as u64) { pc += 342 }
    jlt r3, 6, lbb_11747                            if r3 < (6 as i32 as i64 as u64) { pc += 341 }
    ja lbb_11621                                    if true { pc += 214 }
lbb_11407:
    jeq r9, 15, lbb_11601                           if r9 == (15 as i32 as i64 as u64) { pc += 193 }
    jeq r9, 16, lbb_11565                           if r9 == (16 as i32 as i64 as u64) { pc += 156 }
    jeq r9, 17, lbb_11411                           if r9 == (17 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11654                                    if true { pc += 243 }
lbb_11411:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_8435                      
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11420                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12844                                    if true { pc += 1424 }
lbb_11420:
    ldxb r1, [r10-0x3f]                     
    stxdw [r10-0x5c8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_10232                     
    ldxdw r1, [r10-0x38]                    
    stxdw [r10-0x640], r1                   
    ldxdw r6, [r10-0x40]                    
    lddw r1, 0x8000000000000001                     r1 load str located at -9223372036854775807
    jeq r6, r1, lbb_13147                           if r6 == r1 { pc += 1714 }
lbb_11433:
    mov64 r2, r6                                    r2 = r6
    lddw r1, 0xffffffff00000000                     r1 load str located at -4294967296
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    ldxdw r1, [r10-0x30]                    
    stxdw [r10-0x658], r1                   
    ja lbb_11796                                    if true { pc += 356 }
lbb_11440:
    jeq r9, 21, lbb_11610                           if r9 == (21 as i32 as i64 as u64) { pc += 169 }
    jeq r9, 22, lbb_11703                           if r9 == (22 as i32 as i64 as u64) { pc += 261 }
    jeq r9, 23, lbb_11444                           if r9 == (23 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11654                                    if true { pc += 210 }
lbb_11444:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_8435                      
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 245 }
    ja lbb_12844                                    if true { pc += 1391 }
lbb_11453:
    jeq r9, 87, lbb_11770                           if r9 == (87 as i32 as i64 as u64) { pc += 316 }
    jeq r9, 88, lbb_11703                           if r9 == (88 as i32 as i64 as u64) { pc += 248 }
    jeq r9, 89, lbb_11457                           if r9 == (89 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11654                                    if true { pc += 197 }
lbb_11457:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_17745                     
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 232 }
    ja lbb_12844                                    if true { pc += 1378 }
lbb_11466:
    jeq r9, 27, lbb_11583                           if r9 == (27 as i32 as i64 as u64) { pc += 116 }
    jeq r9, 28, lbb_11469                           if r9 == (28 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11654                                    if true { pc += 185 }
lbb_11469:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_17745                     
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 220 }
    ja lbb_12844                                    if true { pc += 1366 }
lbb_11478:
    jeq r9, 46, lbb_11703                           if r9 == (46 as i32 as i64 as u64) { pc += 224 }
    jeq r9, 47, lbb_11481                           if r9 == (47 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11654                                    if true { pc += 173 }
lbb_11481:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_8435                      
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11490                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12844                                    if true { pc += 1354 }
lbb_11490:
    ldxb r1, [r10-0x3f]                     
    stxdw [r10-0x5c8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_10232                     
    ldxdw r1, [r10-0x38]                    
    stxdw [r10-0x640], r1                   
    ldxdw r6, [r10-0x40]                    
    lddw r1, 0x8000000000000001                     r1 load str located at -9223372036854775807
    jeq r6, r1, lbb_13147                           if r6 == r1 { pc += 1644 }
    ja lbb_11433                                    if true { pc += -71 }
lbb_11504:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_8435                      
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 185 }
    ja lbb_12844                                    if true { pc += 1331 }
lbb_11513:
    mov64 r8, r4                                    r8 = r4
    jeq r2, 0, lbb_11772                            if r2 == (0 as i32 as i64 as u64) { pc += 257 }
    add64 r3, -2                                    r3 += -2   ///  r3 = r3.wrapping_add(-2 as i32 as i64 as u64)
    ldxdw r1, [r10-0x4b8]                   
    ldxb r6, [r1+0x1]                       
    stxdw [r10-0x88], r3                    
    add64 r1, 2                                     r1 += 2   ///  r1 = r1.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r10-0x90], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_8435                      
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11529                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12844                                    if true { pc += 1315 }
lbb_11529:
    stxdw [r10-0x5c8], r6                   
    ldxb r1, [r10-0x3f]                     
    stxdw [r10-0x650], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_8435                      
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11540                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12844                                    if true { pc += 1304 }
lbb_11540:
    ldxb r1, [r10-0x3f]                     
    stxdw [r10-0x660], r1                   
    ja lbb_11699                                    if true { pc += 156 }
lbb_11543:
    jlt r3, 5, lbb_11747                            if r3 < (5 as i32 as i64 as u64) { pc += 203 }
    add64 r3, -5                                    r3 += -5   ///  r3 = r3.wrapping_add(-5 as i32 as i64 as u64)
    ldxdw r2, [r10-0x4b8]                   
    ldxw r1, [r2+0x1]                       
    stxdw [r10-0x648], r1                   
    stxdw [r10-0x88], r3                    
    add64 r2, 5                                     r2 += 5   ///  r2 = r2.wrapping_add(5 as i32 as i64 as u64)
    stxdw [r10-0x90], r2                    
lbb_11551:
    ja lbb_11649                                    if true { pc += 97 }
lbb_11552:
    jgt r2, 7, lbb_11554                            if r2 > (7 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11747                                    if true { pc += 193 }
lbb_11554:
    add64 r0, -9                                    r0 += -9   ///  r0 = r0.wrapping_add(-9 as i32 as i64 as u64)
    ldxdw r1, [r10-0x4b8]                   
    ldxdw r6, [r1+0x1]                      
    stxdw [r10-0x88], r0                    
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r10-0x90], r1                    
    mov64 r8, r6                                    r8 = r6
    lddw r1, 0xffffffff00000000                     r1 load str located at -4294967296
    and64 r8, r1                                    r8 &= r1   ///  r8 = r8.and(r1)
    ja lbb_11649                                    if true { pc += 84 }
lbb_11565:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_17745                     
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 124 }
    ja lbb_12844                                    if true { pc += 1270 }
lbb_11574:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_8435                      
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 115 }
    ja lbb_12844                                    if true { pc += 1261 }
lbb_11583:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_17745                     
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 106 }
    ja lbb_12844                                    if true { pc += 1252 }
lbb_11592:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_17745                     
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 97 }
    ja lbb_12844                                    if true { pc += 1243 }
lbb_11601:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_17745                     
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 88 }
    ja lbb_12844                                    if true { pc += 1234 }
lbb_11610:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_8435                      
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 79 }
    ja lbb_12844                                    if true { pc += 1225 }
lbb_11619:
    jeq r2, 0, lbb_11747                            if r2 == (0 as i32 as i64 as u64) { pc += 127 }
    jlt r3, 6, lbb_11747                            if r3 < (6 as i32 as i64 as u64) { pc += 126 }
lbb_11621:
    ldxb r1, [r1+0x0]                       
    stxdw [r10-0x5c8], r1                   
    add64 r3, -6                                    r3 += -6   ///  r3 = r3.wrapping_add(-6 as i32 as i64 as u64)
    ldxdw r2, [r10-0x4b8]                   
    ldxw r1, [r2+0x2]                       
    stxdw [r10-0x648], r1                   
    stxdw [r10-0x88], r3                    
    add64 r2, 6                                     r2 += 6   ///  r2 = r2.wrapping_add(6 as i32 as i64 as u64)
    stxdw [r10-0x90], r2                    
lbb_11630:
    ja lbb_11649                                    if true { pc += 18 }
lbb_11631:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_17745                     
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 58 }
    ja lbb_12844                                    if true { pc += 1204 }
lbb_11640:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_17745                     
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 49 }
    ja lbb_12844                                    if true { pc += 1195 }
lbb_11649:
    ja lbb_11703                                    if true { pc += 53 }
lbb_11650:
    jslt r9, 94, lbb_11703                          if (r9 as i64) < (94 as i32 as i64) { pc += 52 }
    jeq r9, 94, lbb_11688                           if r9 == (94 as i32 as i64 as u64) { pc += 36 }
    jeq r9, 95, lbb_11679                           if r9 == (95 as i32 as i64 as u64) { pc += 26 }
    jeq r9, 96, lbb_11703                           if r9 == (96 as i32 as i64 as u64) { pc += 49 }
lbb_11654:
    lddw r1, 0x100034cb0 --> b"\x00\x00\x00\x00\xb48\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295183536
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100010468 --> b"a#4\x00\x00\x00\x00\x00\xbf4\x00\x00\x00\x00\x00\x00W\x04\x00\x00\x10\x00…        r1 load str located at 4295033960
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r9, r10                                   r9 = r10
    add64 r9, -88                                   r9 += -88   ///  r9 = r9.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    mov64 r6, r4                                    r6 = r4
    call function_8487                      
    mov64 r1, r9                                    r1 = r9
    call function_8277                      
    ja lbb_11801                                    if true { pc += 122 }
lbb_11679:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_8435                      
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 10 }
    ja lbb_12844                                    if true { pc += 1156 }
lbb_11688:
    mov64 r8, r4                                    r8 = r4
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_8435                      
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_11697                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12844                                    if true { pc += 1147 }
lbb_11697:
    ldxb r1, [r10-0x3f]                     
    stxdw [r10-0x5c8], r1                   
lbb_11699:
    ldxdw r5, [r10-0x4b0]                   
    mov64 r4, r8                                    r4 = r8
    ldxdw r1, [r10-0x4c0]                   
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_11703:
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    or64 r8, r6                                     r8 |= r6   ///  r8 = r8.or(r6)
    ldxdw r3, [r10-0x88]                    
    jeq r3, 0, lbb_11805                            if r3 == (0 as i32 as i64 as u64) { pc += 97 }
    jeq r3, 1, lbb_11818                            if r3 == (1 as i32 as i64 as u64) { pc += 109 }
    jeq r3, 2, lbb_11831                            if r3 == (2 as i32 as i64 as u64) { pc += 121 }
    stxdw [r10-0x568], r8                   
    stxdw [r10-0x4d8], r9                   
    mov64 r6, r7                                    r6 = r7
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    ldxdw r0, [r10-0x90]                    
    mov64 r2, r7                                    r2 = r7
    ldxb r7, [r0+0x0]                       
    ldxb r8, [r0+0x1]                       
    ldxb r9, [r0+0x2]                       
    ldxw r1, [r10-0x5e]                     
    stxw [r10-0x64], r1                     
    ldxb r1, [r10-0x5a]                     
    stxb [r10-0x60], r1                     
    add64 r3, -3                                    r3 += -3   ///  r3 = r3.wrapping_add(-3 as i32 as i64 as u64)
    stxdw [r10-0x88], r3                    
    add64 r0, 3                                     r0 += 3   ///  r0 = r0.wrapping_add(3 as i32 as i64 as u64)
    stxdw [r10-0x4b8], r0                   
    stxdw [r10-0x90], r0                    
    ldxdw r1, [r10-0x80]                    
    ldxdw r0, [r10-0x4c8]                   
    jne r2, r1, lbb_11134                           if r2 != r1 { pc += -597 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    stxdw [r10-0x4c8], r7                   
    mov64 r7, r4                                    r7 = r4
    stxdw [r10-0x668], r8                   
    mov64 r8, r3                                    r8 = r3
    call function_8568                      
    mov64 r3, r8                                    r3 = r8
    ldxdw r8, [r10-0x668]                   
    ldxdw r1, [r10-0x4c0]                   
    mov64 r4, r7                                    r4 = r7
    ldxdw r7, [r10-0x4c8]                   
    ldxdw r5, [r10-0x4b0]                   
    ldxdw r0, [r10-0x78]                    
    ja lbb_11134                                    if true { pc += -612 }
lbb_11746:
    jgt r2, 7, lbb_11752                            if r2 > (7 as i32 as i64 as u64) { pc += 5 }
lbb_11747:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    mov64 r6, r4                                    r6 = r4
    call function_19705                     
    ja lbb_11801                                    if true { pc += 49 }
lbb_11752:
    ldxdw r3, [r10-0x4d8]                   
    mov64 r2, r3                                    r2 = r3
    add64 r2, -9                                    r2 += -9   ///  r2 = r2.wrapping_add(-9 as i32 as i64 as u64)
    jgt r2, 7, lbb_11757                            if r2 > (7 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11747                                    if true { pc += -10 }
lbb_11757:
    ldxdw r6, [r1+0x0]                      
    add64 r3, -17                                   r3 += -17   ///  r3 = r3.wrapping_add(-17 as i32 as i64 as u64)
    ldxdw r2, [r10-0x4b8]                   
    ldxdw r1, [r2+0x9]                      
    stxdw [r10-0x640], r1                   
    stxdw [r10-0x88], r3                    
    add64 r2, 17                                    r2 += 17   ///  r2 = r2.wrapping_add(17 as i32 as i64 as u64)
    stxdw [r10-0x90], r2                    
    mov64 r8, r6                                    r8 = r6
    lddw r1, 0xffffffff00000000                     r1 load str located at -4294967296
    and64 r8, r1                                    r8 &= r1   ///  r8 = r8.and(r1)
    ja lbb_11649                                    if true { pc += -121 }
lbb_11770:
    mov64 r8, r4                                    r8 = r4
    jgt r3, 8, lbb_11777                            if r3 > (8 as i32 as i64 as u64) { pc += 5 }
lbb_11772:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    stxdw [r10-0x4b8], r0                   
    ja lbb_12846                                    if true { pc += 1069 }
lbb_11777:
    add64 r3, -9                                    r3 += -9   ///  r3 = r3.wrapping_add(-9 as i32 as i64 as u64)
    ldxdw r1, [r10-0x4b8]                   
    ldxdw r6, [r1+0x1]                      
    stxdw [r10-0x88], r3                    
    add64 r1, 9                                     r1 += 9   ///  r1 = r1.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r10-0x90], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -144                                  r2 += -144   ///  r2 = r2.wrapping_add(-144 as i32 as i64 as u64)
    call function_8435                      
    ldxb r1, [r10-0x40]                     
    jne r1, 0, lbb_12844                            if r1 != (0 as i32 as i64 as u64) { pc += 1054 }
    mov64 r2, r6                                    r2 = r6
    lddw r1, 0xffffffff00000000                     r1 load str located at -4294967296
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    ldxb r1, [r10-0x3f]                     
    stxdw [r10-0x5c8], r1                   
lbb_11796:
    ldxdw r5, [r10-0x4b0]                   
    mov64 r4, r8                                    r4 = r8
    mov64 r8, r2                                    r8 = r2
    ldxdw r1, [r10-0x4c0]                   
    ja lbb_11703                                    if true { pc += -98 }
lbb_11801:
    mov64 r3, r6                                    r3 = r6
    ldxdw r5, [r10-0x4b0]                   
    stxdw [r10-0x4b8], r0                   
    ja lbb_11843                                    if true { pc += 38 }
lbb_11805:
    mov64 r6, r4                                    r6 = r4
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    stxdw [r10-0x4b8], r0                   
    jeq r9, 17, lbb_12886                           if r9 == (17 as i32 as i64 as u64) { pc += 1075 }
    ldxdw r5, [r10-0x4b0]                   
    mov64 r3, r6                                    r3 = r6
    jeq r9, 47, lbb_12849                           if r9 == (47 as i32 as i64 as u64) { pc += 1035 }
    jeq r9, 75, lbb_11816                           if r9 == (75 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11817                                    if true { pc += 1 }
lbb_11816:
    jne r8, 0, lbb_12893                            if r8 != (0 as i32 as i64 as u64) { pc += 1076 }
lbb_11817:
    ja lbb_11843                                    if true { pc += 25 }
lbb_11818:
    mov64 r6, r4                                    r6 = r4
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    stxdw [r10-0x4b8], r0                   
    jeq r9, 17, lbb_12862                           if r9 == (17 as i32 as i64 as u64) { pc += 1038 }
    mov64 r3, r6                                    r3 = r6
    jeq r9, 47, lbb_12855                           if r9 == (47 as i32 as i64 as u64) { pc += 1029 }
    ldxdw r5, [r10-0x4b0]                   
    jeq r9, 75, lbb_11829                           if r9 == (75 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11830                                    if true { pc += 1 }
lbb_11829:
    jne r8, 0, lbb_12869                            if r8 != (0 as i32 as i64 as u64) { pc += 1039 }
lbb_11830:
    ja lbb_11843                                    if true { pc += 12 }
lbb_11831:
    mov64 r6, r4                                    r6 = r4
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    stxdw [r10-0x4b8], r0                   
    jeq r9, 17, lbb_12878                           if r9 == (17 as i32 as i64 as u64) { pc += 1041 }
    jeq r9, 47, lbb_12870                           if r9 == (47 as i32 as i64 as u64) { pc += 1032 }
    ldxdw r5, [r10-0x4b0]                   
    mov64 r3, r6                                    r3 = r6
    jeq r9, 75, lbb_11842                           if r9 == (75 as i32 as i64 as u64) { pc += 1 }
    ja lbb_11843                                    if true { pc += 1 }
lbb_11842:
    jne r8, 0, lbb_12885                            if r8 != (0 as i32 as i64 as u64) { pc += 1042 }
lbb_11843:
    ldxdw r9, [r10-0x78]                    
    jeq r3, 35, lbb_11853                           if r3 == (35 as i32 as i64 as u64) { pc += 8 }
    mov64 r6, r9                                    r6 = r9
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_11861                                    if true { pc += 13 }
lbb_11848:
    ldxdw r2, [r6+0x0]                      
    jne r2, 0, lbb_11879                            if r2 != (0 as i32 as i64 as u64) { pc += 29 }
lbb_11850:
    add64 r6, 40                                    r6 += 40   ///  r6 = r6.wrapping_add(40 as i32 as i64 as u64)
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    jne r7, 0, lbb_11861                            if r7 != (0 as i32 as i64 as u64) { pc += 8 }
lbb_11853:
    ldxdw r2, [r10-0x80]                    
    jeq r2, 0, lbb_11885                            if r2 == (0 as i32 as i64 as u64) { pc += 30 }
    mul64 r2, 40                                    r2 *= 40   ///  r2 = r2.wrapping_mul(40 as u64)
    mov64 r1, r9                                    r1 = r9
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ldxdw r5, [r10-0x4b0]                   
    ja lbb_11885                                    if true { pc += 24 }
lbb_11861:
    ldxb r1, [r6-0x8]                       
    jeq r1, 17, lbb_11873                           if r1 == (17 as i32 as i64 as u64) { pc += 10 }
    jeq r1, 47, lbb_11866                           if r1 == (47 as i32 as i64 as u64) { pc += 2 }
    jeq r1, 75, lbb_11848                           if r1 == (75 as i32 as i64 as u64) { pc += -17 }
    ja lbb_11850                                    if true { pc += -16 }
lbb_11866:
    ldxdw r2, [r6+0x0]                      
    mov64 r1, r2                                    r1 = r2
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    jeq r1, r3, lbb_11850                           if r1 == r3 { pc += -22 }
    ja lbb_11879                                    if true { pc += 6 }
lbb_11873:
    ldxdw r2, [r6+0x0]                      
    mov64 r1, r2                                    r1 = r2
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    jeq r1, r3, lbb_11850                           if r1 == r3 { pc += -29 }
lbb_11879:
    ldxdw r1, [r6+0x8]                      
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6768                      
    ldxdw r5, [r10-0x4b0]                   
    ja lbb_11850                                    if true { pc += -35 }
lbb_11885:
    ldxdw r9, [r10-0x4b8]                   
lbb_11886:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x5c8], r1                   
    mov64 r1, r9                                    r1 = r9
    and64 r1, 3                                     r1 &= 3   ///  r1 = r1.and(3)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -2                                    r2 += -2   ///  r2 = r2.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    stxdw [r10-0x568], r3                   
    jlt r2, 2, lbb_11916                            if r2 < (2 as i32 as i64 as u64) { pc += 21 }
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    stxdw [r10-0x568], r2                   
    jeq r1, 0, lbb_11916                            if r1 == (0 as i32 as i64 as u64) { pc += 18 }
    ldxdw r7, [r9-0x1]                      
    ldxdw r6, [r9+0x7]                      
    ldxdw r2, [r6+0x0]                      
    mov64 r1, r7                                    r1 = r7
    callx r2                                
    ldxdw r2, [r6+0x8]                      
    jeq r2, 0, lbb_11908                            if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r3, [r6+0x10]                     
    mov64 r1, r7                                    r1 = r7
    call function_6768                      
lbb_11908:
    add64 r9, -1                                    r9 += -1   ///  r9 = r9.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x568], r1                   
    ldxdw r5, [r10-0x4b0]                   
lbb_11916:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    lddw r1, 0xe83607d25cff24fd                     r1 load str located at -1714174008083143427
    jeq r5, r1, lbb_11921                           if r5 == r1 { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_11921:
    ldxdw r1, [r10-0x528]                   
    or64 r9, r1                                     r9 |= r1   ///  r9 = r9.or(r1)
    ldxdw r2, [r10-0x510]                   
    ldxdw r6, [r10-0x508]                   
    ldxdw r7, [r10-0x5d0]                   
    lddw r1, 0xd0875430b9e3561d                     r1 load str located at -3420672823710755299
    ldxdw r0, [r10-0x530]                   
    ldxdw r3, [r10-0x538]                   
    ldxdw r4, [r10-0x540]                   
    ldxdw r8, [r10-0x548]                   
    jeq r5, r1, lbb_11937                           if r5 == r1 { pc += 4 }
    stxdw [r10-0x528], r9                   
    lddw r1, 0xb328e792b9e3561d                     r1 load str located at -5536921124482099683
    jne r5, r1, lbb_12632                           if r5 != r1 { pc += 695 }
lbb_11937:
    ldxdw r8, [r10-0x4d0]                   
    ldxh r1, [r8+0x0]                       
    mul64 r1, 33                                    r1 *= 33   ///  r1 = r1.wrapping_mul(33 as u64)
    add64 r8, r1                                    r8 += r1   ///  r8 = r8.wrapping_add(r1)
    ldxh r3, [r8+0x22]                      
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jlt r3, 8, lbb_12611                            if r3 < (8 as i32 as i64 as u64) { pc += 667 }
    lddw r1, 0x10000000000                          r1 load str located at 1099511627776
    stxdw [r10-0x4c0], r1                   
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x4b8], r1                   
    ldxdw r4, [r8+0x24]                     
    lddw r1, 0xc88775e1919ec6f7                     r1 load str located at -3997096532596832521
    jsgt r4, r1, lbb_12030                          if (r4 as i64) > (r1 as i64) { pc += 77 }
    lddw r1, 0x885b5beb4c3f4b41                     r1 load str located at -8621195995516023999
    jeq r4, r1, lbb_12041                           if r4 == r1 { pc += 85 }
    lddw r1, 0xa569c4ba92b864de                     r1 load str located at -6527469879292304162
    jeq r4, r1, lbb_12041                           if r4 == r1 { pc += 82 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    lddw r5, 0xc3c420f16c7f4dcd                     r5 load str located at -4340307919598826035
    jeq r4, r5, lbb_11965                           if r4 == r5 { pc += 1 }
    ja lbb_12603                                    if true { pc += 638 }
lbb_11965:
    add64 r3, -8                                    r3 += -8   ///  r3 = r3.wrapping_add(-8 as i32 as i64 as u64)
    stxdw [r10-0x50], r3                    
    add64 r8, 44                                    r8 += 44   ///  r8 = r8.wrapping_add(44 as i32 as i64 as u64)
    stxdw [r10-0x58], r8                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -88                                   r2 += -88   ///  r2 = r2.wrapping_add(-88 as i32 as i64 as u64)
    call function_8977                      
    ldxdw r8, [r10-0x38]                    
    ldxdw r4, [r10-0x40]                    
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    jeq r4, r1, lbb_12392                           if r4 == r1 { pc += 413 }
    ldxdw r7, [r10-0x30]                    
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0x4d0], r4                   
    stxdw [r10-0x4c8], r8                   
    jlt r1, 8, lbb_12106                            if r1 < (8 as i32 as i64 as u64) { pc += 122 }
    ldxdw r2, [r10-0x58]                    
    ldxdw r5, [r2+0x0]                      
    mov64 r3, r2                                    r3 = r2
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x58], r3                    
    mov64 r3, r1                                    r3 = r1
    add64 r3, -8                                    r3 += -8   ///  r3 = r3.wrapping_add(-8 as i32 as i64 as u64)
    stxdw [r10-0x50], r3                    
    jlt r3, 2, lbb_11994                            if r3 < (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12122                                    if true { pc += 128 }
lbb_11994:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    mov64 r8, r0                                    r8 = r0
    jeq r7, 0, lbb_12005                            if r7 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r6, [r10-0x4c8]                   
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_12010                                    if true { pc += 8 }
lbb_12002:
    add64 r6, 40                                    r6 += 40   ///  r6 = r6.wrapping_add(40 as i32 as i64 as u64)
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    jne r7, 0, lbb_12010                            if r7 != (0 as i32 as i64 as u64) { pc += 5 }
lbb_12005:
    ldxdw r6, [r10-0x508]                   
    ldxdw r7, [r10-0x5d0]                   
    ldxdw r2, [r10-0x4d0]                   
    jeq r2, 0, lbb_12392                            if r2 == (0 as i32 as i64 as u64) { pc += 383 }
    ja lbb_12367                                    if true { pc += 357 }
lbb_12010:
    ldxdw r2, [r6-0x8]                      
    mov64 r4, r2                                    r4 = r2
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 31, lbb_12019                           if r4 != (31 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_12019:
    jlt r4, 34, lbb_12021                           if r4 < (34 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_12021:
    jeq r2, 0, lbb_12002                            if r2 == (0 as i32 as i64 as u64) { pc += -20 }
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_12002                            if r1 != (0 as i32 as i64 as u64) { pc += -23 }
    ldxdw r1, [r6+0x0]                      
    lsh64 r2, 4                                     r2 <<= 4   ///  r2 = r2.wrapping_shl(4)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_12002                                    if true { pc += -28 }
lbb_12030:
    lddw r1, 0xc88775e1919ec6f8                     r1 load str located at -3997096532596832520
    jeq r4, r1, lbb_11965                           if r4 == r1 { pc += -68 }
    lddw r1, 0x65879cc54d18aca8                     r1 load str located at 7315988490902613160
    jeq r4, r1, lbb_11965                           if r4 == r1 { pc += -71 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    lddw r5, 0xe756017af6d57b5f                     r5 load str located at -1777231375312258209
    jne r4, r5, lbb_12603                           if r4 != r5 { pc += 562 }
lbb_12041:
    add64 r3, -8                                    r3 += -8   ///  r3 = r3.wrapping_add(-8 as i32 as i64 as u64)
    stxdw [r10-0x50], r3                    
    add64 r8, 44                                    r8 += 44   ///  r8 = r8.wrapping_add(44 as i32 as i64 as u64)
    stxdw [r10-0x58], r8                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -88                                   r2 += -88   ///  r2 = r2.wrapping_add(-88 as i32 as i64 as u64)
    call function_8977                      
    ldxdw r8, [r10-0x38]                    
    ldxdw r4, [r10-0x40]                    
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    jeq r4, r1, lbb_12594                           if r4 == r1 { pc += 539 }
    ldxdw r7, [r10-0x30]                    
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0x4d0], r4                   
    stxdw [r10-0x4c8], r8                   
    jlt r1, 8, lbb_12114                            if r1 < (8 as i32 as i64 as u64) { pc += 54 }
    ldxdw r2, [r10-0x58]                    
    ldxdw r5, [r2+0x0]                      
    mov64 r3, r2                                    r3 = r2
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x58], r3                    
    mov64 r3, r1                                    r3 = r1
    add64 r3, -8                                    r3 += -8   ///  r3 = r3.wrapping_add(-8 as i32 as i64 as u64)
    stxdw [r10-0x50], r3                    
    jlt r3, 2, lbb_12070                            if r3 < (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12167                                    if true { pc += 97 }
lbb_12070:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    mov64 r8, r0                                    r8 = r0
    jeq r7, 0, lbb_12081                            if r7 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r6, [r10-0x4c8]                   
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_12086                                    if true { pc += 8 }
lbb_12078:
    add64 r6, 40                                    r6 += 40   ///  r6 = r6.wrapping_add(40 as i32 as i64 as u64)
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    jne r7, 0, lbb_12086                            if r7 != (0 as i32 as i64 as u64) { pc += 5 }
lbb_12081:
    ldxdw r6, [r10-0x508]                   
    ldxdw r7, [r10-0x5d0]                   
    ldxdw r2, [r10-0x4d0]                   
    jeq r2, 0, lbb_12594                            if r2 == (0 as i32 as i64 as u64) { pc += 509 }
    ja lbb_12569                                    if true { pc += 483 }
lbb_12086:
    ldxdw r2, [r6-0x8]                      
    mov64 r4, r2                                    r4 = r2
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 31, lbb_12095                           if r4 != (31 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_12095:
    jlt r4, 34, lbb_12097                           if r4 < (34 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_12097:
    jeq r2, 0, lbb_12078                            if r2 == (0 as i32 as i64 as u64) { pc += -20 }
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_12078                            if r1 != (0 as i32 as i64 as u64) { pc += -23 }
    ldxdw r1, [r6+0x0]                      
    lsh64 r2, 4                                     r2 <<= 4   ///  r2 = r2.wrapping_shl(4)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_12078                                    if true { pc += -28 }
lbb_12106:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    mov64 r8, r0                                    r8 = r0
    jeq r7, 0, lbb_12363                            if r7 == (0 as i32 as i64 as u64) { pc += 252 }
    ldxdw r6, [r10-0x4c8]                   
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_12372                                    if true { pc += 258 }
lbb_12114:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    mov64 r8, r0                                    r8 = r0
    jeq r7, 0, lbb_12565                            if r7 == (0 as i32 as i64 as u64) { pc += 446 }
    ldxdw r6, [r10-0x4c8]                   
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_12574                                    if true { pc += 452 }
lbb_12122:
    ldxh r0, [r2+0x8]                       
    mov64 r3, r2                                    r3 = r2
    add64 r3, 10                                    r3 += 10   ///  r3 = r3.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r10-0x58], r3                    
    mov64 r3, r1                                    r3 = r1
    add64 r3, -10                                   r3 += -10   ///  r3 = r3.wrapping_add(-10 as i32 as i64 as u64)
    stxdw [r10-0x50], r3                    
    jlt r3, 2, lbb_12131                            if r3 < (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12316                                    if true { pc += 185 }
lbb_12131:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    mov64 r8, r0                                    r8 = r0
    jeq r7, 0, lbb_12142                            if r7 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r6, [r10-0x4c8]                   
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_12147                                    if true { pc += 8 }
lbb_12139:
    add64 r6, 40                                    r6 += 40   ///  r6 = r6.wrapping_add(40 as i32 as i64 as u64)
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    jne r7, 0, lbb_12147                            if r7 != (0 as i32 as i64 as u64) { pc += 5 }
lbb_12142:
    ldxdw r6, [r10-0x508]                   
    ldxdw r7, [r10-0x5d0]                   
    ldxdw r2, [r10-0x4d0]                   
    jeq r2, 0, lbb_12392                            if r2 == (0 as i32 as i64 as u64) { pc += 246 }
    ja lbb_12367                                    if true { pc += 220 }
lbb_12147:
    ldxdw r2, [r6-0x8]                      
    mov64 r4, r2                                    r4 = r2
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 31, lbb_12156                           if r4 != (31 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_12156:
    jlt r4, 34, lbb_12158                           if r4 < (34 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_12158:
    jeq r2, 0, lbb_12139                            if r2 == (0 as i32 as i64 as u64) { pc += -20 }
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_12139                            if r1 != (0 as i32 as i64 as u64) { pc += -23 }
    ldxdw r1, [r6+0x0]                      
    lsh64 r2, 4                                     r2 <<= 4   ///  r2 = r2.wrapping_shl(4)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_12139                                    if true { pc += -28 }
lbb_12167:
    ldxh r0, [r2+0x8]                       
    mov64 r3, r2                                    r3 = r2
    add64 r3, 10                                    r3 += 10   ///  r3 = r3.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r10-0x58], r3                    
    mov64 r3, r1                                    r3 = r1
    add64 r3, -10                                   r3 += -10   ///  r3 = r3.wrapping_add(-10 as i32 as i64 as u64)
    stxdw [r10-0x50], r3                    
    jlt r3, 2, lbb_12176                            if r3 < (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12422                                    if true { pc += 246 }
lbb_12176:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    mov64 r8, r0                                    r8 = r0
    jeq r7, 0, lbb_12187                            if r7 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r6, [r10-0x4c8]                   
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_12192                                    if true { pc += 8 }
lbb_12184:
    add64 r6, 40                                    r6 += 40   ///  r6 = r6.wrapping_add(40 as i32 as i64 as u64)
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    jne r7, 0, lbb_12192                            if r7 != (0 as i32 as i64 as u64) { pc += 5 }
lbb_12187:
    ldxdw r6, [r10-0x508]                   
    ldxdw r7, [r10-0x5d0]                   
    ldxdw r2, [r10-0x4d0]                   
    jeq r2, 0, lbb_12594                            if r2 == (0 as i32 as i64 as u64) { pc += 403 }
    ja lbb_12569                                    if true { pc += 377 }
lbb_12192:
    ldxdw r2, [r6-0x8]                      
    mov64 r4, r2                                    r4 = r2
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 31, lbb_12201                           if r4 != (31 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_12201:
    jlt r4, 34, lbb_12203                           if r4 < (34 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_12203:
    jeq r2, 0, lbb_12184                            if r2 == (0 as i32 as i64 as u64) { pc += -20 }
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_12184                            if r1 != (0 as i32 as i64 as u64) { pc += -23 }
    ldxdw r1, [r6+0x0]                      
    lsh64 r2, 4                                     r2 <<= 4   ///  r2 = r2.wrapping_shl(4)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_12184                                    if true { pc += -28 }
lbb_12212:
    ldxdw r9, [r10-0x78]                    
    ldxdw r7, [r10-0x80]                    
    stxdw [r10-0x4b8], r9                   
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    jeq r7, r1, lbb_11886                           if r7 == r1 { pc += -332 }
lbb_12218:
    jlt r3, 8, lbb_12263                            if r3 < (8 as i32 as i64 as u64) { pc += 44 }
    mov64 r1, r3                                    r1 = r3
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    jeq r1, 8, lbb_12223                            if r1 == (8 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12272                                    if true { pc += 49 }
lbb_12223:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    ldxdw r5, [r10-0x4b0]                   
    mov64 r9, r0                                    r9 = r0
    jeq r6, 0, lbb_12237                            if r6 == (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r8, [r10-0x4b8]                   
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_12239                                    if true { pc += 7 }
lbb_12232:
    ldxdw r2, [r8+0x0]                      
    jne r2, 0, lbb_12257                            if r2 != (0 as i32 as i64 as u64) { pc += 23 }
lbb_12234:
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    jne r6, 0, lbb_12239                            if r6 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_12237:
    jeq r7, 0, lbb_11886                            if r7 == (0 as i32 as i64 as u64) { pc += -352 }
    ja lbb_13116                                    if true { pc += 877 }
lbb_12239:
    ldxb r1, [r8-0x8]                       
    jeq r1, 17, lbb_12251                           if r1 == (17 as i32 as i64 as u64) { pc += 10 }
    jeq r1, 47, lbb_12244                           if r1 == (47 as i32 as i64 as u64) { pc += 2 }
    jeq r1, 75, lbb_12232                           if r1 == (75 as i32 as i64 as u64) { pc += -11 }
    ja lbb_12234                                    if true { pc += -10 }
lbb_12244:
    ldxdw r2, [r8+0x0]                      
    mov64 r1, r2                                    r1 = r2
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    jeq r1, r3, lbb_12234                           if r1 == r3 { pc += -16 }
    ja lbb_12257                                    if true { pc += 6 }
lbb_12251:
    ldxdw r2, [r8+0x0]                      
    mov64 r1, r2                                    r1 = r2
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    jeq r1, r3, lbb_12234                           if r1 == r3 { pc += -23 }
lbb_12257:
    ldxdw r1, [r8+0x8]                      
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6768                      
    ldxdw r5, [r10-0x4b0]                   
    ja lbb_12234                                    if true { pc += -29 }
lbb_12263:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    ldxdw r5, [r10-0x4b0]                   
    mov64 r9, r0                                    r9 = r0
    jeq r6, 0, lbb_13115                            if r6 == (0 as i32 as i64 as u64) { pc += 846 }
    ldxdw r8, [r10-0x4b8]                   
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_13123                                    if true { pc += 851 }
lbb_12272:
    mov64 r1, r3                                    r1 = r3
    and64 r1, -2                                    r1 &= -2   ///  r1 = r1.and(-2)
    jeq r1, 16, lbb_12276                           if r1 == (16 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12473                                    if true { pc += 197 }
lbb_12276:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    ldxdw r5, [r10-0x4b0]                   
    mov64 r9, r0                                    r9 = r0
    jeq r6, 0, lbb_12290                            if r6 == (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r8, [r10-0x4b8]                   
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_12292                                    if true { pc += 7 }
lbb_12285:
    ldxdw r2, [r8+0x0]                      
    jne r2, 0, lbb_12310                            if r2 != (0 as i32 as i64 as u64) { pc += 23 }
lbb_12287:
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    jne r6, 0, lbb_12292                            if r6 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_12290:
    jeq r7, 0, lbb_11886                            if r7 == (0 as i32 as i64 as u64) { pc += -405 }
    ja lbb_13116                                    if true { pc += 824 }
lbb_12292:
    ldxb r1, [r8-0x8]                       
    jeq r1, 17, lbb_12304                           if r1 == (17 as i32 as i64 as u64) { pc += 10 }
    jeq r1, 47, lbb_12297                           if r1 == (47 as i32 as i64 as u64) { pc += 2 }
    jeq r1, 75, lbb_12285                           if r1 == (75 as i32 as i64 as u64) { pc += -11 }
    ja lbb_12287                                    if true { pc += -10 }
lbb_12297:
    ldxdw r2, [r8+0x0]                      
    mov64 r1, r2                                    r1 = r2
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    jeq r1, r3, lbb_12287                           if r1 == r3 { pc += -16 }
    ja lbb_12310                                    if true { pc += 6 }
lbb_12304:
    ldxdw r2, [r8+0x0]                      
    mov64 r1, r2                                    r1 = r2
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    jeq r1, r3, lbb_12287                           if r1 == r3 { pc += -23 }
lbb_12310:
    ldxdw r1, [r8+0x8]                      
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6768                      
    ldxdw r5, [r10-0x4b0]                   
    ja lbb_12287                                    if true { pc += -29 }
lbb_12316:
    ldxh r3, [r2+0xa]                       
    add64 r2, 12                                    r2 += 12   ///  r2 = r2.wrapping_add(12 as i32 as i64 as u64)
    stxdw [r10-0x58], r2                    
    add64 r1, -12                                   r1 += -12   ///  r1 = r1.wrapping_add(-12 as i32 as i64 as u64)
    stxdw [r10-0x50], r1                    
    jeq r1, 0, lbb_12524                            if r1 == (0 as i32 as i64 as u64) { pc += 202 }
    mov64 r1, 21                                    r1 = 21 as i32 as i64 as u64
    lddw r2, 0x1000336f0 --> b"Not all bytes read"        r2 load str located at 4295177968
    mov64 r3, 18                                    r3 = 18 as i32 as i64 as u64
    call function_8239                      
    mov64 r8, r0                                    r8 = r0
    jeq r7, 0, lbb_12335                            if r7 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r6, [r10-0x4c8]                   
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_12340                                    if true { pc += 8 }
lbb_12332:
    add64 r6, 40                                    r6 += 40   ///  r6 = r6.wrapping_add(40 as i32 as i64 as u64)
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    jne r7, 0, lbb_12340                            if r7 != (0 as i32 as i64 as u64) { pc += 5 }
lbb_12335:
    ldxdw r6, [r10-0x508]                   
    ldxdw r7, [r10-0x5d0]                   
    ldxdw r2, [r10-0x4d0]                   
    jeq r2, 0, lbb_12392                            if r2 == (0 as i32 as i64 as u64) { pc += 53 }
    ja lbb_12367                                    if true { pc += 27 }
lbb_12340:
    ldxdw r2, [r6-0x8]                      
    mov64 r4, r2                                    r4 = r2
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 31, lbb_12349                           if r4 != (31 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_12349:
    jlt r4, 34, lbb_12351                           if r4 < (34 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_12351:
    jeq r2, 0, lbb_12332                            if r2 == (0 as i32 as i64 as u64) { pc += -20 }
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_12332                            if r1 != (0 as i32 as i64 as u64) { pc += -23 }
    ldxdw r1, [r6+0x0]                      
    lsh64 r2, 4                                     r2 <<= 4   ///  r2 = r2.wrapping_shl(4)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_12332                                    if true { pc += -28 }
lbb_12360:
    add64 r6, 40                                    r6 += 40   ///  r6 = r6.wrapping_add(40 as i32 as i64 as u64)
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    jne r7, 0, lbb_12372                            if r7 != (0 as i32 as i64 as u64) { pc += 9 }
lbb_12363:
    ldxdw r6, [r10-0x508]                   
    ldxdw r7, [r10-0x5d0]                   
    ldxdw r2, [r10-0x4d0]                   
    jeq r2, 0, lbb_12392                            if r2 == (0 as i32 as i64 as u64) { pc += 25 }
lbb_12367:
    mul64 r2, 40                                    r2 *= 40   ///  r2 = r2.wrapping_mul(40 as u64)
    ldxdw r1, [r10-0x4c8]                   
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_12392                                    if true { pc += 20 }
lbb_12372:
    ldxdw r2, [r6-0x8]                      
    mov64 r4, r2                                    r4 = r2
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 31, lbb_12381                           if r4 != (31 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_12381:
    jlt r4, 34, lbb_12383                           if r4 < (34 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_12383:
    jeq r2, 0, lbb_12360                            if r2 == (0 as i32 as i64 as u64) { pc += -24 }
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_12360                            if r1 != (0 as i32 as i64 as u64) { pc += -27 }
    ldxdw r1, [r6+0x0]                      
    lsh64 r2, 4                                     r2 <<= 4   ///  r2 = r2.wrapping_shl(4)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_12360                                    if true { pc += -32 }
lbb_12392:
    mov64 r3, r8                                    r3 = r8
    and64 r3, 3                                     r3 &= 3   ///  r3 = r3.and(3)
    mov64 r4, r3                                    r4 = r3
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jlt r4, 2, lbb_12603                            if r4 < (2 as i32 as i64 as u64) { pc += 203 }
    jeq r3, 0, lbb_12603                            if r3 == (0 as i32 as i64 as u64) { pc += 202 }
    ldxdw r7, [r8-0x1]                      
    ldxdw r6, [r8+0x7]                      
    ldxdw r2, [r6+0x0]                      
    mov64 r1, r7                                    r1 = r7
    callx r2                                
    ldxdw r2, [r6+0x8]                      
    jeq r2, 0, lbb_12411                            if r2 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_12408:
    ldxdw r3, [r6+0x10]                     
    mov64 r1, r7                                    r1 = r7
    call function_6768                      
lbb_12411:
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r6, [r10-0x508]                   
    ldxdw r7, [r10-0x5d0]                   
    ja lbb_12603                                    if true { pc += 181 }
lbb_12422:
    ldxh r6, [r2+0xa]                       
    mov64 r3, r2                                    r3 = r2
    add64 r3, 12                                    r3 += 12   ///  r3 = r3.wrapping_add(12 as i32 as i64 as u64)
    stxdw [r10-0x58], r3                    
    mov64 r3, r1                                    r3 = r1
    add64 r3, -12                                   r3 += -12   ///  r3 = r3.wrapping_add(-12 as i32 as i64 as u64)
    stxdw [r10-0x50], r3                    
    jeq r3, 0, lbb_12526                            if r3 == (0 as i32 as i64 as u64) { pc += 96 }
    add64 r2, 13                                    r2 += 13   ///  r2 = r2.wrapping_add(13 as i32 as i64 as u64)
    stxdw [r10-0x58], r2                    
    add64 r1, -13                                   r1 += -13   ///  r1 = r1.wrapping_add(-13 as i32 as i64 as u64)
    stxdw [r10-0x50], r1                    
    jeq r1, 0, lbb_12901                            if r1 == (0 as i32 as i64 as u64) { pc += 466 }
    mov64 r1, 21                                    r1 = 21 as i32 as i64 as u64
    lddw r2, 0x1000336f0 --> b"Not all bytes read"        r2 load str located at 4295177968
    mov64 r3, 18                                    r3 = 18 as i32 as i64 as u64
    call function_8239                      
    mov64 r8, r0                                    r8 = r0
    jeq r7, 0, lbb_12448                            if r7 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r6, [r10-0x4c8]                   
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_12453                                    if true { pc += 8 }
lbb_12445:
    add64 r6, 40                                    r6 += 40   ///  r6 = r6.wrapping_add(40 as i32 as i64 as u64)
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    jne r7, 0, lbb_12453                            if r7 != (0 as i32 as i64 as u64) { pc += 5 }
lbb_12448:
    ldxdw r6, [r10-0x508]                   
    ldxdw r7, [r10-0x5d0]                   
    ldxdw r2, [r10-0x4d0]                   
    jeq r2, 0, lbb_12594                            if r2 == (0 as i32 as i64 as u64) { pc += 142 }
    ja lbb_12569                                    if true { pc += 116 }
lbb_12453:
    ldxdw r2, [r6-0x8]                      
    mov64 r4, r2                                    r4 = r2
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 31, lbb_12462                           if r4 != (31 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_12462:
    jlt r4, 34, lbb_12464                           if r4 < (34 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_12464:
    jeq r2, 0, lbb_12445                            if r2 == (0 as i32 as i64 as u64) { pc += -20 }
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_12445                            if r1 != (0 as i32 as i64 as u64) { pc += -23 }
    ldxdw r1, [r6+0x0]                      
    lsh64 r2, 4                                     r2 <<= 4   ///  r2 = r2.wrapping_shl(4)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_12445                                    if true { pc += -28 }
lbb_12473:
    jeq r3, 18, lbb_13028                           if r3 == (18 as i32 as i64 as u64) { pc += 554 }
    jeq r3, 19, lbb_12476                           if r3 == (19 as i32 as i64 as u64) { pc += 1 }
    ja lbb_13068                                    if true { pc += 592 }
lbb_12476:
    ldxdw r1, [r10-0x90]                    
    ldxb r1, [r1+0x12]                      
    stxdw [r10-0x5c8], r1                   
    jeq r6, 0, lbb_12488                            if r6 == (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r8, [r10-0x4b8]                   
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_12500                                    if true { pc += 17 }
lbb_12483:
    ldxdw r2, [r8+0x0]                      
    jne r2, 0, lbb_12518                            if r2 != (0 as i32 as i64 as u64) { pc += 33 }
lbb_12485:
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    jne r6, 0, lbb_12500                            if r6 != (0 as i32 as i64 as u64) { pc += 12 }
lbb_12488:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x568], r1                   
    jeq r7, 0, lbb_11916                            if r7 == (0 as i32 as i64 as u64) { pc += -575 }
    mul64 r7, 40                                    r7 *= 40   ///  r7 = r7.wrapping_mul(40 as u64)
    ldxdw r1, [r10-0x4b8]                   
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ldxdw r5, [r10-0x4b0]                   
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x568], r1                   
    ja lbb_11916                                    if true { pc += -584 }
lbb_12500:
    ldxb r1, [r8-0x8]                       
    jeq r1, 17, lbb_12512                           if r1 == (17 as i32 as i64 as u64) { pc += 10 }
    jeq r1, 47, lbb_12505                           if r1 == (47 as i32 as i64 as u64) { pc += 2 }
    jeq r1, 75, lbb_12483                           if r1 == (75 as i32 as i64 as u64) { pc += -21 }
    ja lbb_12485                                    if true { pc += -20 }
lbb_12505:
    ldxdw r2, [r8+0x0]                      
    mov64 r1, r2                                    r1 = r2
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    jeq r1, r3, lbb_12485                           if r1 == r3 { pc += -26 }
    ja lbb_12518                                    if true { pc += 6 }
lbb_12512:
    ldxdw r2, [r8+0x0]                      
    mov64 r1, r2                                    r1 = r2
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    jeq r1, r3, lbb_12485                           if r1 == r3 { pc += -33 }
lbb_12518:
    ldxdw r1, [r8+0x8]                      
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6768                      
    ldxdw r5, [r10-0x4b0]                   
    ja lbb_12485                                    if true { pc += -39 }
lbb_12524:
    stxdw [r10-0x530], r3                   
    ja lbb_12902                                    if true { pc += 376 }
lbb_12526:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    mov64 r8, r0                                    r8 = r0
    jeq r7, 0, lbb_12537                            if r7 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r6, [r10-0x4c8]                   
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_12542                                    if true { pc += 8 }
lbb_12534:
    add64 r6, 40                                    r6 += 40   ///  r6 = r6.wrapping_add(40 as i32 as i64 as u64)
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    jne r7, 0, lbb_12542                            if r7 != (0 as i32 as i64 as u64) { pc += 5 }
lbb_12537:
    ldxdw r6, [r10-0x508]                   
    ldxdw r7, [r10-0x5d0]                   
    ldxdw r2, [r10-0x4d0]                   
    jeq r2, 0, lbb_12594                            if r2 == (0 as i32 as i64 as u64) { pc += 53 }
    ja lbb_12569                                    if true { pc += 27 }
lbb_12542:
    ldxdw r2, [r6-0x8]                      
    mov64 r4, r2                                    r4 = r2
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 31, lbb_12551                           if r4 != (31 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_12551:
    jlt r4, 34, lbb_12553                           if r4 < (34 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_12553:
    jeq r2, 0, lbb_12534                            if r2 == (0 as i32 as i64 as u64) { pc += -20 }
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_12534                            if r1 != (0 as i32 as i64 as u64) { pc += -23 }
    ldxdw r1, [r6+0x0]                      
    lsh64 r2, 4                                     r2 <<= 4   ///  r2 = r2.wrapping_shl(4)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_12534                                    if true { pc += -28 }
lbb_12562:
    add64 r6, 40                                    r6 += 40   ///  r6 = r6.wrapping_add(40 as i32 as i64 as u64)
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    jne r7, 0, lbb_12574                            if r7 != (0 as i32 as i64 as u64) { pc += 9 }
lbb_12565:
    ldxdw r6, [r10-0x508]                   
    ldxdw r7, [r10-0x5d0]                   
    ldxdw r2, [r10-0x4d0]                   
    jeq r2, 0, lbb_12594                            if r2 == (0 as i32 as i64 as u64) { pc += 25 }
lbb_12569:
    mul64 r2, 40                                    r2 *= 40   ///  r2 = r2.wrapping_mul(40 as u64)
    ldxdw r1, [r10-0x4c8]                   
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_12594                                    if true { pc += 20 }
lbb_12574:
    ldxdw r2, [r6-0x8]                      
    mov64 r4, r2                                    r4 = r2
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 31, lbb_12583                           if r4 != (31 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_12583:
    jlt r4, 34, lbb_12585                           if r4 < (34 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_12585:
    jeq r2, 0, lbb_12562                            if r2 == (0 as i32 as i64 as u64) { pc += -24 }
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_12562                            if r1 != (0 as i32 as i64 as u64) { pc += -27 }
    ldxdw r1, [r6+0x0]                      
    lsh64 r2, 4                                     r2 <<= 4   ///  r2 = r2.wrapping_shl(4)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_12562                                    if true { pc += -32 }
lbb_12594:
    mov64 r3, r8                                    r3 = r8
    and64 r3, 3                                     r3 &= 3   ///  r3 = r3.and(3)
    mov64 r4, r3                                    r4 = r3
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jlt r4, 2, lbb_12603                            if r4 < (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12835                                    if true { pc += 232 }
lbb_12603:
    ldxdw r3, [r10-0x4b8]                   
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    or64 r0, r1                                     r0 |= r1   ///  r0 = r0.or(r1)
    ldxdw r1, [r10-0x4c0]                   
    or64 r0, r1                                     r0 |= r1   ///  r0 = r0.or(r1)
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    ldxdw r2, [r10-0x510]                   
    ldxdw r5, [r10-0x4b0]                   
lbb_12611:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x5d8], r1                   
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    lddw r1, 0xffffffffffff                         r1 load str located at 281474976710655
    jgt r0, r1, lbb_12618                           if r0 > r1 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_12618:
    stxdw [r10-0x5b8], r3                   
    mov64 r8, r0                                    r8 = r0
    lddw r1, 0x100000000                            r1 load str located at 4294967296
    and64 r8, r1                                    r8 &= r1   ///  r8 = r8.and(r1)
    mov64 r4, r0                                    r4 = r0
    lddw r1, 0x10000000000                          r1 load str located at 1099511627776
    and64 r4, r1                                    r4 &= r1   ///  r4 = r4.and(r1)
    mov64 r3, r0                                    r3 = r0
    rsh64 r3, 16                                    r3 >>= 16   ///  r3 = r3.wrapping_shr(16)
    rsh64 r4, 40                                    r4 >>= 40   ///  r4 = r4.wrapping_shr(40)
    rsh64 r8, 32                                    r8 >>= 32   ///  r8 = r8.wrapping_shr(32)
    stxdw [r10-0x528], r9                   
lbb_12632:
    stxdw [r10-0x548], r8                   
    stxdw [r10-0x540], r4                   
    stxdw [r10-0x538], r3                   
    stxdw [r10-0x530], r0                   
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x4c8], r1                   
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    lddw r1, 0x4055bc62c3351cae                     r1 load str located at 4635818523815648430
    jeq r5, r1, lbb_12643                           if r5 == r1 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_12643:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r9, [r10-0x4a0]                   
    ldxdw r0, [r10-0x4e0]                   
    lddw r1, 0x52142a73b3f727ae                     r1 load str located at 5914398887073228718
    ldxdw r8, [r10-0x588]                   
    jeq r5, r1, lbb_12651                           if r5 == r1 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_12651:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x4b8], r1                   
    lddw r1, 0x2939f530c0fd63a4                     r1 load str located at 2970675018972619684
    jeq r5, r1, lbb_12658                           if r5 == r1 { pc += 2 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x4b8], r1                   
lbb_12658:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x4c0], r1                   
    lddw r1, 0x6859164a61d89e53                     r1 load str located at 7519065561596730963
    jeq r5, r1, lbb_12665                           if r5 == r1 { pc += 2 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x4c0], r1                   
lbb_12665:
    lddw r1, 0xefde0edc6184f2b2                     r1 load str located at -1162475314123312462
    jeq r5, r1, lbb_12670                           if r5 == r1 { pc += 2 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x4c8], r1                   
lbb_12670:
    ldxdw r1, [r10-0x558]                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    stxdw [r10-0x510], r2                   
    ldxdw r1, [r10-0x5b0]                   
    ldxdw r2, [r10-0x4c8]                   
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    stxdw [r10-0x4c8], r2                   
    ldxdw r1, [r10-0x5a8]                   
    ldxdw r2, [r10-0x4c0]                   
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    stxdw [r10-0x4c0], r2                   
    ldxdw r1, [r10-0x5a0]                   
    ldxdw r2, [r10-0x4b8]                   
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    stxdw [r10-0x4b8], r2                   
    ldxdw r1, [r10-0x598]                   
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    stxdw [r10-0x4d8], r4                   
    ldxdw r1, [r10-0x590]                   
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    stxdw [r10-0x4d0], r3                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1176                                 r1 += -1176   ///  r1 = r1.wrapping_add(-1176 as i32 as i64 as u64)
    ldxdw r2, [r10-0x98]                    
    stxdw [r10-0x4b0], r2                   
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
lbb_12696:
    jeq r2, 0, lbb_12702                            if r2 == (0 as i32 as i64 as u64) { pc += 5 }
    add64 r2, -8                                    r2 += -8   ///  r2 = r2.wrapping_add(-8 as i32 as i64 as u64)
    ldxdw r3, [r1+0x0]                      
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    jeq r3, r5, lbb_12747                           if r3 == r5 { pc += 46 }
    ja lbb_12696                                    if true { pc += -6 }
lbb_12702:
    ldxdw r3, [r10-0x4b0]                   
    jgt r3, 127, lbb_13149                          if r3 > (127 as i32 as i64 as u64) { pc += 445 }
    ldxdw r4, [r10-0x580]                   
    ldxb r1, [r4+0x2]                       
    xor64 r1, 168                                   r1 ^= 168   ///  r1 = r1.xor(168)
    ldxb r2, [r4+0x3]                       
    xor64 r2, 181                                   r2 ^= 181   ///  r2 = r2.xor(181)
    lsh64 r2, 8                                     r2 <<= 8   ///  r2 = r2.wrapping_shl(8)
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    ldxb r1, [r4+0x4]                       
    xor64 r1, 169                                   r1 ^= 169   ///  r1 = r1.xor(169)
    lsh64 r1, 16                                    r1 <<= 16   ///  r1 = r1.wrapping_shl(16)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    ldxb r2, [r4+0x5]                       
    xor64 r2, 173                                   r2 ^= 173   ///  r2 = r2.xor(173)
    lsh64 r2, 24                                    r2 <<= 24   ///  r2 = r2.wrapping_shl(24)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    ldxb r2, [r4+0x6]                       
    xor64 r2, 112                                   r2 ^= 112   ///  r2 = r2.xor(112)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1176                                 r2 += -1176   ///  r2 = r2.wrapping_add(-1176 as i32 as i64 as u64)
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    ldxb r3, [r4+0x7]                       
    xor64 r3, 91                                    r3 ^= 91   ///  r3 = r3.xor(91)
    lsh64 r3, 40                                    r3 <<= 40   ///  r3 = r3.wrapping_shl(40)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    ldxb r3, [r4+0x8]                       
    xor64 r3, 91                                    r3 ^= 91   ///  r3 = r3.xor(91)
    lsh64 r3, 48                                    r3 <<= 48   ///  r3 = r3.wrapping_shl(48)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    ldxb r3, [r4+0x9]                       
    xor64 r3, 251                                   r3 ^= 251   ///  r3 = r3.xor(251)
    lsh64 r3, 56                                    r3 <<= 56   ///  r3 = r3.wrapping_shl(56)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    stxdw [r2+0x0], r1                      
    ldxdw r1, [r10-0x98]                    
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x4b0], r1                   
    stxdw [r10-0x98], r1                    
    ldxdw r6, [r10-0x508]                   
    ldxdw r9, [r10-0x4a0]                   
    ldxdw r7, [r10-0x5d0]                   
lbb_12747:
    lddw r1, 0xf92a0c49a9d32ad                      r1 load str located at 1122136023436702381
    jsgt r5, r1, lbb_12766                          if (r5 as i64) > (r1 as i64) { pc += 16 }
    ldxdw r2, [r10-0x560]                   
    ldxdw r1, [r10-0x550]                   
    ldxdw r3, [r10-0x4e8]                   
    lddw r4, 0xe83607d25cff24fc                     r4 load str located at -1714174008083143428
    jsgt r5, r4, lbb_12795                          if (r5 as i64) > (r4 as i64) { pc += 39 }
    lddw r4, 0xb328e792b9e3561c                     r4 load str located at -5536921124482099684
    jsgt r5, r4, lbb_12815                          if (r5 as i64) > (r4 as i64) { pc += 56 }
    lddw r4, 0x959b6a82f67cccac                     r4 load str located at -7666416829954470740
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2317 }
    lddw r4, 0xa12a3db13be376a2                     r4 load str located at -6833581653158037854
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2320 }
    ja lbb_10444                                    if true { pc += -2322 }
lbb_12766:
    ldxdw r2, [r10-0x560]                   
    ldxdw r1, [r10-0x550]                   
    ldxdw r3, [r10-0x4e8]                   
    lddw r4, 0x3cc6f006d17cccab                     r4 load str located at 4379451599739473067
    jsgt r5, r4, lbb_12782                          if (r5 as i64) > (r4 as i64) { pc += 10 }
    lddw r4, 0x26eaa412d4046aab                     r4 load str located at 2804234118764980907
    jsgt r5, r4, lbb_12805                          if (r5 as i64) > (r4 as i64) { pc += 30 }
    lddw r4, 0xf92a0c49a9d32ae                      r4 load str located at 1122136023436702382
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2333 }
    lddw r4, 0x25d42e9e4c5f68ae                     r4 load str located at 2725854931887024302
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2336 }
    ja lbb_10444                                    if true { pc += -2338 }
lbb_12782:
    lddw r4, 0x52142a73b3f727ad                     r4 load str located at 5914398887073228717
    jsgt r5, r4, lbb_10435                          if (r5 as i64) > (r4 as i64) { pc += -2350 }
    lddw r4, 0x3cc6f006d17cccac                     r4 load str located at 4379451599739473068
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2343 }
    lddw r4, 0x3e30e49d807cccac                     r4 load str located at 4481332994350304428
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2346 }
    lddw r4, 0x4055bc62c3351cae                     r4 load str located at 4635818523815648430
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2349 }
    ja lbb_10444                                    if true { pc += -2351 }
lbb_12795:
    lddw r4, 0xfb5b5b70ada9b5a7                     r4 load str located at -334573207800924761
    jsgt r5, r4, lbb_12825                          if (r5 as i64) > (r4 as i64) { pc += 27 }
    lddw r4, 0xe83607d25cff24fd                     r4 load str located at -1714174008083143427
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2356 }
    lddw r4, 0xefde0edc6184f2b2                     r4 load str located at -1162475314123312462
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2359 }
    ja lbb_10444                                    if true { pc += -2361 }
lbb_12805:
    lddw r4, 0x26eaa412d4046aac                     r4 load str located at 2804234118764980908
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2363 }
    lddw r4, 0x2939f530c0fd63a4                     r4 load str located at 2970675018972619684
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2366 }
    lddw r4, 0x3a655514f07d921d                     r4 load str located at 4207862975270064669
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2369 }
    ja lbb_10444                                    if true { pc += -2371 }
lbb_12815:
    lddw r4, 0xb328e792b9e3561d                     r4 load str located at -5536921124482099683
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2373 }
    lddw r4, 0xc94c7a95c2efb3ab                     r4 load str located at -3941640790216821845
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2376 }
    lddw r4, 0xd0875430b9e3561d                     r4 load str located at -3420672823710755299
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2379 }
    ja lbb_10444                                    if true { pc += -2381 }
lbb_12825:
    lddw r4, 0xfb5b5b70ada9b5a8                     r4 load str located at -334573207800924760
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2383 }
    lddw r4, 0xfd7a72e9f7faffad                     r4 load str located at -181706485741715539
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2386 }
    lddw r4, 0xad27f3e228c2224                      r4 load str located at 779825590332826148
    jeq r5, r4, lbb_10445                           if r5 == r4 { pc += -2389 }
    ja lbb_10444                                    if true { pc += -2391 }
lbb_12835:
    jeq r3, 0, lbb_12603                            if r3 == (0 as i32 as i64 as u64) { pc += -233 }
    ldxdw r7, [r8-0x1]                      
    ldxdw r6, [r8+0x7]                      
    ldxdw r2, [r6+0x0]                      
    mov64 r1, r7                                    r1 = r7
    callx r2                                
    ldxdw r2, [r6+0x8]                      
    jeq r2, 0, lbb_12411                            if r2 == (0 as i32 as i64 as u64) { pc += -432 }
    ja lbb_12408                                    if true { pc += -436 }
lbb_12844:
    ldxdw r1, [r10-0x38]                    
lbb_12845:
    stxdw [r10-0x4b8], r1                   
lbb_12846:
    ldxdw r5, [r10-0x4b0]                   
    mov64 r3, r8                                    r3 = r8
    ja lbb_11843                                    if true { pc += -1006 }
lbb_12849:
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    jeq r1, r2, lbb_11817                           if r1 == r2 { pc += -1037 }
    ja lbb_12893                                    if true { pc += 38 }
lbb_12855:
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    ldxdw r5, [r10-0x4b0]                   
    jeq r1, r2, lbb_11830                           if r1 == r2 { pc += -1031 }
    ja lbb_12869                                    if true { pc += 7 }
lbb_12862:
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    ldxdw r5, [r10-0x4b0]                   
    mov64 r3, r6                                    r3 = r6
    jeq r1, r2, lbb_11830                           if r1 == r2 { pc += -1039 }
lbb_12869:
    ja lbb_12893                                    if true { pc += 23 }
lbb_12870:
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    ldxdw r5, [r10-0x4b0]                   
    mov64 r3, r6                                    r3 = r6
    jeq r1, r2, lbb_11843                           if r1 == r2 { pc += -1034 }
    ja lbb_12885                                    if true { pc += 7 }
lbb_12878:
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    ldxdw r5, [r10-0x4b0]                   
    mov64 r3, r6                                    r3 = r6
    jeq r1, r2, lbb_11843                           if r1 == r2 { pc += -1042 }
lbb_12885:
    ja lbb_12893                                    if true { pc += 7 }
lbb_12886:
    mov64 r1, r8                                    r1 = r8
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    ldxdw r5, [r10-0x4b0]                   
    mov64 r3, r6                                    r3 = r6
    jeq r1, r2, lbb_11817                           if r1 == r2 { pc += -1076 }
lbb_12893:
    lsh64 r8, 1                                     r8 <<= 1   ///  r8 = r8.wrapping_shl(1)
    ldxdw r1, [r10-0x640]                   
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6768                      
    mov64 r3, r6                                    r3 = r6
    ldxdw r5, [r10-0x4b0]                   
    ja lbb_11843                                    if true { pc += -1058 }
lbb_12901:
    stxdw [r10-0x530], r6                   
lbb_12902:
    stxdw [r10-0x528], r0                   
    stxdw [r10-0x4d8], r5                   
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, r7                                    r2 = r7
    mul64 r2, 40                                    r2 *= 40   ///  r2 = r2.wrapping_mul(40 as u64)
    stxdw [r10-0x4c0], r2                   
lbb_12908:
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r10-0x4c0]                   
    jeq r1, r6, lbb_12917                           if r1 == r6 { pc += 6 }
    mov64 r1, r8                                    r1 = r8
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    call function_17335                     
    mov64 r1, r6                                    r1 = r6
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    jne r0, 0, lbb_12908                            if r0 != (0 as i32 as i64 as u64) { pc += -9 }
lbb_12917:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r2, 2                                     r2 = 2 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    lddw r4, 0x800000000000001d                     r4 load str located at -9223372036854775779
lbb_12922:
    ldxdw r3, [r10-0x4c0]                   
    jeq r3, r1, lbb_12974                           if r3 == r1 { pc += 50 }
    mov64 r3, r8                                    r3 = r8
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    ldxdw r3, [r3+0x0]                      
    jne r3, r4, lbb_12922                           if r3 != r4 { pc += -8 }
    jeq r2, 1, lbb_12974                            if r2 == (1 as i32 as i64 as u64) { pc += 43 }
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    jge r2, r7, lbb_12974                           if r2 >= r7 { pc += 41 }
    mov64 r2, r8                                    r2 = r8
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxdw r2, [r2-0x50]                     
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    jlt r2, 34, lbb_12941                           if r2 < (34 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 31                                    r2 = 31 as i32 as i64 as u64
lbb_12941:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jgt r2, 30, lbb_12951                           if r2 > (30 as i32 as i64 as u64) { pc += 8 }
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    lsh64 r3, r2                                    r3 <<= r2   ///  r3 = r3.wrapping_shl(r2 as u32)
    and64 r3, 1619558400                            r3 &= 1619558400   ///  r3 = r3.and(1619558400)
    jne r3, 0, lbb_12948                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_12951                                    if true { pc += 3 }
lbb_12948:
    mov64 r2, r8                                    r2 = r8
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxdw r4, [r2-0x48]                     
lbb_12951:
    ldxdw r2, [r10-0x630]                   
    mov64 r1, r2                                    r1 = r2
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    jlt r4, r2, lbb_12958                           if r4 < r2 { pc += 3 }
    mov64 r1, r4                                    r1 = r4
    ldxdw r2, [r10-0x630]                   
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
lbb_12958:
    stxdw [r10-0x4c8], r4                   
    call function_23211                     
    stxdw [r10-0x4b8], r0                   
    ldxdw r1, [r10-0x4c8]                   
    call function_23211                     
    ldxdw r1, [r10-0x4b8]                   
    mov64 r2, r0                                    r2 = r0
    call function_26063                     
    mov64 r1, r0                                    r1 = r0
    lddw r2, 0x3f1a36e2eb1c432d                     r2 load str located at 4547007122018943789
    call function_25957                     
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jsgt r0, 0, lbb_12973                           if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_12973:
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
lbb_12974:
    stxdw [r10-0x4c8], r8                   
    stxdw [r10-0x4b8], r5                   
    jeq r7, 0, lbb_12983                            if r7 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r8, [r10-0x4c8]                   
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_12990                                    if true { pc += 10 }
lbb_12980:
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    jne r7, 0, lbb_12990                            if r7 != (0 as i32 as i64 as u64) { pc += 7 }
lbb_12983:
    ldxdw r2, [r10-0x4d0]                   
    jeq r2, 0, lbb_13010                            if r2 == (0 as i32 as i64 as u64) { pc += 25 }
    mul64 r2, 40                                    r2 *= 40   ///  r2 = r2.wrapping_mul(40 as u64)
    ldxdw r1, [r10-0x4c8]                   
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_13010                                    if true { pc += 20 }
lbb_12990:
    ldxdw r2, [r8-0x8]                      
    mov64 r4, r2                                    r4 = r2
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 31, lbb_12999                           if r4 != (31 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_12999:
    jlt r4, 34, lbb_13001                           if r4 < (34 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_13001:
    jeq r2, 0, lbb_12980                            if r2 == (0 as i32 as i64 as u64) { pc += -22 }
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_12980                            if r1 != (0 as i32 as i64 as u64) { pc += -25 }
    ldxdw r1, [r8+0x0]                      
    lsh64 r2, 4                                     r2 <<= 4   ///  r2 = r2.wrapping_shl(4)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ja lbb_12980                                    if true { pc += -30 }
lbb_13010:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r7, [r10-0x5d0]                   
    ldxdw r1, [r10-0x4c0]                   
    jeq r1, r6, lbb_13016                           if r1 == r6 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_13016:
    ldxdw r1, [r10-0x4d8]                   
    jeq r1, 0, lbb_13019                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_13019:
    ldxdw r1, [r10-0x530]                   
    ldxdw r4, [r10-0x528]                   
    mov64 r0, r4                                    r0 = r4
    lsh64 r0, 16                                    r0 <<= 16   ///  r0 = r0.wrapping_shl(16)
    lsh64 r3, 40                                    r3 <<= 40   ///  r3 = r3.wrapping_shl(40)
    stxdw [r10-0x4c0], r3                   
    lsh64 r2, 48                                    r2 <<= 48   ///  r2 = r2.wrapping_shl(48)
    ldxdw r6, [r10-0x508]                   
    ja lbb_12603                                    if true { pc += -425 }
lbb_13028:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    ldxdw r5, [r10-0x4b0]                   
    mov64 r9, r0                                    r9 = r0
    jeq r6, 0, lbb_13042                            if r6 == (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r8, [r10-0x4b8]                   
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_13044                                    if true { pc += 7 }
lbb_13037:
    ldxdw r2, [r8+0x0]                      
    jne r2, 0, lbb_13062                            if r2 != (0 as i32 as i64 as u64) { pc += 23 }
lbb_13039:
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    jne r6, 0, lbb_13044                            if r6 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_13042:
    jeq r7, 0, lbb_11886                            if r7 == (0 as i32 as i64 as u64) { pc += -1157 }
    ja lbb_13116                                    if true { pc += 72 }
lbb_13044:
    ldxb r1, [r8-0x8]                       
    jeq r1, 17, lbb_13056                           if r1 == (17 as i32 as i64 as u64) { pc += 10 }
    jeq r1, 47, lbb_13049                           if r1 == (47 as i32 as i64 as u64) { pc += 2 }
    jeq r1, 75, lbb_13037                           if r1 == (75 as i32 as i64 as u64) { pc += -11 }
    ja lbb_13039                                    if true { pc += -10 }
lbb_13049:
    ldxdw r2, [r8+0x0]                      
    mov64 r1, r2                                    r1 = r2
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    jeq r1, r3, lbb_13039                           if r1 == r3 { pc += -16 }
    ja lbb_13062                                    if true { pc += 6 }
lbb_13056:
    ldxdw r2, [r8+0x0]                      
    mov64 r1, r2                                    r1 = r2
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    jeq r1, r3, lbb_13039                           if r1 == r3 { pc += -23 }
lbb_13062:
    ldxdw r1, [r8+0x8]                      
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6768                      
    ldxdw r5, [r10-0x4b0]                   
    ja lbb_13039                                    if true { pc += -29 }
lbb_13068:
    mov64 r1, 21                                    r1 = 21 as i32 as i64 as u64
    lddw r2, 0x1000336f0 --> b"Not all bytes read"        r2 load str located at 4295177968
    mov64 r3, 18                                    r3 = 18 as i32 as i64 as u64
    call function_8239                      
    ldxdw r5, [r10-0x4b0]                   
    mov64 r9, r0                                    r9 = r0
    jeq r6, 0, lbb_13084                            if r6 == (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r8, [r10-0x4b8]                   
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_13086                                    if true { pc += 7 }
lbb_13079:
    ldxdw r2, [r8+0x0]                      
    jne r2, 0, lbb_13104                            if r2 != (0 as i32 as i64 as u64) { pc += 23 }
lbb_13081:
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    jne r6, 0, lbb_13086                            if r6 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_13084:
    jeq r7, 0, lbb_11886                            if r7 == (0 as i32 as i64 as u64) { pc += -1199 }
    ja lbb_13116                                    if true { pc += 30 }
lbb_13086:
    ldxb r1, [r8-0x8]                       
    jeq r1, 17, lbb_13098                           if r1 == (17 as i32 as i64 as u64) { pc += 10 }
    jeq r1, 47, lbb_13091                           if r1 == (47 as i32 as i64 as u64) { pc += 2 }
    jeq r1, 75, lbb_13079                           if r1 == (75 as i32 as i64 as u64) { pc += -11 }
    ja lbb_13081                                    if true { pc += -10 }
lbb_13091:
    ldxdw r2, [r8+0x0]                      
    mov64 r1, r2                                    r1 = r2
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    jeq r1, r3, lbb_13081                           if r1 == r3 { pc += -16 }
    ja lbb_13104                                    if true { pc += 6 }
lbb_13098:
    ldxdw r2, [r8+0x0]                      
    mov64 r1, r2                                    r1 = r2
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    jeq r1, r3, lbb_13081                           if r1 == r3 { pc += -23 }
lbb_13104:
    ldxdw r1, [r8+0x8]                      
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6768                      
    ldxdw r5, [r10-0x4b0]                   
    ja lbb_13081                                    if true { pc += -29 }
lbb_13110:
    ldxdw r2, [r8+0x0]                      
    jne r2, 0, lbb_13141                            if r2 != (0 as i32 as i64 as u64) { pc += 29 }
lbb_13112:
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    jne r6, 0, lbb_13123                            if r6 != (0 as i32 as i64 as u64) { pc += 8 }
lbb_13115:
    jeq r7, 0, lbb_11886                            if r7 == (0 as i32 as i64 as u64) { pc += -1230 }
lbb_13116:
    mul64 r7, 40                                    r7 *= 40   ///  r7 = r7.wrapping_mul(40 as u64)
    ldxdw r1, [r10-0x4b8]                   
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    ldxdw r5, [r10-0x4b0]                   
    ja lbb_11886                                    if true { pc += -1237 }
lbb_13123:
    ldxb r1, [r8-0x8]                       
    jeq r1, 17, lbb_13135                           if r1 == (17 as i32 as i64 as u64) { pc += 10 }
    jeq r1, 47, lbb_13128                           if r1 == (47 as i32 as i64 as u64) { pc += 2 }
    jeq r1, 75, lbb_13110                           if r1 == (75 as i32 as i64 as u64) { pc += -17 }
    ja lbb_13112                                    if true { pc += -16 }
lbb_13128:
    ldxdw r2, [r8+0x0]                      
    mov64 r1, r2                                    r1 = r2
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    jeq r1, r3, lbb_13112                           if r1 == r3 { pc += -22 }
    ja lbb_13141                                    if true { pc += 6 }
lbb_13135:
    ldxdw r2, [r8+0x0]                      
    mov64 r1, r2                                    r1 = r2
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    jeq r1, r3, lbb_13112                           if r1 == r3 { pc += -29 }
lbb_13141:
    ldxdw r1, [r8+0x8]                      
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6768                      
    ldxdw r5, [r10-0x4b0]                   
    ja lbb_13112                                    if true { pc += -35 }
lbb_13147:
    ldxdw r1, [r10-0x640]                   
    ja lbb_12845                                    if true { pc += -304 }
lbb_13149:
    ldxdw r1, [r10-0x5e8]                   
    stw [r1+0x8], 47                        
    stdw [r1+0x0], 2                        
    ja lbb_13468                                    if true { pc += 315 }
lbb_13153:
    mov64 r7, r6                                    r7 = r6
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r1, [r10-0x628]                   
    jeq r1, 0, lbb_13158                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_13158:
    stxdw [r10-0x558], r2                   
    ldxdw r1, [r10-0x4b0]                   
    call function_23211                     
    stxdw [r10-0x4b0], r0                   
    ldxdw r2, [r10-0x5f0]                   
    ldxdw r3, [r10-0x560]                   
    ldxdw r1, [r10-0x510]                   
    ldxdw r1, [r10-0x550]                   
    ldxdw r1, [r10-0x4e8]                   
    stxdw [r10-0x4e8], r1                   
    stxdw [r10-0x508], r7                   
    ldxdw r1, [r10-0x4f0]                   
    stxdw [r10-0x4f0], r1                   
    ldxdw r1, [r10-0x4f8]                   
    stxdw [r10-0x4f8], r1                   
    ldxdw r1, [r10-0x500]                   
    stxdw [r10-0x500], r1                   
    ldxdw r1, [r10-0x4a0]                   
    ldxdw r8, [r10-0x5c0]                   
    ldxdw r9, [r10-0x4e0]                   
    ldxdw r0, [r10-0x588]                   
    ldxdw r1, [r10-0x4a8]                   
    ldxdw r1, [r10-0x518]                   
    ldxdw r1, [r10-0x520]                   
    ldxdw r1, [r10-0x530]                   
    ldxdw r1, [r10-0x5d8]                   
    stxdw [r10-0x5d8], r1                   
    ldxdw r1, [r10-0x538]                   
    ldxdw r1, [r10-0x528]                   
    stxdw [r10-0x528], r1                   
    ldxdw r1, [r10-0x5b8]                   
    stxdw [r10-0x5b8], r1                   
    ldxdw r1, [r10-0x540]                   
    ldxdw r1, [r10-0x548]                   
    ldxdw r4, [r10-0x5c8]                   
    ldxdw r1, [r10-0x568]                   
    stxdw [r10-0x568], r1                   
    ldxdw r6, [r10-0x4d0]                   
    ldxdw r5, [r10-0x4d8]                   
    ldxdw r1, [r10-0x4c8]                   
    ldxdw r7, [r10-0x4b8]                   
    ldxdw r7, [r10-0x4c0]                   
lbb_13200:
    stxdw [r10-0x560], r3                   
    ldxdw r3, [r10-0x558]                   
    stxdw [r10-0x558], r3                   
    ldxdw r3, [r10-0x4b0]                   
    stxdw [r10-0x4b0], r3                   
    stxdw [r10-0x4c8], r1                   
    stxdw [r10-0x4d8], r5                   
    stxdw [r10-0x4d0], r6                   
    stxdw [r10-0x5c8], r4                   
    ldxdw r1, [r10-0x548]                   
    stxdw [r10-0x548], r1                   
    ldxdw r1, [r10-0x540]                   
    stxdw [r10-0x540], r1                   
    ldxdw r1, [r10-0x5b8]                   
    stxdw [r10-0x5b8], r1                   
    ldxdw r1, [r10-0x538]                   
    stxdw [r10-0x538], r1                   
    ldxdw r1, [r10-0x530]                   
    stxdw [r10-0x530], r1                   
    ldxdw r1, [r10-0x520]                   
    stxdw [r10-0x520], r1                   
    ldxdw r1, [r10-0x518]                   
    stxdw [r10-0x518], r1                   
    ldxdw r1, [r10-0x4a8]                   
    stxdw [r10-0x4a8], r1                   
    stxdw [r10-0x588], r0                   
    stxdw [r10-0x4e0], r9                   
    stxdw [r10-0x5c0], r8                   
    ldxdw r1, [r10-0x4a0]                   
    stxdw [r10-0x4a0], r1                   
    ldxdw r1, [r10-0x500]                   
    stxdw [r10-0x500], r1                   
    ldxdw r1, [r10-0x4f8]                   
    stxdw [r10-0x4f8], r1                   
    ldxdw r1, [r10-0x4f0]                   
    stxdw [r10-0x4f0], r1                   
    ldxdw r1, [r10-0x508]                   
    stxdw [r10-0x508], r1                   
    ldxdw r1, [r10-0x4e8]                   
    stxdw [r10-0x4e8], r1                   
    ldxdw r1, [r10-0x550]                   
    stxdw [r10-0x550], r1                   
    ldxdw r1, [r10-0x510]                   
    stxdw [r10-0x510], r1                   
    ldxdw r6, [r10-0x568]                   
    mov64 r1, r6                                    r1 = r6
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    ldxdw r8, [r10-0x5d8]                   
    ldxdw r4, [r10-0x528]                   
    ldxdw r5, [r10-0x4b8]                   
    ldxdw r0, [r10-0x4c0]                   
    jne r1, 0, lbb_13272                            if r1 != (0 as i32 as i64 as u64) { pc += 20 }
    mov64 r2, r8                                    r2 = r8
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_13272                            if r2 != (0 as i32 as i64 as u64) { pc += 17 }
    ldxdw r2, [r10-0x4d0]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_13272                            if r2 != (0 as i32 as i64 as u64) { pc += 14 }
    mov64 r2, r4                                    r2 = r4
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_13272                            if r2 != (0 as i32 as i64 as u64) { pc += 11 }
    ldxdw r2, [r10-0x4d8]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_13272                            if r2 != (0 as i32 as i64 as u64) { pc += 8 }
    mov64 r2, r5                                    r2 = r5
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_13272                            if r2 != (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r2, r0                                    r2 = r0
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_13272                            if r2 != (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r2, [r10-0x4c8]                   
    stxdw [r10-0x5f8], r2                   
lbb_13272:
    stxdw [r10-0x4c0], r0                   
    stxdw [r10-0x4b8], r5                   
    stxdw [r10-0x568], r6                   
    stxdw [r10-0x528], r4                   
    ldxdw r2, [r10-0x560]                   
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    lddw r2, 0x3ff0000000000000                     r2 load str located at 4607182418800017408
    jne r1, 0, lbb_13282                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_13282:
    stxdw [r10-0x580], r2                   
    ldxdw r1, [r10-0x510]                   
    call function_23211                     
    mov64 r7, r0                                    r7 = r0
    ldxdw r1, [r10-0x508]                   
    call function_23211                     
    mov64 r9, r0                                    r9 = r0
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r9                                    r2 = r9
    call function_26063                     
    stxdw [r10-0x590], r0                   
    mov64 r1, r7                                    r1 = r7
    lddw r2, 0x4059000000000000                     r2 load str located at 4636737291354636288
    call function_26063                     
    mov64 r1, r0                                    r1 = r0
    mov64 r2, r9                                    r2 = r9
    call function_26175                     
    stxdw [r10-0x598], r0                   
    ldxdw r1, [r10-0x4a0]                   
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    call function_23274                     
    ldxdw r1, [r10-0x4b0]                   
    mov64 r2, r0                                    r2 = r0
    call function_26175                     
    mov64 r9, r0                                    r9 = r0
    ldxdw r1, [r10-0x558]                   
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_13317                            if r1 != (0 as i32 as i64 as u64) { pc += 6 }
    lddw r1, 0xffffffff                             r1 load str located at 4294967295
    ldxdw r2, [r10-0x620]                   
    jlt r2, r1, lbb_13316                           if r2 < r1 { pc += 1 }
    stxdw [r10-0x620], r1                   
lbb_13316:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
lbb_13317:
    mov64 r7, r6                                    r7 = r6
    ldxdw r1, [r10-0x5e8]                   
    add64 r1, 16                                    r1 += 16   ///  r1 = r1.wrapping_add(16 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1176                                 r2 += -1176   ///  r2 = r2.wrapping_add(-1176 as i32 as i64 as u64)
    mov64 r3, 1032                                  r3 = 1032 as i32 as i64 as u64
    call function_23152                     
    lddw r1, 0x9e3779b97f4a7c15                     r1 load str located at -7046029254386353131
    ldxdw r2, [r10-0x578]                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 30                                    r1 >>= 30   ///  r1 = r1.wrapping_shr(30)
    ldxdw r0, [r10-0x508]                   
    jlt r0, 255, lbb_13333                          if r0 < (255 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 255                                   r0 = 255 as i32 as i64 as u64
lbb_13333:
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    lddw r2, 0xffffffff                             r2 load str located at 4294967295
    ldxdw r4, [r10-0x638]                   
    ldxdw r5, [r10-0x510]                   
    ldxdw r6, [r10-0x4f0]                   
    jlt r5, r2, lbb_13342                           if r5 < r2 { pc += 2 }
    lddw r5, 0xffffffff                             r5 load str located at 4294967295
lbb_13342:
    lddw r2, 0xbf58476d1ce4e5b9                     r2 load str located at -4658895280553007687
    mul64 r1, r2                                    r1 *= r2   ///  r1 = r1.wrapping_mul(r2)
    ldxdw r3, [r10-0x5e8]                   
    ldxdw r2, [r10-0x618]                   
    stxb [r3+0x4d7], r2                     
    ldxdw r2, [r10-0x5c8]                   
    stxb [r3+0x4d6], r2                     
    ldxdw r2, [r10-0x4a0]                   
    stxb [r3+0x4d5], r2                     
    ldxdw r2, [r10-0x500]                   
    stxb [r3+0x4d4], r2                     
    ldxdw r2, [r10-0x4f8]                   
    stxb [r3+0x4d3], r2                     
    stxb [r3+0x4d2], r6                     
    ldxdw r2, [r10-0x4e8]                   
    stxb [r3+0x4d1], r2                     
    stxb [r3+0x4d0], r0                     
    ldxdw r2, [r10-0x548]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4ce], r2                     
    ldxdw r2, [r10-0x5b8]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4cd], r2                     
    ldxdw r2, [r10-0x540]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4cc], r2                     
    ldxdw r2, [r10-0x520]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4cb], r2                     
    ldxdw r2, [r10-0x550]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4ca], r2                     
    ldxdw r2, [r10-0x4a8]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4c9], r2                     
    ldxdw r2, [r10-0x518]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4c8], r2                     
    ldxdw r2, [r10-0x588]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4c7], r2                     
    ldxdw r2, [r10-0x4e0]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4c6], r2                     
    ldxdw r2, [r10-0x5c0]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4c5], r2                     
    ldxdw r2, [r10-0x5f8]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4c4], r2                     
    ldxdw r2, [r10-0x4c8]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4c3], r2                     
    ldxdw r2, [r10-0x4c0]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4c2], r2                     
    ldxdw r2, [r10-0x4b8]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4c1], r2                     
    ldxdw r2, [r10-0x4d8]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4c0], r2                     
    ldxdw r2, [r10-0x4d0]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4bf], r2                     
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    stxb [r3+0x4be], r8                     
    ldxdw r2, [r10-0x528]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4bd], r2                     
    ldxdw r2, [r10-0x568]                   
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    stxb [r3+0x4bc], r2                     
    ldxdw r2, [r10-0x538]                   
    stxh [r3+0x4ba], r2                     
    ldxdw r2, [r10-0x530]                   
    stxh [r3+0x4b8], r2                     
    ldxdw r2, [r10-0x610]                   
    stxw [r3+0x4b4], r2                     
    stxw [r3+0x4b0], r5                     
    ldxdw r2, [r10-0x608]                   
    stxdw [r3+0x4a8], r2                    
    lddw r2, 0x7fefffffffffffff                     r2 load str located at 9218868437227405311
    stxdw [r3+0x4a0], r2                    
    stxdw [r3+0x498], r2                    
    stxdw [r3+0x490], r2                    
    stxdw [r3+0x488], r2                    
    stxdw [r3+0x480], r2                    
    stxdw [r3+0x478], r2                    
    stxdw [r3+0x470], r2                    
    stxdw [r3+0x468], r2                    
    stxdw [r3+0x460], r2                    
    stxdw [r3+0x458], r2                    
    stxdw [r3+0x450], r2                    
    stxdw [r3+0x448], r2                    
    stxdw [r3+0x440], r9                    
    ldxdw r2, [r10-0x598]                   
    stxdw [r3+0x438], r2                    
    ldxdw r2, [r10-0x590]                   
    stxdw [r3+0x430], r2                    
    ldxdw r2, [r10-0x580]                   
    stxdw [r3+0x428], r2                    
    ldxdw r2, [r10-0x600]                   
    stxdw [r3+0x420], r2                    
    ldxdw r2, [r10-0x570]                   
    div64 r2, 1000000                               r2 /= 1000000   ///  r2 = r2 / (1000000 as u64)
    stxdw [r3+0x418], r2                    
    ldxdw r2, [r10-0x620]                   
    stxw [r3+0xc], r2                       
    stxw [r3+0x8], r7                       
    stxw [r3+0x4], r4                       
    ldxdw r2, [r10-0x560]                   
    stxw [r3+0x0], r2                       
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 27                                    r2 >>= 27   ///  r2 = r2.wrapping_shr(27)
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    lddw r1, 0x94d049bb133111eb                     r1 load str located at -7723592293110705685
    mul64 r2, r1                                    r2 *= r1   ///  r2 = r2.wrapping_mul(r1)
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 31                                    r1 >>= 31   ///  r1 = r1.wrapping_shr(31)
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    mod64 r1, 100                                   r1 %= 100   ///  r1 = r1 % (100 as u64)
    stxb [r3+0x4cf], r1                     
lbb_13468:
    ldxdw r2, [r10-0x5f0]                   
    ldxb r1, [r2+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxb [r2+0x0], r1                       
    ja lbb_10434                                    if true { pc += -3039 }
lbb_13473:
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, r7                                    r2 = r7
    call function_20091                     

function_13476:
    jne r3, 1048576, lbb_13480                      if r3 != (1048576 as i32 as i64 as u64) { pc += 3 }
    mov64 r3, r2                                    r3 = r2
    and64 r3, 3                                     r3 &= 3   ///  r3 = r3.and(3)
    jeq r3, 0, lbb_13486                            if r3 == (0 as i32 as i64 as u64) { pc += 6 }
lbb_13480:
    lddw r2, 0x2600000000                           r2 load str located at 163208757248
    stxdw [r1+0x4], r2                      
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_13484:
    stxw [r1+0x0], r2                       
    exit                                    
lbb_13486:
    stxdw [r1+0x8], r2                      
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ja lbb_13484                                    if true { pc += -5 }

function_13489:
    add64 r2, 12472                                 r2 += 12472   ///  r2 = r2.wrapping_add(12472 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    and64 r3, 4                                     r3 &= 4   ///  r3 = r3.and(4)
    jne r3, 0, lbb_13496                            if r3 != (0 as i32 as i64 as u64) { pc += 3 }
    stxdw [r1+0x8], r2                      
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ja lbb_13500                                    if true { pc += 4 }
lbb_13496:
    lddw r2, 0x2600000000                           r2 load str located at 163208757248
    stxdw [r1+0x4], r2                      
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_13500:
    stxw [r1+0x0], r2                       
    exit                                    

function_13502:
    add64 r2, 16568                                 r2 += 16568   ///  r2 = r2.wrapping_add(16568 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    and64 r3, 4                                     r3 &= 4   ///  r3 = r3.and(4)
    jne r3, 0, lbb_13509                            if r3 != (0 as i32 as i64 as u64) { pc += 3 }
    stxdw [r1+0x8], r2                      
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ja lbb_13513                                    if true { pc += 4 }
lbb_13509:
    lddw r2, 0x2600000000                           r2 load str located at 163208757248
    stxdw [r1+0x4], r2                      
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_13513:
    stxw [r1+0x0], r2                       
    exit                                    

function_13515:
    stxdw [r10-0xdf0], r1                   
    ldxb r1, [r2+0x0]                       
    stxdw [r10-0xd40], r1                   
    mov64 r1, r2                                    r1 = r2
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xd38], r1                   
    stdw [r10-0xd30], 3680                  
    ldxb r1, [r2+0x7]                       
    stxdw [r10-0xdf8], r1                   
    ldxb r1, [r2+0x3]                       
    stxb [r10-0xd26], r1                    
    ldxh r1, [r2+0x1]                       
    stxh [r10-0xd28], r1                    
    ldxh r1, [r2+0x4]                       
    stxdw [r10-0xe00], r1                   
    ldxb r1, [r2+0x6]                       
    stxdw [r10-0xe08], r1                   
    mov64 r1, 3680                                  r1 = 3680 as i32 as i64 as u64
    mov64 r2, 4                                     r2 = 4 as i32 as i64 as u64
    call function_6742                      
    mov64 r7, r0                                    r7 = r0
    jne r7, 0, lbb_13540                            if r7 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 4                                     r1 = 4 as i32 as i64 as u64
    mov64 r2, 3680                                  r2 = 3680 as i32 as i64 as u64
    call function_20094                     
lbb_13540:
    stb [r7+0x0], 0                         
    mov64 r1, r7                                    r1 = r7
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -88                                   r2 += -88   ///  r2 = r2.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    lddw r6, 0x3f80000000000000                     r6 load str located at 4575657221408423936
    stxdw [r7+0x54], r6                     
    stb [r7+0x5c], 0                        
    mov64 r1, r7                                    r1 = r7
    add64 r1, 93                                    r1 += 93   ///  r1 = r1.wrapping_add(93 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -176                                  r2 += -176   ///  r2 = r2.wrapping_add(-176 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0xb0], r6                     
    stb [r7+0xb8], 0                        
    mov64 r1, r7                                    r1 = r7
    add64 r1, 185                                   r1 += 185   ///  r1 = r1.wrapping_add(185 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -264                                  r2 += -264   ///  r2 = r2.wrapping_add(-264 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x10c], r6                    
    stb [r7+0x114], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 277                                   r1 += 277   ///  r1 = r1.wrapping_add(277 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -352                                  r2 += -352   ///  r2 = r2.wrapping_add(-352 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x168], r6                    
    stb [r7+0x170], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 369                                   r1 += 369   ///  r1 = r1.wrapping_add(369 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -440                                  r2 += -440   ///  r2 = r2.wrapping_add(-440 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x1c4], r6                    
    stb [r7+0x1cc], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 461                                   r1 += 461   ///  r1 = r1.wrapping_add(461 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -528                                  r2 += -528   ///  r2 = r2.wrapping_add(-528 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x220], r6                    
    stb [r7+0x228], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 553                                   r1 += 553   ///  r1 = r1.wrapping_add(553 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -616                                  r2 += -616   ///  r2 = r2.wrapping_add(-616 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x27c], r6                    
    stb [r7+0x284], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 645                                   r1 += 645   ///  r1 = r1.wrapping_add(645 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -704                                  r2 += -704   ///  r2 = r2.wrapping_add(-704 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x2d8], r6                    
    stb [r7+0x2e0], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 737                                   r1 += 737   ///  r1 = r1.wrapping_add(737 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -792                                  r2 += -792   ///  r2 = r2.wrapping_add(-792 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x334], r6                    
    stb [r7+0x33c], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 829                                   r1 += 829   ///  r1 = r1.wrapping_add(829 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -875                                  r2 += -875   ///  r2 = r2.wrapping_add(-875 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x390], r6                    
    stb [r7+0x398], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 921                                   r1 += 921   ///  r1 = r1.wrapping_add(921 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -958                                  r2 += -958   ///  r2 = r2.wrapping_add(-958 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x3ec], r6                    
    stb [r7+0x3f4], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 1013                                  r1 += 1013   ///  r1 = r1.wrapping_add(1013 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1041                                 r2 += -1041   ///  r2 = r2.wrapping_add(-1041 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x448], r6                    
    stb [r7+0x450], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 1105                                  r1 += 1105   ///  r1 = r1.wrapping_add(1105 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1124                                 r2 += -1124   ///  r2 = r2.wrapping_add(-1124 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x4a4], r6                    
    stb [r7+0x4ac], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 1197                                  r1 += 1197   ///  r1 = r1.wrapping_add(1197 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1207                                 r2 += -1207   ///  r2 = r2.wrapping_add(-1207 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x500], r6                    
    stb [r7+0x508], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 1289                                  r1 += 1289   ///  r1 = r1.wrapping_add(1289 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1290                                 r2 += -1290   ///  r2 = r2.wrapping_add(-1290 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x55c], r6                    
    stb [r7+0x564], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 1381                                  r1 += 1381   ///  r1 = r1.wrapping_add(1381 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1373                                 r2 += -1373   ///  r2 = r2.wrapping_add(-1373 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x5b8], r6                    
    stb [r7+0x5c0], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 1473                                  r1 += 1473   ///  r1 = r1.wrapping_add(1473 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1456                                 r2 += -1456   ///  r2 = r2.wrapping_add(-1456 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x614], r6                    
    stb [r7+0x61c], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 1565                                  r1 += 1565   ///  r1 = r1.wrapping_add(1565 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1539                                 r2 += -1539   ///  r2 = r2.wrapping_add(-1539 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x670], r6                    
    stb [r7+0x678], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 1657                                  r1 += 1657   ///  r1 = r1.wrapping_add(1657 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1622                                 r2 += -1622   ///  r2 = r2.wrapping_add(-1622 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x6cc], r6                    
    stb [r7+0x6d4], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 1749                                  r1 += 1749   ///  r1 = r1.wrapping_add(1749 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1705                                 r2 += -1705   ///  r2 = r2.wrapping_add(-1705 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x728], r6                    
    stb [r7+0x730], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 1841                                  r1 += 1841   ///  r1 = r1.wrapping_add(1841 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1788                                 r2 += -1788   ///  r2 = r2.wrapping_add(-1788 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x784], r6                    
    stb [r7+0x78c], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 1933                                  r1 += 1933   ///  r1 = r1.wrapping_add(1933 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1871                                 r2 += -1871   ///  r2 = r2.wrapping_add(-1871 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x7e0], r6                    
    stb [r7+0x7e8], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 2025                                  r1 += 2025   ///  r1 = r1.wrapping_add(2025 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1954                                 r2 += -1954   ///  r2 = r2.wrapping_add(-1954 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x83c], r6                    
    stb [r7+0x844], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 2117                                  r1 += 2117   ///  r1 = r1.wrapping_add(2117 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2037                                 r2 += -2037   ///  r2 = r2.wrapping_add(-2037 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x898], r6                    
    stb [r7+0x8a0], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 2209                                  r1 += 2209   ///  r1 = r1.wrapping_add(2209 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2120                                 r2 += -2120   ///  r2 = r2.wrapping_add(-2120 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x8f4], r6                    
    stb [r7+0x8fc], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 2301                                  r1 += 2301   ///  r1 = r1.wrapping_add(2301 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2203                                 r2 += -2203   ///  r2 = r2.wrapping_add(-2203 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x950], r6                    
    stb [r7+0x958], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 2393                                  r1 += 2393   ///  r1 = r1.wrapping_add(2393 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2286                                 r2 += -2286   ///  r2 = r2.wrapping_add(-2286 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0x9ac], r6                    
    stb [r7+0x9b4], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 2485                                  r1 += 2485   ///  r1 = r1.wrapping_add(2485 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2369                                 r2 += -2369   ///  r2 = r2.wrapping_add(-2369 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0xa08], r6                    
    stb [r7+0xa10], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 2577                                  r1 += 2577   ///  r1 = r1.wrapping_add(2577 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2452                                 r2 += -2452   ///  r2 = r2.wrapping_add(-2452 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0xa64], r6                    
    stb [r7+0xa6c], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 2669                                  r1 += 2669   ///  r1 = r1.wrapping_add(2669 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2535                                 r2 += -2535   ///  r2 = r2.wrapping_add(-2535 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0xac0], r6                    
    stb [r7+0xac8], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 2761                                  r1 += 2761   ///  r1 = r1.wrapping_add(2761 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2618                                 r2 += -2618   ///  r2 = r2.wrapping_add(-2618 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0xb1c], r6                    
    stb [r7+0xb24], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 2853                                  r1 += 2853   ///  r1 = r1.wrapping_add(2853 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2701                                 r2 += -2701   ///  r2 = r2.wrapping_add(-2701 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0xb78], r6                    
    stb [r7+0xb80], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 2945                                  r1 += 2945   ///  r1 = r1.wrapping_add(2945 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2784                                 r2 += -2784   ///  r2 = r2.wrapping_add(-2784 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0xbd4], r6                    
    stb [r7+0xbdc], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 3037                                  r1 += 3037   ///  r1 = r1.wrapping_add(3037 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2867                                 r2 += -2867   ///  r2 = r2.wrapping_add(-2867 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0xc30], r6                    
    stb [r7+0xc38], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 3129                                  r1 += 3129   ///  r1 = r1.wrapping_add(3129 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2950                                 r2 += -2950   ///  r2 = r2.wrapping_add(-2950 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0xc8c], r6                    
    stb [r7+0xc94], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 3221                                  r1 += 3221   ///  r1 = r1.wrapping_add(3221 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3033                                 r2 += -3033   ///  r2 = r2.wrapping_add(-3033 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0xce8], r6                    
    stb [r7+0xcf0], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 3313                                  r1 += 3313   ///  r1 = r1.wrapping_add(3313 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3116                                 r2 += -3116   ///  r2 = r2.wrapping_add(-3116 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0xd44], r6                    
    stb [r7+0xd4c], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 3405                                  r1 += 3405   ///  r1 = r1.wrapping_add(3405 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3199                                 r2 += -3199   ///  r2 = r2.wrapping_add(-3199 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0xda0], r6                    
    stb [r7+0xda8], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 3497                                  r1 += 3497   ///  r1 = r1.wrapping_add(3497 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3282                                 r2 += -3282   ///  r2 = r2.wrapping_add(-3282 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r7+0xdfc], r6                    
    stb [r7+0xe04], 0                       
    mov64 r1, r7                                    r1 = r7
    add64 r1, 3589                                  r1 += 3589   ///  r1 = r1.wrapping_add(3589 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3365                                 r2 += -3365   ///  r2 = r2.wrapping_add(-3365 as i32 as i64 as u64)
    mov64 r3, 83                                    r3 = 83 as i32 as i64 as u64
    call function_23152                     
    stxdw [r10-0xd88], r7                   
    stxdw [r7+0xe58], r6                    
    ldxdw r1, [r10-0xd40]                   
    jeq r1, 0, lbb_13927                            if r1 == (0 as i32 as i64 as u64) { pc += 62 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xd60], r1                   
    ldxdw r1, [r10-0xd30]                   
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_13934                                    if true { pc += 64 }
lbb_13870:
    ldxdw r5, [r10-0xd60]                   
    jeq r5, 3680, lbb_14760                         if r5 == (3680 as i32 as i64 as u64) { pc += 888 }
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    and64 r6, 255                                   r6 &= 255   ///  r6 = r6.and(255)
    lsh64 r6, 8                                     r6 <<= 8   ///  r6 = r6.wrapping_shl(8)
    ldxdw r2, [r10-0xd70]                   
    or64 r6, r2                                     r6 |= r2   ///  r6 = r6.or(r2)
    ldxdw r8, [r10-0xd98]                   
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    ldxdw r2, [r10-0xd80]                   
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    or64 r8, r2                                     r8 |= r2   ///  r8 = r8.or(r2)
    ldxdw r2, [r10-0xd68]                   
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    or64 r6, r2                                     r6 |= r2   ///  r6 = r6.or(r2)
    ldxdw r9, [r10-0xd50]                   
    rsh64 r9, 32                                    r9 >>= 32   ///  r9 = r9.wrapping_shr(32)
    ldxdw r2, [r10-0xd88]                   
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    ldxdw r3, [r10-0xdd0]                   
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 16                                    r4 >>= 16   ///  r4 = r4.wrapping_shr(16)
    stxb [r2+0x2f], r4                      
    stxh [r2+0x2d], r3                      
    ldxdw r3, [r10-0xd90]                   
    stxw [r2+0x58], r3                      
    stxw [r2+0x54], r7                      
    ldxdw r3, [r10-0xde8]                   
    stxdw [r2+0x4c], r3                     
    ldxdw r3, [r10-0xde0]                   
    stxdw [r2+0x44], r3                     
    ldxdw r3, [r10-0xdd8]                   
    stxw [r2+0x40], r3                      
    ldxdw r3, [r10-0xd58]                   
    stxw [r2+0x3c], r3                      
    ldxdw r3, [r10-0xdc8]                   
    stxdw [r2+0x34], r3                     
    ldxdw r3, [r10-0xdb8]                   
    stxw [r2+0x30], r3                      
    ldxdw r3, [r10-0xdc0]                   
    stxb [r2+0x2c], r3                      
    ldxdw r3, [r10-0xda0]                   
    stxdw [r2+0x24], r3                     
    ldxdw r3, [r10-0xda8]                   
    stxdw [r2+0x1c], r3                     
    ldxdw r3, [r10-0xdb0]                   
    stxw [r2+0x18], r3                      
    stxdw [r2+0x10], r8                     
    stxw [r2+0xc], r9                       
    ldxdw r3, [r10-0xd78]                   
    stxdw [r2+0x4], r3                      
    stxw [r2+0x0], r6                       
    add64 r5, 92                                    r5 += 92   ///  r5 = r5.wrapping_add(92 as i32 as i64 as u64)
    stxdw [r10-0xd60], r5                   
    ldxdw r2, [r10-0xd40]                   
    jlt r0, r2, lbb_13934                           if r0 < r2 { pc += 7 }
lbb_13927:
    mov64 r1, 16                                    r1 = 16 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6742                      
    jne r0, 0, lbb_14689                            if r0 != (0 as i32 as i64 as u64) { pc += 758 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 16                                    r2 = 16 as i32 as i64 as u64
    call function_20094                     
lbb_13934:
    jeq r1, 0, lbb_14709                            if r1 == (0 as i32 as i64 as u64) { pc += 774 }
    ldxdw r2, [r10-0xd38]                   
    ldxb r4, [r2+0x0]                       
    mov64 r3, r2                                    r3 = r2
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0xd38], r3                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    stxdw [r10-0xd30], r3                   
    stxb [r10-0x3be], r4                    
    stxdw [r10-0xd48], r0                   
    stxdw [r10-0xd70], r4                   
    jsgt r4, 6, lbb_13975                           if (r4 as i64) > (6 as i32 as i64) { pc += 28 }
    jsgt r4, 2, lbb_13991                           if (r4 as i64) > (2 as i32 as i64) { pc += 43 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xd68], r1                   
    jeq r4, 0, lbb_14436                            if r4 == (0 as i32 as i64 as u64) { pc += 485 }
    jeq r4, 1, lbb_14085                            if r4 == (1 as i32 as i64 as u64) { pc += 133 }
    jeq r4, 2, lbb_13954                            if r4 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14245                                    if true { pc += 291 }
lbb_13954:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17655                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_13962                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14631                                    if true { pc += 669 }
lbb_13962:
    ldxdw r1, [r10-0xd30]                   
    jeq r1, 0, lbb_14709                            if r1 == (0 as i32 as i64 as u64) { pc += 745 }
    ldxw r8, [r10-0x50]                     
    ldxw r9, [r10-0x54]                     
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    ldxdw r2, [r10-0xd38]                   
    ldxb r6, [r2+0x0]                       
    stxdw [r10-0xd30], r1                   
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0xd38], r2                   
    stxb [r10-0x36b], r6                    
    jlt r6, 6, lbb_14197                            if r6 < (6 as i32 as i64 as u64) { pc += 223 }
    ja lbb_14405                                    if true { pc += 430 }
lbb_13975:
    jsgt r4, 9, lbb_14006                           if (r4 as i64) > (9 as i32 as i64) { pc += 30 }
    jeq r4, 7, lbb_14132                            if r4 == (7 as i32 as i64 as u64) { pc += 155 }
    jeq r4, 8, lbb_14100                            if r4 == (8 as i32 as i64 as u64) { pc += 122 }
    jeq r4, 9, lbb_13980                            if r4 == (9 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14245                                    if true { pc += 265 }
lbb_13980:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_13988                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14631                                    if true { pc += 643 }
lbb_13988:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xd68], r1                   
    ja lbb_14093                                    if true { pc += 102 }
lbb_13991:
    jsgt r4, 4, lbb_14032                           if (r4 as i64) > (4 as i32 as i64) { pc += 40 }
    jeq r4, 3, lbb_14395                            if r4 == (3 as i32 as i64 as u64) { pc += 402 }
    jeq r4, 4, lbb_13995                            if r4 == (4 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14245                                    if true { pc += 250 }
lbb_13995:
    jeq r3, 0, lbb_14709                            if r3 == (0 as i32 as i64 as u64) { pc += 713 }
    mov64 r3, r1                                    r3 = r1
    add64 r3, -2                                    r3 += -2   ///  r3 = r3.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r4, [r2+0x1]                       
    stxdw [r10-0xd30], r3                   
    mov64 r3, r2                                    r3 = r2
    add64 r3, 2                                     r3 += 2   ///  r3 = r3.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r10-0xd38], r3                   
    stxb [r10-0x36b], r4                    
    jeq r4, 0, lbb_14158                            if r4 == (0 as i32 as i64 as u64) { pc += 153 }
    ja lbb_14405                                    if true { pc += 399 }
lbb_14006:
    jsgt r4, 11, lbb_14243                          if (r4 as i64) > (11 as i32 as i64) { pc += 236 }
    jeq r4, 10, lbb_14049                           if r4 == (10 as i32 as i64 as u64) { pc += 41 }
    jeq r4, 11, lbb_14010                           if r4 == (11 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14245                                    if true { pc += 235 }
lbb_14010:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jne r1, 0, lbb_14031                            if r1 != (0 as i32 as i64 as u64) { pc += 14 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xd68], r1                   
    ldxdw r1, [r10-0xd40]                   
    ldxw r7, [r10-0x44]                     
    ldxw r9, [r10-0x48]                     
    ldxdw r6, [r10-0x50]                    
    ldxw r8, [r10-0x54]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_14160                            if r1 == (0 as i32 as i64 as u64) { pc += 129 }
lbb_14031:
    ja lbb_14070                                    if true { pc += 38 }
lbb_14032:
    jeq r4, 5, lbb_14072                            if r4 == (5 as i32 as i64 as u64) { pc += 39 }
    jeq r4, 6, lbb_14035                            if r4 == (6 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14245                                    if true { pc += 210 }
lbb_14035:
    jeq r3, 0, lbb_14709                            if r3 == (0 as i32 as i64 as u64) { pc += 673 }
    add64 r1, -2                                    r1 += -2   ///  r1 = r1.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r3, [r2+0x1]                       
    stxdw [r10-0xd30], r1                   
    add64 r2, 2                                     r2 += 2   ///  r2 = r2.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r10-0xd38], r2                   
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xd68], r1                   
    stxb [r10-0x36b], r3                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jeq r3, 0, lbb_14048                            if r3 == (0 as i32 as i64 as u64) { pc += 2 }
    jne r3, 1, lbb_14405                            if r3 != (1 as i32 as i64 as u64) { pc += 358 }
lbb_14047:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
lbb_14048:
    ja lbb_14436                                    if true { pc += 387 }
lbb_14049:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jne r1, 0, lbb_14070                            if r1 != (0 as i32 as i64 as u64) { pc += 14 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xd68], r1                   
    ldxw r1, [r10-0x44]                     
    stxdw [r10-0xd98], r1                   
    ldxw r8, [r10-0x48]                     
    ldxdw r6, [r10-0x50]                    
    ldxw r9, [r10-0x54]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_14151                            if r1 == (0 as i32 as i64 as u64) { pc += 81 }
lbb_14070:
    ldxdw r8, [r10-0x50]                    
    ja lbb_14713                                    if true { pc += 641 }
lbb_14072:
    jeq r3, 0, lbb_14709                            if r3 == (0 as i32 as i64 as u64) { pc += 636 }
    add64 r1, -2                                    r1 += -2   ///  r1 = r1.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r3, [r2+0x1]                       
    stxdw [r10-0xd30], r1                   
    add64 r2, 2                                     r2 += 2   ///  r2 = r2.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r10-0xd38], r2                   
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xd68], r1                   
    stxb [r10-0x36b], r3                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jeq r3, 0, lbb_14048                            if r3 == (0 as i32 as i64 as u64) { pc += -35 }
    jne r3, 1, lbb_14405                            if r3 != (1 as i32 as i64 as u64) { pc += 321 }
    ja lbb_14047                                    if true { pc += -38 }
lbb_14085:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_14093                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14631                                    if true { pc += 538 }
lbb_14093:
    ldxdw r8, [r10-0x50]                    
    ldxw r9, [r10-0x54]                     
    ldxdw r1, [r10-0x48]                    
    stxdw [r10-0xd80], r1                   
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    stxdw [r10-0xd98], r1                   
    ja lbb_14130                                    if true { pc += 30 }
lbb_14100:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_14108                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14631                                    if true { pc += 523 }
lbb_14108:
    ldxdw r6, [r10-0x48]                    
    ldxdw r8, [r10-0x50]                    
    ldxw r9, [r10-0x54]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_14119                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14631                                    if true { pc += 512 }
lbb_14119:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xd68], r1                   
    ldxdw r1, [r10-0x48]                    
    stxdw [r10-0xda0], r1                   
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0xda8], r1                   
    ldxw r1, [r10-0x54]                     
    stxdw [r10-0xdb0], r1                   
    stxdw [r10-0xd80], r6                   
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    stxdw [r10-0xd98], r6                   
lbb_14130:
    ldxdw r0, [r10-0xd48]                   
    ja lbb_14436                                    if true { pc += 304 }
lbb_14132:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_14140                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14631                                    if true { pc += 491 }
lbb_14140:
    ldxdw r6, [r10-0x48]                    
    ldxdw r8, [r10-0x50]                    
    ldxw r9, [r10-0x54]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_14119                            if r1 == (0 as i32 as i64 as u64) { pc += -31 }
    ja lbb_14070                                    if true { pc += -81 }
lbb_14151:
    stxw [r10-0xa8], r8                     
    stxdw [r10-0xb0], r6                    
    mov64 r1, r9                                    r1 = r9
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    jne r1, 8, lbb_14166                            if r1 != (8 as i32 as i64 as u64) { pc += 10 }
lbb_14156:
    ldxdw r8, [r10-0xac]                    
    ja lbb_14713                                    if true { pc += 555 }
lbb_14158:
    jlt r1, 10, lbb_14709                           if r1 < (10 as i32 as i64 as u64) { pc += 550 }
    ja lbb_14419                                    if true { pc += 259 }
lbb_14160:
    stxw [r10-0xa8], r9                     
    stxdw [r10-0xb0], r6                    
    mov64 r1, r8                                    r1 = r8
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    jne r1, 8, lbb_14601                            if r1 != (8 as i32 as i64 as u64) { pc += 436 }
    ja lbb_14156                                    if true { pc += -10 }
lbb_14166:
    ldxdw r1, [r10-0x48]                    
    stxdw [r10-0xda0], r1                   
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0xda8], r1                   
    ldxw r1, [r10-0x54]                     
    stxdw [r10-0xdb0], r1                   
    stxw [r10-0x2c0], r6                    
    ldxdw r6, [r10-0xac]                    
    stxdw [r10-0x2bc], r6                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jne r1, 0, lbb_14196                            if r1 != (0 as i32 as i64 as u64) { pc += 14 }
    stxdw [r10-0xd78], r9                   
    stxdw [r10-0xd80], r6                   
    ldxw r1, [r10-0x44]                     
    stxdw [r10-0xd58], r1                   
    ldxw r6, [r10-0x48]                     
    ldxdw r9, [r10-0x50]                    
    ldxw r8, [r10-0x54]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_14216                            if r1 == (0 as i32 as i64 as u64) { pc += 20 }
lbb_14196:
    ja lbb_14070                                    if true { pc += -127 }
lbb_14197:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17655                     
    ldxw r1, [r10-0x58]                     
    jne r1, 0, lbb_14631                            if r1 != (0 as i32 as i64 as u64) { pc += 427 }
    ldxw r1, [r10-0x54]                     
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    or64 r1, r8                                     r1 |= r8   ///  r1 = r1.or(r8)
    mov64 r8, r1                                    r8 = r1
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xd68], r1                   
    ldxw r1, [r10-0x50]                     
    stxdw [r10-0xd80], r1                   
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xd98], r1                   
    ldxdw r0, [r10-0xd48]                   
    ja lbb_14436                                    if true { pc += 220 }
lbb_14216:
    stxw [r10-0xa8], r6                     
    stxdw [r10-0xb0], r9                    
    mov64 r1, r8                                    r1 = r8
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r2, r8                                    r2 = r8
    jne r1, 8, lbb_14223                            if r1 != (8 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14156                                    if true { pc += -67 }
lbb_14223:
    stxdw [r10-0xdb8], r9                   
    ldxdw r1, [r10-0xd80]                   
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    stxdw [r10-0xd80], r1                   
    ldxdw r1, [r10-0x48]                    
    stxdw [r10-0xde8], r1                   
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0xde0], r1                   
    ldxw r1, [r10-0x54]                     
    stxdw [r10-0xdd8], r1                   
    ldxdw r8, [r10-0x2c0]                   
    ldxdw r1, [r10-0xac]                    
    stxdw [r10-0xdc8], r1                   
    stxdw [r10-0xdc0], r2                   
    lddw r1, 0xffffff00                             r1 load str located at 4294967040
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    rsh64 r2, 8                                     r2 >>= 8   ///  r2 = r2.wrapping_shr(8)
    stxdw [r10-0xdd0], r2                   
    ja lbb_14597                                    if true { pc += 354 }
lbb_14243:
    jeq r4, 12, lbb_14263                           if r4 == (12 as i32 as i64 as u64) { pc += 19 }
    jeq r4, 13, lbb_14318                           if r4 == (13 as i32 as i64 as u64) { pc += 73 }
lbb_14245:
    lddw r1, 0x100034cb0 --> b"\x00\x00\x00\x00\xb48\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295183536
    stxdw [r10-0x58], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    stxdw [r10-0x48], r1                    
    lddw r1, 0x100010468 --> b"a#4\x00\x00\x00\x00\x00\xbf4\x00\x00\x00\x00\x00\x00W\x04\x00\x00\x10\x00…        r1 load str located at 4295033960
    stxdw [r10-0xa8], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -958                                  r1 += -958   ///  r1 = r1.wrapping_add(-958 as i32 as i64 as u64)
    stxdw [r10-0xb0], r1                    
    stdw [r10-0x38], 0                      
    stdw [r10-0x50], 1                      
    stdw [r10-0x40], 1                      
    mov64 r8, r10                                   r8 = r10
    add64 r8, -264                                  r8 += -264   ///  r8 = r8.wrapping_add(-264 as i32 as i64 as u64)
    ja lbb_14753                                    if true { pc += 490 }
lbb_14263:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_14271                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14631                                    if true { pc += 360 }
lbb_14271:
    ldxdw r6, [r10-0x48]                    
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0xd50], r1                   
    ldxw r8, [r10-0x54]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_14283                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14631                                    if true { pc += 348 }
lbb_14283:
    stxdw [r10-0xd80], r6                   
    ldxdw r1, [r10-0x48]                    
    stxdw [r10-0xda0], r1                   
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0xda8], r1                   
    ldxw r6, [r10-0x54]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_14297                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14631                                    if true { pc += 334 }
lbb_14297:
    stxdw [r10-0xd78], r8                   
    ldxdw r1, [r10-0xd40]                   
    ldxdw r8, [r10-0x48]                    
    stxw [r10-0x260], r8                    
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0xdb8], r1                   
    stxdw [r10-0x268], r1                   
    ldxw r9, [r10-0x54]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_14313                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14631                                    if true { pc += 318 }
lbb_14313:
    stxdw [r10-0xdb0], r6                   
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xd68], r1                   
    ldxdw r1, [r10-0x264]                   
    ja lbb_14372                                    if true { pc += 54 }
lbb_14318:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_14326                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14631                                    if true { pc += 305 }
lbb_14326:
    ldxdw r6, [r10-0x48]                    
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0xd50], r1                   
    ldxw r8, [r10-0x54]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_14338                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14631                                    if true { pc += 293 }
lbb_14338:
    stxdw [r10-0xd80], r6                   
    ldxdw r1, [r10-0x48]                    
    stxdw [r10-0xda0], r1                   
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0xda8], r1                   
    ldxw r6, [r10-0x54]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_14352                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14631                                    if true { pc += 279 }
lbb_14352:
    stxdw [r10-0xd78], r8                   
    ldxdw r1, [r10-0xd40]                   
    ldxdw r8, [r10-0x48]                    
    stxw [r10-0x208], r8                    
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0xdb8], r1                   
    stxdw [r10-0x210], r1                   
    ldxw r9, [r10-0x54]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_14368                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14631                                    if true { pc += 263 }
lbb_14368:
    stxdw [r10-0xdb0], r6                   
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xd68], r1                   
    ldxdw r1, [r10-0x20c]                   
lbb_14372:
    stxdw [r10-0xdc8], r1                   
    ldxdw r1, [r10-0x48]                    
    stxdw [r10-0xde8], r1                   
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0xde0], r1                   
    ldxw r1, [r10-0x54]                     
    stxdw [r10-0xdd8], r1                   
    stxdw [r10-0xdc0], r9                   
    lddw r1, 0xffffff00                             r1 load str located at 4294967040
    and64 r9, r1                                    r9 &= r1   ///  r9 = r9.and(r1)
    ldxdw r1, [r10-0xd80]                   
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    stxdw [r10-0xd98], r1                   
    rsh64 r8, 32                                    r8 >>= 32   ///  r8 = r8.wrapping_shr(32)
    rsh64 r9, 8                                     r9 >>= 8   ///  r9 = r9.wrapping_shr(8)
    stxdw [r10-0xdd0], r9                   
    ldxdw r0, [r10-0xd48]                   
    ldxdw r1, [r10-0xd40]                   
    mov64 r1, r8                                    r1 = r8
    ldxdw r9, [r10-0xd78]                   
    ldxdw r8, [r10-0xd50]                   
    ja lbb_14436                                    if true { pc += 41 }
lbb_14395:
    jeq r3, 0, lbb_14709                            if r3 == (0 as i32 as i64 as u64) { pc += 313 }
    mov64 r3, r1                                    r3 = r1
    add64 r3, -2                                    r3 += -2   ///  r3 = r3.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r4, [r2+0x1]                       
    stxdw [r10-0xd30], r3                   
    mov64 r3, r2                                    r3 = r2
    add64 r3, 2                                     r3 += 2   ///  r3 = r3.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r10-0xd38], r3                   
    stxb [r10-0x36b], r4                    
    jeq r4, 0, lbb_14418                            if r4 == (0 as i32 as i64 as u64) { pc += 13 }
lbb_14405:
    lddw r1, 0x100034cb0 --> b"\x00\x00\x00\x00\xb48\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295183536
    stxdw [r10-0x58], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -352                                  r1 += -352   ///  r1 = r1.wrapping_add(-352 as i32 as i64 as u64)
    stxdw [r10-0x48], r1                    
    lddw r1, 0x100010468 --> b"a#4\x00\x00\x00\x00\x00\xbf4\x00\x00\x00\x00\x00\x00W\x04\x00\x00\x10\x00…        r1 load str located at 4295033960
    stxdw [r10-0x158], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -875                                  r1 += -875   ///  r1 = r1.wrapping_add(-875 as i32 as i64 as u64)
    stxdw [r10-0x160], r1                   
    ja lbb_14748                                    if true { pc += 330 }
lbb_14418:
    jlt r1, 10, lbb_14709                           if r1 < (10 as i32 as i64 as u64) { pc += 290 }
lbb_14419:
    ldxh r3, [r2+0x6]                       
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    ldxw r6, [r2+0x2]                       
    or64 r6, r3                                     r6 |= r3   ///  r6 = r6.or(r3)
    ldxh r9, [r2+0x8]                       
    add64 r2, 10                                    r2 += 10   ///  r2 = r2.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r10-0xd38], r2                   
    add64 r1, -10                                   r1 += -10   ///  r1 = r1.wrapping_add(-10 as i32 as i64 as u64)
    stxdw [r10-0xd30], r1                   
    mov64 r8, r9                                    r8 = r9
    rsh64 r8, 8                                     r8 >>= 8   ///  r8 = r8.wrapping_shr(8)
    lsh64 r9, 48                                    r9 <<= 48   ///  r9 = r9.wrapping_shl(48)
    or64 r9, r6                                     r9 |= r6   ///  r9 = r9.or(r6)
    mov64 r1, r6                                    r1 = r6
    rsh64 r1, 8                                     r1 >>= 8   ///  r1 = r1.wrapping_shr(8)
    stxdw [r10-0xd68], r1                   
    rsh64 r9, 24                                    r9 >>= 24   ///  r9 = r9.wrapping_shr(24)
lbb_14436:
    ldxdw r2, [r10-0xd40]                   
    stxw [r10-0x318], r9                    
    stxdw [r10-0x314], r8                   
    ldxdw r9, [r10-0xd30]                   
    jeq r9, 0, lbb_14709                            if r9 == (0 as i32 as i64 as u64) { pc += 268 }
    stxdw [r10-0xd58], r1                   
    ldxdw r1, [r10-0x318]                   
    stxdw [r10-0xd78], r1                   
    ldxdw r2, [r10-0xd38]                   
    ldxb r4, [r2+0x0]                       
    mov64 r1, r2                                    r1 = r2
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0xd38], r1                   
    mov64 r1, r9                                    r1 = r9
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxdw [r10-0xd30], r1                   
    stxb [r10-0x160], r4                    
    stxdw [r10-0xd50], r8                   
    jsgt r4, 5, lbb_14474                           if (r4 as i64) > (5 as i32 as i64) { pc += 19 }
    jsgt r4, 2, lbb_14504                           if (r4 as i64) > (2 as i32 as i64) { pc += 48 }
    jeq r4, 0, lbb_14539                            if r4 == (0 as i32 as i64 as u64) { pc += 82 }
    jeq r4, 1, lbb_14581                            if r4 == (1 as i32 as i64 as u64) { pc += 123 }
    jeq r4, 2, lbb_14460                            if r4 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14664                                    if true { pc += 204 }
lbb_14460:
    jlt r9, 5, lbb_14709                            if r9 < (5 as i32 as i64 as u64) { pc += 248 }
    ldxw r1, [r2+0x1]                       
    add64 r2, 5                                     r2 += 5   ///  r2 = r2.wrapping_add(5 as i32 as i64 as u64)
    stxdw [r10-0xd38], r2                   
    add64 r9, -5                                    r9 += -5   ///  r9 = r9.wrapping_add(-5 as i32 as i64 as u64)
    stxdw [r10-0xd30], r9                   
    stxdw [r10-0xd90], r1                   
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r2, r1                                    r2 = r1
    call function_25947                     
    jne r0, 0, lbb_14742                            if r0 != (0 as i32 as i64 as u64) { pc += 270 }
    mov64 r7, 2                                     r7 = 2 as i32 as i64 as u64
    ja lbb_14594                                    if true { pc += 120 }
lbb_14474:
    jsgt r4, 8, lbb_14493                           if (r4 as i64) > (8 as i32 as i64) { pc += 18 }
    jeq r4, 6, lbb_14523                            if r4 == (6 as i32 as i64 as u64) { pc += 47 }
    jeq r4, 7, lbb_14567                            if r4 == (7 as i32 as i64 as u64) { pc += 90 }
    jeq r4, 8, lbb_14479                            if r4 == (8 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14664                                    if true { pc += 185 }
lbb_14479:
    jlt r9, 5, lbb_14709                            if r9 < (5 as i32 as i64 as u64) { pc += 229 }
    ldxw r1, [r2+0x1]                       
    add64 r2, 5                                     r2 += 5   ///  r2 = r2.wrapping_add(5 as i32 as i64 as u64)
    stxdw [r10-0xd38], r2                   
    add64 r9, -5                                    r9 += -5   ///  r9 = r9.wrapping_add(-5 as i32 as i64 as u64)
    stxdw [r10-0xd30], r9                   
    stxdw [r10-0xd90], r1                   
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r2, r1                                    r2 = r1
    call function_25947                     
    jne r0, 0, lbb_14742                            if r0 != (0 as i32 as i64 as u64) { pc += 251 }
    mov64 r7, 8                                     r7 = 8 as i32 as i64 as u64
    ja lbb_14594                                    if true { pc += 101 }
lbb_14493:
    jsgt r4, 10, lbb_14662                          if (r4 as i64) > (10 as i32 as i64) { pc += 168 }
    jeq r4, 9, lbb_14537                            if r4 == (9 as i32 as i64 as u64) { pc += 42 }
    jeq r4, 10, lbb_14497                           if r4 == (10 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14664                                    if true { pc += 167 }
lbb_14497:
    jlt r9, 5, lbb_14709                            if r9 < (5 as i32 as i64 as u64) { pc += 211 }
    ldxw r1, [r2+0x1]                       
    stxdw [r10-0xd90], r1                   
    add64 r2, 5                                     r2 += 5   ///  r2 = r2.wrapping_add(5 as i32 as i64 as u64)
    stxdw [r10-0xd38], r2                   
    mov64 r7, 10                                    r7 = 10 as i32 as i64 as u64
    ja lbb_14685                                    if true { pc += 181 }
lbb_14504:
    jeq r4, 3, lbb_14553                            if r4 == (3 as i32 as i64 as u64) { pc += 48 }
    mov64 r7, 4                                     r7 = 4 as i32 as i64 as u64
    jeq r4, 4, lbb_13870                            if r4 == (4 as i32 as i64 as u64) { pc += -637 }
    jeq r4, 5, lbb_14509                            if r4 == (5 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14664                                    if true { pc += 155 }
lbb_14509:
    jlt r9, 5, lbb_14709                            if r9 < (5 as i32 as i64 as u64) { pc += 199 }
    ldxw r1, [r2+0x1]                       
    add64 r2, 5                                     r2 += 5   ///  r2 = r2.wrapping_add(5 as i32 as i64 as u64)
    stxdw [r10-0xd38], r2                   
    add64 r9, -5                                    r9 += -5   ///  r9 = r9.wrapping_add(-5 as i32 as i64 as u64)
    stxdw [r10-0xd30], r9                   
    stxdw [r10-0xd90], r1                   
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r2, r1                                    r2 = r1
    call function_25947                     
    jne r0, 0, lbb_14742                            if r0 != (0 as i32 as i64 as u64) { pc += 221 }
    mov64 r7, 5                                     r7 = 5 as i32 as i64 as u64
    ja lbb_14594                                    if true { pc += 71 }
lbb_14523:
    jlt r9, 5, lbb_14709                            if r9 < (5 as i32 as i64 as u64) { pc += 185 }
    ldxw r1, [r2+0x1]                       
    add64 r2, 5                                     r2 += 5   ///  r2 = r2.wrapping_add(5 as i32 as i64 as u64)
    stxdw [r10-0xd38], r2                   
    add64 r9, -5                                    r9 += -5   ///  r9 = r9.wrapping_add(-5 as i32 as i64 as u64)
    stxdw [r10-0xd30], r9                   
    stxdw [r10-0xd90], r1                   
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r2, r1                                    r2 = r1
    call function_25947                     
    jne r0, 0, lbb_14742                            if r0 != (0 as i32 as i64 as u64) { pc += 207 }
    mov64 r7, 6                                     r7 = 6 as i32 as i64 as u64
    ja lbb_14594                                    if true { pc += 57 }
lbb_14537:
    mov64 r7, 9                                     r7 = 9 as i32 as i64 as u64
    ja lbb_13870                                    if true { pc += -669 }
lbb_14539:
    jlt r9, 5, lbb_14709                            if r9 < (5 as i32 as i64 as u64) { pc += 169 }
    ldxw r1, [r2+0x1]                       
    add64 r2, 5                                     r2 += 5   ///  r2 = r2.wrapping_add(5 as i32 as i64 as u64)
    stxdw [r10-0xd38], r2                   
    add64 r9, -5                                    r9 += -5   ///  r9 = r9.wrapping_add(-5 as i32 as i64 as u64)
    stxdw [r10-0xd30], r9                   
    stxdw [r10-0xd90], r1                   
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r2, r1                                    r2 = r1
    call function_25947                     
    jne r0, 0, lbb_14742                            if r0 != (0 as i32 as i64 as u64) { pc += 191 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ja lbb_14594                                    if true { pc += 41 }
lbb_14553:
    jlt r9, 5, lbb_14709                            if r9 < (5 as i32 as i64 as u64) { pc += 155 }
    ldxw r1, [r2+0x1]                       
    add64 r2, 5                                     r2 += 5   ///  r2 = r2.wrapping_add(5 as i32 as i64 as u64)
    stxdw [r10-0xd38], r2                   
    add64 r9, -5                                    r9 += -5   ///  r9 = r9.wrapping_add(-5 as i32 as i64 as u64)
    stxdw [r10-0xd30], r9                   
    stxdw [r10-0xd90], r1                   
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r2, r1                                    r2 = r1
    call function_25947                     
    jne r0, 0, lbb_14742                            if r0 != (0 as i32 as i64 as u64) { pc += 177 }
    mov64 r7, 3                                     r7 = 3 as i32 as i64 as u64
    ja lbb_14594                                    if true { pc += 27 }
lbb_14567:
    jlt r9, 5, lbb_14709                            if r9 < (5 as i32 as i64 as u64) { pc += 141 }
    ldxw r1, [r2+0x1]                       
    add64 r2, 5                                     r2 += 5   ///  r2 = r2.wrapping_add(5 as i32 as i64 as u64)
    stxdw [r10-0xd38], r2                   
    add64 r9, -5                                    r9 += -5   ///  r9 = r9.wrapping_add(-5 as i32 as i64 as u64)
    stxdw [r10-0xd30], r9                   
    stxdw [r10-0xd90], r1                   
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r2, r1                                    r2 = r1
    call function_25947                     
    jne r0, 0, lbb_14742                            if r0 != (0 as i32 as i64 as u64) { pc += 163 }
    mov64 r7, 7                                     r7 = 7 as i32 as i64 as u64
    ja lbb_14594                                    if true { pc += 13 }
lbb_14581:
    jlt r9, 5, lbb_14709                            if r9 < (5 as i32 as i64 as u64) { pc += 127 }
    ldxw r1, [r2+0x1]                       
    add64 r2, 5                                     r2 += 5   ///  r2 = r2.wrapping_add(5 as i32 as i64 as u64)
    stxdw [r10-0xd38], r2                   
    add64 r9, -5                                    r9 += -5   ///  r9 = r9.wrapping_add(-5 as i32 as i64 as u64)
    stxdw [r10-0xd30], r9                   
    stxdw [r10-0xd90], r1                   
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r2, r1                                    r2 = r1
    call function_25947                     
    jne r0, 0, lbb_14742                            if r0 != (0 as i32 as i64 as u64) { pc += 149 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
lbb_14594:
    mov64 r1, r9                                    r1 = r9
    ldxdw r0, [r10-0xd48]                   
    ja lbb_13870                                    if true { pc += -727 }
lbb_14597:
    ldxdw r0, [r10-0xd48]                   
    ldxdw r1, [r10-0xd58]                   
    ldxdw r9, [r10-0xd78]                   
    ja lbb_14436                                    if true { pc += -165 }
lbb_14601:
    ldxdw r1, [r10-0x48]                    
    stxdw [r10-0xda0], r1                   
    mov64 r2, r7                                    r2 = r7
    ldxdw r7, [r10-0x50]                    
    ldxw r1, [r10-0x54]                     
    stxdw [r10-0xdb0], r1                   
    stxw [r10-0x1ac], r2                    
    ldxdw r1, [r10-0xac]                    
    stxdw [r10-0x1b4], r1                   
    stxw [r10-0x1b8], r6                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jne r1, 0, lbb_14631                            if r1 != (0 as i32 as i64 as u64) { pc += 13 }
    stxdw [r10-0xd78], r8                   
    ldxw r1, [r10-0x44]                     
    stxdw [r10-0xd58], r1                   
    ldxw r6, [r10-0x48]                     
    ldxdw r9, [r10-0x50]                    
    ldxw r8, [r10-0x54]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -3384                                 r2 += -3384   ///  r2 = r2.wrapping_add(-3384 as i32 as i64 as u64)
    call function_17454                     
    ldxw r1, [r10-0x58]                     
    jeq r1, 0, lbb_14632                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
lbb_14631:
    ja lbb_14070                                    if true { pc += -562 }
lbb_14632:
    stxw [r10-0xa8], r6                     
    stxdw [r10-0xb0], r9                    
    mov64 r1, r8                                    r1 = r8
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r2, r8                                    r2 = r8
    jne r1, 8, lbb_14639                            if r1 != (8 as i32 as i64 as u64) { pc += 1 }
    ja lbb_14156                                    if true { pc += -483 }
lbb_14639:
    stxdw [r10-0xdb8], r9                   
    stxdw [r10-0xda8], r7                   
    ldxdw r1, [r10-0x48]                    
    stxdw [r10-0xde8], r1                   
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0xde0], r1                   
    ldxw r1, [r10-0x54]                     
    stxdw [r10-0xdd8], r1                   
    ldxdw r8, [r10-0x1b8]                   
    ldxdw r1, [r10-0xac]                    
    stxdw [r10-0xdc8], r1                   
    stxdw [r10-0xdc0], r2                   
    lddw r1, 0xffffff00                             r1 load str located at 4294967040
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    ldxdw r1, [r10-0x1b0]                   
    stxdw [r10-0xd80], r1                   
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    stxdw [r10-0xd98], r1                   
    rsh64 r2, 8                                     r2 >>= 8   ///  r2 = r2.wrapping_shr(8)
    stxdw [r10-0xdd0], r2                   
    ldxdw r1, [r10-0xd40]                   
    ja lbb_14597                                    if true { pc += -65 }
lbb_14662:
    jeq r4, 11, lbb_14679                           if r4 == (11 as i32 as i64 as u64) { pc += 16 }
    jeq r4, 12, lbb_14677                           if r4 == (12 as i32 as i64 as u64) { pc += 13 }
lbb_14664:
    lddw r1, 0x100034cb0 --> b"\x00\x00\x00\x00\xb48\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295183536
    stxdw [r10-0x58], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -264                                  r1 += -264   ///  r1 = r1.wrapping_add(-264 as i32 as i64 as u64)
    stxdw [r10-0x48], r1                    
    lddw r1, 0x100010468 --> b"a#4\x00\x00\x00\x00\x00\xbf4\x00\x00\x00\x00\x00\x00W\x04\x00\x00\x10\x00…        r1 load str located at 4295033960
    stxdw [r10-0x100], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -352                                  r1 += -352   ///  r1 = r1.wrapping_add(-352 as i32 as i64 as u64)
    stxdw [r10-0x108], r1                   
    ja lbb_14748                                    if true { pc += 71 }
lbb_14677:
    mov64 r7, 12                                    r7 = 12 as i32 as i64 as u64
    ja lbb_13870                                    if true { pc += -809 }
lbb_14679:
    jlt r9, 5, lbb_14709                            if r9 < (5 as i32 as i64 as u64) { pc += 29 }
    ldxw r1, [r2+0x1]                       
    stxdw [r10-0xd90], r1                   
    add64 r2, 5                                     r2 += 5   ///  r2 = r2.wrapping_add(5 as i32 as i64 as u64)
    stxdw [r10-0xd38], r2                   
    mov64 r7, 11                                    r7 = 11 as i32 as i64 as u64
lbb_14685:
    add64 r9, -5                                    r9 += -5   ///  r9 = r9.wrapping_add(-5 as i32 as i64 as u64)
    stxdw [r10-0xd30], r9                   
    mov64 r1, r9                                    r1 = r9
    ja lbb_13870                                    if true { pc += -819 }
lbb_14689:
    ldxdw r2, [r10-0xe08]                   
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    ldxdw r1, [r10-0xe00]                   
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    ldxdw r2, [r10-0xdf8]                   
    lsh64 r2, 24                                    r2 <<= 24   ///  r2 = r2.wrapping_shl(24)
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    ldxdw r1, [r10-0xd40]                   
    stxb [r0+0x0], r1                       
    ldxh r1, [r10-0xd28]                    
    stxh [r0+0x1], r1                       
    ldxb r1, [r10-0xd26]                    
    stxb [r0+0x3], r1                       
    ldxdw r1, [r10-0xd88]                   
    stxdw [r0+0x8], r1                      
    stxw [r0+0x4], r2                       
    ldxdw r1, [r10-0xdf0]                   
    stxdw [r1+0x8], r0                      
    stw [r1+0x0], 0                         
    ja lbb_14741                                    if true { pc += 32 }
lbb_14709:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
lbb_14712:
    mov64 r8, r0                                    r8 = r0
lbb_14713:
    mov64 r1, r8                                    r1 = r8
    and64 r1, 3                                     r1 &= 3   ///  r1 = r1.and(3)
    mov64 r2, r1                                    r2 = r1
    add64 r2, -2                                    r2 += -2   ///  r2 = r2.wrapping_add(-2 as i32 as i64 as u64)
    jlt r2, 2, lbb_14734                            if r2 < (2 as i32 as i64 as u64) { pc += 16 }
    jeq r1, 0, lbb_14734                            if r1 == (0 as i32 as i64 as u64) { pc += 15 }
    ldxdw r9, [r8-0x1]                      
    ldxdw r6, [r8+0x7]                      
    ldxdw r2, [r6+0x0]                      
    mov64 r1, r9                                    r1 = r9
    callx r2                                
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    ldxdw r2, [r6+0x8]                      
    jeq r2, 0, lbb_14730                            if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r3, [r6+0x10]                     
    mov64 r1, r9                                    r1 = r9
    call function_6768                      
lbb_14730:
    mov64 r1, r8                                    r1 = r8
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
lbb_14734:
    ldxdw r1, [r10-0xdf0]                   
    stw [r1+0x8], 37                        
    stdw [r1+0x0], 1                        
    ldxdw r1, [r10-0xd88]                   
    mov64 r2, 3680                                  r2 = 3680 as i32 as i64 as u64
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
    call function_6768                      
lbb_14741:
    exit                                    
lbb_14742:
    mov64 r1, 20                                    r1 = 20 as i32 as i64 as u64
    lddw r2, 0x100033697 --> b"For portability reasons we do not allow to deserialize NaNs."        r2 load str located at 4295177879
    mov64 r3, 60                                    r3 = 60 as i32 as i64 as u64
    call function_8239                      
    ja lbb_14712                                    if true { pc += -36 }
lbb_14748:
    stdw [r10-0x38], 0                      
    stdw [r10-0x50], 1                      
    stdw [r10-0x40], 1                      
    mov64 r8, r10                                   r8 = r10
    add64 r8, -176                                  r8 += -176   ///  r8 = r8.wrapping_add(-176 as i32 as i64 as u64)
lbb_14753:
    mov64 r2, r10                                   r2 = r10
    add64 r2, -88                                   r2 += -88   ///  r2 = r2.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    call function_20113                     
    mov64 r1, r8                                    r1 = r8
    call function_8277                      
    ja lbb_14712                                    if true { pc += -48 }
lbb_14760:
    mov64 r1, 40                                    r1 = 40 as i32 as i64 as u64
    mov64 r2, 40                                    r2 = 40 as i32 as i64 as u64
    lddw r3, 0x100034c08 --> b"\x00\x00\x00\x00U7\x03\x00=\x00\x00\x00\x00\x00\x00\x00\x95\x00\x00\x00\x…        r3 load str located at 4295183368
    call function_20679                     

function_14765:
    mov64 r6, r1                                    r6 = r1
    ldxw r1, [r2+0x0]                       
    jsgt r1, 5, lbb_14780                           if (r1 as i64) > (5 as i32 as i64) { pc += 12 }
    jsgt r1, 2, lbb_14814                           if (r1 as i64) > (2 as i32 as i64) { pc += 45 }
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    jlt r1, 2, lbb_14827                            if r1 < (2 as i32 as i64 as u64) { pc += 56 }
lbb_14771:
    ldxw r1, [r3+0x8]                       
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    ldxw r2, [r2+0x4]                       
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r7, r3                                    r7 = r3
    call function_25901                     
    ja lbb_14835                                    if true { pc += 55 }
lbb_14780:
    jsgt r1, 8, lbb_14792                           if (r1 as i64) > (8 as i32 as i64) { pc += 11 }
    jeq r1, 6, lbb_14783                            if r1 == (6 as i32 as i64 as u64) { pc += 1 }
    jeq r1, 7, lbb_14816                            if r1 == (7 as i32 as i64 as u64) { pc += 33 }
lbb_14783:
    ldxw r1, [r3+0x8]                       
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    ldxw r2, [r2+0x4]                       
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r7, r3                                    r7 = r3
    call function_24261                     
    ja lbb_14824                                    if true { pc += 32 }
lbb_14792:
    mov64 r4, r1                                    r4 = r1
    add64 r4, -10                                   r4 += -10   ///  r4 = r4.wrapping_add(-10 as i32 as i64 as u64)
    jlt r4, 2, lbb_14839                            if r4 < (2 as i32 as i64 as u64) { pc += 44 }
    jeq r1, 9, lbb_14863                            if r1 == (9 as i32 as i64 as u64) { pc += 67 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r2, 280001                                r2 = 280001 as i32 as i64 as u64
lbb_14799:
    mov64 r4, r1                                    r4 = r1
    mov64 r1, r5                                    r1 = r5
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r0, r2                                    r0 = r2
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r5, r4                                    r5 = r4
    jne r0, 0, lbb_14799                            if r0 != (0 as i32 as i64 as u64) { pc += -8 }
    stxw [r10-0x4], r4                      
    mov64 r2, r10                                   r2 = r10
    add64 r2, -4                                    r2 += -4   ///  r2 = r2.wrapping_add(-4 as i32 as i64 as u64)
    stxw [r10-0x4], r1                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -4                                    r1 += -4   ///  r1 = r1.wrapping_add(-4 as i32 as i64 as u64)
    ja lbb_14863                                    if true { pc += 49 }
lbb_14814:
    jeq r1, 3, lbb_14771                            if r1 == (3 as i32 as i64 as u64) { pc += -44 }
    jeq r1, 4, lbb_14862                            if r1 == (4 as i32 as i64 as u64) { pc += 46 }
lbb_14816:
    ldxw r1, [r3+0x8]                       
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    ldxw r2, [r2+0x4]                       
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r7, r3                                    r7 = r3
    call function_25901                     
lbb_14824:
    stxw [r7+0x8], r0                       
    stb [r7+0x1], 0                         
    ja lbb_14836                                    if true { pc += 9 }
lbb_14827:
    ldxw r1, [r3+0x8]                       
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    ldxw r2, [r2+0x4]                       
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r7, r3                                    r7 = r3
    call function_24261                     
lbb_14835:
    stxw [r7+0x8], r0                       
lbb_14836:
    stxw [r6+0x8], r0                       
    ldxdw r1, [r7+0x0]                      
    ja lbb_14866                                    if true { pc += 27 }
lbb_14839:
    ldxw r1, [r2+0x4]                       
    div64 r1, 5                                     r1 /= 5   ///  r1 = r1 / (5 as u64)
    mov64 r2, r1                                    r2 = r1
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
lbb_14847:
    mov64 r4, r2                                    r4 = r2
    mov64 r2, r5                                    r2 = r5
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r0, r1                                    r0 = r1
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r5, r4                                    r5 = r4
    jne r0, 0, lbb_14847                            if r0 != (0 as i32 as i64 as u64) { pc += -8 }
    stxw [r10-0x4], r4                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -4                                    r1 += -4   ///  r1 = r1.wrapping_add(-4 as i32 as i64 as u64)
    stxw [r10-0x4], r2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -4                                    r1 += -4   ///  r1 = r1.wrapping_add(-4 as i32 as i64 as u64)
    ja lbb_14863                                    if true { pc += 1 }
lbb_14862:
    stb [r3+0x1], 0                         
lbb_14863:
    ldxw r1, [r3+0x8]                       
    stxw [r6+0x8], r1                       
    ldxdw r1, [r3+0x0]                      
lbb_14866:
    stxdw [r6+0x0], r1                      
    exit                                    

function_14868:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxb r1, [r6+0x0]                       
    jsgt r1, 6, lbb_14886                           if (r1 as i64) > (6 as i32 as i64) { pc += 13 }
    jsgt r1, 2, lbb_14895                           if (r1 as i64) > (2 as i32 as i64) { pc += 21 }
    jeq r1, 0, lbb_15133                            if r1 == (0 as i32 as i64 as u64) { pc += 258 }
    jeq r1, 1, lbb_14968                            if r1 == (1 as i32 as i64 as u64) { pc += 92 }
    ldxdw r8, [r7+0x0]                      
    ldxb r1, [r6+0x4]                       
    jne r1, 0, lbb_14880                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15055                                    if true { pc += 175 }
lbb_14880:
    mov64 r1, r6                                    r1 = r6
    add64 r1, 5                                     r1 += 5   ///  r1 = r1.wrapping_add(5 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    call function_15449                     
    mov64 r7, r0                                    r7 = r0
    ja lbb_15056                                    if true { pc += 170 }
lbb_14886:
    jsgt r1, 9, lbb_14912                           if (r1 as i64) > (9 as i32 as i64) { pc += 25 }
    jeq r1, 7, lbb_14960                            if r1 == (7 as i32 as i64 as u64) { pc += 72 }
    jeq r1, 8, lbb_14970                            if r1 == (8 as i32 as i64 as u64) { pc += 81 }
    add64 r6, 4                                     r6 += 4   ///  r6 = r6.wrapping_add(4 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    xor64 r0, 1                                     r0 ^= 1   ///  r0 = r0.xor(1)
    ja lbb_15133                                    if true { pc += 238 }
lbb_14895:
    jsgt r1, 4, lbb_14928                           if (r1 as i64) > (4 as i32 as i64) { pc += 32 }
    jeq r1, 3, lbb_14979                            if r1 == (3 as i32 as i64 as u64) { pc += 82 }
    ldxdw r1, [r6+0x1]                      
    ldxdw r2, [r7+0x0]                      
    ldxdw r3, [r2+0x410]                    
    add64 r2, 16                                    r2 += 16   ///  r2 = r2.wrapping_add(16 as i32 as i64 as u64)
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    ja lbb_14908                                    if true { pc += 5 }
lbb_14903:
    jeq r3, 0, lbb_15133                            if r3 == (0 as i32 as i64 as u64) { pc += 229 }
    add64 r3, -8                                    r3 += -8   ///  r3 = r3.wrapping_add(-8 as i32 as i64 as u64)
    ldxdw r4, [r2+0x0]                      
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jeq r4, r1, lbb_15133                           if r4 == r1 { pc += 225 }
lbb_14908:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r3, 0, lbb_14903                            if r3 == (0 as i32 as i64 as u64) { pc += -7 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_14903                                    if true { pc += -9 }
lbb_14912:
    jsgt r1, 11, lbb_14936                          if (r1 as i64) > (11 as i32 as i64) { pc += 23 }
    jeq r1, 10, lbb_14994                           if r1 == (10 as i32 as i64 as u64) { pc += 80 }
    mov64 r1, r6                                    r1 = r6
    add64 r1, 4                                     r1 += 4   ///  r1 = r1.wrapping_add(4 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    jne r0, 0, lbb_14920                            if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15027                                    if true { pc += 107 }
lbb_14920:
    mov64 r1, r6                                    r1 = r6
    add64 r1, 44                                    r1 += 44   ///  r1 = r1.wrapping_add(44 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_15133                            if r1 != (0 as i32 as i64 as u64) { pc += 206 }
    ja lbb_14959                                    if true { pc += 31 }
lbb_14928:
    jeq r1, 5, lbb_15020                            if r1 == (5 as i32 as i64 as u64) { pc += 91 }
    ldxdw r1, [r7+0x0]                      
    ldxb r2, [r6+0x1]                       
    jeq r2, 0, lbb_15085                            if r2 == (0 as i32 as i64 as u64) { pc += 153 }
    ldxw r1, [r1+0x8]                       
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_15133                            if r1 == (0 as i32 as i64 as u64) { pc += 198 }
    ja lbb_15120                                    if true { pc += 184 }
lbb_14936:
    jeq r1, 12, lbb_15126                           if r1 == (12 as i32 as i64 as u64) { pc += 189 }
    mov64 r1, r6                                    r1 = r6
    add64 r1, 4                                     r1 += 4   ///  r1 = r1.wrapping_add(4 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_14945                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15133                                    if true { pc += 188 }
lbb_14945:
    mov64 r1, r6                                    r1 = r6
    add64 r1, 24                                    r1 += 24   ///  r1 = r1.wrapping_add(24 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_15133                            if r1 != (0 as i32 as i64 as u64) { pc += 181 }
    mov64 r1, r6                                    r1 = r6
    add64 r1, 44                                    r1 += 44   ///  r1 = r1.wrapping_add(44 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_15133                            if r1 != (0 as i32 as i64 as u64) { pc += 174 }
lbb_14959:
    ja lbb_15149                                    if true { pc += 189 }
lbb_14960:
    mov64 r1, r6                                    r1 = r6
    add64 r1, 4                                     r1 += 4   ///  r1 = r1.wrapping_add(4 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jne r1, 0, lbb_14977                            if r1 != (0 as i32 as i64 as u64) { pc += 10 }
    ja lbb_15133                                    if true { pc += 165 }
lbb_14968:
    add64 r6, 4                                     r6 += 4   ///  r6 = r6.wrapping_add(4 as i32 as i64 as u64)
    ja lbb_15122                                    if true { pc += 152 }
lbb_14970:
    mov64 r1, r6                                    r1 = r6
    add64 r1, 4                                     r1 += 4   ///  r1 = r1.wrapping_add(4 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_15133                            if r1 != (0 as i32 as i64 as u64) { pc += 156 }
lbb_14977:
    add64 r6, 24                                    r6 += 24   ///  r6 = r6.wrapping_add(24 as i32 as i64 as u64)
    ja lbb_15122                                    if true { pc += 143 }
lbb_14979:
    ldxdw r1, [r6+0x1]                      
    ldxdw r2, [r7+0x0]                      
    ldxdw r3, [r2+0x410]                    
    add64 r2, 16                                    r2 += 16   ///  r2 = r2.wrapping_add(16 as i32 as i64 as u64)
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    ja lbb_14990                                    if true { pc += 5 }
lbb_14985:
    jeq r3, 0, lbb_15133                            if r3 == (0 as i32 as i64 as u64) { pc += 147 }
    add64 r3, -8                                    r3 += -8   ///  r3 = r3.wrapping_add(-8 as i32 as i64 as u64)
    ldxdw r4, [r2+0x0]                      
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jeq r4, r1, lbb_15133                           if r4 == r1 { pc += 143 }
lbb_14990:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_14985                            if r3 != (0 as i32 as i64 as u64) { pc += -7 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_14985                                    if true { pc += -9 }
lbb_14994:
    mov64 r1, r6                                    r1 = r6
    add64 r1, 4                                     r1 += 4   ///  r1 = r1.wrapping_add(4 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    jne r0, 0, lbb_15000                            if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15047                                    if true { pc += 47 }
lbb_15000:
    mov64 r1, r6                                    r1 = r6
    add64 r1, 24                                    r1 += 24   ///  r1 = r1.wrapping_add(24 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    mov64 r8, r0                                    r8 = r0
    mov64 r1, r6                                    r1 = r6
    add64 r1, 44                                    r1 += 44   ///  r1 = r1.wrapping_add(44 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jne r1, 0, lbb_15013                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15017                                    if true { pc += 4 }
lbb_15013:
    add64 r6, 64                                    r6 += 64   ///  r6 = r6.wrapping_add(64 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
lbb_15017:
    jeq r8, 0, lbb_15133                            if r8 == (0 as i32 as i64 as u64) { pc += 115 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ja lbb_15133                                    if true { pc += 113 }
lbb_15020:
    ldxdw r1, [r7+0x0]                      
    ldxb r2, [r6+0x1]                       
    jeq r2, 0, lbb_15089                            if r2 == (0 as i32 as i64 as u64) { pc += 66 }
    ldxw r1, [r1+0x8]                       
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_15133                            if r1 != (0 as i32 as i64 as u64) { pc += 107 }
    ja lbb_15120                                    if true { pc += 93 }
lbb_15027:
    mov64 r1, r6                                    r1 = r6
    add64 r1, 24                                    r1 += 24   ///  r1 = r1.wrapping_add(24 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    mov64 r8, r0                                    r8 = r0
    mov64 r1, r6                                    r1 = r6
    add64 r1, 44                                    r1 += 44   ///  r1 = r1.wrapping_add(44 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_15043                            if r0 != (0 as i32 as i64 as u64) { pc += 5 }
    add64 r6, 64                                    r6 += 64   ///  r6 = r6.wrapping_add(64 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    mov64 r1, r0                                    r1 = r0
lbb_15043:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_15133                            if r8 == (0 as i32 as i64 as u64) { pc += 88 }
    mov64 r0, r1                                    r0 = r1
    ja lbb_15133                                    if true { pc += 86 }
lbb_15047:
    mov64 r1, r6                                    r1 = r6
    add64 r1, 44                                    r1 += 44   ///  r1 = r1.wrapping_add(44 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jne r1, 0, lbb_14959                            if r1 != (0 as i32 as i64 as u64) { pc += -95 }
    ja lbb_15133                                    if true { pc += 78 }
lbb_15055:
    ldxw r7, [r6+0x8]                       
lbb_15056:
    ldxb r1, [r6+0xc]                       
    jne r1, 0, lbb_15060                            if r1 != (0 as i32 as i64 as u64) { pc += 2 }
    ldxw r1, [r6+0x10]                      
    ja lbb_15065                                    if true { pc += 5 }
lbb_15060:
    mov64 r1, r6                                    r1 = r6
    add64 r1, 13                                    r1 += 13   ///  r1 = r1.wrapping_add(13 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    call function_15449                     
    mov64 r1, r0                                    r1 = r0
lbb_15065:
    ldxb r2, [r6+0x1]                       
    jsgt r2, 2, lbb_15076                           if (r2 as i64) > (2 as i32 as i64) { pc += 9 }
    jeq r2, 0, lbb_15114                            if r2 == (0 as i32 as i64 as u64) { pc += 46 }
    jeq r2, 1, lbb_15100                            if r2 == (1 as i32 as i64 as u64) { pc += 31 }
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jgt r7, r1, lbb_15133                           if r7 > r1 { pc += 58 }
    ja lbb_15106                                    if true { pc += 30 }
lbb_15076:
    jeq r2, 3, lbb_15093                            if r2 == (3 as i32 as i64 as u64) { pc += 16 }
    jeq r2, 4, lbb_15107                            if r2 == (4 as i32 as i64 as u64) { pc += 29 }
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jle r7, r1, lbb_15133                           if r7 <= r1 { pc += 49 }
    ja lbb_15113                                    if true { pc += 28 }
lbb_15085:
    ldxw r1, [r1+0x0]                       
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_15133                            if r1 == (0 as i32 as i64 as u64) { pc += 45 }
    ja lbb_15120                                    if true { pc += 31 }
lbb_15089:
    ldxw r1, [r1+0x0]                       
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_15133                            if r1 != (0 as i32 as i64 as u64) { pc += 41 }
    ja lbb_15120                                    if true { pc += 27 }
lbb_15093:
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jge r7, r1, lbb_15133                           if r7 >= r1 { pc += 34 }
    ja lbb_15120                                    if true { pc += 20 }
lbb_15100:
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r7, r1, lbb_15133                           if r7 != r1 { pc += 27 }
lbb_15106:
    ja lbb_15120                                    if true { pc += 13 }
lbb_15107:
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r7, r1, lbb_15133                           if r7 < r1 { pc += 20 }
lbb_15113:
    ja lbb_15120                                    if true { pc += 6 }
lbb_15114:
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r7, r1, lbb_15133                           if r7 == r1 { pc += 13 }
lbb_15120:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_15133                                    if true { pc += 11 }
lbb_15122:
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    ja lbb_15133                                    if true { pc += 7 }
lbb_15126:
    mov64 r1, r6                                    r1 = r6
    add64 r1, 4                                     r1 += 4   ///  r1 = r1.wrapping_add(4 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jne r1, 0, lbb_15135                            if r1 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_15133:
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    exit                                    
lbb_15135:
    mov64 r1, r6                                    r1 = r6
    add64 r1, 24                                    r1 += 24   ///  r1 = r1.wrapping_add(24 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_15133                            if r1 == (0 as i32 as i64 as u64) { pc += -9 }
    mov64 r1, r6                                    r1 = r6
    add64 r1, 44                                    r1 += 44   ///  r1 = r1.wrapping_add(44 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    call function_15151                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_15133                            if r1 == (0 as i32 as i64 as u64) { pc += -16 }
lbb_15149:
    add64 r6, 64                                    r6 += 64   ///  r6 = r6.wrapping_add(64 as i32 as i64 as u64)
    ja lbb_15122                                    if true { pc += -29 }

function_15151:
    ldxb r3, [r1+0x0]                       
    jsgt r3, 3, lbb_15170                           if (r3 as i64) > (3 as i32 as i64) { pc += 17 }
    jsgt r3, 1, lbb_15181                           if (r3 as i64) > (1 as i32 as i64) { pc += 27 }
    jeq r3, 0, lbb_15237                            if r3 == (0 as i32 as i64 as u64) { pc += 82 }
    ldxdw r1, [r1+0x1]                      
    ldxdw r2, [r2+0x0]                      
    ldxdw r3, [r2+0x410]                    
    add64 r2, 16                                    r2 += 16   ///  r2 = r2.wrapping_add(16 as i32 as i64 as u64)
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    ja lbb_15166                                    if true { pc += 5 }
lbb_15161:
    jeq r3, 0, lbb_15437                            if r3 == (0 as i32 as i64 as u64) { pc += 275 }
    add64 r3, -8                                    r3 += -8   ///  r3 = r3.wrapping_add(-8 as i32 as i64 as u64)
    ldxdw r4, [r2+0x0]                      
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jeq r4, r1, lbb_15437                           if r4 == r1 { pc += 271 }
lbb_15166:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_15161                            if r3 != (0 as i32 as i64 as u64) { pc += -7 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ja lbb_15161                                    if true { pc += -9 }
lbb_15170:
    jsgt r3, 5, lbb_15189                           if (r3 as i64) > (5 as i32 as i64) { pc += 18 }
    jeq r3, 4, lbb_15242                            if r3 == (4 as i32 as i64 as u64) { pc += 70 }
    ldxdw r2, [r2+0x0]                      
    ldxb r3, [r1+0x1]                       
    jsgt r3, 6, lbb_15176                           if (r3 as i64) > (6 as i32 as i64) { pc += 1 }
    ja lbb_15317                                    if true { pc += 141 }
lbb_15176:
    jsgt r3, 10, lbb_15339                          if (r3 as i64) > (10 as i32 as i64) { pc += 162 }
    jsgt r3, 8, lbb_15382                           if (r3 as i64) > (8 as i32 as i64) { pc += 204 }
    jeq r3, 7, lbb_15406                            if r3 == (7 as i32 as i64 as u64) { pc += 227 }
    mov64 r1, 1225                                  r1 = 1225 as i32 as i64 as u64
    ja lbb_15419                                    if true { pc += 238 }
lbb_15181:
    jeq r3, 2, lbb_15249                            if r3 == (2 as i32 as i64 as u64) { pc += 67 }
    ldxdw r2, [r2+0x0]                      
    ldxb r1, [r1+0x1]                       
    jeq r1, 0, lbb_15323                            if r1 == (0 as i32 as i64 as u64) { pc += 138 }
    ldxw r1, [r2+0x8]                       
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_15437                            if r1 != (0 as i32 as i64 as u64) { pc += 249 }
    ja lbb_15436                                    if true { pc += 247 }
lbb_15189:
    jeq r3, 6, lbb_15264                            if r3 == (6 as i32 as i64 as u64) { pc += 74 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxdw r9, [r2+0x8]                      
    jeq r9, 0, lbb_15437                            if r9 == (0 as i32 as i64 as u64) { pc += 244 }
    ldxw r1, [r9+0x0]                       
    jeq r1, 0, lbb_15437                            if r1 == (0 as i32 as i64 as u64) { pc += 242 }
    ldxdw r2, [r2+0x0]                      
    add64 r2, 1064                                  r2 += 1064   ///  r2 = r2.wrapping_add(1064 as i32 as i64 as u64)
    stxdw [r10-0x10], r2                    
    add64 r9, 24                                    r9 += 24   ///  r9 = r9.wrapping_add(24 as i32 as i64 as u64)
    stxdw [r10-0x8], r9                     
    ja lbb_15204                                    if true { pc += 3 }
lbb_15201:
    ldxw r1, [r7+0x0]                       
    ldxdw r9, [r10-0x8]                     
    jeq r1, 0, lbb_15437                            if r1 == (0 as i32 as i64 as u64) { pc += 233 }
lbb_15204:
    mov64 r7, r1                                    r7 = r1
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    jgt r1, 40, lbb_15440                           if r1 > (40 as i32 as i64 as u64) { pc += 231 }
    mov64 r1, r7                                    r1 = r7
    lsh64 r1, 4                                     r1 <<= 4   ///  r1 = r1.wrapping_shl(4)
    mov64 r8, r9                                    r8 = r9
    add64 r8, r1                                    r8 += r1   ///  r8 = r8.wrapping_add(r1)
    ldxw r1, [r8+0x8]                       
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    lddw r2, 0xffffffff                             r2 load str located at 4294967295
    jeq r1, r2, lbb_15347                           if r1 == r2 { pc += 129 }
    jlt r1, 16, lbb_15220                           if r1 < (16 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15445                                    if true { pc += 225 }
lbb_15220:
    lsh64 r1, 3                                     r1 <<= 3   ///  r1 = r1.wrapping_shl(3)
    ldxdw r2, [r10-0x10]                    
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxdw r1, [r2+0x0]                      
    call function_25752                     
    lsh64 r7, 4                                     r7 <<= 4   ///  r7 = r7.wrapping_shl(4)
    add64 r9, r7                                    r9 += r7   ///  r9 = r9.wrapping_add(r7)
    ldxw r1, [r8+0x4]                       
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r7, r9                                    r7 = r9
    add64 r7, 4                                     r7 += 4   ///  r7 = r7.wrapping_add(4 as i32 as i64 as u64)
    mov64 r2, r0                                    r2 = r0
    call function_23430                     
    jslt r0, 0, lbb_15201                           if (r0 as i64) < (0 as i32 as i64) { pc += -34 }
    mov64 r7, r9                                    r7 = r9
    ja lbb_15201                                    if true { pc += -36 }
lbb_15237:
    ldxdw r6, [r2+0x0]                      
    ldxb r2, [r1+0x4]                       
    jne r2, 0, lbb_15273                            if r2 != (0 as i32 as i64 as u64) { pc += 33 }
    ldxw r7, [r1+0x8]                       
    ja lbb_15281                                    if true { pc += 39 }
lbb_15242:
    ldxdw r2, [r2+0x0]                      
    ldxb r1, [r1+0x1]                       
    jeq r1, 0, lbb_15327                            if r1 == (0 as i32 as i64 as u64) { pc += 82 }
    ldxw r1, [r2+0x8]                       
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_15437                            if r1 == (0 as i32 as i64 as u64) { pc += 189 }
    ja lbb_15436                                    if true { pc += 187 }
lbb_15249:
    ldxdw r1, [r1+0x1]                      
    ldxdw r2, [r2+0x0]                      
    ldxdw r3, [r2+0x410]                    
    add64 r2, 16                                    r2 += 16   ///  r2 = r2.wrapping_add(16 as i32 as i64 as u64)
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    ja lbb_15260                                    if true { pc += 5 }
lbb_15255:
    jeq r3, 0, lbb_15437                            if r3 == (0 as i32 as i64 as u64) { pc += 181 }
    add64 r3, -8                                    r3 += -8   ///  r3 = r3.wrapping_add(-8 as i32 as i64 as u64)
    ldxdw r4, [r2+0x0]                      
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jeq r4, r1, lbb_15437                           if r4 == r1 { pc += 177 }
lbb_15260:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jeq r3, 0, lbb_15255                            if r3 == (0 as i32 as i64 as u64) { pc += -7 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ja lbb_15255                                    if true { pc += -9 }
lbb_15264:
    ldxdw r2, [r2+0x0]                      
    ldxb r3, [r1+0x1]                       
    jsgt r3, 6, lbb_15303                           if (r3 as i64) > (6 as i32 as i64) { pc += 36 }
    jsgt r3, 2, lbb_15331                           if (r3 as i64) > (2 as i32 as i64) { pc += 63 }
    mov64 r1, 1212                                  r1 = 1212 as i32 as i64 as u64
    jeq r3, 0, lbb_15425                            if r3 == (0 as i32 as i64 as u64) { pc += 155 }
    jeq r3, 1, lbb_15394                            if r3 == (1 as i32 as i64 as u64) { pc += 123 }
    mov64 r1, 1215                                  r1 = 1215 as i32 as i64 as u64
    ja lbb_15425                                    if true { pc += 152 }
lbb_15273:
    mov64 r2, r1                                    r2 = r1
    add64 r2, 5                                     r2 += 5   ///  r2 = r2.wrapping_add(5 as i32 as i64 as u64)
    mov64 r7, r1                                    r7 = r1
    mov64 r1, r2                                    r1 = r2
    mov64 r2, r6                                    r2 = r6
    call function_15449                     
    mov64 r1, r7                                    r1 = r7
    mov64 r7, r0                                    r7 = r0
lbb_15281:
    ldxb r2, [r1+0xc]                       
    jne r2, 0, lbb_15285                            if r2 != (0 as i32 as i64 as u64) { pc += 2 }
    ldxw r0, [r1+0x10]                      
    ja lbb_15292                                    if true { pc += 7 }
lbb_15285:
    mov64 r2, r1                                    r2 = r1
    add64 r2, 13                                    r2 += 13   ///  r2 = r2.wrapping_add(13 as i32 as i64 as u64)
    mov64 r8, r1                                    r8 = r1
    mov64 r1, r2                                    r1 = r2
    mov64 r2, r6                                    r2 = r6
    call function_15449                     
    mov64 r1, r8                                    r1 = r8
lbb_15292:
    ldxb r1, [r1+0x1]                       
    jsgt r1, 2, lbb_15308                           if (r1 as i64) > (2 as i32 as i64) { pc += 14 }
    jeq r1, 0, lbb_15355                            if r1 == (0 as i32 as i64 as u64) { pc += 60 }
    jeq r1, 1, lbb_15369                            if r1 == (1 as i32 as i64 as u64) { pc += 73 }
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jgt r7, r0, lbb_15437                           if r7 > r0 { pc += 135 }
    ja lbb_15436                                    if true { pc += 133 }
lbb_15303:
    jsgt r3, 10, lbb_15343                          if (r3 as i64) > (10 as i32 as i64) { pc += 39 }
    jsgt r3, 8, lbb_15385                           if (r3 as i64) > (8 as i32 as i64) { pc += 80 }
    jeq r3, 7, lbb_15408                            if r3 == (7 as i32 as i64 as u64) { pc += 102 }
    mov64 r1, 1225                                  r1 = 1225 as i32 as i64 as u64
    ja lbb_15425                                    if true { pc += 117 }
lbb_15308:
    jeq r1, 3, lbb_15362                            if r1 == (3 as i32 as i64 as u64) { pc += 53 }
    jeq r1, 4, lbb_15430                            if r1 == (4 as i32 as i64 as u64) { pc += 120 }
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jle r7, r0, lbb_15437                           if r7 <= r0 { pc += 121 }
    ja lbb_15436                                    if true { pc += 119 }
lbb_15317:
    jsgt r3, 2, lbb_15335                           if (r3 as i64) > (2 as i32 as i64) { pc += 17 }
    mov64 r1, 1212                                  r1 = 1212 as i32 as i64 as u64
    jeq r3, 0, lbb_15419                            if r3 == (0 as i32 as i64 as u64) { pc += 99 }
    jeq r3, 1, lbb_15396                            if r3 == (1 as i32 as i64 as u64) { pc += 75 }
    mov64 r1, 1215                                  r1 = 1215 as i32 as i64 as u64
    ja lbb_15419                                    if true { pc += 96 }
lbb_15323:
    ldxw r1, [r2+0x0]                       
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_15437                            if r1 != (0 as i32 as i64 as u64) { pc += 111 }
    ja lbb_15436                                    if true { pc += 109 }
lbb_15327:
    ldxw r1, [r2+0x0]                       
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_15437                            if r1 == (0 as i32 as i64 as u64) { pc += 107 }
    ja lbb_15436                                    if true { pc += 105 }
lbb_15331:
    jsgt r3, 4, lbb_15376                           if (r3 as i64) > (4 as i32 as i64) { pc += 44 }
    jeq r3, 3, lbb_15398                            if r3 == (3 as i32 as i64 as u64) { pc += 65 }
    mov64 r1, 1220                                  r1 = 1220 as i32 as i64 as u64
    ja lbb_15425                                    if true { pc += 90 }
lbb_15335:
    jsgt r3, 4, lbb_15379                           if (r3 as i64) > (4 as i32 as i64) { pc += 43 }
    jeq r3, 3, lbb_15400                            if r3 == (3 as i32 as i64 as u64) { pc += 63 }
    mov64 r1, 1220                                  r1 = 1220 as i32 as i64 as u64
    ja lbb_15419                                    if true { pc += 80 }
lbb_15339:
    jsgt r3, 12, lbb_15388                          if (r3 as i64) > (12 as i32 as i64) { pc += 48 }
    jeq r3, 11, lbb_15410                           if r3 == (11 as i32 as i64 as u64) { pc += 69 }
    mov64 r1, 1229                                  r1 = 1229 as i32 as i64 as u64
    ja lbb_15419                                    if true { pc += 76 }
lbb_15343:
    jsgt r3, 12, lbb_15391                          if (r3 as i64) > (12 as i32 as i64) { pc += 47 }
    jeq r3, 11, lbb_15412                           if r3 == (11 as i32 as i64 as u64) { pc += 67 }
    mov64 r1, 1229                                  r1 = 1229 as i32 as i64 as u64
    ja lbb_15425                                    if true { pc += 78 }
lbb_15347:
    ldxw r1, [r8+0x4]                       
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_23922                     
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jeq r0, 0, lbb_15436                            if r0 == (0 as i32 as i64 as u64) { pc += 82 }
    ja lbb_15437                                    if true { pc += 82 }
lbb_15355:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jeq r7, r0, lbb_15437                           if r7 == r0 { pc += 76 }
    ja lbb_15436                                    if true { pc += 74 }
lbb_15362:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jge r7, r0, lbb_15437                           if r7 >= r0 { pc += 69 }
    ja lbb_15436                                    if true { pc += 67 }
lbb_15369:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jne r7, r0, lbb_15437                           if r7 != r0 { pc += 62 }
    ja lbb_15436                                    if true { pc += 60 }
lbb_15376:
    jeq r3, 5, lbb_15402                            if r3 == (5 as i32 as i64 as u64) { pc += 25 }
    mov64 r1, 1222                                  r1 = 1222 as i32 as i64 as u64
    ja lbb_15425                                    if true { pc += 46 }
lbb_15379:
    jeq r3, 5, lbb_15404                            if r3 == (5 as i32 as i64 as u64) { pc += 24 }
    mov64 r1, 1222                                  r1 = 1222 as i32 as i64 as u64
    ja lbb_15419                                    if true { pc += 37 }
lbb_15382:
    jeq r3, 9, lbb_15414                            if r3 == (9 as i32 as i64 as u64) { pc += 31 }
    mov64 r1, 1226                                  r1 = 1226 as i32 as i64 as u64
    ja lbb_15419                                    if true { pc += 34 }
lbb_15385:
    jeq r3, 9, lbb_15416                            if r3 == (9 as i32 as i64 as u64) { pc += 30 }
    mov64 r1, 1226                                  r1 = 1226 as i32 as i64 as u64
    ja lbb_15425                                    if true { pc += 37 }
lbb_15388:
    jeq r3, 13, lbb_15418                           if r3 == (13 as i32 as i64 as u64) { pc += 29 }
    mov64 r1, 1227                                  r1 = 1227 as i32 as i64 as u64
    ja lbb_15419                                    if true { pc += 28 }
lbb_15391:
    jeq r3, 13, lbb_15424                           if r3 == (13 as i32 as i64 as u64) { pc += 32 }
    mov64 r1, 1227                                  r1 = 1227 as i32 as i64 as u64
    ja lbb_15425                                    if true { pc += 31 }
lbb_15394:
    mov64 r1, 1214                                  r1 = 1214 as i32 as i64 as u64
    ja lbb_15425                                    if true { pc += 29 }
lbb_15396:
    mov64 r1, 1214                                  r1 = 1214 as i32 as i64 as u64
    ja lbb_15419                                    if true { pc += 21 }
lbb_15398:
    mov64 r1, 1213                                  r1 = 1213 as i32 as i64 as u64
    ja lbb_15425                                    if true { pc += 25 }
lbb_15400:
    mov64 r1, 1213                                  r1 = 1213 as i32 as i64 as u64
    ja lbb_15419                                    if true { pc += 17 }
lbb_15402:
    mov64 r1, 1221                                  r1 = 1221 as i32 as i64 as u64
    ja lbb_15425                                    if true { pc += 21 }
lbb_15404:
    mov64 r1, 1221                                  r1 = 1221 as i32 as i64 as u64
    ja lbb_15419                                    if true { pc += 13 }
lbb_15406:
    mov64 r1, 1223                                  r1 = 1223 as i32 as i64 as u64
    ja lbb_15419                                    if true { pc += 11 }
lbb_15408:
    mov64 r1, 1223                                  r1 = 1223 as i32 as i64 as u64
    ja lbb_15425                                    if true { pc += 15 }
lbb_15410:
    mov64 r1, 1228                                  r1 = 1228 as i32 as i64 as u64
    ja lbb_15419                                    if true { pc += 7 }
lbb_15412:
    mov64 r1, 1228                                  r1 = 1228 as i32 as i64 as u64
    ja lbb_15425                                    if true { pc += 11 }
lbb_15414:
    mov64 r1, 1224                                  r1 = 1224 as i32 as i64 as u64
    ja lbb_15419                                    if true { pc += 3 }
lbb_15416:
    mov64 r1, 1224                                  r1 = 1224 as i32 as i64 as u64
    ja lbb_15425                                    if true { pc += 7 }
lbb_15418:
    mov64 r1, 1230                                  r1 = 1230 as i32 as i64 as u64
lbb_15419:
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxb r1, [r2+0x0]                       
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_15437                            if r1 != (0 as i32 as i64 as u64) { pc += 14 }
    ja lbb_15436                                    if true { pc += 12 }
lbb_15424:
    mov64 r1, 1230                                  r1 = 1230 as i32 as i64 as u64
lbb_15425:
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxb r1, [r2+0x0]                       
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_15437                            if r1 == (0 as i32 as i64 as u64) { pc += 8 }
    ja lbb_15436                                    if true { pc += 6 }
lbb_15430:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jlt r7, r0, lbb_15437                           if r7 < r0 { pc += 1 }
lbb_15436:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_15437:
    and64 r6, 1                                     r6 &= 1   ///  r6 = r6.and(1)
    mov64 r0, r6                                    r0 = r6
    exit                                    
lbb_15440:
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 40                                    r2 = 40 as i32 as i64 as u64
    lddw r3, 0x100034bd8 --> b"\x00\x00\x00\x00@7\x03\x00\x15\x00\x00\x00\x00\x00\x00\x00\xff\x00\x00\x0…        r3 load str located at 4295183320
    call function_20679                     
lbb_15445:
    mov64 r2, 16                                    r2 = 16 as i32 as i64 as u64
    lddw r3, 0x100034c20 --> b"\x00\x00\x00\x00\xb07\x03\x003\x00\x00\x00\x00\x00\x00\x00\xdd\x00\x00\x0…        r3 load str located at 4295183392
    call function_20679                     

function_15449:
    ldxb r1, [r1+0x0]                       
    jsgt r1, 8, lbb_15459                           if (r1 as i64) > (8 as i32 as i64) { pc += 8 }
    jsgt r1, 3, lbb_15464                           if (r1 as i64) > (3 as i32 as i64) { pc += 12 }
    jsgt r1, 1, lbb_15480                           if (r1 as i64) > (1 as i32 as i64) { pc += 27 }
    jeq r1, 0, lbb_15493                            if r1 == (0 as i32 as i64 as u64) { pc += 39 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxw r1, [r2+0x8]                       
    jeq r1, 0, lbb_15519                            if r1 == (0 as i32 as i64 as u64) { pc += 62 }
    ldxw r0, [r2+0xc]                       
    ja lbb_15519                                    if true { pc += 60 }
lbb_15459:
    jsgt r1, 12, lbb_15468                          if (r1 as i64) > (12 as i32 as i64) { pc += 8 }
    jsgt r1, 10, lbb_15483                          if (r1 as i64) > (10 as i32 as i64) { pc += 22 }
    jeq r1, 9, lbb_15498                            if r1 == (9 as i32 as i64 as u64) { pc += 36 }
    ldxb r0, [r2+0x4d5]                     
    ja lbb_15519                                    if true { pc += 55 }
lbb_15464:
    jsgt r1, 5, lbb_15472                           if (r1 as i64) > (5 as i32 as i64) { pc += 7 }
    jeq r1, 4, lbb_15516                            if r1 == (4 as i32 as i64 as u64) { pc += 50 }
    ldxb r0, [r2+0x4d0]                     
    ja lbb_15519                                    if true { pc += 51 }
lbb_15468:
    jsgt r1, 14, lbb_15476                          if (r1 as i64) > (14 as i32 as i64) { pc += 7 }
    jeq r1, 13, lbb_15518                           if r1 == (13 as i32 as i64 as u64) { pc += 48 }
    ldxb r0, [r2+0x4d6]                     
    ja lbb_15519                                    if true { pc += 47 }
lbb_15472:
    jeq r1, 6, lbb_15489                            if r1 == (6 as i32 as i64 as u64) { pc += 16 }
    jeq r1, 7, lbb_15512                            if r1 == (7 as i32 as i64 as u64) { pc += 38 }
    ldxb r0, [r2+0x4d3]                     
    ja lbb_15519                                    if true { pc += 43 }
lbb_15476:
    jeq r1, 15, lbb_15491                           if r1 == (15 as i32 as i64 as u64) { pc += 14 }
    jeq r1, 16, lbb_15514                           if r1 == (16 as i32 as i64 as u64) { pc += 36 }
    ldxb r0, [r2+0x4d7]                     
    ja lbb_15519                                    if true { pc += 39 }
lbb_15480:
    jeq r1, 2, lbb_15505                            if r1 == (2 as i32 as i64 as u64) { pc += 24 }
    ldxb r0, [r2+0x4cf]                     
    ja lbb_15519                                    if true { pc += 36 }
lbb_15483:
    jeq r1, 11, lbb_15500                           if r1 == (11 as i32 as i64 as u64) { pc += 16 }
    ldxdw r0, [r2+0x420]                    
    lddw r1, 0xffffffff                             r1 load str located at 4294967295
    jlt r0, r1, lbb_15519                           if r0 < r1 { pc += 31 }
    ja lbb_15504                                    if true { pc += 15 }
lbb_15489:
    ldxb r0, [r2+0x4d1]                     
    ja lbb_15519                                    if true { pc += 28 }
lbb_15491:
    ldxh r0, [r2+0x4b8]                     
    ja lbb_15519                                    if true { pc += 26 }
lbb_15493:
    mov64 r0, 200000                                r0 = 200000 as i32 as i64 as u64
    ldxw r1, [r2+0x0]                       
    jeq r1, 0, lbb_15519                            if r1 == (0 as i32 as i64 as u64) { pc += 23 }
    ldxw r0, [r2+0x4]                       
    ja lbb_15519                                    if true { pc += 21 }
lbb_15498:
    ldxb r0, [r2+0x4d4]                     
    ja lbb_15519                                    if true { pc += 19 }
lbb_15500:
    ldxdw r0, [r2+0x418]                    
    lddw r1, 0xffffffff                             r1 load str located at 4294967295
    jlt r0, r1, lbb_15519                           if r0 < r1 { pc += 15 }
lbb_15504:
    ja lbb_15509                                    if true { pc += 4 }
lbb_15505:
    ldxdw r0, [r2+0x410]                    
    lddw r1, 0xffffffff                             r1 load str located at 4294967295
    jlt r0, r1, lbb_15519                           if r0 < r1 { pc += 10 }
lbb_15509:
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    ja lbb_15519                                    if true { pc += 7 }
lbb_15512:
    ldxb r0, [r2+0x4d2]                     
    ja lbb_15519                                    if true { pc += 5 }
lbb_15514:
    ldxh r0, [r2+0x4ba]                     
    ja lbb_15519                                    if true { pc += 3 }
lbb_15516:
    ldxw r0, [r2+0x4b0]                     
    ja lbb_15519                                    if true { pc += 1 }
lbb_15518:
    ldxw r0, [r2+0x4b4]                     
lbb_15519:
    exit                                    

function_15520:
    mov64 r9, r3                                    r9 = r3
    mov64 r7, r2                                    r7 = r2
    stxdw [r10-0x38], r1                    
    ldxdw r1, [r7+0x10]                     
    stxdw [r10-0x40], r1                    
    ldxdw r6, [r7+0x8]                      
    mov64 r1, 1024                                  r1 = 1024 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_6742                      
    mov64 r8, r0                                    r8 = r0
    jne r8, 0, lbb_15534                            if r8 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 1024                                  r2 = 1024 as i32 as i64 as u64
    call function_20091                     
lbb_15534:
    ldxdw r1, [r9+0x18]                     
    stxdw [r8+0x18], r1                     
    ldxdw r1, [r9+0x10]                     
    stxdw [r8+0x10], r1                     
    ldxdw r1, [r9+0x8]                      
    stxdw [r8+0x8], r1                      
    ldxdw r1, [r9+0x0]                      
    stxdw [r8+0x0], r1                      
    ldxdw r1, [r10-0x40]                    
    stxdw [r8+0x28], r1                     
    stxdw [r8+0x20], r6                     
    stdw [r8+0x30], 0                       
    stdw [r8+0x38], 0                       
    stxdw [r10-0x30], r8                    
    stdw [r10-0x28], 64                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_19746                     
    mov64 r1, r8                                    r1 = r8
    mov64 r2, 1024                                  r2 = 1024 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6768                      
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r7+0x8], r6                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_19856                     
    ldxw r1, [r10-0x30]                     
    jne r1, 0, lbb_15568                            if r1 != (0 as i32 as i64 as u64) { pc += 4 }
    ldxdw r1, [r10-0x28]                    
    stxdw [r7+0x10], r1                     
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ja lbb_15570                                    if true { pc += 2 }
lbb_15568:
    ldxw r2, [r10-0x28]                     
    ldxw r1, [r10-0x2c]                     
lbb_15570:
    ldxdw r3, [r10-0x38]                    
    stxw [r3+0x4], r2                       
    stxw [r3+0x0], r1                       
    exit                                    

function_15574:
    jne r3, 1728, lbb_15581                         if r3 != (1728 as i32 as i64 as u64) { pc += 6 }
    mov64 r3, r2                                    r3 = r2
    and64 r3, 7                                     r3 &= 7   ///  r3 = r3.and(7)
    jne r3, 0, lbb_15581                            if r3 != (0 as i32 as i64 as u64) { pc += 3 }
    stxdw [r1+0x8], r2                      
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ja lbb_15583                                    if true { pc += 2 }
lbb_15581:
    stw [r1+0x4], 3                         
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_15583:
    stxw [r1+0x0], r2                       
    exit                                    

function_15585:
    jlt r3, 704, lbb_15589                          if r3 < (704 as i32 as i64 as u64) { pc += 3 }
    mov64 r3, r2                                    r3 = r2
    and64 r3, 7                                     r3 &= 7   ///  r3 = r3.and(7)
    jeq r3, 0, lbb_15593                            if r3 == (0 as i32 as i64 as u64) { pc += 4 }
lbb_15589:
    stw [r1+0x4], 3                         
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_15591:
    stxw [r1+0x0], r2                       
    exit                                    
lbb_15593:
    stxdw [r1+0x8], r2                      
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ja lbb_15591                                    if true { pc += -5 }

function_15596:
    mov64 r7, r5                                    r7 = r5
    stxdw [r10-0x90], r3                    
    mov64 r9, r2                                    r9 = r2
    mov64 r8, r1                                    r8 = r1
    ldxdw r1, [r7-0xff0]                    
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r7-0xff8]                    
    stxdw [r10-0x1000], r1                  
    ldxdw r1, [r7-0xfe8]                    
    stxdw [r10-0xa0], r1                    
    stxdw [r10-0xff0], r1                   
    ldxdw r6, [r7-0x1000]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r3, r6                                    r3 = r6
    stxdw [r10-0x98], r4                    
    call function_16885                     
    ldxw r1, [r10-0x18]                     
    jne r1, 0, lbb_15646                            if r1 != (0 as i32 as i64 as u64) { pc += 30 }
    stxdw [r10-0xc0], r6                    
    stxdw [r10-0xa8], r9                    
    stxdw [r10-0xb0], r8                    
    ldxdw r1, [r7-0xfc8]                    
    stxdw [r10-0xb8], r1                    
    ldxdw r6, [r7-0xfd0]                    
    ldxdw r8, [r7-0xfd8]                    
    ldxdw r9, [r7-0xfe0]                    
    ldxdw r1, [r10-0x10]                    
    stxdw [r10-0xff8], r1                   
    stdw [r10-0x1000], 1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0xa0]                    
    ldxdw r7, [r10-0x90]                    
    mov64 r3, r7                                    r3 = r7
    ldxdw r4, [r10-0x98]                    
    call function_18816                     
    ldxdw r2, [r10-0x20]                    
    ldxdw r1, [r10-0x28]                    
    jeq r1, 0, lbb_15650                            if r1 == (0 as i32 as i64 as u64) { pc += 12 }
    stxdw [r10-0x18], r2                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    call function_19150                     
    lddw r1, 0x2800000000                           r1 load str located at 171798691840
    ldxdw r8, [r10-0xb0]                    
    ja lbb_15647                                    if true { pc += 1 }
lbb_15646:
    ldxdw r1, [r10-0x14]                    
lbb_15647:
    stxdw [r8+0x0], r1                      
    stb [r8+0x8], 2                         
lbb_15649:
    exit                                    
lbb_15650:
    stxdw [r10-0xc8], r8                    
    ldxdw r1, [r10-0xb8]                    
    ldxdw r1, [r10-0xa0]                    
    stxdw [r10-0xd8], r9                    
    stxdw [r10-0xd0], r6                    
    ldxdw r6, [r10-0x98]                    
    jne r6, 0, lbb_15658                            if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, r2                                    r7 = r2
lbb_15658:
    stxdw [r10-0xe8], r2                    
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    mov64 r9, r1                                    r9 = r1
    jne r6, 0, lbb_15663                            if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 160                                   r2 = 160 as i32 as i64 as u64
lbb_15663:
    ldxdw r1, [r10-0xa8]                    
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, r7                                    r2 = r7
    call function_17214                     
    stxdw [r10-0xe0], r0                    
    mov64 r1, 576                                   r1 = 576 as i32 as i64 as u64
    jne r6, 0, lbb_15671                            if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 580                                   r1 = 580 as i32 as i64 as u64
lbb_15671:
    mov64 r0, 56                                    r0 = 56 as i32 as i64 as u64
    ldxdw r5, [r10-0xc0]                    
    ldxdw r2, [r10-0xc8]                    
    jne r6, 0, lbb_15676                            if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 60                                    r0 = 60 as i32 as i64 as u64
lbb_15676:
    ldxdw r4, [r9+0x10]                     
    mov64 r3, r5                                    r3 = r5
    sub64 r3, r4                                    r3 -= r4   ///  r3 = r3.wrapping_sub(r4)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jgt r3, r5, lbb_15682                           if r3 > r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_15682:
    stxdw [r10-0x90], r7                    
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r5, [r10-0xa8]                    
    ldxdw r6, [r10-0xb8]                    
    jne r4, 0, lbb_15688                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, r3                                    r2 = r3
lbb_15688:
    mov64 r3, r9                                    r3 = r9
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
    ldxw r7, [r3+0x0]                       
    mov64 r3, r5                                    r3 = r5
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    ldxw r8, [r3+0x0]                       
    mov64 r1, r5                                    r1 = r5
    add64 r1, 296                                   r1 += 296   ///  r1 = r1.wrapping_add(296 as i32 as i64 as u64)
    stxdw [r10-0x98], r2                    
    call function_17214                     
    jgt r0, 1000, lbb_15700                         if r0 > (1000 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 1000                                  r0 = 1000 as i32 as i64 as u64
lbb_15700:
    mov64 r6, 100000                                r6 = 100000 as i32 as i64 as u64
    jlt r0, 100000, lbb_15703                       if r0 < (100000 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 100000                                r0 = 100000 as i32 as i64 as u64
lbb_15703:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -56                                   r1 += -56   ///  r1 = r1.wrapping_add(-56 as i32 as i64 as u64)
    mov64 r2, r0                                    r2 = r0
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0xe0]                    
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_25903                     
    mov64 r1, 100                                   r1 = 100 as i32 as i64 as u64
    ldxdw r2, [r10-0xa8]                    
    ldxw r2, [r2+0x248]                     
    jeq r2, 0, lbb_15715                            if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r2                                    r1 = r2
lbb_15715:
    ldxdw r2, [r9+0x20]                     
    jgt r2, 1000, lbb_15718                         if r2 > (1000 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 1000                                  r2 = 1000 as i32 as i64 as u64
lbb_15718:
    mov64 r9, r8                                    r9 = r8
    jlt r2, 100000, lbb_15721                       if r2 < (100000 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 100000                                r2 = 100000 as i32 as i64 as u64
lbb_15721:
    jgt r9, r1, lbb_15723                           if r9 > r1 { pc += 1 }
    mov64 r6, r2                                    r6 = r2
lbb_15723:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    ldxdw r2, [r10-0x38]                    
    ldxdw r3, [r10-0x30]                    
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_25903                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    ldxdw r2, [r10-0x48]                    
    ldxdw r3, [r10-0x40]                    
    mov64 r4, 1000000                               r4 = 1000000 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_24950                     
    ldxdw r6, [r10-0xa8]                    
    mov64 r1, r6                                    r1 = r6
    add64 r1, 432                                   r1 += 432   ///  r1 = r1.wrapping_add(432 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    call function_17214                     
    mov64 r2, r0                                    r2 = r0
    add64 r2, r7                                    r2 += r7   ///  r2 = r2.wrapping_add(r7)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r2, r0, lbb_15747                           if r2 < r0 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_15747:
    ldxdw r4, [r10-0x58]                    
    mov64 r3, r2                                    r3 = r2
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    ldxdw r8, [r10-0x90]                    
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r3, r2, lbb_15754                           if r3 < r2 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_15754:
    ldxdw r2, [r10-0x50]                    
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    ldxdw r2, [r10-0xd0]                    
    ldxdw r4, [r10-0xd8]                    
    jne r1, 0, lbb_15761                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15765                                    if true { pc += 4 }
lbb_15761:
    ldxdw r8, [r10-0xb0]                    
    stb [r8+0x8], 2                         
    stw [r8+0x0], 23                        
    ja lbb_15649                                    if true { pc += -116 }
lbb_15765:
    ldxdw r1, [r4+0x0]                      
    stxdw [r10-0xfd0], r2                   
    ldxdw r2, [r10-0xb8]                    
    stxdw [r10-0xfc8], r2                   
    ldxdw r2, [r10-0xc8]                    
    stxdw [r10-0xfd8], r2                   
    stxdw [r10-0xfe0], r1                   
    ldxdw r1, [r10-0xe8]                    
    stxdw [r10-0xfe8], r1                   
    stxdw [r10-0xff0], r9                   
    ldxdw r1, [r10-0x98]                    
    stxdw [r10-0xff8], r1                   
    stxdw [r10-0x1000], r8                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r6                                    r2 = r6
    ldxdw r4, [r10-0xc0]                    
    call function_15987                     
    ldxb r7, [r10-0x18]                     
    jeq r7, 2, lbb_15787                            if r7 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_15790                                    if true { pc += 3 }
lbb_15787:
    ldxdw r1, [r10-0x14]                    
    ldxdw r8, [r10-0xb0]                    
    ja lbb_15647                                    if true { pc += -143 }
lbb_15790:
    ldxb r1, [r10-0x15]                     
    stxb [r10-0x2], r1                      
    ldxh r1, [r10-0x17]                     
    stxh [r10-0x4], r1                      
    ldxw r8, [r10-0x10]                     
    mov64 r6, r8                                    r6 = r8
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    arsh64 r6, 32                                   r6 >>= 32 (signed)   ///  r6 = (r6 as i64).wrapping_shr(32)
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_25868                     
    jsgt r0, 0, lbb_15803                           if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    ja lbb_15846                                    if true { pc += 43 }
lbb_15803:
    ldxw r1, [r10-0x14]                     
    stxdw [r10-0x98], r1                    
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r6                                    r2 = r6
    call function_25947                     
    jne r0, 0, lbb_15846                            if r0 != (0 as i32 as i64 as u64) { pc += 37 }
    mov64 r6, r8                                    r6 = r8
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    arsh64 r6, 32                                   r6 >>= 32 (signed)   ///  r6 = (r6 as i64).wrapping_shr(32)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    call function_23093                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_26141                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jslt r0, 0, lbb_15822                           if (r0 as i64) < (0 as i32 as i64) { pc += 1 }
    ldxdw r1, [r10-0x68]                    
lbb_15822:
    stxdw [r10-0x90], r1                    
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jslt r0, 0, lbb_15826                           if (r0 as i64) < (0 as i32 as i64) { pc += 1 }
    ldxdw r9, [r10-0x60]                    
lbb_15826:
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 2139095039                            r2 = 2139095039 as i32 as i64 as u64
    call function_25834                     
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
    mov64 r3, -1                                    r3 = -1 as i32 as i64 as u64
    jsgt r0, 0, lbb_15833                           if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    mov64 r3, r9                                    r3 = r9
lbb_15833:
    ldxdw r4, [r10-0xb0]                    
    jsgt r0, 0, lbb_15836                           if (r0 as i64) > (0 as i32 as i64) { pc += 1 }
    ldxdw r1, [r10-0x90]                    
lbb_15836:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, 10000001, lbb_15839                     if r1 < (10000001 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_15839:
    jeq r3, 0, lbb_15841                            if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_15841:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_15850                            if r2 != (0 as i32 as i64 as u64) { pc += 7 }
    stb [r4+0x8], 2                         
    stdw [r4+0x0], 23                       
    ja lbb_15649                                    if true { pc += -197 }
lbb_15846:
    lddw r1, 0x3200000000                           r1 load str located at 214748364800
    ldxdw r8, [r10-0xb0]                    
    ja lbb_15647                                    if true { pc += -203 }
lbb_15850:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jgt r1, 10000000, lbb_15854                     if r1 > (10000000 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_15854:
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    neg64 r3                                        r3 = -r3   ///  r3 = (r3 as i64).wrapping_neg() as u64
    mov64 r2, 10000000                              r2 = 10000000 as i32 as i64 as u64
    sub64 r2, r1                                    r2 -= r1   ///  r2 = r2.wrapping_sub(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -120                                  r1 += -120   ///  r1 = r1.wrapping_add(-120 as i32 as i64 as u64)
    ldxdw r9, [r10-0xe8]                    
    mov64 r4, r9                                    r4 = r9
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_25903                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -136                                  r1 += -136   ///  r1 = r1.wrapping_add(-136 as i32 as i64 as u64)
    ldxdw r2, [r10-0x78]                    
    ldxdw r3, [r10-0x70]                    
    mov64 r4, 10000000                              r4 = 10000000 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_24950                     
    ldxdw r1, [r10-0x88]                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jle r1, r9, lbb_15875                           if r1 <= r9 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_15875:
    mov64 r4, r8                                    r4 = r8
    ldxdw r3, [r10-0x80]                    
    jeq r3, 0, lbb_15879                            if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_15879:
    ldxdw r8, [r10-0xb0]                    
    jeq r3, 0, lbb_15882                            if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, r6                                    r2 = r6
lbb_15882:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_15887                            if r2 != (0 as i32 as i64 as u64) { pc += 3 }
    lddw r1, 0x3100000000                           r1 load str located at 210453397504
    ja lbb_15647                                    if true { pc += -240 }
lbb_15887:
    ldxb r2, [r10-0x2]                      
    stxb [r8+0xb], r2                       
    ldxh r2, [r10-0x4]                      
    stxh [r8+0x9], r2                       
    stxw [r8+0x10], r4                      
    ldxdw r2, [r10-0x98]                    
    stxw [r8+0xc], r2                       
    stxb [r8+0x8], r7                       
    stxdw [r8+0x0], r1                      
    ja lbb_15649                                    if true { pc += -248 }

function_15897:
    mov64 r6, r1                                    r6 = r1
    ldxdw r7, [r2+0x3]                      
    ldxh r1, [r2+0x0]                       
    stxh [r6+0x8], r1                       
    ldxb r1, [r2+0x2]                       
    stxb [r6+0xa], r1                       
    add64 r2, 11                                    r2 += 11   ///  r2 = r2.wrapping_add(11 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    add64 r1, 19                                    r1 += 19   ///  r1 = r1.wrapping_add(19 as i32 as i64 as u64)
    mov64 r3, 1013                                  r3 = 1013 as i32 as i64 as u64
    call function_23152                     
    stxdw [r6+0xb], r7                      
    stw [r6+0x0], 0                         
    exit                                    

function_15911:
    mov64 r4, r3                                    r4 = r3
    mov64 r7, r2                                    r7 = r2
    stxdw [r10-0x20], r1                    
    ldxb r1, [r4+0xc9]                      
    jne r1, 0, lbb_15949                            if r1 != (0 as i32 as i64 as u64) { pc += 33 }
lbb_15916:
    ldxdw r8, [r4+0x18]                     
    ldxdw r6, [r4+0x0]                      
    ldxdw r3, [r4+0x10]                     
    stdw [r10-0x1000], 0                    
    add64 r4, 32                                    r4 += 32   ///  r4 = r4.wrapping_add(32 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r4                                    r2 = r4
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_18816                     
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x18]                    
    jne r2, 0, lbb_15958                            if r2 != (0 as i32 as i64 as u64) { pc += 27 }
    stxdw [r10-0x28], r6                    
    jslt r8, 0, lbb_15977                           if (r8 as i64) < (0 as i32 as i64) { pc += 44 }
    jslt r1, 0, lbb_15977                           if (r1 as i64) < (0 as i32 as i64) { pc += 43 }
    sub64 r8, r1                                    r8 -= r1   ///  r8 = r8.wrapping_sub(r1)
    mov64 r1, r8                                    r1 = r8
    mov64 r2, 2                                     r2 = 2 as i32 as i64 as u64
    call function_26065                     
    jsgt r8, 1, lbb_15967                           if (r8 as i64) > (1 as i32 as i64) { pc += 28 }
    ldxdw r3, [r10-0x28]                    
    jslt r8, -1, lbb_15942                          if (r8 as i64) < (-1 as i32 as i64) { pc += 1 }
    ja lbb_15975                                    if true { pc += 33 }
lbb_15942:
    ldxdw r1, [r7+0x280]                    
    jslt r1, 0, lbb_15945                           if (r1 as i64) < (0 as i32 as i64) { pc += 1 }
    ja lbb_15974                                    if true { pc += 29 }
lbb_15945:
    sub64 r1, r0                                    r1 -= r0   ///  r1 = r1.wrapping_sub(r0)
    ldxdw r2, [r7+0x298]                    
    jsgt r1, r2, lbb_15974                          if (r1 as i64) > (r2 as i64) { pc += 26 }
    ja lbb_15975                                    if true { pc += 26 }
lbb_15949:
    ldxdw r1, [r7+0x238]                    
    ldxdw r2, [r4+0x8]                      
    jlt r2, r1, lbb_15916                           if r2 < r1 { pc += -36 }
    ldxb r1, [r4+0xd4]                      
    jne r1, 0, lbb_15963                            if r1 != (0 as i32 as i64 as u64) { pc += 9 }
    ldxw r1, [r7+0x244]                     
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxw [r7+0x244], r1                     
    ja lbb_15916                                    if true { pc += -42 }
lbb_15958:
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -8                                    r1 += -8   ///  r1 = r1.wrapping_add(-8 as i32 as i64 as u64)
    call function_19150                     
    ja lbb_15977                                    if true { pc += 14 }
lbb_15963:
    ldxw r1, [r7+0x240]                     
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxw [r7+0x240], r1                     
    ja lbb_15916                                    if true { pc += -51 }
lbb_15967:
    ldxdw r1, [r7+0x280]                    
    ldxdw r3, [r10-0x28]                    
    jslt r1, 1, lbb_15974                           if (r1 as i64) < (1 as i32 as i64) { pc += 4 }
    mov64 r2, r0                                    r2 = r0
    sub64 r2, r1                                    r2 -= r1   ///  r2 = r2.wrapping_sub(r1)
    ldxdw r1, [r7+0x298]                    
    jsle r2, r1, lbb_15975                          if (r2 as i64) <= (r1 as i64) { pc += 1 }
lbb_15974:
    stxdw [r7+0x288], r3                    
lbb_15975:
    stxdw [r7+0x280], r0                    
    mov64 r9, 26                                    r9 = 26 as i32 as i64 as u64
lbb_15977:
    ldxdw r1, [r10-0x20]                    
    stxw [r1+0x0], r9                       
    stw [r1+0x4], 52                        
    exit                                    

function_15981:
    stxdw [r1+0x8], r2                      
    stw [r1+0x0], 0                         
    exit                                    

function_15984:
    stxdw [r1+0x8], r2                      
    stw [r1+0x0], 0                         
    exit                                    

function_15987:
    ldxdw r6, [r5-0xfe0]                    
    ldxb r8, [r6+0x0]                       
    mov64 r0, r8                                    r0 = r8
    and64 r0, 8                                     r0 &= 8   ///  r0 = r0.and(8)
    jne r0, 0, lbb_16048                            if r0 != (0 as i32 as i64 as u64) { pc += 56 }
    mov64 r0, r8                                    r0 = r8
    and64 r0, 7                                     r0 &= 7   ///  r0 = r0.and(7)
    jeq r0, 7, lbb_16048                            if r0 == (7 as i32 as i64 as u64) { pc += 53 }
    ldxdw r0, [r5-0xfc8]                    
    stxdw [r10-0x9c8], r0                   
    ldxdw r9, [r5-0xfd0]                    
    ldxdw r0, [r5-0xfd8]                    
    stxdw [r10-0x9b8], r0                   
    mov64 r0, r6                                    r0 = r6
    ldxdw r6, [r5-0xfe8]                    
    stxdw [r10-0x9d8], r6                   
    ldxdw r7, [r5-0xff0]                    
    ldxdw r6, [r5-0xff8]                    
    stxdw [r10-0x9d0], r6                   
    ldxdw r5, [r5-0x1000]                   
    stxdw [r10-0x9c0], r5                   
    mov64 r5, r8                                    r5 = r8
    add64 r5, 1                                     r5 += 1   ///  r5 = r5.wrapping_add(1 as i32 as i64 as u64)
    stxb [r0+0x0], r5                       
    ldxdw r5, [r0+0x50]                     
    jne r5, 1048576, lbb_16051                      if r5 != (1048576 as i32 as i64 as u64) { pc += 38 }
    mov64 r5, r0                                    r5 = r0
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x9f0], r5                   
    and64 r5, 3                                     r5 &= 3   ///  r5 = r5.and(3)
    jeq r5, 0, lbb_16019                            if r5 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_16051                                    if true { pc += 32 }
lbb_16019:
    stxdw [r10-0x9e8], r0                   
    mov64 r8, r3                                    r8 = r3
    stxdw [r10-0x9f8], r2                   
    stxdw [r10-0x9e0], r1                   
    stxdw [r10-0xfe8], r9                   
    ldxdw r1, [r10-0x9c8]                   
    stxdw [r10-0xfe0], r1                   
    ldxdw r1, [r10-0x9d8]                   
    stxdw [r10-0xff0], r1                   
    stxdw [r10-0xff8], r7                   
    ldxdw r1, [r10-0x9d0]                   
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1240                                 r1 += -1240   ///  r1 = r1.wrapping_add(-1240 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0x9b8]                   
    mov64 r3, r4                                    r3 = r4
    ldxdw r4, [r10-0x9c0]                   
    call function_10318                     
    ldxw r7, [r10-0x4d0]                    
    ldxw r6, [r10-0x4d4]                    
    ldxw r9, [r10-0x4d8]                    
    jeq r9, 2, lbb_16043                            if r9 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_16057                                    if true { pc += 14 }
lbb_16043:
    ldxdw r1, [r10-0x9e0]                   
    stxw [r1+0x8], r7                       
    stxw [r1+0x4], r6                       
    stb [r1+0x0], 2                         
    ja lbb_16165                                    if true { pc += 117 }
lbb_16048:
    stb [r1+0x0], 2                         
    stdw [r1+0x4], 11                       
    ja lbb_16180                                    if true { pc += 129 }
lbb_16051:
    lddw r2, 0x2600000000                           r2 load str located at 163208757248
    stxdw [r1+0x4], r2                      
    stb [r1+0x0], 2                         
    stxb [r0+0x0], r8                       
    ja lbb_16180                                    if true { pc += 123 }
lbb_16057:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2468                                 r1 += -2468   ///  r1 = r1.wrapping_add(-2468 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1228                                 r2 += -1228   ///  r2 = r2.wrapping_add(-1228 as i32 as i64 as u64)
    mov64 r3, 1228                                  r3 = 1228 as i32 as i64 as u64
    call function_23152                     
    stxw [r10-0x9a8], r7                    
    stxw [r10-0x9ac], r6                    
    stxw [r10-0x9b0], r9                    
    mov64 r7, r8                                    r7 = r8
    mov64 r1, r7                                    r1 = r7
    call function_25991                     
    jgt r7, 999999, lbb_16161                       if r7 > (999999 as i32 as i64 as u64) { pc += 91 }
    stxdw [r10-0x9b8], r0                   
    ldxdw r1, [r10-0x9f8]                   
    ldxb r2, [r1+0x291]                     
    ldxb r7, [r1+0x290]                     
    mov64 r4, r10                                   r4 = r10
    add64 r4, -2480                                 r4 += -2480   ///  r4 = r4.wrapping_add(-2480 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    ldxdw r9, [r10-0x9f0]                   
    mov64 r3, r9                                    r3 = r9
    call function_16322                     
    mov64 r8, r0                                    r8 = r0
    rsh64 r8, 32                                    r8 >>= 32   ///  r8 = r8.wrapping_shr(32)
    mov64 r1, r0                                    r1 = r0
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    ldxdw r6, [r10-0x9e8]                   
    jeq r1, 26, lbb_16088                           if r1 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_16167                                    if true { pc += 79 }
lbb_16088:
    ldxdw r3, [r10-0x508]                   
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r9                                    r2 = r9
    stxdw [r10-0x9c0], r3                   
    call function_16352                     
    mov64 r2, r0                                    r2 = r0
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mov64 r1, r0                                    r1 = r0
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jeq r1, 26, lbb_16100                           if r1 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_16170                                    if true { pc += 70 }
lbb_16100:
    stxdw [r10-0x9c8], r2                   
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r9                                    r2 = r9
    ldxdw r3, [r10-0x9c0]                   
    call function_16377                     
    mov64 r7, r0                                    r7 = r0
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r1, r0                                    r1 = r0
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jeq r1, 26, lbb_16112                           if r1 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_16173                                    if true { pc += 61 }
lbb_16112:
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2480                                 r2 += -2480   ///  r2 = r2.wrapping_add(-2480 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    call function_16402                     
    mov64 r2, r0                                    r2 = r0
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mov64 r1, r0                                    r1 = r0
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jeq r1, 26, lbb_16123                           if r1 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_16170                                    if true { pc += 47 }
lbb_16123:
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    arsh64 r8, 32                                   r8 >>= 32 (signed)   ///  r8 = (r8 as i64).wrapping_shr(32)
    stxdw [r10-0x9c0], r2                   
    ldxdw r2, [r10-0x9c8]                   
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r1, r8                                    r1 = r8
    call function_23173                     
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    arsh64 r7, 32                                   r7 >>= 32 (signed)   ///  r7 = (r7 as i64).wrapping_shr(32)
    mov64 r1, r0                                    r1 = r0
    mov64 r2, r7                                    r2 = r7
    call function_23173                     
    mov64 r1, r0                                    r1 = r0
    ldxdw r2, [r10-0x9b8]                   
    call function_25901                     
    ldxdw r2, [r10-0x9c0]                   
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r1, r0                                    r1 = r0
    call function_25901                     
    ldxdw r1, [r10-0x9f8]                   
    ldxdw r2, [r1+0x270]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1240                                 r1 += -1240   ///  r1 = r1.wrapping_add(-1240 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -2480                                 r5 += -2480   ///  r5 = r5.wrapping_add(-2480 as i32 as i64 as u64)
    mov64 r3, r0                                    r3 = r0
    mov64 r4, r9                                    r4 = r9
    call function_16190                     
    ldxb r1, [r10-0x4d8]                    
    jeq r1, 2, lbb_16156                            if r1 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_16181                                    if true { pc += 25 }
lbb_16156:
    ldxdw r1, [r10-0x4d4]                   
    ldxdw r2, [r10-0x9e0]                   
    stxdw [r2+0x4], r1                      
    stb [r2+0x0], 2                         
    ja lbb_16177                                    if true { pc += 16 }
lbb_16161:
    ldxdw r1, [r10-0x9e0]                   
    stxw [r1+0x4], r0                       
    stw [r1+0x8], 1232348160                
    sth [r1+0x0], 256                       
lbb_16165:
    ldxdw r6, [r10-0x9e8]                   
    ja lbb_16177                                    if true { pc += 10 }
lbb_16167:
    ldxdw r1, [r10-0x9e0]                   
    stxw [r1+0x8], r8                       
    ja lbb_16175                                    if true { pc += 5 }
lbb_16170:
    ldxdw r1, [r10-0x9e0]                   
    stxw [r1+0x8], r2                       
    ja lbb_16175                                    if true { pc += 2 }
lbb_16173:
    ldxdw r1, [r10-0x9e0]                   
    stxw [r1+0x8], r7                       
lbb_16175:
    stxw [r1+0x4], r0                       
    stb [r1+0x0], 2                         
lbb_16177:
    ldxb r1, [r6+0x0]                       
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxb [r6+0x0], r1                       
lbb_16180:
    exit                                    
lbb_16181:
    ldxb r2, [r10-0x4d5]                    
    ldxdw r3, [r10-0x9e0]                   
    stxb [r3+0x3], r2                       
    ldxh r2, [r10-0x4d7]                    
    stxh [r3+0x1], r2                       
    ldxdw r2, [r10-0x4d4]                   
    stxdw [r3+0x4], r2                      
    stxb [r3+0x0], r1                       
    ja lbb_16177                                    if true { pc += -13 }

function_16190:
    stxdw [r10-0x40], r5                    
    mov64 r7, r4                                    r7 = r4
    mov64 r6, r3                                    r6 = r3
    mov64 r9, r2                                    r9 = r2
    mov64 r8, r1                                    r8 = r1
    mov64 r2, r7                                    r2 = r7
    add64 r2, 4152                                  r2 += 4152   ///  r2 = r2.wrapping_add(4152 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_13515                     
    ldxw r1, [r10-0x30]                     
    jne r1, 0, lbb_16291                            if r1 != (0 as i32 as i64 as u64) { pc += 89 }
    ldxdw r1, [r10-0x40]                    
    stxdw [r10-0x38], r6                    
    stxdw [r10-0x60], r9                    
    mov64 r1, r9                                    r1 = r9
    call function_23347                     
    ldxdw r6, [r10-0x28]                    
    add64 r7, 56                                    r7 += 56   ///  r7 = r7.wrapping_add(56 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    and64 r1, 4                                     r1 &= 4   ///  r1 = r1.and(4)
    jeq r1, 0, lbb_16213                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_16295                                    if true { pc += 82 }
lbb_16213:
    stxdw [r10-0x70], r0                    
    stxdw [r10-0x68], r8                    
    stxdw [r10-0x28], r7                    
    ldxdw r1, [r10-0x40]                    
    stxdw [r10-0x30], r1                    
    ldxdw r2, [r10-0x38]                    
    stxw [r10-0x20], r2                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x40], r1                    
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x48], r1                    
    ldxb r1, [r6+0x0]                       
    stxdw [r10-0x50], r2                    
    jeq r1, 0, lbb_16240                            if r1 == (0 as i32 as i64 as u64) { pc += 13 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x48], r1                    
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x40], r1                    
    mov64 r7, 84                                    r7 = 84 as i32 as i64 as u64
    ldxdw r1, [r10-0x38]                    
    stxdw [r10-0x50], r1                    
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ja lbb_16245                                    if true { pc += 9 }
lbb_16236:
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
    add64 r7, 92                                    r7 += 92   ///  r7 = r7.wrapping_add(92 as i32 as i64 as u64)
    ldxb r1, [r6+0x0]                       
    jlt r8, r1, lbb_16245                           if r8 < r1 { pc += 5 }
lbb_16240:
    mov64 r2, 1232348160                            r2 = 1232348160 as i32 as i64 as u64
    ldxdw r1, [r10-0x60]                    
    jeq r1, 0, lbb_16299                            if r1 == (0 as i32 as i64 as u64) { pc += 56 }
    ldxdw r2, [r10-0x70]                    
    ja lbb_16299                                    if true { pc += 54 }
lbb_16245:
    jne r8, 40, lbb_16251                           if r8 != (40 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 40                                    r1 = 40 as i32 as i64 as u64
    mov64 r2, 40                                    r2 = 40 as i32 as i64 as u64
    lddw r3, 0x100034bf0 --> b"\x00\x00\x00\x00U7\x03\x00=\x00\x00\x00\x00\x00\x00\x00t\x00\x00\x00\x0f\…        r3 load str located at 4295183344
    call function_20679                     
lbb_16251:
    ldxdw r9, [r6+0x8]                      
    add64 r9, r7                                    r9 += r7   ///  r9 = r9.wrapping_add(r7)
    mov64 r1, r9                                    r1 = r9
    add64 r1, -84                                   r1 += -84   ///  r1 = r1.wrapping_add(-84 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -48                                   r2 += -48   ///  r2 = r2.wrapping_add(-48 as i32 as i64 as u64)
    call function_14868                     
    jeq r0, 0, lbb_16236                            if r0 == (0 as i32 as i64 as u64) { pc += -23 }
    ldxdw r1, [r10-0x38]                    
    stxw [r10-0x4], r1                      
    ldxdw r1, [r10-0x50]                    
    stxw [r10-0x8], r1                      
    ldxdw r1, [r10-0x58]                    
    stxh [r10-0xa], r1                      
    ldxdw r1, [r10-0x48]                    
    stxb [r10-0xb], r1                      
    ldxdw r1, [r10-0x40]                    
    stxb [r10-0xc], r1                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -12                                   r3 += -12   ///  r3 = r3.wrapping_add(-12 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    call function_14765                     
    ldxw r1, [r10-0x10]                     
    stxdw [r10-0x38], r1                    
    ldxw r1, [r10-0x14]                     
    stxdw [r10-0x50], r1                    
    ldxh r1, [r10-0x16]                     
    stxdw [r10-0x58], r1                    
    ldxb r1, [r10-0x17]                     
    stxdw [r10-0x48], r1                    
    ldxb r1, [r10-0x18]                     
    stxdw [r10-0x40], r1                    
    ldxw r1, [r9+0x0]                       
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lsh64 r2, r1                                    r2 <<= r1   ///  r2 = r2.wrapping_shl(r1 as u32)
    and64 r2, 1139                                  r2 &= 1139   ///  r2 = r2.and(1139)
    jne r2, 0, lbb_16236                            if r2 != (0 as i32 as i64 as u64) { pc += -54 }
    ja lbb_16240                                    if true { pc += -51 }
lbb_16291:
    ldxdw r1, [r10-0x2c]                    
    stxdw [r8+0x4], r1                      
    stb [r8+0x0], 2                         
    ja lbb_16321                                    if true { pc += 26 }
lbb_16295:
    stxw [r8+0x8], r7                       
    stw [r8+0x4], 2                         
    stb [r8+0x0], 2                         
    ja lbb_16313                                    if true { pc += 14 }
lbb_16299:
    ldxdw r7, [r10-0x68]                    
    ldxdw r1, [r10-0x50]                    
    stxw [r7+0x4], r1                       
    ldxdw r1, [r10-0x58]                    
    stxh [r7+0x2], r1                       
    ldxdw r1, [r10-0x48]                    
    stxb [r7+0x1], r1                       
    ldxdw r1, [r10-0x40]                    
    stxb [r7+0x0], r1                       
    ldxdw r1, [r10-0x38]                    
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    call function_23192                     
    stxw [r7+0x8], r0                       
lbb_16313:
    ldxdw r1, [r6+0x8]                      
    mov64 r2, 3680                                  r2 = 3680 as i32 as i64 as u64
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
    call function_6768                      
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 16                                    r2 = 16 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
lbb_16321:
    exit                                    

function_16322:
    lddw r0, 0x3f8000000000001a                     r0 load str located at 4575657221408423962
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    ldxb r5, [r4+0x4cf]                     
    jge r5, r2, lbb_16351                           if r5 >= r2 { pc += 24 }
    add64 r3, 8248                                  r3 += 8248   ///  r3 = r3.wrapping_add(8248 as i32 as i64 as u64)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r5, [r4+0x410]                    
    add64 r4, 16                                    r4 += 16   ///  r4 = r4.wrapping_add(16 as i32 as i64 as u64)
    lsh64 r5, 3                                     r5 <<= 3   ///  r5 = r5.wrapping_shl(3)
lbb_16332:
    jgt r2, 383, lbb_16351                          if r2 > (383 as i32 as i64 as u64) { pc += 18 }
    mov64 r6, r2                                    r6 = r2
    lsh64 r6, 3                                     r6 <<= 3   ///  r6 = r6.wrapping_shl(3)
    mov64 r7, r3                                    r7 = r3
    add64 r7, r6                                    r7 += r6   ///  r7 = r7.wrapping_add(r6)
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    ldxdw r6, [r7+0x0]                      
    jeq r6, 0, lbb_16332                            if r6 == (0 as i32 as i64 as u64) { pc += -8 }
    mov64 r7, r5                                    r7 = r5
    mov64 r8, r4                                    r8 = r4
lbb_16342:
    jeq r7, 0, lbb_16332                            if r7 == (0 as i32 as i64 as u64) { pc += -11 }
    add64 r7, -8                                    r7 += -8   ///  r7 = r7.wrapping_add(-8 as i32 as i64 as u64)
    ldxdw r9, [r8+0x0]                      
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    jne r9, r6, lbb_16342                           if r9 != r6 { pc += -5 }
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    call function_24886                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    or64 r0, 26                                     r0 |= 26   ///  r0 = r0.or(26)
lbb_16351:
    exit                                    

function_16352:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    lddw r4, 0x2600000000                           r4 load str located at 163208757248
    mov64 r5, r2                                    r5 = r2
    add64 r5, 12472                                 r5 += 12472   ///  r5 = r5.wrapping_add(12472 as i32 as i64 as u64)
    and64 r5, 4                                     r5 &= 4   ///  r5 = r5.and(4)
    jne r5, 0, lbb_16375                            if r5 != (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    add64 r2, 12480                                 r2 += 12480   ///  r2 = r2.wrapping_add(12480 as i32 as i64 as u64)
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
lbb_16362:
    lddw r4, 0x3f80000000000000                     r4 load str located at 4575657221408423936
    jeq r5, 4088, lbb_16375                         if r5 == (4088 as i32 as i64 as u64) { pc += 10 }
    mov64 r4, r2                                    r4 = r2
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r4, [r4+0x0]                      
    jne r4, r3, lbb_16362                           if r4 != r3 { pc += -8 }
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    call function_24886                     
    mov64 r4, r0                                    r4 = r0
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
lbb_16375:
    or64 r0, r4                                     r0 |= r4   ///  r0 = r0.or(r4)
    exit                                    

function_16377:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    lddw r4, 0x2600000000                           r4 load str located at 163208757248
    mov64 r5, r2                                    r5 = r2
    add64 r5, 16568                                 r5 += 16568   ///  r5 = r5.wrapping_add(16568 as i32 as i64 as u64)
    and64 r5, 4                                     r5 &= 4   ///  r5 = r5.and(4)
    jne r5, 0, lbb_16400                            if r5 != (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    add64 r2, 16576                                 r2 += 16576   ///  r2 = r2.wrapping_add(16576 as i32 as i64 as u64)
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
lbb_16387:
    lddw r4, 0x3f80000000000000                     r4 load str located at 4575657221408423936
    jeq r5, 20472, lbb_16400                        if r5 == (20472 as i32 as i64 as u64) { pc += 10 }
    mov64 r4, r2                                    r4 = r2
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r4, [r4+0x0]                      
    jne r4, r3, lbb_16387                           if r4 != r3 { pc += -8 }
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    call function_24886                     
    mov64 r4, r0                                    r4 = r0
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
lbb_16400:
    or64 r0, r4                                     r0 |= r4   ///  r0 = r0.or(r4)
    exit                                    

function_16402:
    mov64 r6, r2                                    r6 = r2
    stxdw [r10-0x8], r1                     
    ldxw r1, [r1+0x3054]                    
    mov64 r0, r1                                    r0 = r1
    add64 r0, -1                                    r0 += -1   ///  r0 = r0.wrapping_add(-1 as i32 as i64 as u64)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    mov64 r5, r1                                    r5 = r1
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    ldxb r4, [r6+0x4bc]                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_16416                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_16416:
    arsh64 r5, 32                                   r5 >>= 32 (signed)   ///  r5 = (r5 as i64).wrapping_shr(32)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r0, 8388607, lbb_16420                      if r0 < (8388607 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_16420:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jsgt r5, -1, lbb_16423                          if (r5 as i64) > (-1 as i32 as i64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_16423:
    mov64 r5, r1                                    r5 = r1
    and64 r5, 2147483647                            r5 &= 2147483647   ///  r5 = r5.and(2147483647)
    add64 r5, -8388608                              r5 += -8388608   ///  r5 = r5.wrapping_add(-8388608 as i32 as i64 as u64)
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    jlt r5, 2130706432, lbb_16430                   if r5 < (2130706432 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_16430:
    and64 r2, r0                                    r2 &= r0   ///  r2 = r2.and(r0)
    or64 r4, r2                                     r4 |= r2   ///  r4 = r4.or(r2)
    and64 r3, r4                                    r3 &= r4   ///  r3 = r3.and(r4)
    ldxb r2, [r6+0x4bd]                     
    mov64 r0, r1                                    r0 = r1
    mov64 r4, r3                                    r4 = r3
    stxdw [r10-0x10], r6                    
    jne r2, 0, lbb_16794                            if r2 != (0 as i32 as i64 as u64) { pc += 356 }
lbb_16438:
    ldxb r2, [r6+0x4be]                     
    mov64 r1, r0                                    r1 = r0
    mov64 r3, r4                                    r3 = r4
    jeq r2, 0, lbb_16498                            if r2 == (0 as i32 as i64 as u64) { pc += 56 }
    ldxdw r1, [r10-0x8]                     
    ldxw r2, [r1+0x308c]                    
    mov64 r7, r2                                    r7 = r2
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    arsh64 r7, 32                                   r7 >>= 32 (signed)   ///  r7 = (r7 as i64).wrapping_shr(32)
    mov64 r5, r2                                    r5 = r2
    and64 r5, 2147483647                            r5 &= 2147483647   ///  r5 = r5.and(2147483647)
    mov64 r6, r5                                    r6 = r5
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r6, 8388607, lbb_16455                      if r6 < (8388607 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_16455:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jslt r7, 0, lbb_16458                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_16458:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jsgt r5, 2139095040, lbb_16461                  if (r5 as i64) > (2139095040 as i32 as i64) { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_16461:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jeq r5, 2139095040, lbb_16464                   if r5 == (2139095040 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_16464:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jeq r5, 0, lbb_16467                            if r5 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_16467:
    add64 r5, -8388608                              r5 += -8388608   ///  r5 = r5.wrapping_add(-8388608 as i32 as i64 as u64)
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    jlt r5, 2130706432, lbb_16472                   if r5 < (2130706432 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_16472:
    and64 r1, r6                                    r1 &= r6   ///  r1 = r1.and(r6)
    and64 r3, r6                                    r3 &= r6   ///  r3 = r3.and(r6)
    or64 r8, r3                                     r8 |= r3   ///  r8 = r8.or(r3)
    or64 r8, r7                                     r8 |= r7   ///  r8 = r8.or(r7)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    or64 r8, r1                                     r8 |= r1   ///  r8 = r8.or(r1)
    mov64 r3, r8                                    r3 = r8
    xor64 r3, 1                                     r3 ^= 1   ///  r3 = r3.xor(1)
    mov64 r1, r0                                    r1 = r0
    jne r8, 0, lbb_16483                            if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r2                                    r1 = r2
lbb_16483:
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    ldxdw r6, [r10-0x10]                    
    jne r8, 0, lbb_16498                            if r8 != (0 as i32 as i64 as u64) { pc += 11 }
    xor64 r4, 1                                     r4 ^= 1   ///  r4 = r4.xor(1)
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jne r4, 0, lbb_16498                            if r4 != (0 as i32 as i64 as u64) { pc += 8 }
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r1, r0                                    r1 = r0
    call function_23173                     
    mov64 r1, r0                                    r1 = r0
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_16498:
    ldxb r2, [r6+0x4bf]                     
    mov64 r0, r1                                    r0 = r1
    mov64 r4, r3                                    r4 = r3
    jeq r2, 0, lbb_16556                            if r2 == (0 as i32 as i64 as u64) { pc += 54 }
    ldxdw r2, [r10-0x8]                     
    ldxw r2, [r2+0x30b4]                    
    mov64 r7, r2                                    r7 = r2
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    arsh64 r7, 32                                   r7 >>= 32 (signed)   ///  r7 = (r7 as i64).wrapping_shr(32)
    mov64 r0, r2                                    r0 = r2
    and64 r0, 2147483647                            r0 &= 2147483647   ///  r0 = r0.and(2147483647)
    mov64 r6, r0                                    r6 = r0
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r6, 8388607, lbb_16515                      if r6 < (8388607 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_16515:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jslt r7, 0, lbb_16518                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_16518:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jsgt r0, 2139095040, lbb_16521                  if (r0 as i64) > (2139095040 as i32 as i64) { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_16521:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jeq r0, 2139095040, lbb_16524                   if r0 == (2139095040 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_16524:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jeq r0, 0, lbb_16527                            if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_16527:
    add64 r0, -8388608                              r0 += -8388608   ///  r0 = r0.wrapping_add(-8388608 as i32 as i64 as u64)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jlt r0, 2130706432, lbb_16532                   if r0 < (2130706432 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_16532:
    and64 r4, r6                                    r4 &= r6   ///  r4 = r4.and(r6)
    and64 r5, r6                                    r5 &= r6   ///  r5 = r5.and(r6)
    or64 r8, r5                                     r8 |= r5   ///  r8 = r8.or(r5)
    or64 r8, r7                                     r8 |= r7   ///  r8 = r8.or(r7)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    or64 r8, r4                                     r8 |= r4   ///  r8 = r8.or(r4)
    mov64 r4, r8                                    r4 = r8
    xor64 r4, 1                                     r4 ^= 1   ///  r4 = r4.xor(1)
    mov64 r0, r1                                    r0 = r1
    jne r8, 0, lbb_16543                            if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, r2                                    r0 = r2
lbb_16543:
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    ldxdw r6, [r10-0x10]                    
    jne r8, 0, lbb_16556                            if r8 != (0 as i32 as i64 as u64) { pc += 9 }
    xor64 r3, 1                                     r3 ^= 1   ///  r3 = r3.xor(1)
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_16556                            if r3 != (0 as i32 as i64 as u64) { pc += 6 }
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    call function_23173                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
lbb_16556:
    ldxb r2, [r6+0x4c0]                     
    mov64 r1, r0                                    r1 = r0
    mov64 r3, r4                                    r3 = r4
    jeq r2, 0, lbb_16616                            if r2 == (0 as i32 as i64 as u64) { pc += 56 }
    ldxdw r1, [r10-0x8]                     
    ldxw r2, [r1+0x3058]                    
    mov64 r7, r2                                    r7 = r2
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    arsh64 r7, 32                                   r7 >>= 32 (signed)   ///  r7 = (r7 as i64).wrapping_shr(32)
    mov64 r5, r2                                    r5 = r2
    and64 r5, 2147483647                            r5 &= 2147483647   ///  r5 = r5.and(2147483647)
    mov64 r6, r5                                    r6 = r5
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r6, 8388607, lbb_16573                      if r6 < (8388607 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_16573:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jslt r7, 0, lbb_16576                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_16576:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jsgt r5, 2139095040, lbb_16579                  if (r5 as i64) > (2139095040 as i32 as i64) { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_16579:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jeq r5, 2139095040, lbb_16582                   if r5 == (2139095040 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_16582:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jeq r5, 0, lbb_16585                            if r5 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_16585:
    add64 r5, -8388608                              r5 += -8388608   ///  r5 = r5.wrapping_add(-8388608 as i32 as i64 as u64)
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    jlt r5, 2130706432, lbb_16590                   if r5 < (2130706432 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_16590:
    and64 r1, r6                                    r1 &= r6   ///  r1 = r1.and(r6)
    and64 r3, r6                                    r3 &= r6   ///  r3 = r3.and(r6)
    or64 r8, r3                                     r8 |= r3   ///  r8 = r8.or(r3)
    or64 r8, r7                                     r8 |= r7   ///  r8 = r8.or(r7)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    or64 r8, r1                                     r8 |= r1   ///  r8 = r8.or(r1)
    mov64 r3, r8                                    r3 = r8
    xor64 r3, 1                                     r3 ^= 1   ///  r3 = r3.xor(1)
    mov64 r1, r0                                    r1 = r0
    jne r8, 0, lbb_16601                            if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r2                                    r1 = r2
lbb_16601:
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    ldxdw r6, [r10-0x10]                    
    jne r8, 0, lbb_16616                            if r8 != (0 as i32 as i64 as u64) { pc += 11 }
    xor64 r4, 1                                     r4 ^= 1   ///  r4 = r4.xor(1)
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jne r4, 0, lbb_16616                            if r4 != (0 as i32 as i64 as u64) { pc += 8 }
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r1, r0                                    r1 = r0
    call function_23173                     
    mov64 r1, r0                                    r1 = r0
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_16616:
    ldxb r2, [r6+0x4c1]                     
    mov64 r0, r1                                    r0 = r1
    mov64 r4, r3                                    r4 = r3
    jeq r2, 0, lbb_16674                            if r2 == (0 as i32 as i64 as u64) { pc += 54 }
    ldxdw r2, [r10-0x8]                     
    ldxw r2, [r2+0x3090]                    
    mov64 r7, r2                                    r7 = r2
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    arsh64 r7, 32                                   r7 >>= 32 (signed)   ///  r7 = (r7 as i64).wrapping_shr(32)
    mov64 r0, r2                                    r0 = r2
    and64 r0, 2147483647                            r0 &= 2147483647   ///  r0 = r0.and(2147483647)
    mov64 r6, r0                                    r6 = r0
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r6, 8388607, lbb_16633                      if r6 < (8388607 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_16633:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jslt r7, 0, lbb_16636                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_16636:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jsgt r0, 2139095040, lbb_16639                  if (r0 as i64) > (2139095040 as i32 as i64) { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_16639:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jeq r0, 2139095040, lbb_16642                   if r0 == (2139095040 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_16642:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jeq r0, 0, lbb_16645                            if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_16645:
    add64 r0, -8388608                              r0 += -8388608   ///  r0 = r0.wrapping_add(-8388608 as i32 as i64 as u64)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jlt r0, 2130706432, lbb_16650                   if r0 < (2130706432 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_16650:
    and64 r4, r6                                    r4 &= r6   ///  r4 = r4.and(r6)
    and64 r5, r6                                    r5 &= r6   ///  r5 = r5.and(r6)
    or64 r8, r5                                     r8 |= r5   ///  r8 = r8.or(r5)
    or64 r8, r7                                     r8 |= r7   ///  r8 = r8.or(r7)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    or64 r8, r4                                     r8 |= r4   ///  r8 = r8.or(r4)
    mov64 r4, r8                                    r4 = r8
    xor64 r4, 1                                     r4 ^= 1   ///  r4 = r4.xor(1)
    mov64 r0, r1                                    r0 = r1
    jne r8, 0, lbb_16661                            if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, r2                                    r0 = r2
lbb_16661:
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    ldxdw r6, [r10-0x10]                    
    jne r8, 0, lbb_16674                            if r8 != (0 as i32 as i64 as u64) { pc += 9 }
    xor64 r3, 1                                     r3 ^= 1   ///  r3 = r3.xor(1)
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_16674                            if r3 != (0 as i32 as i64 as u64) { pc += 6 }
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    call function_23173                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
lbb_16674:
    ldxb r2, [r6+0x4c2]                     
    mov64 r1, r0                                    r1 = r0
    mov64 r3, r4                                    r3 = r4
    jeq r2, 0, lbb_16734                            if r2 == (0 as i32 as i64 as u64) { pc += 56 }
    ldxdw r1, [r10-0x8]                     
    ldxw r2, [r1+0x3094]                    
    mov64 r7, r2                                    r7 = r2
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    arsh64 r7, 32                                   r7 >>= 32 (signed)   ///  r7 = (r7 as i64).wrapping_shr(32)
    mov64 r5, r2                                    r5 = r2
    and64 r5, 2147483647                            r5 &= 2147483647   ///  r5 = r5.and(2147483647)
    mov64 r6, r5                                    r6 = r5
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r6, 8388607, lbb_16691                      if r6 < (8388607 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_16691:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jslt r7, 0, lbb_16694                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_16694:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jsgt r5, 2139095040, lbb_16697                  if (r5 as i64) > (2139095040 as i32 as i64) { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_16697:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jeq r5, 2139095040, lbb_16700                   if r5 == (2139095040 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_16700:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jeq r5, 0, lbb_16703                            if r5 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_16703:
    add64 r5, -8388608                              r5 += -8388608   ///  r5 = r5.wrapping_add(-8388608 as i32 as i64 as u64)
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    jlt r5, 2130706432, lbb_16708                   if r5 < (2130706432 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_16708:
    and64 r1, r6                                    r1 &= r6   ///  r1 = r1.and(r6)
    and64 r3, r6                                    r3 &= r6   ///  r3 = r3.and(r6)
    or64 r8, r3                                     r8 |= r3   ///  r8 = r8.or(r3)
    or64 r8, r7                                     r8 |= r7   ///  r8 = r8.or(r7)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    or64 r8, r1                                     r8 |= r1   ///  r8 = r8.or(r1)
    mov64 r3, r8                                    r3 = r8
    xor64 r3, 1                                     r3 ^= 1   ///  r3 = r3.xor(1)
    mov64 r1, r0                                    r1 = r0
    jne r8, 0, lbb_16719                            if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r2                                    r1 = r2
lbb_16719:
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    ldxdw r6, [r10-0x10]                    
    jne r8, 0, lbb_16734                            if r8 != (0 as i32 as i64 as u64) { pc += 11 }
    xor64 r4, 1                                     r4 ^= 1   ///  r4 = r4.xor(1)
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jne r4, 0, lbb_16734                            if r4 != (0 as i32 as i64 as u64) { pc += 8 }
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r1, r0                                    r1 = r0
    call function_23173                     
    mov64 r1, r0                                    r1 = r0
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_16734:
    ldxb r2, [r6+0x4c3]                     
    jeq r2, 0, lbb_16783                            if r2 == (0 as i32 as i64 as u64) { pc += 47 }
    ldxdw r2, [r10-0x8]                     
    ldxw r0, [r2+0x3098]                    
    mov64 r7, r0                                    r7 = r0
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    arsh64 r7, 32                                   r7 >>= 32 (signed)   ///  r7 = (r7 as i64).wrapping_shr(32)
    mov64 r5, r0                                    r5 = r0
    and64 r5, 2147483647                            r5 &= 2147483647   ///  r5 = r5.and(2147483647)
    mov64 r6, r5                                    r6 = r5
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r6, 8388607, lbb_16749                      if r6 < (8388607 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_16749:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jslt r7, 0, lbb_16752                           if (r7 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_16752:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jsgt r5, 2139095040, lbb_16755                  if (r5 as i64) > (2139095040 as i32 as i64) { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_16755:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jeq r5, 2139095040, lbb_16758                   if r5 == (2139095040 as i32 as i64 as u64) { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_16758:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jeq r5, 0, lbb_16761                            if r5 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_16761:
    add64 r5, -8388608                              r5 += -8388608   ///  r5 = r5.wrapping_add(-8388608 as i32 as i64 as u64)
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    jlt r5, 2130706432, lbb_16766                   if r5 < (2130706432 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_16766:
    and64 r2, r6                                    r2 &= r6   ///  r2 = r2.and(r6)
    and64 r4, r6                                    r4 &= r6   ///  r4 = r4.and(r6)
    or64 r8, r4                                     r8 |= r4   ///  r8 = r8.or(r4)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    or64 r8, r7                                     r8 |= r7   ///  r8 = r8.or(r7)
    or64 r8, r2                                     r8 |= r2   ///  r8 = r8.or(r2)
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    jne r8, 0, lbb_16783                            if r8 != (0 as i32 as i64 as u64) { pc += 9 }
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jeq r3, 0, lbb_16789                            if r3 == (0 as i32 as i64 as u64) { pc += 13 }
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    mov64 r2, r0                                    r2 = r0
    call function_23173                     
    ja lbb_16789                                    if true { pc += 6 }
lbb_16783:
    lddw r2, 0x3f8000000000001a                     r2 load str located at 4575657221408423962
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    mov64 r0, r1                                    r0 = r1
    jne r3, 0, lbb_16789                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_16792                                    if true { pc += 3 }
lbb_16789:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    or64 r0, 26                                     r0 |= 26   ///  r0 = r0.or(26)
    mov64 r2, r0                                    r2 = r0
lbb_16792:
    mov64 r0, r2                                    r0 = r2
    exit                                    
lbb_16794:
    ldxdw r2, [r10-0x8]                     
    ldxw r2, [r2+0x3088]                    
    mov64 r6, r2                                    r6 = r2
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    arsh64 r6, 32                                   r6 >>= 32 (signed)   ///  r6 = (r6 as i64).wrapping_shr(32)
    mov64 r0, r2                                    r0 = r2
    and64 r0, 2147483647                            r0 &= 2147483647   ///  r0 = r0.and(2147483647)
    mov64 r7, r0                                    r7 = r0
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r7, 8388607, lbb_16807                      if r7 < (8388607 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_16807:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jslt r6, 0, lbb_16810                           if (r6 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_16810:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jsgt r0, 2139095040, lbb_16813                  if (r0 as i64) > (2139095040 as i32 as i64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_16813:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jeq r0, 2139095040, lbb_16816                   if r0 == (2139095040 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_16816:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jeq r0, 0, lbb_16819                            if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_16819:
    add64 r0, -8388608                              r0 += -8388608   ///  r0 = r0.wrapping_add(-8388608 as i32 as i64 as u64)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jlt r0, 2130706432, lbb_16824                   if r0 < (2130706432 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_16824:
    and64 r4, r9                                    r4 &= r9   ///  r4 = r4.and(r9)
    and64 r5, r9                                    r5 &= r9   ///  r5 = r5.and(r9)
    or64 r8, r5                                     r8 |= r5   ///  r8 = r8.or(r5)
    or64 r8, r7                                     r8 |= r7   ///  r8 = r8.or(r7)
    or64 r8, r6                                     r8 |= r6   ///  r8 = r8.or(r6)
    or64 r8, r4                                     r8 |= r4   ///  r8 = r8.or(r4)
    mov64 r4, r8                                    r4 = r8
    xor64 r4, 1                                     r4 ^= 1   ///  r4 = r4.xor(1)
    mov64 r0, r1                                    r0 = r1
    jne r8, 0, lbb_16835                            if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, r2                                    r0 = r2
lbb_16835:
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    ldxdw r6, [r10-0x10]                    
    jne r8, 0, lbb_16438                            if r8 != (0 as i32 as i64 as u64) { pc += -401 }
    xor64 r3, 1                                     r3 ^= 1   ///  r3 = r3.xor(1)
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_16438                            if r3 != (0 as i32 as i64 as u64) { pc += -404 }
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    call function_23173                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ja lbb_16438                                    if true { pc += -411 }

function_16849:
    mov64 r0, r3                                    r0 = r3
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r4, [r7+0x250]                    
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    mov64 r5, r4                                    r5 = r4
    arsh64 r5, 63                                   r5 >>= 63 (signed)   ///  r5 = (r5 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, r0                                    r2 = r0
    call function_25903                     
    ldxdw r1, [r10-0x10]                    
    mov64 r3, r1                                    r3 = r1
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r4, [r10-0x8]                     
    jne r4, r3, lbb_16867                           if r4 != r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_16867:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_16879                            if r2 != (0 as i32 as i64 as u64) { pc += 10 }
    ldxdw r2, [r7+0x258]                    
    jeq r2, 0, lbb_16879                            if r2 == (0 as i32 as i64 as u64) { pc += 8 }
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    jne r1, r3, lbb_16875                           if r1 != r3 { pc += 1 }
    jeq r2, -1, lbb_16879                           if r2 == (-1 as i32 as i64 as u64) { pc += 4 }
lbb_16875:
    call function_26065                     
    stxdw [r6+0x8], r0                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_16883                                    if true { pc += 4 }
lbb_16879:
    lddw r1, 0x3300000000                           r1 load str located at 219043332096
    stxdw [r6+0x4], r1                      
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_16883:
    stxw [r6+0x0], r1                       
    exit                                    

function_16885:
    mov64 r7, r5                                    r7 = r5
    stxdw [r10-0x70], r4                    
    mov64 r8, r3                                    r8 = r3
    mov64 r9, r2                                    r9 = r2
    stxdw [r10-0x68], r1                    
    stdw [r10-0x1000], 0                    
    ldxdw r6, [r7-0xff0]                    
    ldxdw r3, [r7-0x1000]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r6                                    r2 = r6
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_18816                     
    ldxdw r1, [r10-0x18]                    
    ldxdw r2, [r10-0x20]                    
    jne r2, 0, lbb_16937                            if r2 != (0 as i32 as i64 as u64) { pc += 35 }
    ldxdw r2, [r10-0x70]                    
    stxdw [r10-0x78], r8                    
    ldxdw r7, [r7-0xff8]                    
    jslt r7, 0, lbb_16941                           if (r7 as i64) < (0 as i32 as i64) { pc += 35 }
    jslt r1, 0, lbb_16941                           if (r1 as i64) < (0 as i32 as i64) { pc += 34 }
    sub64 r7, r1                                    r7 -= r1   ///  r7 = r7.wrapping_sub(r1)
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 2                                     r2 = 2 as i32 as i64 as u64
    call function_26065                     
    ldxdw r1, [r10-0x70]                    
    jne r1, 0, lbb_16914                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_16949                                    if true { pc += 35 }
lbb_16914:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jsgt r7, 1, lbb_16917                           if (r7 as i64) > (1 as i32 as i64) { pc += 1 }
    ja lbb_16952                                    if true { pc += 35 }
lbb_16917:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r7, r9                                    r7 = r9
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r0                                    r3 = r0
    call function_16849                     
    ldxw r1, [r10-0x10]                     
    jeq r1, 0, lbb_16926                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_16960                                    if true { pc += 34 }
lbb_16926:
    ldxdw r0, [r10-0x8]                     
lbb_16927:
    ldxdw r1, [r7+0x260]                    
    ldxdw r3, [r7+0x268]                    
    neg64 r3                                        r3 = -r3   ///  r3 = (r3 as i64).wrapping_neg() as u64
    jsge r1, r3, lbb_17023                          if (r1 as i64) >= (r3 as i64) { pc += 92 }
    lddw r1, 0x100033603 --> b"assertion failed: min <= max"        r1 load str located at 4295177731
    mov64 r2, 28                                    r2 = 28 as i32 as i64 as u64
    lddw r3, 0x100034940 --> b"\x00\x00\x00\x00\x1f6\x03\x00P\x00\x00\x00\x00\x00\x00\x00\x8b\x03\x00\x0…        r3 load str located at 4295182656
    call function_20650                     
lbb_16937:
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_19150                     
lbb_16941:
    lddw r1, 0x3400000000                           r1 load str located at 223338299392
lbb_16943:
    ldxdw r2, [r10-0x68]                    
    stxdw [r2+0x4], r1                      
    mov64 r1, r2                                    r1 = r2
lbb_16946:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
lbb_16947:
    stxw [r1+0x0], r7                       
    exit                                    
lbb_16949:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jslt r7, -1, lbb_16952                          if (r7 as i64) < (-1 as i32 as i64) { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_16952:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r7, r9                                    r7 = r9
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r0                                    r3 = r0
    call function_16849                     
    ldxw r1, [r10-0x10]                     
    jeq r1, 0, lbb_16967                            if r1 == (0 as i32 as i64 as u64) { pc += 7 }
lbb_16960:
    ldxw r1, [r10-0x8]                      
    ldxw r2, [r10-0xc]                      
    ldxdw r3, [r10-0x68]                    
    stxw [r3+0x8], r1                       
    mov64 r1, r3                                    r1 = r3
    stxw [r1+0x4], r2                       
    ja lbb_16946                                    if true { pc += -21 }
lbb_16967:
    ldxdw r0, [r10-0x8]                     
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    jne r8, 0, lbb_16927                            if r8 != (0 as i32 as i64 as u64) { pc += -43 }
    mov64 r8, r0                                    r8 = r0
    ldxdw r2, [r7+0x288]                    
    ldxdw r3, [r10-0x78]                    
    mov64 r1, r3                                    r1 = r3
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jgt r1, r3, lbb_16979                           if r1 > r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_16979:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jne r2, 0, lbb_16982                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r1                                    r4 = r1
lbb_16982:
    jslt r4, 100, lbb_16984                         if (r4 as i64) < (100 as i32 as i64) { pc += 1 }
    mov64 r4, 100                                   r4 = 100 as i32 as i64 as u64
lbb_16984:
    ldxdw r2, [r9+0x278]                    
    mov64 r5, r4                                    r5 = r4
    arsh64 r5, 63                                   r5 >>= 63 (signed)   ///  r5 = (r5 as i64).wrapping_shr(63)
    mov64 r3, r2                                    r3 = r2
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_25903                     
    ldxdw r4, [r10-0x30]                    
    mov64 r1, r4                                    r1 = r4
    arsh64 r1, 63                                   r1 >>= 63 (signed)   ///  r1 = (r1 as i64).wrapping_shr(63)
    ldxdw r2, [r10-0x28]                    
    jne r2, r1, lbb_16998                           if r2 != r1 { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_16998:
    and64 r7, 1                                     r7 &= 1   ///  r7 = r7.and(1)
    jne r7, 0, lbb_17030                            if r7 != (0 as i32 as i64 as u64) { pc += 30 }
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r2                                    r3 = r2
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    mov64 r5, r4                                    r5 = r4
    arsh64 r5, 63                                   r5 >>= 63 (signed)   ///  r5 = (r5 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    call function_25903                     
    ldxdw r1, [r10-0x40]                    
    mov64 r3, r1                                    r3 = r1
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r4, [r10-0x38]                    
    jne r4, r3, lbb_17015                           if r4 != r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_17015:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    mov64 r7, r9                                    r7 = r9
    jne r2, 0, lbb_17030                            if r2 != (0 as i32 as i64 as u64) { pc += 12 }
    mov64 r2, 100                                   r2 = 100 as i32 as i64 as u64
    call function_26065                     
    jsgt r8, 0, lbb_17059                           if (r8 as i64) > (0 as i32 as i64) { pc += 38 }
    jsgt r0, r8, lbb_16927                          if (r0 as i64) > (r8 as i64) { pc += -95 }
    ja lbb_17060                                    if true { pc += 37 }
lbb_17023:
    mov64 r2, r0                                    r2 = r0
    jslt r0, r1, lbb_17026                          if (r0 as i64) < (r1 as i64) { pc += 1 }
    mov64 r2, r1                                    r2 = r1
lbb_17026:
    jslt r0, r3, lbb_17028                          if (r0 as i64) < (r3 as i64) { pc += 1 }
    mov64 r3, r2                                    r3 = r2
lbb_17028:
    jslt r3, -10000000, lbb_17030                   if (r3 as i64) < (-10000000 as i32 as i64) { pc += 1 }
    ja lbb_17033                                    if true { pc += 3 }
lbb_17030:
    lddw r1, 0x3000000000                           r1 load str located at 206158430208
    ja lbb_16943                                    if true { pc += -90 }
lbb_17033:
    mov64 r2, r3                                    r2 = r3
    add64 r2, 10000000                              r2 += 10000000   ///  r2 = r2.wrapping_add(10000000 as i32 as i64 as u64)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r2, r3, lbb_17038                           if r2 < r3 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_17038:
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    ldxdw r4, [r6+0x8]                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_25903                     
    ldxdw r3, [r10-0x48]                    
    jgt r3, 9999999, lbb_17030                      if r3 > (9999999 as i32 as i64 as u64) { pc += -17 }
    ldxdw r2, [r10-0x50]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r4, 10000000                              r4 = 10000000 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_24950                     
    ldxdw r1, [r10-0x60]                    
    ldxdw r2, [r10-0x68]                    
    stxdw [r2+0x8], r1                      
    mov64 r1, r2                                    r1 = r2
    ja lbb_16947                                    if true { pc += -112 }
lbb_17059:
    jslt r0, r8, lbb_16927                          if (r0 as i64) < (r8 as i64) { pc += -133 }
lbb_17060:
    mov64 r0, r8                                    r0 = r8
    ja lbb_16927                                    if true { pc += -135 }

function_17062:
    mov64 r8, r3                                    r8 = r3
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r7                                    r1 = r7
    add64 r1, 24                                    r1 += 24   ///  r1 = r1.wrapping_add(24 as i32 as i64 as u64)
    mov64 r2, -1                                    r2 = -1 as i32 as i64 as u64
    call function_17130                     
    jne r0, 0, lbb_17075                            if r0 != (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, r7                                    r1 = r7
    add64 r1, 160                                   r1 += 160   ///  r1 = r1.wrapping_add(160 as i32 as i64 as u64)
    mov64 r2, -1                                    r2 = -1 as i32 as i64 as u64
    call function_17130                     
    jeq r0, 0, lbb_17082                            if r0 == (0 as i32 as i64 as u64) { pc += 7 }
lbb_17075:
    stxdw [r10-0x8], r0                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -8                                    r1 += -8   ///  r1 = r1.wrapping_add(-8 as i32 as i64 as u64)
    call function_19150                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 19                                    r2 = 19 as i32 as i64 as u64
    ja lbb_17113                                    if true { pc += 31 }
lbb_17082:
    mov64 r1, r7                                    r1 = r7
    add64 r1, 296                                   r1 += 296   ///  r1 = r1.wrapping_add(296 as i32 as i64 as u64)
    mov64 r2, 100000                                r2 = 100000 as i32 as i64 as u64
    call function_17130                     
    jeq r0, 0, lbb_17094                            if r0 == (0 as i32 as i64 as u64) { pc += 7 }
    stxdw [r10-0x8], r0                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -8                                    r1 += -8   ///  r1 = r1.wrapping_add(-8 as i32 as i64 as u64)
    call function_19150                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 20                                    r2 = 20 as i32 as i64 as u64
    ja lbb_17113                                    if true { pc += 19 }
lbb_17094:
    mov64 r1, r7                                    r1 = r7
    add64 r1, 432                                   r1 += 432   ///  r1 = r1.wrapping_add(432 as i32 as i64 as u64)
    mov64 r2, -1                                    r2 = -1 as i32 as i64 as u64
    call function_17130                     
    jeq r0, 0, lbb_17106                            if r0 == (0 as i32 as i64 as u64) { pc += 7 }
    stxdw [r10-0x8], r0                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -8                                    r1 += -8   ///  r1 = r1.wrapping_add(-8 as i32 as i64 as u64)
    call function_19150                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 25                                    r2 = 25 as i32 as i64 as u64
    ja lbb_17113                                    if true { pc += 7 }
lbb_17106:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r2, 21                                    r2 = 21 as i32 as i64 as u64
    ldxdw r3, [r7+0x250]                    
    jeq r3, 0, lbb_17113                            if r3 == (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r2, 22                                    r2 = 22 as i32 as i64 as u64
    ldxdw r3, [r7+0x258]                    
    jne r3, 0, lbb_17116                            if r3 != (0 as i32 as i64 as u64) { pc += 3 }
lbb_17113:
    stxw [r6+0x4], r2                       
    stxw [r6+0x0], r1                       
    exit                                    
lbb_17116:
    ldxw r1, [r8+0x240]                     
    stxw [r7+0x240], r1                     
    ldxw r1, [r8+0x244]                     
    stxw [r7+0x244], r1                     
    ldxdw r1, [r8+0x8]                      
    stxdw [r7+0x8], r1                      
    ldxdw r1, [r8+0x10]                     
    stxdw [r7+0x10], r1                     
    ldxdw r1, [r8+0x280]                    
    stxdw [r7+0x280], r1                    
    ldxdw r1, [r8+0x288]                    
    stxdw [r7+0x288], r1                    
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ja lbb_17113                                    if true { pc += -17 }

function_17130:
    ldxdw r3, [r1+0x80]                     
    mov64 r4, r3                                    r4 = r3
    add64 r4, -9                                    r4 += -9   ///  r4 = r4.wrapping_add(-9 as i32 as i64 as u64)
    jgt r4, -9, lbb_17152                           if r4 > (-9 as i32 as i64 as u64) { pc += 18 }
    lddw r1, 0x100034c78 --> b"\x00\x00\x00\x00\x858\x03\x00)\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295183480
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x10002cf40 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295151424
    stxdw [r10-0x8], r1                     
    lddw r1, 0x100033798 --> b"\x08\x00\x00\x00\x00\x00\x00\x00 (bytes  or lessprogram-api/solfi-v2-api/…        r1 load str located at 4295178136
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 2                      
    stdw [r10-0x28], 1                      
    mov64 r6, r10                                   r6 = r10
    add64 r6, -120                                  r6 += -120   ///  r6 = r6.wrapping_add(-120 as i32 as i64 as u64)
    ja lbb_17205                                    if true { pc += 53 }
lbb_17152:
    ldxdw r4, [r1+0x0]                      
    jne r4, 0, lbb_17177                            if r4 != (0 as i32 as i64 as u64) { pc += 23 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    jeq r3, 0, lbb_17213                            if r3 == (0 as i32 as i64 as u64) { pc += 56 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    add64 r1, 72                                    r1 += 72   ///  r1 = r1.wrapping_add(72 as i32 as i64 as u64)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ja lbb_17171                                    if true { pc += 10 }
lbb_17161:
    ldxdw r0, [r1-0x8]                      
    ldxdw r6, [r1+0x0]                      
    jle r6, r0, lbb_17174                           if r6 <= r0 { pc += 10 }
    jgt r0, r2, lbb_17188                           if r0 > r2 { pc += 23 }
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r6, r5                                    r6 = r5
    jlt r4, r3, lbb_17171                           if r4 < r3 { pc += 1 }
    ja lbb_17213                                    if true { pc += 42 }
lbb_17171:
    stxdw [r10-0x60], r4                    
    ldxdw r5, [r1-0x40]                     
    jgt r5, r6, lbb_17161                           if r5 > r6 { pc += -13 }
lbb_17174:
    lddw r1, 0x100034c58 --> b"\x00\x00\x00\x0098\x03\x007\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00p8…        r1 load str located at 4295183448
    ja lbb_17179                                    if true { pc += 2 }
lbb_17177:
    lddw r1, 0x100034c68 --> b"\x00\x00\x00\x00p8\x03\x00\x15\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295183464
lbb_17179:
    stxdw [r10-0x40], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 0                      
    stdw [r10-0x30], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    call function_8945                      
    ja lbb_17213                                    if true { pc += 25 }
lbb_17188:
    lddw r1, 0x100034c38 --> b"\x00\x00\x00\x00\x138\x03\x00\x09\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295183416
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x10002cf40 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295151424
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 2                      
    stdw [r10-0x28], 1                      
    mov64 r6, r10                                   r6 = r10
    add64 r6, -88                                   r6 += -88   ///  r6 = r6.wrapping_add(-88 as i32 as i64 as u64)
lbb_17205:
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    call function_20113                     
    call function_19908                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r0                                    r2 = r0
    call function_8918                      
lbb_17213:
    exit                                    

function_17214:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r3, [r1+0x80]                     
    jeq r3, 0, lbb_17285                            if r3 == (0 as i32 as i64 as u64) { pc += 68 }
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    jeq r3, 0, lbb_17279                            if r3 == (0 as i32 as i64 as u64) { pc += 60 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r6, [r1+0x0]                      
    jge r6, r2, lbb_17282                           if r6 >= r2 { pc += 60 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ldxdw r7, [r1+0x8]                      
    mov64 r5, r7                                    r5 = r7
    jgt r7, r2, lbb_17286                           if r7 > r2 { pc += 60 }
    jeq r3, 1, lbb_17279                            if r3 == (1 as i32 as i64 as u64) { pc += 52 }
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jge r7, r2, lbb_17282                           if r7 >= r2 { pc += 53 }
    mov64 r0, 2                                     r0 = 2 as i32 as i64 as u64
    ldxdw r8, [r1+0x10]                     
    mov64 r5, r8                                    r5 = r8
    mov64 r6, r7                                    r6 = r7
    jgt r8, r2, lbb_17286                           if r8 > r2 { pc += 52 }
    jeq r3, 2, lbb_17279                            if r3 == (2 as i32 as i64 as u64) { pc += 44 }
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    jge r8, r2, lbb_17282                           if r8 >= r2 { pc += 45 }
    mov64 r0, 3                                     r0 = 3 as i32 as i64 as u64
    ldxdw r7, [r1+0x18]                     
    mov64 r5, r7                                    r5 = r7
    mov64 r6, r8                                    r6 = r8
    jgt r7, r2, lbb_17286                           if r7 > r2 { pc += 44 }
    jeq r3, 3, lbb_17279                            if r3 == (3 as i32 as i64 as u64) { pc += 36 }
    mov64 r4, 3                                     r4 = 3 as i32 as i64 as u64
    jge r7, r2, lbb_17282                           if r7 >= r2 { pc += 37 }
    mov64 r0, 4                                     r0 = 4 as i32 as i64 as u64
    ldxdw r8, [r1+0x20]                     
    mov64 r5, r8                                    r5 = r8
    mov64 r6, r7                                    r6 = r7
    jgt r8, r2, lbb_17286                           if r8 > r2 { pc += 36 }
    jeq r3, 4, lbb_17279                            if r3 == (4 as i32 as i64 as u64) { pc += 28 }
    mov64 r4, 4                                     r4 = 4 as i32 as i64 as u64
    jge r8, r2, lbb_17282                           if r8 >= r2 { pc += 29 }
    mov64 r0, 5                                     r0 = 5 as i32 as i64 as u64
    ldxdw r7, [r1+0x28]                     
    mov64 r5, r7                                    r5 = r7
    mov64 r6, r8                                    r6 = r8
    jgt r7, r2, lbb_17286                           if r7 > r2 { pc += 28 }
    jeq r3, 5, lbb_17279                            if r3 == (5 as i32 as i64 as u64) { pc += 20 }
    mov64 r4, 5                                     r4 = 5 as i32 as i64 as u64
    jge r7, r2, lbb_17282                           if r7 >= r2 { pc += 21 }
    mov64 r0, 6                                     r0 = 6 as i32 as i64 as u64
    ldxdw r8, [r1+0x30]                     
    mov64 r5, r8                                    r5 = r8
    mov64 r6, r7                                    r6 = r7
    jgt r8, r2, lbb_17286                           if r8 > r2 { pc += 20 }
    jeq r3, 6, lbb_17279                            if r3 == (6 as i32 as i64 as u64) { pc += 12 }
    mov64 r4, 6                                     r4 = 6 as i32 as i64 as u64
    jge r8, r2, lbb_17282                           if r8 >= r2 { pc += 13 }
    mov64 r0, 7                                     r0 = 7 as i32 as i64 as u64
    ldxdw r5, [r1+0x38]                     
    mov64 r6, r8                                    r6 = r8
    jgt r5, r2, lbb_17286                           if r5 > r2 { pc += 13 }
    jeq r3, 7, lbb_17279                            if r3 == (7 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    lddw r3, 0x100034c98 --> b"\x00\x00\x00\x00\xe37\x03\x000\x00\x00\x00\x00\x00\x00\x00;\x00\x00\x00\x…        r3 load str located at 4295183512
    call function_20679                     
lbb_17279:
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    ja lbb_17284                                    if true { pc += 2 }
lbb_17282:
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
lbb_17284:
    ldxdw r0, [r1+0x40]                     
lbb_17285:
    exit                                    
lbb_17286:
    lsh64 r0, 3                                     r0 <<= 3   ///  r0 = r0.wrapping_shl(3)
    add64 r1, 64                                    r1 += 64   ///  r1 = r1.wrapping_add(64 as i32 as i64 as u64)
    mov64 r3, r1                                    r3 = r1
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    ldxdw r1, [r1+0x0]                      
    ldxdw r0, [r3+0x0]                      
    sub64 r0, r1                                    r0 -= r1   ///  r0 = r0.wrapping_sub(r1)
    sub64 r2, r6                                    r2 -= r6   ///  r2 = r2.wrapping_sub(r6)
    mul64 r0, r2                                    r0 *= r2   ///  r0 = r0.wrapping_mul(r2)
    sub64 r5, r6                                    r5 -= r6   ///  r5 = r5.wrapping_sub(r6)
    mov64 r2, r5                                    r2 = r5
    rsh64 r2, 1                                     r2 >>= 1   ///  r2 = r2.wrapping_shr(1)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    div64 r0, r5                                    r0 /= r5   ///  r0 = r0 / r5
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
    ja lbb_17285                                    if true { pc += -19 }

function_17304:
    ldxb r2, [r1+0x0]                       
    xor64 r2, 168                                   r2 ^= 168   ///  r2 = r2.xor(168)
    ldxb r0, [r1+0x1]                       
    xor64 r0, 181                                   r0 ^= 181   ///  r0 = r0.xor(181)
    lsh64 r0, 8                                     r0 <<= 8   ///  r0 = r0.wrapping_shl(8)
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    ldxb r2, [r1+0x2]                       
    xor64 r2, 169                                   r2 ^= 169   ///  r2 = r2.xor(169)
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    ldxb r2, [r1+0x3]                       
    xor64 r2, 173                                   r2 ^= 173   ///  r2 = r2.xor(173)
    lsh64 r2, 24                                    r2 <<= 24   ///  r2 = r2.wrapping_shl(24)
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    ldxb r2, [r1+0x4]                       
    xor64 r2, 112                                   r2 ^= 112   ///  r2 = r2.xor(112)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    ldxb r2, [r1+0x5]                       
    xor64 r2, 91                                    r2 ^= 91   ///  r2 = r2.xor(91)
    lsh64 r2, 40                                    r2 <<= 40   ///  r2 = r2.wrapping_shl(40)
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    ldxb r2, [r1+0x6]                       
    xor64 r2, 91                                    r2 ^= 91   ///  r2 = r2.xor(91)
    lsh64 r2, 48                                    r2 <<= 48   ///  r2 = r2.wrapping_shl(48)
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    ldxb r1, [r1+0x7]                       
    xor64 r1, 251                                   r1 ^= 251   ///  r1 = r1.xor(251)
    lsh64 r1, 56                                    r1 <<= 56   ///  r1 = r1.wrapping_shl(56)
    or64 r0, r1                                     r0 |= r1   ///  r0 = r0.or(r1)
    exit                                    

function_17335:
    ldxdw r3, [r1+0x0]                      
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    jlt r3, 34, lbb_17341                           if r3 < (34 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 31                                    r3 = 31 as i32 as i64 as u64
lbb_17341:
    mov64 r2, 16                                    r2 = 16 as i32 as i64 as u64
    jsgt r3, 6, lbb_17345                           if (r3 as i64) > (6 as i32 as i64) { pc += 2 }
    jsgt r3, 3, lbb_17360                           if (r3 as i64) > (3 as i32 as i64) { pc += 16 }
    ja lbb_17363                                    if true { pc += 18 }
lbb_17345:
    mov64 r4, r3                                    r4 = r3
    add64 r4, -7                                    r4 += -7   ///  r4 = r4.wrapping_add(-7 as i32 as i64 as u64)
    jlt r4, 16, lbb_17363                           if r4 < (16 as i32 as i64 as u64) { pc += 15 }
    jgt r3, 33, lbb_17369                           if r3 > (33 as i32 as i64 as u64) { pc += 20 }
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    lsh64 r4, r3                                    r4 <<= r3   ///  r4 = r4.wrapping_shl(r3 as u32)
    lddw r5, 0x37d000000                            r5 load str located at 14982053888
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    jne r4, 0, lbb_17363                            if r4 != (0 as i32 as i64 as u64) { pc += 8 }
    jeq r3, 25, lbb_17362                           if r3 == (25 as i32 as i64 as u64) { pc += 6 }
    jeq r3, 31, lbb_17358                           if r3 == (31 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17369                                    if true { pc += 11 }
lbb_17358:
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    ja lbb_17363                                    if true { pc += 3 }
lbb_17360:
    jeq r3, 4, lbb_17362                            if r3 == (4 as i32 as i64 as u64) { pc += 1 }
    jeq r3, 5, lbb_17362                            if r3 == (5 as i32 as i64 as u64) { pc += 0 }
lbb_17362:
    mov64 r2, 17                                    r2 = 17 as i32 as i64 as u64
lbb_17363:
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    ldxb r1, [r1+0x0]                       
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_17368                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_17368:
    exit                                    
lbb_17369:
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    ja lbb_17363                                    if true { pc += -8 }

function_17371:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -33                                   r1 += -33   ///  r1 = r1.wrapping_add(-33 as i32 as i64 as u64)
    stxdw [r10-0x38], r1                    
    stxdw [r10-0x48], r3                    
    stxdw [r10-0x58], r2                    
    lddw r1, 0x1000338ae --> b"marketUnexpected variant index: src/lib.rsUnable t"        r1 load str located at 4295178414
    stxdw [r10-0x68], r1                    
    stxb [r10-0x21], r4                     
    stdw [r10-0x30], 1                      
    stdw [r10-0x40], 32                     
    stdw [r10-0x50], 32                     
    stdw [r10-0x60], 6                      
    lddw r1, 0x961634be972aba81                     r1 load str located at -7631854525348136319
    stxdw [r10-0x8], r1                     
    lddw r1, 0x27a193550384248c                     r1 load str located at 2855725632070100108
    stxdw [r10-0x10], r1                    
    lddw r1, 0xfe9c3fe63cdd3704                     r1 load str located at -100134833612835068
    stxdw [r10-0x18], r1                    
    lddw r1, 0xf4c9fbb437348706                     r1 load str located at -807837906697419002
    stxdw [r10-0x20], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -137                                  r1 += -137   ///  r1 = r1.wrapping_add(-137 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -104                                  r2 += -104   ///  r2 = r2.wrapping_add(-104 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -32                                   r4 += -32   ///  r4 = r4.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
    call function_18258                     
    ldxdw r1, [r10-0x71]                    
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x79]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x81]                    
    stxdw [r6+0x8], r1                      
    ldxdw r1, [r10-0x89]                    
    stxdw [r6+0x0], r1                      
    ldxb r1, [r10-0x69]                     
    stxb [r6+0x20], r1                      
    exit                                    

function_17416:
    mov64 r6, r1                                    r6 = r1
    stxdw [r10-0x30], r3                    
    stxdw [r10-0x40], r4                    
    stxdw [r10-0x50], r2                    
    stdw [r10-0x28], 32                     
    stdw [r10-0x38], 32                     
    stdw [r10-0x48], 32                     
    lddw r1, 0x59f8e9dbd87b8e04                     r1 load str located at 6483188794038914564
    stxdw [r10-0x8], r1                     
    lddw r1, 0x8410ffda99135a0b                     r1 load str located at -8930356746739557877
    stxdw [r10-0x10], r1                    
    lddw r1, 0x830d8e1429103dbb                     r1 load str located at -9003383862804333125
    stxdw [r10-0x18], r1                    
    lddw r1, 0xf189244e8f25978c                     r1 load str located at -1042261918931904628
    stxdw [r10-0x20], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -113                                  r1 += -113   ///  r1 = r1.wrapping_add(-113 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -80                                   r2 += -80   ///  r2 = r2.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -32                                   r4 += -32   ///  r4 = r4.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, 3                                     r3 = 3 as i32 as i64 as u64
    call function_18258                     
    ldxdw r1, [r10-0x59]                    
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x61]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x69]                    
    stxdw [r6+0x8], r1                      
    ldxdw r1, [r10-0x71]                    
    stxdw [r6+0x0], r1                      
    ldxb r1, [r10-0x51]                     
    stxb [r6+0x20], r1                      
    exit                                    

function_17454:
    mov64 r6, r1                                    r6 = r1
    ldxdw r3, [r2+0x8]                      
    jeq r3, 0, lbb_17499                            if r3 == (0 as i32 as i64 as u64) { pc += 42 }
    ldxdw r1, [r2+0x0]                      
    ldxb r7, [r1+0x0]                       
    mov64 r4, r1                                    r4 = r1
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r2+0x0], r4                      
    mov64 r4, r3                                    r4 = r3
    add64 r4, -1                                    r4 += -1   ///  r4 = r4.wrapping_add(-1 as i32 as i64 as u64)
    stxdw [r2+0x8], r4                      
    stxb [r10-0x79], r7                     
    jsgt r7, 3, lbb_17505                           if (r7 as i64) > (3 as i32 as i64) { pc += 38 }
    jsgt r7, 1, lbb_17518                           if (r7 as i64) > (1 as i32 as i64) { pc += 50 }
    jeq r7, 0, lbb_17621                            if r7 == (0 as i32 as i64 as u64) { pc += 152 }
    jeq r7, 1, lbb_17471                            if r7 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17542                                    if true { pc += 71 }
lbb_17471:
    jeq r4, 0, lbb_17599                            if r4 == (0 as i32 as i64 as u64) { pc += 127 }
    mov64 r4, r3                                    r4 = r3
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r5, [r1+0x1]                       
    stxdw [r2+0x8], r4                      
    mov64 r4, r1                                    r4 = r1
    add64 r4, 2                                     r4 += 2   ///  r4 = r4.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r2+0x0], r4                      
    stxb [r10-0x59], r5                     
    jeq r5, 0, lbb_17597                            if r5 == (0 as i32 as i64 as u64) { pc += 116 }
lbb_17481:
    lddw r1, 0x100034cb0 --> b"\x00\x00\x00\x00\xb48\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295183536
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100010468 --> b"a#4\x00\x00\x00\x00\x00\xbf4\x00\x00\x00\x00\x00\x00W\x04\x00\x00\x10\x00…        r1 load str located at 4295033960
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    ja lbb_17559                                    if true { pc += 60 }
lbb_17499:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    stxdw [r6+0x8], r0                      
    stw [r6+0x0], 1                         
    ja lbb_17631                                    if true { pc += 126 }
lbb_17505:
    jsgt r7, 5, lbb_17540                           if (r7 as i64) > (5 as i32 as i64) { pc += 34 }
    jeq r7, 4, lbb_17566                            if r7 == (4 as i32 as i64 as u64) { pc += 59 }
    jeq r7, 5, lbb_17509                            if r7 == (5 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17542                                    if true { pc += 33 }
lbb_17509:
    jeq r4, 0, lbb_17599                            if r4 == (0 as i32 as i64 as u64) { pc += 89 }
    add64 r3, -2                                    r3 += -2   ///  r3 = r3.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r8, [r1+0x1]                       
    stxdw [r2+0x8], r3                      
    add64 r1, 2                                     r1 += 2   ///  r1 = r1.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r2+0x0], r1                      
    stxb [r10-0x59], r8                     
    jlt r8, 15, lbb_17532                           if r8 < (15 as i32 as i64 as u64) { pc += 15 }
    ja lbb_17481                                    if true { pc += -37 }
lbb_17518:
    jeq r7, 2, lbb_17577                            if r7 == (2 as i32 as i64 as u64) { pc += 58 }
    jeq r7, 3, lbb_17521                            if r7 == (3 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17542                                    if true { pc += 21 }
lbb_17521:
    jeq r4, 0, lbb_17599                            if r4 == (0 as i32 as i64 as u64) { pc += 77 }
    add64 r3, -2                                    r3 += -2   ///  r3 = r3.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r4, [r1+0x1]                       
    stxdw [r2+0x8], r3                      
    add64 r1, 2                                     r1 += 2   ///  r1 = r1.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r2+0x0], r1                      
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    stxb [r10-0x59], r4                     
    jeq r4, 0, lbb_17532                            if r4 == (0 as i32 as i64 as u64) { pc += 2 }
    jne r4, 1, lbb_17481                            if r4 != (1 as i32 as i64 as u64) { pc += -50 }
lbb_17531:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
lbb_17532:
    stxdw [r6+0x10], r2                     
    stxw [r6+0xc], r3                       
    stxw [r6+0x8], r9                       
    stxh [r6+0x6], r1                       
    stxb [r6+0x5], r8                       
    stxb [r6+0x4], r7                       
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_17630                                    if true { pc += 90 }
lbb_17540:
    jeq r7, 6, lbb_17588                            if r7 == (6 as i32 as i64 as u64) { pc += 47 }
    jeq r7, 7, lbb_17532                            if r7 == (7 as i32 as i64 as u64) { pc += -10 }
lbb_17542:
    lddw r1, 0x100034cb0 --> b"\x00\x00\x00\x00\xb48\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295183536
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100010468 --> b"a#4\x00\x00\x00\x00\x00\xbf4\x00\x00\x00\x00\x00\x00W\x04\x00\x00\x10\x00…        r1 load str located at 4295033960
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -121                                  r1 += -121   ///  r1 = r1.wrapping_add(-121 as i32 as i64 as u64)
    stxdw [r10-0x58], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -120                                  r7 += -120   ///  r7 = r7.wrapping_add(-120 as i32 as i64 as u64)
lbb_17559:
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_20113                     
    mov64 r1, r7                                    r1 = r7
    call function_8277                      
    ja lbb_17628                                    if true { pc += 62 }
lbb_17566:
    jeq r4, 0, lbb_17599                            if r4 == (0 as i32 as i64 as u64) { pc += 32 }
    add64 r3, -2                                    r3 += -2   ///  r3 = r3.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r4, [r1+0x1]                       
    stxdw [r2+0x8], r3                      
    add64 r1, 2                                     r1 += 2   ///  r1 = r1.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r2+0x0], r1                      
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    stxb [r10-0x59], r4                     
    jeq r4, 0, lbb_17532                            if r4 == (0 as i32 as i64 as u64) { pc += -43 }
    jne r4, 1, lbb_17481                            if r4 != (1 as i32 as i64 as u64) { pc += -95 }
    ja lbb_17531                                    if true { pc += -46 }
lbb_17577:
    jeq r4, 0, lbb_17599                            if r4 == (0 as i32 as i64 as u64) { pc += 21 }
    mov64 r4, r3                                    r4 = r3
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r5, [r1+0x1]                       
    stxdw [r2+0x8], r4                      
    mov64 r4, r1                                    r4 = r1
    add64 r4, 2                                     r4 += 2   ///  r4 = r4.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r2+0x0], r4                      
    stxb [r10-0x59], r5                     
    jeq r5, 0, lbb_17603                            if r5 == (0 as i32 as i64 as u64) { pc += 16 }
    ja lbb_17481                                    if true { pc += -107 }
lbb_17588:
    jeq r4, 0, lbb_17599                            if r4 == (0 as i32 as i64 as u64) { pc += 10 }
    add64 r3, -2                                    r3 += -2   ///  r3 = r3.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r8, [r1+0x1]                       
    stxdw [r2+0x8], r3                      
    add64 r1, 2                                     r1 += 2   ///  r1 = r1.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r2+0x0], r1                      
    stxb [r10-0x59], r8                     
    jlt r8, 15, lbb_17532                           if r8 < (15 as i32 as i64 as u64) { pc += -64 }
    ja lbb_17481                                    if true { pc += -116 }
lbb_17597:
    jlt r3, 10, lbb_17599                           if r3 < (10 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17604                                    if true { pc += 5 }
lbb_17599:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    ja lbb_17628                                    if true { pc += 25 }
lbb_17603:
    jlt r3, 10, lbb_17599                           if r3 < (10 as i32 as i64 as u64) { pc += -5 }
lbb_17604:
    ldxh r9, [r1+0x8]                       
    ldxw r8, [r1+0x2]                       
    ldxh r4, [r1+0x6]                       
    add64 r1, 10                                    r1 += 10   ///  r1 = r1.wrapping_add(10 as i32 as i64 as u64)
    stxdw [r2+0x0], r1                      
    add64 r3, -10                                   r3 += -10   ///  r3 = r3.wrapping_add(-10 as i32 as i64 as u64)
    stxdw [r2+0x8], r3                      
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    or64 r8, r4                                     r8 |= r4   ///  r8 = r8.or(r4)
    mov64 r3, r9                                    r3 = r9
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    lsh64 r9, 48                                    r9 <<= 48   ///  r9 = r9.wrapping_shl(48)
    or64 r9, r8                                     r9 |= r8   ///  r9 = r9.or(r8)
    mov64 r1, r8                                    r1 = r8
    rsh64 r1, 8                                     r1 >>= 8   ///  r1 = r1.wrapping_shr(8)
    rsh64 r9, 24                                    r9 >>= 24   ///  r9 = r9.wrapping_shr(24)
    ja lbb_17532                                    if true { pc += -89 }
lbb_17621:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r8, r2                                    r8 = r2
    call function_17655                     
    ldxw r1, [r10-0x40]                     
    jeq r1, 0, lbb_17632                            if r1 == (0 as i32 as i64 as u64) { pc += 5 }
lbb_17627:
    ldxdw r0, [r10-0x38]                    
lbb_17628:
    stxdw [r6+0x8], r0                      
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_17630:
    stxw [r6+0x0], r1                       
lbb_17631:
    exit                                    
lbb_17632:
    mov64 r2, r8                                    r2 = r8
    ldxdw r1, [r2+0x8]                      
    jeq r1, 0, lbb_17599                            if r1 == (0 as i32 as i64 as u64) { pc += -36 }
    ldxw r4, [r10-0x38]                     
    ldxw r9, [r10-0x3c]                     
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    ldxdw r3, [r2+0x0]                      
    ldxb r8, [r3+0x0]                       
    stxdw [r2+0x8], r1                      
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r2+0x0], r3                      
    stxb [r10-0x59], r8                     
    jlt r8, 6, lbb_17646                            if r8 < (6 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17481                                    if true { pc += -165 }
lbb_17646:
    stxdw [r10-0x88], r4                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    call function_17655                     
    ldxw r1, [r10-0x40]                     
    jne r1, 0, lbb_17627                            if r1 != (0 as i32 as i64 as u64) { pc += -25 }
    ldxdw r2, [r10-0x3c]                    
    ldxdw r3, [r10-0x88]                    
    ja lbb_17532                                    if true { pc += -123 }

function_17655:
    mov64 r6, r1                                    r6 = r1
    ldxdw r4, [r2+0x8]                      
    jeq r4, 0, lbb_17687                            if r4 == (0 as i32 as i64 as u64) { pc += 29 }
    ldxdw r3, [r2+0x0]                      
    ldxb r1, [r3+0x0]                       
    mov64 r5, r3                                    r5 = r3
    add64 r5, 1                                     r5 += 1   ///  r5 = r5.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r2+0x0], r5                      
    mov64 r5, r4                                    r5 = r4
    add64 r5, -1                                    r5 += -1   ///  r5 = r5.wrapping_add(-1 as i32 as i64 as u64)
    stxdw [r2+0x8], r5                      
    stxb [r10-0x79], r1                     
    jeq r1, 0, lbb_17725                            if r1 == (0 as i32 as i64 as u64) { pc += 57 }
    jeq r1, 1, lbb_17693                            if r1 == (1 as i32 as i64 as u64) { pc += 24 }
    lddw r1, 0x100034cb0 --> b"\x00\x00\x00\x00\xb48\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295183536
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100010468 --> b"a#4\x00\x00\x00\x00\x00\xbf4\x00\x00\x00\x00\x00\x00W\x04\x00\x00\x10\x00…        r1 load str located at 4295033960
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -121                                  r1 += -121   ///  r1 = r1.wrapping_add(-121 as i32 as i64 as u64)
    stxdw [r10-0x58], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -120                                  r7 += -120   ///  r7 = r7.wrapping_add(-120 as i32 as i64 as u64)
    ja lbb_17718                                    if true { pc += 31 }
lbb_17687:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    stxdw [r6+0x8], r0                      
    stw [r6+0x0], 1                         
    ja lbb_17744                                    if true { pc += 51 }
lbb_17693:
    jeq r5, 0, lbb_17727                            if r5 == (0 as i32 as i64 as u64) { pc += 33 }
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    ldxb r5, [r3+0x1]                       
    stxdw [r2+0x8], r4                      
    add64 r3, 2                                     r3 += 2   ///  r3 = r3.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r2+0x0], r3                      
    stxb [r10-0x59], r5                     
    jlt r5, 18, lbb_17739                           if r5 < (18 as i32 as i64 as u64) { pc += 38 }
    lddw r1, 0x100034cb0 --> b"\x00\x00\x00\x00\xb48\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295183536
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100010468 --> b"a#4\x00\x00\x00\x00\x00\xbf4\x00\x00\x00\x00\x00\x00W\x04\x00\x00\x10\x00…        r1 load str located at 4295033960
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
lbb_17718:
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_20113                     
    mov64 r1, r7                                    r1 = r7
    call function_8277                      
    ja lbb_17730                                    if true { pc += 5 }
lbb_17725:
    jlt r4, 5, lbb_17727                            if r4 < (5 as i32 as i64 as u64) { pc += 1 }
    ja lbb_17733                                    if true { pc += 6 }
lbb_17727:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
lbb_17730:
    stxdw [r6+0x8], r0                      
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ja lbb_17743                                    if true { pc += 10 }
lbb_17733:
    add64 r4, -5                                    r4 += -5   ///  r4 = r4.wrapping_add(-5 as i32 as i64 as u64)
    ldxw r0, [r3+0x1]                       
    stxdw [r2+0x8], r4                      
    add64 r3, 5                                     r3 += 5   ///  r3 = r3.wrapping_add(5 as i32 as i64 as u64)
    stxdw [r2+0x0], r3                      
    ja lbb_17739                                    if true { pc += 0 }
lbb_17739:
    stxw [r6+0x8], r0                       
    stxb [r6+0x5], r5                       
    stxb [r6+0x4], r1                       
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_17743:
    stxw [r6+0x0], r1                       
lbb_17744:
    exit                                    

function_17745:
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r2+0x8]                      
    jeq r1, 0, lbb_17785                            if r1 == (0 as i32 as i64 as u64) { pc += 37 }
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    ldxdw r4, [r2+0x0]                      
    ldxb r3, [r4+0x0]                       
    stxdw [r2+0x8], r1                      
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r2+0x0], r4                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxb [r10-0x59], r3                     
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jeq r3, 0, lbb_17792                            if r3 == (0 as i32 as i64 as u64) { pc += 34 }
    jeq r3, 1, lbb_17791                            if r3 == (1 as i32 as i64 as u64) { pc += 32 }
    lddw r1, 0x100034cb0 --> b"\x00\x00\x00\x00\xb48\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295183536
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100010468 --> b"a#4\x00\x00\x00\x00\x00\xbf4\x00\x00\x00\x00\x00\x00W\x04\x00\x00\x10\x00…        r1 load str located at 4295033960
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_20113                     
    mov64 r1, r7                                    r1 = r7
    call function_8277                      
    stxdw [r6+0x8], r0                      
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ja lbb_17793                                    if true { pc += 8 }
lbb_17785:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    stxdw [r6+0x8], r0                      
    stb [r6+0x0], 1                         
    ja lbb_17794                                    if true { pc += 3 }
lbb_17791:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_17792:
    stxb [r6+0x1], r2                       
lbb_17793:
    stxb [r6+0x0], r1                       
lbb_17794:
    exit                                    

function_17795:
    mov64 r6, r1                                    r6 = r1
    ldxdw r3, [r2+0x8]                      
    jlt r3, 4, lbb_17903                            if r3 < (4 as i32 as i64 as u64) { pc += 105 }
    ldxdw r4, [r2+0x0]                      
    ldxw r9, [r4+0x0]                       
    add64 r4, 4                                     r4 += 4   ///  r4 = r4.wrapping_add(4 as i32 as i64 as u64)
    add64 r3, -4                                    r3 += -4   ///  r3 = r3.wrapping_add(-4 as i32 as i64 as u64)
    stxdw [r2+0x8], r3                      
    stxdw [r2+0x0], r4                      
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jeq r9, 0, lbb_17927                            if r9 == (0 as i32 as i64 as u64) { pc += 119 }
    stxdw [r10-0x88], r4                    
    stxdw [r10-0xa0], r3                    
    stxdw [r10-0x98], r2                    
    stxdw [r10-0xa8], r6                    
    mov64 r6, r9                                    r6 = r9
    jlt r9, 2048, lbb_17815                         if r9 < (2048 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 2048                                  r6 = 2048 as i32 as i64 as u64
lbb_17815:
    mov64 r8, r6                                    r8 = r6
    lsh64 r8, 1                                     r8 <<= 1   ///  r8 = r8.wrapping_shl(1)
    mov64 r1, r8                                    r1 = r8
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_6742                      
    jeq r0, 0, lbb_17931                            if r0 == (0 as i32 as i64 as u64) { pc += 110 }
    stxdw [r10-0x70], r0                    
    stxdw [r10-0x78], r6                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    stdw [r10-0x68], 0                      
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0xa0]                    
    mov64 r2, r4                                    r2 = r4
    ldxdw r3, [r10-0x98]                    
    ldxdw r1, [r10-0x88]                    
    ja lbb_17844                                    if true { pc += 13 }
lbb_17831:
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
    mov64 r1, r0                                    r1 = r0
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    ldxdw r5, [r10-0x80]                    
    stxb [r1+0x1], r5                       
    stxb [r1+0x0], r7                       
    stxdw [r10-0x68], r8                    
    add64 r6, 2                                     r6 += 2   ///  r6 = r6.wrapping_add(2 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jgt r9, r1, lbb_17844                           if r9 > r1 { pc += 1 }
    ja lbb_17921                                    if true { pc += 77 }
lbb_17844:
    jeq r4, r6, lbb_17880                           if r4 == r6 { pc += 35 }
    ldxdw r1, [r10-0x88]                    
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    mov64 r5, r2                                    r5 = r2
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r7, [r1+0x0]                       
    stxdw [r3+0x8], r2                      
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r3+0x0], r1                      
    stxb [r10-0x59], r7                     
    jlt r7, 8, lbb_17879                            if r7 < (8 as i32 as i64 as u64) { pc += 24 }
    lddw r1, 0x100034cb0 --> b"\x00\x00\x00\x00\xb48\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295183536
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100010468 --> b"a#4\x00\x00\x00\x00\x00\xbf4\x00\x00\x00\x00\x00\x00W\x04\x00\x00\x10\x00…        r1 load str located at 4295033960
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -89                                   r1 += -89   ///  r1 = r1.wrapping_add(-89 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r7, r10                                   r7 = r10
    add64 r7, -88                                   r7 += -88   ///  r7 = r7.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    call function_20113                     
    mov64 r1, r7                                    r1 = r7
    call function_8277                      
    ja lbb_17907                                    if true { pc += 28 }
lbb_17879:
    jne r2, 0, lbb_17884                            if r2 != (0 as i32 as i64 as u64) { pc += 4 }
lbb_17880:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    ja lbb_17907                                    if true { pc += 23 }
lbb_17884:
    ldxb r2, [r1+0x0]                       
    stxdw [r10-0x80], r2                    
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r3+0x0], r1                      
    mov64 r2, r5                                    r2 = r5
    add64 r2, -2                                    r2 += -2   ///  r2 = r2.wrapping_add(-2 as i32 as i64 as u64)
    stxdw [r3+0x8], r2                      
    ldxdw r1, [r10-0x78]                    
    jne r8, r1, lbb_17831                           if r8 != r1 { pc += -62 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -120                                  r1 += -120   ///  r1 = r1.wrapping_add(-120 as i32 as i64 as u64)
    stxdw [r10-0x90], r2                    
    call function_8658                      
    ldxdw r2, [r10-0x90]                    
    ldxdw r1, [r10-0x88]                    
    ldxdw r4, [r10-0xa0]                    
    ldxdw r3, [r10-0x98]                    
    ldxdw r0, [r10-0x70]                    
    ja lbb_17831                                    if true { pc += -72 }
lbb_17903:
    lddw r1, 0x100034928 --> b"\x00\x00\x00\x00\xe85\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00%\x00\x00\x0…        r1 load str located at 4295182632
    call function_19705                     
    ja lbb_17916                                    if true { pc += 9 }
lbb_17907:
    ldxdw r2, [r10-0x78]                    
    jeq r2, 0, lbb_17915                            if r2 == (0 as i32 as i64 as u64) { pc += 6 }
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    ldxdw r1, [r10-0x70]                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r6, r0                                    r6 = r0
    call function_6768                      
    mov64 r0, r6                                    r0 = r6
lbb_17915:
    ldxdw r6, [r10-0xa8]                    
lbb_17916:
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    stxdw [r6+0x0], r1                      
    stxdw [r6+0x8], r0                      
    ja lbb_17930                                    if true { pc += 9 }
lbb_17921:
    ldxdw r0, [r10-0x70]                    
    ldxdw r1, [r10-0x78]                    
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    ldxdw r6, [r10-0xa8]                    
    jeq r1, r2, lbb_17916                           if r1 == r2 { pc += -11 }
lbb_17927:
    stxdw [r6+0x10], r8                     
    stxdw [r6+0x8], r0                      
    stxdw [r6+0x0], r1                      
lbb_17930:
    exit                                    
lbb_17931:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, r8                                    r2 = r8
    call function_20091                     

function_17934:
    stxdw [r10-0x228], r3                   
    mov64 r3, r2                                    r3 = r2
    mov64 r2, 11                                    r2 = 11 as i32 as i64 as u64
    stxdw [r10-0x1e8], r2                   
    ldxdw r2, [r3+0x0]                      
    ldxdw r0, [r2+0x0]                      
    ldxdw r8, [r3+0x8]                      
    ldxdw r2, [r8+0x0]                      
    ldxdw r5, [r3+0x10]                     
    stxdw [r10-0x1f0], r5                   
    ldxdw r6, [r5+0x0]                      
    ldxdw r5, [r3+0x18]                     
    stxdw [r10-0x200], r5                   
    ldxdw r7, [r5+0x0]                      
    ldxdw r5, [r3+0x20]                     
    stxdw [r10-0x210], r5                   
    ldxdw r5, [r5+0x0]                      
    ldxdw r3, [r3+0x28]                     
    stxdw [r10-0x220], r3                   
    ldxdw r3, [r3+0x0]                      
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x218], r3                   
    stxdw [r10-0x190], r3                   
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x208], r5                   
    stxdw [r10-0x1a0], r5                   
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x1f8], r7                   
    stxdw [r10-0x1b0], r7                   
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x1c0], r6                   
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x1d0], r2                   
    mov64 r5, r0                                    r5 = r0
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x1e0], r5                   
    sth [r10-0x188], 0                      
    sth [r10-0x198], 0                      
    sth [r10-0x1a8], 0                      
    sth [r10-0x1b8], 0                      
    sth [r10-0x1c8], 1                      
    sth [r10-0x1d8], 257                    
    stb [r10-0x179], 0                      
    ldxb r3, [r0+0x0]                       
    jne r3, 0, lbb_18255                            if r3 != (0 as i32 as i64 as u64) { pc += 276 }
    stxdw [r10-0x240], r4                   
    stxdw [r10-0x230], r1                   
    ldxb r1, [r0+0x1]                       
    stxdw [r10-0x238], r1                   
    ldxb r3, [r0+0x2]                       
    ldxb r7, [r0+0x3]                       
    ldxdw r9, [r0+0x50]                     
    mov64 r4, r0                                    r4 = r0
    add64 r4, 40                                    r4 += 40   ///  r4 = r4.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x158], r4                   
    mov64 r4, r0                                    r4 = r0
    add64 r4, 88                                    r4 += 88   ///  r4 = r4.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x160], r4                   
    stxdw [r10-0x168], r9                   
    add64 r0, 72                                    r0 += 72   ///  r0 = r0.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x170], r0                   
    stxdw [r10-0x178], r5                   
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x1e8], r1                   
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r7, 0, lbb_18001                            if r7 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_18001:
    stxb [r10-0x146], r4                    
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_18005                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_18005:
    stxb [r10-0x147], r4                    
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r1, [r10-0x238]                   
    jne r1, 0, lbb_18010                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_18010:
    stxb [r10-0x148], r4                    
    stdw [r10-0x150], 0                     
    ldxdw r7, [r8+0x0]                      
    mov64 r9, r7                                    r9 = r7
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    ldxdw r1, [r10-0x230]                   
    jne r0, 0, lbb_18255                            if r0 != (0 as i32 as i64 as u64) { pc += 233 }
    ldxb r2, [r7+0x0]                       
    mov64 r3, 11                                    r3 = 11 as i32 as i64 as u64
    stxdw [r10-0x1e8], r3                   
    jne r2, 0, lbb_18255                            if r2 != (0 as i32 as i64 as u64) { pc += 229 }
    ldxb r1, [r7+0x1]                       
    ldxb r2, [r7+0x2]                       
    ldxb r4, [r7+0x3]                       
    ldxdw r3, [r7+0x50]                     
    mov64 r5, r7                                    r5 = r7
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x120], r5                   
    mov64 r5, r7                                    r5 = r7
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x128], r5                   
    stxdw [r10-0x130], r3                   
    add64 r7, 72                                    r7 += 72   ///  r7 = r7.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x138], r7                   
    stxdw [r10-0x140], r9                   
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    stxdw [r10-0x1e8], r3                   
    jne r4, 0, lbb_18044                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_18044:
    stxb [r10-0x10e], r3                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_18048                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_18048:
    stxb [r10-0x10f], r3                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_18052                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_18052:
    stxb [r10-0x110], r2                    
    stdw [r10-0x118], 0                     
    ldxdw r1, [r10-0x1f0]                   
    ldxdw r9, [r1+0x0]                      
    mov64 r7, r9                                    r7 = r9
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    ldxdw r1, [r10-0x230]                   
    jne r0, 0, lbb_18255                            if r0 != (0 as i32 as i64 as u64) { pc += 189 }
    ldxb r2, [r9+0x0]                       
    and64 r2, 136                                   r2 &= 136   ///  r2 = r2.and(136)
    mov64 r3, 11                                    r3 = 11 as i32 as i64 as u64
    stxdw [r10-0x1e8], r3                   
    jne r2, 0, lbb_18255                            if r2 != (0 as i32 as i64 as u64) { pc += 184 }
    ldxb r1, [r9+0x1]                       
    ldxb r2, [r9+0x2]                       
    ldxb r4, [r9+0x3]                       
    ldxdw r3, [r9+0x50]                     
    mov64 r5, r9                                    r5 = r9
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xe8], r5                    
    mov64 r5, r9                                    r5 = r9
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xf0], r5                    
    stxdw [r10-0xf8], r3                    
    add64 r9, 72                                    r9 += 72   ///  r9 = r9.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x100], r9                   
    stxdw [r10-0x108], r7                   
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    stxdw [r10-0x1e8], r3                   
    jne r4, 0, lbb_18089                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_18089:
    stxb [r10-0xd6], r3                     
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_18093                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_18093:
    stxb [r10-0xd7], r3                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_18097                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_18097:
    stxb [r10-0xd8], r2                     
    stdw [r10-0xe0], 0                      
    ldxdw r1, [r10-0x200]                   
    ldxdw r9, [r1+0x0]                      
    mov64 r6, r9                                    r6 = r9
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x1f8]                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    ldxdw r1, [r10-0x230]                   
    jne r0, 0, lbb_18255                            if r0 != (0 as i32 as i64 as u64) { pc += 144 }
    ldxb r2, [r9+0x0]                       
    and64 r2, 136                                   r2 &= 136   ///  r2 = r2.and(136)
    mov64 r3, 11                                    r3 = 11 as i32 as i64 as u64
    stxdw [r10-0x1e8], r3                   
    jne r2, 0, lbb_18255                            if r2 != (0 as i32 as i64 as u64) { pc += 139 }
    ldxb r1, [r9+0x1]                       
    ldxb r2, [r9+0x2]                       
    ldxb r4, [r9+0x3]                       
    ldxdw r3, [r9+0x50]                     
    mov64 r5, r9                                    r5 = r9
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xb0], r5                    
    mov64 r5, r9                                    r5 = r9
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xb8], r5                    
    stxdw [r10-0xc0], r3                    
    add64 r9, 72                                    r9 += 72   ///  r9 = r9.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0xc8], r9                    
    stxdw [r10-0xd0], r6                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    stxdw [r10-0x1e8], r3                   
    jne r4, 0, lbb_18134                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_18134:
    stxb [r10-0x9e], r3                     
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_18138                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_18138:
    stxb [r10-0x9f], r3                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_18142                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_18142:
    stxb [r10-0xa0], r2                     
    stdw [r10-0xa8], 0                      
    ldxdw r1, [r10-0x210]                   
    ldxdw r8, [r1+0x0]                      
    mov64 r6, r8                                    r6 = r8
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x208]                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    ldxdw r1, [r10-0x230]                   
    jne r0, 0, lbb_18255                            if r0 != (0 as i32 as i64 as u64) { pc += 99 }
    ldxb r2, [r8+0x0]                       
    and64 r2, 136                                   r2 &= 136   ///  r2 = r2.and(136)
    mov64 r3, 11                                    r3 = 11 as i32 as i64 as u64
    stxdw [r10-0x1e8], r3                   
    jne r2, 0, lbb_18255                            if r2 != (0 as i32 as i64 as u64) { pc += 94 }
    ldxb r1, [r8+0x1]                       
    ldxb r2, [r8+0x2]                       
    ldxb r4, [r8+0x3]                       
    ldxdw r3, [r8+0x50]                     
    mov64 r5, r8                                    r5 = r8
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r5                    
    mov64 r5, r8                                    r5 = r8
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r5                    
    stxdw [r10-0x88], r3                    
    add64 r8, 72                                    r8 += 72   ///  r8 = r8.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r8                    
    stxdw [r10-0x98], r6                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    stxdw [r10-0x1e8], r3                   
    jne r4, 0, lbb_18179                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_18179:
    stxb [r10-0x66], r3                     
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_18183                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_18183:
    stxb [r10-0x67], r3                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_18187                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_18187:
    stxb [r10-0x68], r2                     
    stdw [r10-0x70], 0                      
    ldxdw r1, [r10-0x220]                   
    ldxdw r7, [r1+0x0]                      
    mov64 r6, r7                                    r6 = r7
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x218]                   
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    call function_23165                     
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    ldxdw r1, [r10-0x230]                   
    jne r0, 0, lbb_18255                            if r0 != (0 as i32 as i64 as u64) { pc += 54 }
    ldxb r2, [r7+0x0]                       
    and64 r2, 136                                   r2 &= 136   ///  r2 = r2.and(136)
    mov64 r3, 11                                    r3 = 11 as i32 as i64 as u64
    stxdw [r10-0x1e8], r3                   
    jne r2, 0, lbb_18255                            if r2 != (0 as i32 as i64 as u64) { pc += 49 }
    ldxb r3, [r7+0x1]                       
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_18211                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_18211:
    ldxb r4, [r7+0x2]                       
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_18215                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_18215:
    ldxb r4, [r7+0x3]                       
    jne r4, 0, lbb_18218                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_18218:
    ldxdw r4, [r7+0x50]                     
    mov64 r5, r7                                    r5 = r7
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r5                    
    mov64 r5, r7                                    r5 = r7
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r5                    
    stxdw [r10-0x50], r4                    
    add64 r7, 72                                    r7 += 72   ///  r7 = r7.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r7                    
    stxdw [r10-0x60], r6                    
    stxb [r10-0x2e], r2                     
    stxb [r10-0x2f], r3                     
    stxb [r10-0x30], r1                     
    stdw [r10-0x38], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -377                                  r1 += -377   ///  r1 = r1.wrapping_add(-377 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -480                                  r1 += -480   ///  r1 = r1.wrapping_add(-480 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100033388 --> b"\x8c\x97%\x8fN$\x89\xf1\xbb=\x10)\x14\x8e\x0d\x83\x0bZ\x13\x99\xda\xff\x1…        r1 load str located at 4295177096
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 1                       
    stdw [r10-0x18], 6                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -376                                  r2 += -376   ///  r2 = r2.wrapping_add(-376 as i32 as i64 as u64)
    mov64 r3, 6                                     r3 = 6 as i32 as i64 as u64
    ldxdw r4, [r10-0x228]                   
    ldxdw r5, [r10-0x240]                   
    syscall [invalid]                       
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    stxdw [r10-0x1e8], r1                   
    ldxdw r1, [r10-0x230]                   
lbb_18255:
    ldxdw r2, [r10-0x1e8]                   
    stxw [r1+0x0], r2                       
    exit                                    

function_18258:
    mov64 r0, r4                                    r0 = r4
    mov64 r6, r1                                    r6 = r1
    stdw [r10-0x20], 0                      
    stdw [r10-0x28], 0                      
    stdw [r10-0x30], 0                      
    stdw [r10-0x38], 0                      
    stb [r10-0x1], 255                      
    mov64 r4, r10                                   r4 = r10
    add64 r4, -56                                   r4 += -56   ///  r4 = r4.wrapping_add(-56 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -1                                    r5 += -1   ///  r5 = r5.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r2                                    r1 = r2
    mov64 r2, r3                                    r2 = r3
    mov64 r3, r0                                    r3 = r0
    syscall [invalid]                       
    jeq r0, 0, lbb_18286                            if r0 == (0 as i32 as i64 as u64) { pc += 12 }
    lddw r1, 0x100034cc0 --> b"\x00\x00\x00\x00\xd88\x03\x001\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295183552
    stxdw [r10-0x38], r1                    
    stdw [r10-0x18], 0                      
    stdw [r10-0x30], 1                      
    stdw [r10-0x20], 0                      
    stdw [r10-0x28], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -56                                   r1 += -56   ///  r1 = r1.wrapping_add(-56 as i32 as i64 as u64)
    lddw r2, 0x100034cd0 --> b"\x00\x00\x00\x00\xce8\x03\x00\x0a\x00\x00\x00\x00\x00\x00\x00d\x02\x00\x0…        r2 load str located at 4295183568
    call function_20640                     
lbb_18286:
    ldxdw r1, [r10-0x20]                    
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x30]                    
    stxdw [r6+0x8], r1                      
    ldxdw r1, [r10-0x38]                    
    stxdw [r6+0x0], r1                      
    ldxb r1, [r10-0x1]                      
    stxb [r6+0x20], r1                      
    exit                                    

function_18297:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x100033909 --> b"()"                  r2 load str located at 4295178505
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_21373                     
    exit                                    

function_18303:
    ldxdw r2, [r1+0x10]                     
    jeq r2, 0, lbb_18308                            if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r1, [r1+0x18]                     
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6768                      
lbb_18308:
    exit                                    

function_18309:
    exit                                    

function_18310:
    exit                                    

function_18311:
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_18316                            if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r1, [r1+0x8]                      
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6768                      
lbb_18316:
    exit                                    

function_18317:
    lddw r2, 0x10003390b --> b"description() is deprecated; use DisplayTryFromInt"        r2 load str located at 4295178507
    stxdw [r1+0x0], r2                      
    stdw [r1+0x8], 40                       
    exit                                    

function_18322:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_19155                     
    ldxdw r1, [r10-0x8]                     
    ldxdw r3, [r1+0x30]                     
    ldxdw r2, [r10-0x10]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    callx r3                                
    ldxdw r1, [r10-0x20]                    
    ldxdw r2, [r10-0x18]                    
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_18337:
    stdw [r1+0x0], 0                        
    exit                                    

function_18339:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_19155                     
    ldxdw r1, [r10-0x8]                     
    ldxdw r3, [r1+0x30]                     
    ldxdw r2, [r10-0x10]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    callx r3                                
    ldxdw r1, [r10-0x20]                    
    ldxdw r2, [r10-0x18]                    
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_18354:
    stdw [r1+0x0], 0                        
    exit                                    

function_18356:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_19155                     
    ldxdw r1, [r10-0x8]                     
    ldxdw r3, [r1+0x30]                     
    ldxdw r2, [r10-0x10]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    callx r3                                
    ldxdw r1, [r10-0x20]                    
    ldxdw r2, [r10-0x18]                    
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_18371:
    exit                                    

function_18372:
    exit                                    

function_18373:
    lddw r2, 0x72f53a70996cb31c                     r2 load str located at 8283591344902681372
    stxdw [r1+0x8], r2                      
    lddw r2, 0xb5ff603b62edc218                     r2 load str located at -5332437625604292072
    stxdw [r1+0x0], r2                      
    exit                                    

function_18380:
    lddw r2, 0x5bb5e6a9fa88d8f9                     r2 load str located at 6608441645963204857
    stxdw [r1+0x8], r2                      
    lddw r2, 0x9df6d35319584c1                      r2 load str located at 711407341380404417
    stxdw [r1+0x0], r2                      
    exit                                    

function_18387:
    lddw r2, 0xb55131cab6a0bc46                     r2 load str located at -5381465333013889978
    stxdw [r1+0x8], r2                      
    lddw r2, 0x67640bae5de3f2b1                     r2 load str located at 7450092527105077937
    stxdw [r1+0x0], r2                      
    exit                                    

function_18394:
    lddw r2, 0x75d6fb826124db4f                     r2 load str located at 8491250684847774543
    stxdw [r1+0x8], r2                      
    lddw r2, 0x97144fa85f6cdcb0                     r2 load str located at -7560330289874150224
    stxdw [r1+0x0], r2                      
    exit                                    

function_18401:
    lddw r2, 0x5dcd9ea8a457a8fe                     r2 load str located at 6759233062901885182
    stxdw [r1+0x8], r2                      
    lddw r2, 0x88a9d62410b90ef8                     r2 load str located at -8599106563099521288
    stxdw [r1+0x0], r2                      
    exit                                    

function_18408:
    lddw r2, 0x88861a174f9fc4a1                     r2 load str located at -8609164950249683807
    stxdw [r1+0x8], r2                      
    lddw r2, 0x8adcec2ea02f91f2                     r2 load str located at -8440611916599029262
    stxdw [r1+0x0], r2                      
    exit                                    

function_18415:
    lddw r3, 0x100034d08 --> b"\x00\x00\x00\x00P=\x02\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x0…        r3 load str located at 4295183624
    stxdw [r1+0x8], r3                      
    add64 r2, 9                                     r2 += 9   ///  r2 = r2.wrapping_add(9 as i32 as i64 as u64)
    stxdw [r1+0x0], r2                      
    exit                                    

function_18421:
    lddw r3, 0x100034d80 --> b"\x00\x00\x00\x00X=\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x0…        r3 load str located at 4295183744
    stxdw [r1+0x8], r3                      
    add64 r2, 16                                    r2 += 16   ///  r2 = r2.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r1+0x0], r2                      
    exit                                    

function_18427:
    lddw r3, 0x100034df8 --> b"\x00\x00\x00\x00H=\x02\x00\x10\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x0…        r3 load str located at 4295183864
    stxdw [r1+0x8], r3                      
    add64 r2, 16                                    r2 += 16   ///  r2 = r2.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r1+0x0], r2                      
    exit                                    

function_18433:
    ldxdw r2, [r1+0x10]                     
    jeq r2, 0, lbb_18441                            if r2 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r3, [r1+0x18]                     
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r3                                    r1 = r3
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6768                      
    mov64 r1, r6                                    r1 = r6
lbb_18441:
    mov64 r2, 40                                    r2 = 40 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    exit                                    

function_18445:
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    exit                                    

function_18449:
    mov64 r2, 16                                    r2 = 16 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    exit                                    

function_18453:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    exit                                    

function_18455:
    lddw r3, 0x100034e70 --> b"\x00\x00\x00\x00H=\x02\x00 \x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x…        r3 load str located at 4295183984
    stxdw [r1+0x8], r3                      
    stxdw [r1+0x0], r2                      
    exit                                    

function_18460:
    lddw r3, 0x100034ee8 --> b"\x00\x00\x00\x00H=\x02\x00\x10\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x0…        r3 load str located at 4295184104
    stxdw [r1+0x8], r3                      
    stxdw [r1+0x0], r2                      
    exit                                    

function_18465:
    lddw r3, 0x100034f60 --> b"\x00\x00\x00\x00\x18=\x02\x00(\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x0…        r3 load str located at 4295184224
    stxdw [r1+0x8], r3                      
    stxdw [r1+0x0], r2                      
    exit                                    

function_18470:
    mov64 r0, r1                                    r0 = r1
    add64 r0, 9                                     r0 += 9   ///  r0 = r0.wrapping_add(9 as i32 as i64 as u64)
    lddw r1, 0x5dcd9ea8a457a8fe                     r1 load str located at 6759233062901885182
    jeq r3, r1, lbb_18476                           if r3 == r1 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_18476:
    lddw r1, 0x88a9d62410b90ef8                     r1 load str located at -8599106563099521288
    jeq r2, r1, lbb_18480                           if r2 == r1 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_18480:
    exit                                    

function_18481:
    mov64 r0, r1                                    r0 = r1
    add64 r0, 16                                    r0 += 16   ///  r0 = r0.wrapping_add(16 as i32 as i64 as u64)
    lddw r1, 0xc9756ff18f43d98c                     r1 load str located at -3930112016529499764
    jeq r3, r1, lbb_18487                           if r3 == r1 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_18487:
    lddw r1, 0xb9251a33c36c6868                     r1 load str located at -5105645792930273176
    jeq r2, r1, lbb_18491                           if r2 == r1 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_18491:
    exit                                    

function_18492:
    mov64 r0, r1                                    r0 = r1
    add64 r0, 16                                    r0 += 16   ///  r0 = r0.wrapping_add(16 as i32 as i64 as u64)
    lddw r1, 0x63eb502cd6cb5d6d                     r1 load str located at 7199936582794304877
    jeq r3, r1, lbb_18498                           if r3 == r1 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_18498:
    lddw r1, 0xb98b1b7157a64178                     r1 load str located at -5076933981314334344
    jeq r2, r1, lbb_18502                           if r2 == r1 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_18502:
    exit                                    

function_18503:
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    exit                                    

function_18507:
    mov64 r2, 16                                    r2 = 16 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    exit                                    

function_18511:
    mov64 r2, 40                                    r2 = 40 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    exit                                    

function_18515:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r7+0x20]                     
    stxdw [r10-0x8], r1                     
    ldxdw r1, [r7+0x18]                     
    stxdw [r10-0x10], r1                    
    ldxdw r1, [r7+0x10]                     
    stxdw [r10-0x18], r1                    
    mov64 r1, 24                                    r1 = 24 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6742                      
    mov64 r8, r0                                    r8 = r0
    jne r8, 0, lbb_18531                            if r8 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    call function_20094                     
lbb_18531:
    ldxdw r1, [r10-0x8]                     
    stxdw [r8+0x10], r1                     
    ldxdw r1, [r10-0x10]                    
    stxdw [r8+0x8], r1                      
    ldxdw r1, [r10-0x18]                    
    stxdw [r8+0x0], r1                      
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 40                                    r2 = 40 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    lddw r1, 0x100034d80 --> b"\x00\x00\x00\x00X=\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x0…        r1 load str located at 4295183744
    stxdw [r6+0x8], r1                      
    stxdw [r6+0x0], r8                      
    exit                                    

function_18546:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r7+0x18]                     
    stxdw [r10-0x8], r1                     
    ldxdw r9, [r7+0x10]                     
    mov64 r1, 16                                    r1 = 16 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6742                      
    mov64 r8, r0                                    r8 = r0
    jne r8, 0, lbb_18559                            if r8 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 16                                    r2 = 16 as i32 as i64 as u64
    call function_20094                     
lbb_18559:
    ldxdw r1, [r10-0x8]                     
    stxdw [r8+0x8], r1                      
    stxdw [r8+0x0], r9                      
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    lddw r1, 0x100034df8 --> b"\x00\x00\x00\x00H=\x02\x00\x10\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x0…        r1 load str located at 4295183864
    stxdw [r6+0x8], r1                      
    stxdw [r6+0x0], r8                      
    exit                                    

function_18571:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r2                                    r1 = r2
    mov64 r2, 16                                    r2 = 16 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    lddw r1, 0x100034d08 --> b"\x00\x00\x00\x00P=\x02\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x0…        r1 load str located at 4295183624
    stxdw [r6+0x8], r1                      
    stdw [r6+0x0], 1                        
    exit                                    

function_18581:
    mov64 r6, r3                                    r6 = r3
    mov64 r8, r2                                    r8 = r2
    mov64 r7, r1                                    r7 = r1
    mov64 r1, 32                                    r1 = 32 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6742                      
    jne r0, 0, lbb_18591                            if r0 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    call function_20094                     
lbb_18591:
    stxdw [r0+0x18], r8                     
    stxdw [r0+0x10], r7                     
    stxb [r0+0x8], r6                       
    lddw r1, 0x100035028 --> b"\x00\x00\x00\x00\x88A\x02\x00\x00\x00\x00\x00\xf8@\x02\x00\x00\x00\x00\x0…        r1 load str located at 4295184424
    stxdw [r0+0x0], r1                      
    exit                                    

function_18598:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, 16                                    r1 = 16 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6742                      
    jne r0, 0, lbb_18606                            if r0 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 16                                    r2 = 16 as i32 as i64 as u64
    call function_20094                     
lbb_18606:
    stxb [r0+0x8], r6                       
    lddw r1, 0x100034fb8 --> b"\x00\x00\x00\x00\xa8A\x02\x00\x00\x00\x00\x00\x98@\x02\x00\x00\x00\x00\x0…        r1 load str located at 4295184312
    stxdw [r0+0x0], r1                      
    exit                                    

function_18611:
    mov64 r6, r2                                    r6 = r2
    ldxdw r2, [r1+0x10]                     
    stxdw [r10-0x8], r2                     
    ldxdw r2, [r1+0x8]                      
    stxdw [r10-0x10], r2                    
    ldxdw r1, [r1+0x0]                      
    stxdw [r10-0x18], r1                    
    mov64 r1, 40                                    r1 = 40 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6742                      
    jne r0, 0, lbb_18625                            if r0 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 40                                    r2 = 40 as i32 as i64 as u64
    call function_20094                     
lbb_18625:
    stxb [r0+0x8], r6                       
    lddw r1, 0x100034ff0 --> b"\x00\x00\x00\x00(A\x02\x00\x00\x00\x00\x00\xc8@\x02\x00\x00\x00\x00\x00(B…        r1 load str located at 4295184368
    stxdw [r0+0x0], r1                      
    ldxdw r1, [r10-0x1f]                    
    stxdw [r0+0x9], r1                      
    ldxdw r1, [r10-0x17]                    
    stxdw [r0+0x11], r1                     
    ldxdw r1, [r10-0xf]                     
    stxdw [r0+0x19], r1                     
    ldxdw r1, [r10-0x8]                     
    stxdw [r0+0x20], r1                     
    exit                                    

function_18638:
    mov64 r2, r1                                    r2 = r1
    ldxdw r1, [r2+0x18]                     
    ldxdw r3, [r2+0x8]                      
    jeq r3, 1, lbb_18655                            if r3 == (1 as i32 as i64 as u64) { pc += 13 }
    jne r3, 0, lbb_18646                            if r3 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_18660                            if r1 == (0 as i32 as i64 as u64) { pc += 14 }
lbb_18646:
    mov64 r6, r10                                   r6 = r10
    add64 r6, -24                                   r6 += -24   ///  r6 = r6.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
    call function_20113                     
    call function_19908                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r0                                    r2 = r0
    call function_18611                     
    ja lbb_18665                                    if true { pc += 10 }
lbb_18655:
    jeq r1, 0, lbb_18657                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_18646                                    if true { pc += -11 }
lbb_18657:
    ldxdw r1, [r2+0x0]                      
    ldxdw r6, [r1+0x8]                      
    ldxdw r7, [r1+0x0]                      
lbb_18660:
    call function_19908                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r0                                    r3 = r0
    call function_18581                     
lbb_18665:
    exit                                    

function_18666:
    call function_19166                     
    exit                                    

function_18668:
    call function_19166                     
    exit                                    

function_18670:
    call function_19166                     
    exit                                    

function_18672:
    stxdw [r10-0x8], r1                     
    mov64 r4, r10                                   r4 = r10
    add64 r4, -8                                    r4 += -8   ///  r4 = r4.wrapping_add(-8 as i32 as i64 as u64)
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x100033933 --> b"TryFromIntError"        r2 load str located at 4295178547
    mov64 r3, 15                                    r3 = 15 as i32 as i64 as u64
    lddw r5, 0x100035060 --> b"\x00\x00\x00\x00H=\x02\x00\x08\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x0…        r5 load str located at 4295184480
    call function_21545                     
    exit                                    

function_18683:
    mov64 r6, r2                                    r6 = r2
    mov64 r2, r1                                    r2 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_19155                     
    ldxdw r1, [r10-0x8]                     
    ldxdw r3, [r1+0x20]                     
    ldxdw r1, [r10-0x10]                    
    mov64 r2, r6                                    r2 = r6
    callx r3                                
    exit                                    

function_18694:
    mov64 r6, r2                                    r6 = r2
    mov64 r2, r1                                    r2 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_19155                     
    ldxdw r1, [r10-0x8]                     
    ldxdw r3, [r1+0x20]                     
    ldxdw r1, [r10-0x10]                    
    mov64 r2, r6                                    r2 = r6
    callx r3                                
    exit                                    

function_18705:
    mov64 r6, r2                                    r6 = r2
    mov64 r2, r1                                    r2 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_19155                     
    ldxdw r1, [r10-0x8]                     
    ldxdw r3, [r1+0x20]                     
    ldxdw r1, [r10-0x10]                    
    mov64 r2, r6                                    r2 = r6
    callx r3                                
    exit                                    

function_18716:
    lddw r2, 0x100033942 --> b"out of range integral type conversion attemptedDiv"        r2 load str located at 4295178562
    stxdw [r1+0x0], r2                      
    stdw [r1+0x8], 47                       
    exit                                    

function_18721:
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r1+0x8]                      
    ldxdw r1, [r1+0x0]                      
    call function_21608                     
    exit                                    

function_18726:
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r1+0x10]                     
    ldxdw r1, [r1+0x8]                      
    call function_21608                     
    exit                                    

function_18731:
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r1+0x10]                     
    ldxdw r1, [r1+0x8]                      
    call function_21842                     
    exit                                    

function_18736:
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r1+0x8]                      
    ldxdw r1, [r1+0x0]                      
    call function_21842                     
    exit                                    

function_18741:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    jne r3, 168, lbb_18812                          if r3 != (168 as i32 as i64 as u64) { pc += 68 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r3, r2                                    r3 = r2
    and64 r3, 7                                     r3 &= 7   ///  r3 = r3.and(7)
    jne r3, 0, lbb_18812                            if r3 != (0 as i32 as i64 as u64) { pc += 64 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -336                                  r1 += -336   ///  r1 = r1.wrapping_add(-336 as i32 as i64 as u64)
    mov64 r3, 168                                   r3 = 168 as i32 as i64 as u64
    call function_23152                     
    lddw r1, 0xf0debc9a78563412                     r1 load str located at -1090226688147180526
    stxdw [r10-0x18], r1                    
    stxdw [r10-0x20], r1                    
    stxdw [r10-0x28], r1                    
    stxdw [r10-0x30], r1                    
    stxdw [r10-0x38], r1                    
    stxdw [r10-0x40], r1                    
    stxdw [r10-0x48], r1                    
    stxdw [r10-0x50], r1                    
    stxdw [r10-0x58], r1                    
    stxdw [r10-0x60], r1                    
    lddw r1, 0x9966cc3355aa00ff                     r1 load str located at -7392997217429487361
    stxdw [r10-0x68], r1                    
    lddw r1, 0xeeddccbbaa998877                     r1 load str located at -1234605616436508553
    stxdw [r10-0x70], r1                    
    lddw r1, 0x6655443322110000                     r1 load str located at 7373874951294615552
    stxdw [r10-0x78], r1                    
    lddw r1, 0x996633cc00ffaa55                     r1 load str located at -7393164786998990251
    stxdw [r10-0x80], r1                    
    lddw r1, 0x4488dd22ee117799                     r1 load str located at 4938440133504497561
    stxdw [r10-0x88], r1                    
    lddw r1, 0x66cc3300ffaa55bb                     r1 load str located at 7407351566499993019
    stxdw [r10-0x98], r1                    
    lddw r1, 0x44dd228877ee1166                     r1 load str located at 4962160333955141990
    stxdw [r10-0xa0], r1                    
    lddw r1, 0x990ff033cc55aaff                     r1 load str located at -7417445955993752833
    stxdw [r10-0xa8], r1                    
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stdw [r10-0x8], 0                       
    stdw [r10-0x10], 0                      
    stdw [r10-0x90], 0                      
lbb_18792:
    mov64 r2, r10                                   r2 = r10
    add64 r2, -168                                  r2 += -168   ///  r2 = r2.wrapping_add(-168 as i32 as i64 as u64)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxb r2, [r2+0x0]                       
    mov64 r3, r10                                   r3 = r10
    add64 r3, -336                                  r3 += -336   ///  r3 = r3.wrapping_add(-336 as i32 as i64 as u64)
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    ldxb r4, [r3+0x0]                       
    xor64 r4, r2                                    r4 ^= r2   ///  r4 = r4.xor(r2)
    stxb [r3+0x0], r4                       
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    jne r1, 168, lbb_18792                          if r1 != (168 as i32 as i64 as u64) { pc += -12 }
    mov64 r1, r6                                    r1 = r6
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -336                                  r2 += -336   ///  r2 = r2.wrapping_add(-336 as i32 as i64 as u64)
    mov64 r3, 168                                   r3 = 168 as i32 as i64 as u64
    call function_23152                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_18814                                    if true { pc += 2 }
lbb_18812:
    stxb [r6+0x1], r1                       
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_18814:
    stxb [r6+0x0], r1                       
    exit                                    

function_18816:
    mov64 r9, r1                                    r9 = r1
    ldxdw r1, [r5-0x1000]                   
    ldxdw r0, [r2+0x8]                      
    jeq r1, 0, lbb_18821                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ldxdw r0, [r5-0xff8]                    
lbb_18821:
    ldxdw r7, [r2+0x0]                      
    mov64 r1, r7                                    r1 = r7
    arsh64 r1, 63                                   r1 >>= 63 (signed)   ///  r1 = (r1 as i64).wrapping_shr(63)
    mov64 r8, r7                                    r8 = r7
    xor64 r8, r1                                    r8 ^= r1   ///  r8 = r8.xor(r1)
    sub64 r8, r1                                    r8 -= r1   ///  r8 = r8.wrapping_sub(r1)
    lddw r1, 0xffffffff                             r1 load str located at 4294967295
    jgt r8, r1, lbb_19038                           if r8 > r1 { pc += 208 }
    stxdw [r10-0x100], r3                   
    stxdw [r10-0xf8], r9                    
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r1, r8                                    r1 = r8
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jeq r1, 0, lbb_18885                            if r1 == (0 as i32 as i64 as u64) { pc += 47 }
    jeq r1, 1, lbb_18913                            if r1 == (1 as i32 as i64 as u64) { pc += 74 }
    stxdw [r10-0x110], r0                   
    stxdw [r10-0x108], r4                   
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r9, 10                                    r9 = 10 as i32 as i64 as u64
    ja lbb_18879                                    if true { pc += 34 }
lbb_18845:
    neg64 r3                                        r3 = -r3   ///  r3 = (r3 as i64).wrapping_neg() as u64
    and64 r3, r6                                    r3 &= r6   ///  r3 = r3.and(r6)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    call function_25903                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, r6                                    r3 = r6
    mov64 r4, r9                                    r4 = r9
    mov64 r5, r6                                    r5 = r6
    call function_25903                     
    ldxdw r6, [r10-0x38]                    
    ldxdw r9, [r10-0x40]                    
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    mov64 r1, r8                                    r1 = r8
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r8, r1                                    r8 = r1
    rsh64 r8, 1                                     r8 >>= 1   ///  r8 = r8.wrapping_shr(1)
    ldxdw r5, [r10-0x48]                    
    ldxdw r4, [r10-0x50]                    
    jgt r1, 3, lbb_18879                            if r1 > (3 as i32 as i64 as u64) { pc += 12 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r2, r4                                    r2 = r4
    mov64 r3, r5                                    r3 = r5
    mov64 r4, r9                                    r4 = r9
    mov64 r5, r6                                    r5 = r6
    call function_25903                     
    ldxdw r5, [r10-0x58]                    
    ldxdw r6, [r10-0x60]                    
    ldxdw r4, [r10-0x108]                   
    ldxdw r0, [r10-0x110]                   
    ja lbb_18885                                    if true { pc += 6 }
lbb_18879:
    mov64 r3, r8                                    r3 = r8
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jeq r3, 0, lbb_18845                            if r3 == (0 as i32 as i64 as u64) { pc += -38 }
    mov64 r2, r9                                    r2 = r9
    ja lbb_18845                                    if true { pc += -40 }
lbb_18885:
    jne r4, 0, lbb_18887                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_18915                                    if true { pc += 28 }
lbb_18887:
    jslt r7, 0, lbb_19000                           if (r7 as i64) < (0 as i32 as i64) { pc += 112 }
    mov64 r1, r6                                    r1 = r6
    or64 r1, r5                                     r1 |= r5   ///  r1 = r1.or(r5)
    jeq r1, 0, lbb_19045                            if r1 == (0 as i32 as i64 as u64) { pc += 154 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -192                                  r1 += -192   ///  r1 = r1.wrapping_add(-192 as i32 as i64 as u64)
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0x100]                   
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    mov64 r6, r0                                    r6 = r0
    call function_24950                     
    mov64 r4, r6                                    r4 = r6
    ldxdw r2, [r10-0xc0]                    
    ldxdw r9, [r10-0xf8]                    
    jeq r4, 0, lbb_19035                            if r4 == (0 as i32 as i64 as u64) { pc += 132 }
lbb_18903:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -240                                  r1 += -240   ///  r1 = r1.wrapping_add(-240 as i32 as i64 as u64)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r3, r8                                    r3 = r8
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_24950                     
    ldxdw r1, [r10-0xe8]                    
    jne r1, 0, lbb_19042                            if r1 != (0 as i32 as i64 as u64) { pc += 131 }
    ldxdw r0, [r10-0xf0]                    
    ja lbb_19075                                    if true { pc += 162 }
lbb_18913:
    mov64 r6, 10                                    r6 = 10 as i32 as i64 as u64
    jne r4, 0, lbb_18887                            if r4 != (0 as i32 as i64 as u64) { pc += -28 }
lbb_18915:
    stxdw [r10-0x108], r5                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    mov64 r2, r0                                    r2 = r0
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0x100]                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_25903                     
    ldxdw r9, [r10-0x68]                    
    ldxdw r8, [r10-0x70]                    
    jsgt r7, 0, lbb_18942                           if (r7 as i64) > (0 as i32 as i64) { pc += 16 }
    mov64 r1, r6                                    r1 = r6
    ldxdw r5, [r10-0x108]                   
    or64 r1, r5                                     r1 |= r5   ///  r1 = r1.or(r5)
    jeq r1, 0, lbb_19051                            if r1 == (0 as i32 as i64 as u64) { pc += 121 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r9                                    r3 = r9
    mov64 r4, r6                                    r4 = r6
    call function_24950                     
    ldxdw r3, [r10-0x78]                    
    ldxdw r0, [r10-0x80]                    
    ldxdw r9, [r10-0xf8]                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jeq r3, 0, lbb_19075                            if r3 == (0 as i32 as i64 as u64) { pc += 134 }
    ja lbb_18997                                    if true { pc += 55 }
lbb_18942:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -160                                  r1 += -160   ///  r1 = r1.wrapping_add(-160 as i32 as i64 as u64)
    ldxdw r2, [r10-0x108]                   
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r8                                    r4 = r8
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_25903                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_25903                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_25903                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0x98]                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_18968                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_18968:
    ldxdw r3, [r10-0xa8]                    
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_18972                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_18972:
    ldxdw r3, [r10-0xa0]                    
    ldxdw r6, [r10-0xb0]                    
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ldxdw r5, [r10-0x108]                   
    jne r5, 0, lbb_18978                            if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_18978:
    add64 r6, r3                                    r6 += r3   ///  r6 = r6.wrapping_add(r3)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r9, 0, lbb_18982                            if r9 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_18982:
    ldxdw r7, [r10-0x88]                    
    mov64 r3, r7                                    r3 = r7
    add64 r3, r6                                    r3 += r6   ///  r3 = r3.wrapping_add(r6)
    ldxdw r9, [r10-0xf8]                    
    jlt r3, r7, lbb_18988                           if r3 < r7 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_18988:
    and64 r5, r0                                    r5 &= r0   ///  r5 = r5.and(r0)
    or64 r5, r4                                     r5 |= r4   ///  r5 = r5.or(r4)
    or64 r5, r2                                     r5 |= r2   ///  r5 = r5.or(r2)
    or64 r5, r1                                     r5 |= r1   ///  r5 = r5.or(r1)
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_19048                            if r5 != (0 as i32 as i64 as u64) { pc += 54 }
    ldxdw r0, [r10-0x90]                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jeq r3, 0, lbb_19075                            if r3 == (0 as i32 as i64 as u64) { pc += 78 }
lbb_18997:
    lddw r1, 0x100035090 --> b"\x00\x00\x00\x00\xa09\x03\x00&\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295184528
    ja lbb_19066                                    if true { pc += 66 }
lbb_19000:
    mov64 r7, r0                                    r7 = r0
    mov64 r1, r10                                   r1 = r10
    add64 r1, -224                                  r1 += -224   ///  r1 = r1.wrapping_add(-224 as i32 as i64 as u64)
    mov64 r2, r5                                    r2 = r5
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r8, [r10-0x100]                   
    mov64 r4, r8                                    r4 = r8
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_25903                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_25903                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0xd8]                    
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_19021                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_19021:
    ldxdw r3, [r10-0xc8]                    
    ldxdw r4, [r10-0xe0]                    
    mov64 r8, r3                                    r8 = r3
    add64 r8, r4                                    r8 += r4   ///  r8 = r8.wrapping_add(r4)
    ldxdw r9, [r10-0xf8]                    
    mov64 r4, r7                                    r4 = r7
    jlt r8, r3, lbb_19029                           if r8 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_19029:
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_19064                            if r1 != (0 as i32 as i64 as u64) { pc += 32 }
    ldxdw r2, [r10-0xd0]                    
    jeq r4, 0, lbb_19035                            if r4 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_18903                                    if true { pc += -132 }
lbb_19035:
    lddw r1, 0x1000350d0 --> b"\x00\x00\x00\x000:\x03\x00$\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00T:…        r1 load str located at 4295184592
    ja lbb_19066                                    if true { pc += 28 }
lbb_19038:
    call function_19908                     
    mov64 r1, r0                                    r1 = r0
    call function_18598                     
    ja lbb_19074                                    if true { pc += 32 }
lbb_19042:
    lddw r1, 0x1000350c0 --> b"\x00\x00\x00\x00\x0b:\x03\x00%\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295184576
    ja lbb_19066                                    if true { pc += 21 }
lbb_19045:
    lddw r1, 0x1000350b0 --> b"\x00\x00\x00\x00\xed9\x03\x00\x1e\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295184560
    ja lbb_19053                                    if true { pc += 5 }
lbb_19048:
    lddw r1, 0x1000350a0 --> b"\x00\x00\x00\x00\xc69\x03\x00'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295184544
    ja lbb_19066                                    if true { pc += 15 }
lbb_19051:
    lddw r1, 0x100035080 --> b"\x00\x00\x00\x00q9\x03\x00/\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x…        r1 load str located at 4295184512
lbb_19053:
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_18638                     
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    ldxdw r9, [r10-0xf8]                    
    ja lbb_19075                                    if true { pc += 11 }
lbb_19064:
    lddw r1, 0x1000350e0 --> b"\x00\x00\x00\x00T:\x03\x00%\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x…        r1 load str located at 4295184608
lbb_19066:
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_18638                     
lbb_19074:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
lbb_19075:
    stxdw [r9+0x8], r0                      
    stxdw [r9+0x0], r6                      
    exit                                    

function_19078:
    ldxdw r3, [r1+0x0]                      
    ldxdw r1, [r1+0x8]                      
    ldxdw r4, [r1+0x20]                     
    mov64 r1, r3                                    r1 = r3
    callx r4                                
    exit                                    

function_19084:
    stw [r10-0x4], 0                        
    mov64 r3, r2                                    r3 = r2
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    jlt r3, 128, lbb_19109                          if r3 < (128 as i32 as i64 as u64) { pc += 20 }
    jlt r3, 2048, lbb_19112                         if r3 < (2048 as i32 as i64 as u64) { pc += 22 }
    mov64 r3, r2                                    r3 = r2
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    jlt r3, 65536, lbb_19095                        if r3 < (65536 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19121                                    if true { pc += 26 }
lbb_19095:
    mov64 r3, r2                                    r3 = r2
    and64 r3, 63                                    r3 &= 63   ///  r3 = r3.and(63)
    or64 r3, 128                                    r3 |= 128   ///  r3 = r3.or(128)
    stxb [r10-0x2], r3                      
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 12                                    r3 >>= 12   ///  r3 = r3.wrapping_shr(12)
    or64 r3, 224                                    r3 |= 224   ///  r3 = r3.or(224)
    stxb [r10-0x4], r3                      
    rsh64 r2, 6                                     r2 >>= 6   ///  r2 = r2.wrapping_shr(6)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    or64 r2, 128                                    r2 |= 128   ///  r2 = r2.or(128)
    stxb [r10-0x3], r2                      
    mov64 r3, 3                                     r3 = 3 as i32 as i64 as u64
    ja lbb_19140                                    if true { pc += 31 }
lbb_19109:
    stxb [r10-0x4], r2                      
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    ja lbb_19140                                    if true { pc += 28 }
lbb_19112:
    mov64 r3, r2                                    r3 = r2
    and64 r3, 63                                    r3 &= 63   ///  r3 = r3.and(63)
    or64 r3, 128                                    r3 |= 128   ///  r3 = r3.or(128)
    stxb [r10-0x3], r3                      
    rsh64 r2, 6                                     r2 >>= 6   ///  r2 = r2.wrapping_shr(6)
    or64 r2, 192                                    r2 |= 192   ///  r2 = r2.or(192)
    stxb [r10-0x4], r2                      
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    ja lbb_19140                                    if true { pc += 19 }
lbb_19121:
    mov64 r3, r2                                    r3 = r2
    and64 r3, 63                                    r3 &= 63   ///  r3 = r3.and(63)
    or64 r3, 128                                    r3 |= 128   ///  r3 = r3.or(128)
    stxb [r10-0x1], r3                      
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 6                                     r3 >>= 6   ///  r3 = r3.wrapping_shr(6)
    and64 r3, 63                                    r3 &= 63   ///  r3 = r3.and(63)
    or64 r3, 128                                    r3 |= 128   ///  r3 = r3.or(128)
    stxb [r10-0x2], r3                      
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 12                                    r3 >>= 12   ///  r3 = r3.wrapping_shr(12)
    and64 r3, 63                                    r3 &= 63   ///  r3 = r3.and(63)
    or64 r3, 128                                    r3 |= 128   ///  r3 = r3.or(128)
    stxb [r10-0x3], r3                      
    rsh64 r2, 18                                    r2 >>= 18   ///  r2 = r2.wrapping_shr(18)
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    or64 r2, 240                                    r2 |= 240   ///  r2 = r2.or(240)
    stxb [r10-0x4], r2                      
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
lbb_19140:
    mov64 r2, r10                                   r2 = r10
    add64 r2, -4                                    r2 += -4   ///  r2 = r2.wrapping_add(-4 as i32 as i64 as u64)
    call function_19326                     
    exit                                    

function_19144:
    mov64 r3, r2                                    r3 = r2
    lddw r2, 0x1000350f0 --> b"\x00\x00\x00\x00\x88W\x02\x00 \x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x0…        r2 load str located at 4295184624
    call function_21025                     
    exit                                    

function_19149:
    exit                                    

function_19150:
    ldxdw r1, [r1+0x0]                      
    ldxdw r2, [r1+0x0]                      
    ldxdw r2, [r2+0x0]                      
    callx r2                                
    exit                                    

function_19155:
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r2+0x0]                      
    ldxdw r3, [r1+0x8]                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    callx r3                                
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x8]                     
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_19166:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r6+0x0]                      
    ldxdw r3, [r1+0x8]                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -168                                  r1 += -168   ///  r1 = r1.wrapping_add(-168 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    callx r3                                
    ldxdw r2, [r10-0xa0]                    
    stxdw [r10-0x90], r2                    
    ldxdw r1, [r10-0xa8]                    
    stxdw [r10-0x98], r1                    
    ldxw r3, [r7+0x34]                      
    and64 r3, 4                                     r3 &= 4   ///  r3 = r3.and(4)
    jne r3, 0, lbb_19318                            if r3 != (0 as i32 as i64 as u64) { pc += 137 }
    lddw r1, 0x100033aa0 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00012345678…        r1 load str located at 4295178912
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100025550 --> b"y\x13\x00\x00\x00\x00\x00\x00y\x11\x08\x00\x00\x00\x00\x00y\x14 \x00\x00\…        r1 load str located at 4295120208
    stxdw [r10-0x58], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -152                                  r1 += -152   ///  r1 = r1.wrapping_add(-152 as i32 as i64 as u64)
    stxdw [r10-0x60], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    ldxdw r2, [r7+0x28]                     
    ldxdw r1, [r7+0x20]                     
    mov64 r3, r10                                   r3 = r10
    add64 r3, -64                                   r3 += -64   ///  r3 = r3.wrapping_add(-64 as i32 as i64 as u64)
    call function_21025                     
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_19321                            if r1 != (0 as i32 as i64 as u64) { pc += 117 }
    ldxdw r1, [r10-0x90]                    
    ldxdw r3, [r1+0x30]                     
    ldxdw r2, [r10-0x98]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -184                                  r1 += -184   ///  r1 = r1.wrapping_add(-184 as i32 as i64 as u64)
    callx r3                                
    ldxdw r8, [r10-0xb8]                    
    jeq r8, 0, lbb_19305                            if r8 == (0 as i32 as i64 as u64) { pc += 93 }
    ldxdw r9, [r10-0xb0]                    
    ldxdw r1, [r7+0x20]                     
    ldxdw r2, [r7+0x28]                     
    ldxdw r4, [r2+0x18]                     
    lddw r2, 0x100033ae6 --> b"\x0a\x0aCaused by:"        r2 load str located at 4295178982
    mov64 r3, 12                                    r3 = 12 as i32 as i64 as u64
    callx r4                                
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_19321                            if r1 != (0 as i32 as i64 as u64) { pc += 98 }
    stxdw [r10-0xe0], r9                    
    ldxdw r3, [r9+0x30]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -200                                  r1 += -200   ///  r1 = r1.wrapping_add(-200 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    callx r3                                
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xe8], r1                    
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0xf0], r1                    
    ldxdw r1, [r10-0xc8]                    
    jne r1, 0, lbb_19279                            if r1 != (0 as i32 as i64 as u64) { pc += 44 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xf0], r1                    
    ja lbb_19279                                    if true { pc += 41 }
lbb_19238:
    ldxdw r8, [r10-0x88]                    
    ldxdw r2, [r10-0x78]                    
    stxdw [r10-0x68], r2                    
    stxdw [r10-0x70], r1                    
    ldxdw r1, [r7+0x20]                     
    ldxdw r2, [r7+0x28]                     
    ldxdw r4, [r2+0x18]                     
    lddw r2, 0x100033af2 --> b"\x0a"                r2 load str located at 4295178994
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    callx r4                                
    jne r0, 0, lbb_19324                            if r0 != (0 as i32 as i64 as u64) { pc += 74 }
    stxdw [r10-0x58], r8                    
    ldxdw r1, [r10-0xf0]                    
    stxdw [r10-0x60], r1                    
    stxdw [r10-0x50], r7                    
    stb [r10-0x48], 0                       
    lddw r1, 0x100033aa0 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00012345678…        r1 load str located at 4295178912
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x100025550 --> b"y\x13\x00\x00\x00\x00\x00\x00y\x11\x08\x00\x00\x00\x00\x00y\x14 \x00\x00\…        r1 load str located at 4295120208
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -64                                   r3 += -64   ///  r3 = r3.wrapping_add(-64 as i32 as i64 as u64)
    lddw r2, 0x1000350f0 --> b"\x00\x00\x00\x00\x88W\x02\x00 \x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x0…        r2 load str located at 4295184624
    call function_21025                     
    jne r0, 0, lbb_19324                            if r0 != (0 as i32 as i64 as u64) { pc += 46 }
    mov64 r8, r9                                    r8 = r9
lbb_19279:
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_19299                            if r8 == (0 as i32 as i64 as u64) { pc += 16 }
    ldxdw r9, [r10-0xe0]                    
    ldxdw r3, [r9+0x30]                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -216                                  r1 += -216   ///  r1 = r1.wrapping_add(-216 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    callx r3                                
    stxdw [r10-0x80], r8                    
    mov64 r1, 16                                    r1 = 16 as i32 as i64 as u64
    mov64 r2, r9                                    r2 = r9
    ldxdw r3, [r10-0xe8]                    
    stxdw [r10-0x88], r3                    
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0xe8], r3                    
    ldxdw r3, [r10-0xd0]                    
    stxdw [r10-0xe0], r3                    
    ldxdw r9, [r10-0xd8]                    
lbb_19299:
    mov64 r3, r10                                   r3 = r10
    add64 r3, -136                                  r3 += -136   ///  r3 = r3.wrapping_add(-136 as i32 as i64 as u64)
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    stxdw [r3+0x0], r2                      
    ldxdw r1, [r10-0x80]                    
    jne r1, 0, lbb_19238                            if r1 != (0 as i32 as i64 as u64) { pc += -67 }
lbb_19305:
    ldxb r1, [r6+0x8]                       
    jne r1, 2, lbb_19322                            if r1 != (2 as i32 as i64 as u64) { pc += 15 }
    ldxdw r1, [r6+0x0]                      
    ldxdw r2, [r1+0x30]                     
    mov64 r1, r6                                    r1 = r6
    callx r2                                
    jne r0, 0, lbb_19322                            if r0 != (0 as i32 as i64 as u64) { pc += 10 }
    lddw r1, 0x100033acc --> b"backtrace capture failed"        r1 load str located at 4295178956
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    lddw r3, 0x100035120 --> b"\x00\x00\x00\x00\xc0:\x03\x00\x0c\x00\x00\x00\x00\x00\x00\x00g\x04\x00\x0…        r3 load str located at 4295184672
    call function_20634                     
lbb_19318:
    ldxdw r3, [r2+0x18]                     
    mov64 r2, r7                                    r2 = r7
    callx r3                                
lbb_19321:
    exit                                    
lbb_19322:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_19321                                    if true { pc += -3 }
lbb_19324:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ja lbb_19321                                    if true { pc += -5 }

function_19326:
    stxdw [r10-0xc8], r3                    
    stxdw [r10-0x98], r2                    
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxb r2, [r1+0x18]                      
    stxdw [r10-0xc0], r2                    
    ldxdw r2, [r1+0x8]                      
    stxdw [r10-0xe0], r2                    
    ldxdw r2, [r1+0x0]                      
    stxdw [r10-0xd0], r2                    
    stxdw [r10-0xd8], r1                    
    ldxdw r1, [r1+0x10]                     
    stxdw [r10-0xb8], r1                    
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0xa0], r1                    
    stxdw [r10-0xb0], r1                    
    stxdw [r10-0xa8], r1                    
    ja lbb_19396                                    if true { pc += 53 }
lbb_19343:
    ldxdw r1, [r10-0xe0]                    
    stxdw [r10-0x80], r1                    
    lddw r1, 0x10002cf40 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295151424
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    stxdw [r10-0x48], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -56                                   r1 += -56   ///  r1 = r1.wrapping_add(-56 as i32 as i64 as u64)
    stxdw [r10-0x58], r1                    
    lddw r1, 0x100035138 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295184696
    stxdw [r10-0x78], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    stxdw [r10-0x68], r1                    
    stdw [r10-0x50], 1                      
    stdw [r10-0x70], 2                      
    stdw [r10-0x60], 1                      
    stb [r10-0x8], 1                        
    stdw [r10-0x10], 32                     
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 5                      
    stdw [r10-0x28], 0                      
    stdw [r10-0x38], 2                      
    ldxdw r1, [r10-0xb8]                    
    ldxdw r2, [r1+0x28]                     
    ldxdw r1, [r1+0x20]                     
    mov64 r3, r10                                   r3 = r10
    add64 r3, -120                                  r3 += -120   ///  r3 = r3.wrapping_add(-120 as i32 as i64 as u64)
    call function_21025                     
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0xc0], r1                    
    jne r0, 0, lbb_19495                            if r0 != (0 as i32 as i64 as u64) { pc += 116 }
lbb_19379:
    ldxdw r2, [r10-0x98]                    
    ldxdw r1, [r10-0xa0]                    
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    sub64 r6, r1                                    r6 -= r1   ///  r6 = r6.wrapping_sub(r1)
    ldxdw r1, [r10-0xb8]                    
    mov64 r3, r6                                    r3 = r6
    call function_21539                     
    mov64 r9, r0                                    r9 = r0
    neg64 r9                                        r9 = -r9   ///  r9 = (r9 as i64).wrapping_neg() as u64
    jne r0, 0, lbb_19495                            if r0 != (0 as i32 as i64 as u64) { pc += 106 }
    ldxdw r1, [r10-0xb0]                    
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0xb0], r1                    
    xor64 r7, 1                                     r7 ^= 1   ///  r7 = r7.xor(1)
    ldxdw r1, [r10-0xa8]                    
    stxdw [r10-0xa0], r1                    
    jne r7, 0, lbb_19495                            if r7 != (0 as i32 as i64 as u64) { pc += 99 }
lbb_19396:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r9, [r10-0xc8]                    
    mov64 r6, r9                                    r6 = r9
    ldxdw r2, [r10-0xa8]                    
    jgt r8, r9, lbb_19453                           if r8 > r9 { pc += 52 }
lbb_19401:
    ldxdw r3, [r10-0x98]                    
    add64 r3, r8                                    r3 += r8   ///  r3 = r3.wrapping_add(r8)
    mov64 r4, r9                                    r4 = r9
    sub64 r4, r8                                    r4 -= r8   ///  r4 = r4.wrapping_sub(r8)
    jlt r4, 16, lbb_19413                           if r4 < (16 as i32 as i64 as u64) { pc += 7 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    call function_21920                     
    ldxdw r2, [r10-0x88]                    
    ldxdw r1, [r10-0x90]                    
    ja lbb_19427                                    if true { pc += 14 }
lbb_19413:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jeq r9, r8, lbb_19427                           if r9 == r8 { pc += 11 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_19417:
    mov64 r2, r3                                    r2 = r3
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxb r0, [r2+0x0]                       
    mov64 r2, r5                                    r2 = r5
    jeq r0, 10, lbb_19427                           if r0 == (10 as i32 as i64 as u64) { pc += 4 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    add64 r5, 1                                     r5 += 1   ///  r5 = r5.wrapping_add(1 as i32 as i64 as u64)
    mov64 r2, r4                                    r2 = r4
    jlt r5, r4, lbb_19417                           if r5 < r4 { pc += -10 }
lbb_19427:
    jeq r1, 1, lbb_19434                            if r1 == (1 as i32 as i64 as u64) { pc += 6 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0xa0]                    
    mov64 r8, r9                                    r8 = r9
    mov64 r6, r9                                    r6 = r9
    jeq r1, 0, lbb_19453                            if r1 == (0 as i32 as i64 as u64) { pc += 20 }
    ja lbb_19451                                    if true { pc += 17 }
lbb_19434:
    mov64 r6, r8                                    r6 = r8
    add64 r6, r2                                    r6 += r2   ///  r6 = r6.wrapping_add(r2)
    mov64 r8, r6                                    r8 = r6
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
    jlt r6, r9, lbb_19444                           if r6 < r9 { pc += 5 }
lbb_19439:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0xa0]                    
    mov64 r6, r9                                    r6 = r9
    jgt r8, r9, lbb_19453                           if r8 > r9 { pc += 10 }
    ja lbb_19401                                    if true { pc += -43 }
lbb_19444:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ldxdw r1, [r10-0x98]                    
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    ldxb r1, [r1+0x0]                       
    mov64 r2, r8                                    r2 = r8
    jeq r1, 10, lbb_19453                           if r1 == (10 as i32 as i64 as u64) { pc += 3 }
    ja lbb_19439                                    if true { pc += -12 }
lbb_19451:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r8, r9                                    r8 = r9
lbb_19453:
    ldxdw r1, [r10-0xc0]                    
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    stxdw [r10-0xa8], r2                    
    jeq r1, 0, lbb_19473                            if r1 == (0 as i32 as i64 as u64) { pc += 16 }
    ldxdw r1, [r10-0xb0]                    
    jeq r1, 0, lbb_19379                            if r1 == (0 as i32 as i64 as u64) { pc += -80 }
    ldxdw r1, [r10-0xb8]                    
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    call function_21602                     
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_19495                            if r0 != (0 as i32 as i64 as u64) { pc += 31 }
    ldxdw r1, [r10-0xd0]                    
    jeq r1, 0, lbb_19488                            if r1 == (0 as i32 as i64 as u64) { pc += 22 }
    ldxdw r1, [r10-0xb8]                    
    lddw r2, 0x100033afb --> b"       "             r2 load str located at 4295179003
    mov64 r3, 7                                     r3 = 7 as i32 as i64 as u64
    call function_21539                     
    jne r0, 0, lbb_19495                            if r0 != (0 as i32 as i64 as u64) { pc += 23 }
    ja lbb_19379                                    if true { pc += -94 }
lbb_19473:
    ldxdw r1, [r10-0xd8]                    
    stb [r1+0x18], 1                        
    ldxdw r1, [r10-0xd0]                    
    jeq r1, 0, lbb_19478                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19343                                    if true { pc += -135 }
lbb_19478:
    ldxdw r1, [r10-0xb8]                    
    lddw r2, 0x100033af7 --> b"    "                r2 load str located at 4295178999
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
    call function_21539                     
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0xc0], r1                    
    jne r0, 0, lbb_19495                            if r0 != (0 as i32 as i64 as u64) { pc += 8 }
    ja lbb_19379                                    if true { pc += -109 }
lbb_19488:
    ldxdw r1, [r10-0xb8]                    
    lddw r2, 0x100033af7 --> b"    "                r2 load str located at 4295178999
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
    call function_21539                     
    jne r0, 0, lbb_19495                            if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19379                                    if true { pc += -116 }
lbb_19495:
    and64 r9, 1                                     r9 &= 1   ///  r9 = r9.and(1)
    mov64 r0, r9                                    r0 = r9
    exit                                    

function_19498:
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jsgt r1, 19, lbb_19510                          if (r1 as i64) > (19 as i32 as i64) { pc += 9 }
    jsgt r1, 9, lbb_19525                           if (r1 as i64) > (9 as i32 as i64) { pc += 23 }
    jsgt r1, 4, lbb_19545                           if (r1 as i64) > (4 as i32 as i64) { pc += 42 }
    jsgt r1, 1, lbb_19581                           if (r1 as i64) > (1 as i32 as i64) { pc += 77 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_19654                            if r1 == (0 as i32 as i64 as u64) { pc += 148 }
    jeq r1, 1, lbb_19508                            if r1 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19653                                    if true { pc += 145 }
lbb_19508:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 144 }
lbb_19510:
    jsgt r1, 29, lbb_19518                          if (r1 as i64) > (29 as i32 as i64) { pc += 7 }
    jsgt r1, 24, lbb_19539                          if (r1 as i64) > (24 as i32 as i64) { pc += 27 }
    jsgt r1, 21, lbb_19557                          if (r1 as i64) > (21 as i32 as i64) { pc += 44 }
    jeq r1, 20, lbb_19605                           if r1 == (20 as i32 as i64 as u64) { pc += 91 }
    jeq r1, 21, lbb_19516                           if r1 == (21 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19653                                    if true { pc += 137 }
lbb_19516:
    mov64 r0, 21                                    r0 = 21 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 136 }
lbb_19518:
    jsgt r1, 34, lbb_19532                          if (r1 as i64) > (34 as i32 as i64) { pc += 13 }
    jsgt r1, 31, lbb_19575                          if (r1 as i64) > (31 as i32 as i64) { pc += 55 }
    jeq r1, 30, lbb_19617                           if r1 == (30 as i32 as i64 as u64) { pc += 96 }
    jeq r1, 31, lbb_19523                           if r1 == (31 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19653                                    if true { pc += 130 }
lbb_19523:
    mov64 r0, 31                                    r0 = 31 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 129 }
lbb_19525:
    jsgt r1, 14, lbb_19551                          if (r1 as i64) > (14 as i32 as i64) { pc += 25 }
    jsgt r1, 11, lbb_19587                          if (r1 as i64) > (11 as i32 as i64) { pc += 60 }
    jeq r1, 10, lbb_19629                           if r1 == (10 as i32 as i64 as u64) { pc += 101 }
    jeq r1, 11, lbb_19530                           if r1 == (11 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19653                                    if true { pc += 123 }
lbb_19530:
    mov64 r0, 11                                    r0 = 11 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 122 }
lbb_19532:
    jsgt r1, 37, lbb_19569                          if (r1 as i64) > (37 as i32 as i64) { pc += 36 }
    jeq r1, 35, lbb_19625                           if r1 == (35 as i32 as i64 as u64) { pc += 91 }
    jeq r1, 36, lbb_19613                           if r1 == (36 as i32 as i64 as u64) { pc += 78 }
    jeq r1, 37, lbb_19537                           if r1 == (37 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19653                                    if true { pc += 116 }
lbb_19537:
    mov64 r0, 37                                    r0 = 37 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 115 }
lbb_19539:
    jsgt r1, 26, lbb_19563                          if (r1 as i64) > (26 as i32 as i64) { pc += 23 }
    jeq r1, 25, lbb_19607                           if r1 == (25 as i32 as i64 as u64) { pc += 66 }
    jeq r1, 26, lbb_19543                           if r1 == (26 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19653                                    if true { pc += 110 }
lbb_19543:
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 109 }
lbb_19545:
    jsgt r1, 6, lbb_19593                           if (r1 as i64) > (6 as i32 as i64) { pc += 47 }
    jeq r1, 5, lbb_19631                            if r1 == (5 as i32 as i64 as u64) { pc += 84 }
    jeq r1, 6, lbb_19549                            if r1 == (6 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19653                                    if true { pc += 104 }
lbb_19549:
    mov64 r0, 6                                     r0 = 6 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 103 }
lbb_19551:
    jsgt r1, 16, lbb_19599                          if (r1 as i64) > (16 as i32 as i64) { pc += 47 }
    jeq r1, 15, lbb_19633                           if r1 == (15 as i32 as i64 as u64) { pc += 80 }
    jeq r1, 16, lbb_19555                           if r1 == (16 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19653                                    if true { pc += 98 }
lbb_19555:
    mov64 r0, 16                                    r0 = 16 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 97 }
lbb_19557:
    jeq r1, 22, lbb_19619                           if r1 == (22 as i32 as i64 as u64) { pc += 61 }
    jeq r1, 23, lbb_19609                           if r1 == (23 as i32 as i64 as u64) { pc += 50 }
    jeq r1, 24, lbb_19561                           if r1 == (24 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19653                                    if true { pc += 92 }
lbb_19561:
    mov64 r0, 24                                    r0 = 24 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 91 }
lbb_19563:
    jeq r1, 27, lbb_19621                           if r1 == (27 as i32 as i64 as u64) { pc += 57 }
    jeq r1, 28, lbb_19611                           if r1 == (28 as i32 as i64 as u64) { pc += 46 }
    jeq r1, 29, lbb_19567                           if r1 == (29 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19653                                    if true { pc += 86 }
lbb_19567:
    mov64 r0, 29                                    r0 = 29 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 85 }
lbb_19569:
    jeq r1, 38, lbb_19627                           if r1 == (38 as i32 as i64 as u64) { pc += 57 }
    jeq r1, 39, lbb_19615                           if r1 == (39 as i32 as i64 as u64) { pc += 44 }
    jeq r1, 40, lbb_19573                           if r1 == (40 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19653                                    if true { pc += 80 }
lbb_19573:
    mov64 r0, 40                                    r0 = 40 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 79 }
lbb_19575:
    jeq r1, 32, lbb_19643                           if r1 == (32 as i32 as i64 as u64) { pc += 67 }
    jeq r1, 33, lbb_19623                           if r1 == (33 as i32 as i64 as u64) { pc += 46 }
    jeq r1, 34, lbb_19579                           if r1 == (34 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19653                                    if true { pc += 74 }
lbb_19579:
    mov64 r0, 34                                    r0 = 34 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 73 }
lbb_19581:
    jeq r1, 2, lbb_19645                            if r1 == (2 as i32 as i64 as u64) { pc += 63 }
    jeq r1, 3, lbb_19635                            if r1 == (3 as i32 as i64 as u64) { pc += 52 }
    jeq r1, 4, lbb_19585                            if r1 == (4 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19653                                    if true { pc += 68 }
lbb_19585:
    mov64 r0, 4                                     r0 = 4 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 67 }
lbb_19587:
    jeq r1, 12, lbb_19647                           if r1 == (12 as i32 as i64 as u64) { pc += 59 }
    jeq r1, 13, lbb_19637                           if r1 == (13 as i32 as i64 as u64) { pc += 48 }
    jeq r1, 14, lbb_19591                           if r1 == (14 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19653                                    if true { pc += 62 }
lbb_19591:
    mov64 r0, 14                                    r0 = 14 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 61 }
lbb_19593:
    jeq r1, 7, lbb_19649                            if r1 == (7 as i32 as i64 as u64) { pc += 55 }
    jeq r1, 8, lbb_19639                            if r1 == (8 as i32 as i64 as u64) { pc += 44 }
    jeq r1, 9, lbb_19597                            if r1 == (9 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19653                                    if true { pc += 56 }
lbb_19597:
    mov64 r0, 9                                     r0 = 9 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 55 }
lbb_19599:
    jeq r1, 17, lbb_19651                           if r1 == (17 as i32 as i64 as u64) { pc += 51 }
    jeq r1, 18, lbb_19641                           if r1 == (18 as i32 as i64 as u64) { pc += 40 }
    jeq r1, 19, lbb_19603                           if r1 == (19 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19653                                    if true { pc += 50 }
lbb_19603:
    mov64 r0, 19                                    r0 = 19 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 49 }
lbb_19605:
    mov64 r0, 20                                    r0 = 20 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 47 }
lbb_19607:
    mov64 r0, 25                                    r0 = 25 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 45 }
lbb_19609:
    mov64 r0, 23                                    r0 = 23 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 43 }
lbb_19611:
    mov64 r0, 28                                    r0 = 28 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 41 }
lbb_19613:
    mov64 r0, 36                                    r0 = 36 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 39 }
lbb_19615:
    mov64 r0, 39                                    r0 = 39 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 37 }
lbb_19617:
    mov64 r0, 30                                    r0 = 30 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 35 }
lbb_19619:
    mov64 r0, 22                                    r0 = 22 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 33 }
lbb_19621:
    mov64 r0, 27                                    r0 = 27 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 31 }
lbb_19623:
    mov64 r0, 33                                    r0 = 33 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 29 }
lbb_19625:
    mov64 r0, 35                                    r0 = 35 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 27 }
lbb_19627:
    mov64 r0, 38                                    r0 = 38 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 25 }
lbb_19629:
    mov64 r0, 10                                    r0 = 10 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 23 }
lbb_19631:
    mov64 r0, 5                                     r0 = 5 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 21 }
lbb_19633:
    mov64 r0, 15                                    r0 = 15 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 19 }
lbb_19635:
    mov64 r0, 3                                     r0 = 3 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 17 }
lbb_19637:
    mov64 r0, 13                                    r0 = 13 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 15 }
lbb_19639:
    mov64 r0, 8                                     r0 = 8 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 13 }
lbb_19641:
    mov64 r0, 18                                    r0 = 18 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 11 }
lbb_19643:
    mov64 r0, 32                                    r0 = 32 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 9 }
lbb_19645:
    mov64 r0, 2                                     r0 = 2 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 7 }
lbb_19647:
    mov64 r0, 12                                    r0 = 12 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 5 }
lbb_19649:
    mov64 r0, 7                                     r0 = 7 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 3 }
lbb_19651:
    mov64 r0, 17                                    r0 = 17 as i32 as i64 as u64
    ja lbb_19654                                    if true { pc += 1 }
lbb_19653:
    mov64 r0, 41                                    r0 = 41 as i32 as i64 as u64
lbb_19654:
    exit                                    

function_19655:
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_6742                      
    mov64 r6, r0                                    r6 = r0
    jne r6, 0, lbb_19663                            if r6 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    call function_20091                     
lbb_19663:
    lddw r1, 0x706e6920666f2068                     r1 load str located at 8101528367564529768
    stxdw [r6+0x10], r1                     
    lddw r1, 0x74676e656c206465                     r1 load str located at 8387794212885652581
    stxdw [r6+0x8], r1                      
    lddw r1, 0x7463657078656e55                     r1 load str located at 8386658464824651349
    stxdw [r6+0x0], r1                      
    sth [r6+0x18], 29813                    
    mov64 r1, 24                                    r1 = 24 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6742                      
    jne r0, 0, lbb_19680                            if r0 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    call function_20094                     
lbb_19680:
    stxdw [r0+0x8], r6                      
    stdw [r0+0x10], 26                      
    stdw [r0+0x0], 26                       
    mov64 r1, 20                                    r1 = 20 as i32 as i64 as u64
    mov64 r2, r0                                    r2 = r0
    lddw r3, 0x100035178 --> b"\x00\x00\x00\x00hh\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x0…        r3 load str located at 4295184760
    call function_19910                     
    exit                                    

function_19689:
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_19694                            if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r1, [r1+0x8]                      
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6768                      
lbb_19694:
    exit                                    

function_19695:
    stdw [r1+0x0], 0                        
    exit                                    

function_19697:
    exit                                    

function_19698:
    lddw r2, 0x1b204ea073ccb1b7                     r2 load str located at 1954648689323323831
    stxdw [r1+0x8], r2                      
    lddw r2, 0xe61d2cabdb5e3d                       r2 load str located at 64771322342497853
    stxdw [r1+0x0], r2                      
    exit                                    

function_19705:
    mov64 r6, r1                                    r6 = r1
    mov64 r7, r6                                    r7 = r6
    and64 r7, 3                                     r7 &= 3   ///  r7 = r7.and(3)
    jsgt r7, 1, lbb_19712                           if (r7 as i64) > (1 as i32 as i64) { pc += 3 }
    jeq r7, 0, lbb_19719                            if r7 == (0 as i32 as i64 as u64) { pc += 9 }
    ldxb r1, [r6+0xf]                       
    ja lbb_19720                                    if true { pc += 8 }
lbb_19712:
    mov64 r0, r6                                    r0 = r6
    jeq r7, 2, lbb_19745                            if r7 == (2 as i32 as i64 as u64) { pc += 31 }
    mov64 r1, r6                                    r1 = r6
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    call function_19498                     
    mov64 r1, r0                                    r1 = r0
    ja lbb_19720                                    if true { pc += 1 }
lbb_19719:
    ldxb r1, [r6+0x10]                      
lbb_19720:
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    mov64 r0, r6                                    r0 = r6
    jne r1, 37, lbb_19745                           if r1 != (37 as i32 as i64 as u64) { pc += 22 }
    call function_19655                     
    mov64 r1, r7                                    r1 = r7
    add64 r1, -2                                    r1 += -2   ///  r1 = r1.wrapping_add(-2 as i32 as i64 as u64)
    jlt r1, 2, lbb_19745                            if r1 < (2 as i32 as i64 as u64) { pc += 18 }
    jeq r7, 0, lbb_19745                            if r7 == (0 as i32 as i64 as u64) { pc += 17 }
    mov64 r8, r0                                    r8 = r0
    ldxdw r7, [r6-0x1]                      
    ldxdw r9, [r6+0x7]                      
    ldxdw r2, [r9+0x0]                      
    mov64 r1, r7                                    r1 = r7
    callx r2                                
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    ldxdw r2, [r9+0x8]                      
    jeq r2, 0, lbb_19740                            if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r3, [r9+0x10]                     
    mov64 r1, r7                                    r1 = r7
    call function_6768                      
lbb_19740:
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    call function_6768                      
    mov64 r0, r8                                    r0 = r8
lbb_19745:
    exit                                    

function_19746:
    syscall [invalid]                       
    exit                                    

function_19748:
    lddw r3, 0xffffffff00000000                     r3 load str located at -4294967296
    mov64 r4, r2                                    r4 = r2
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jsgt r4, 12, lbb_19767                          if (r4 as i64) > (12 as i32 as i64) { pc += 9 }
    jsgt r4, 5, lbb_19775                           if (r4 as i64) > (5 as i32 as i64) { pc += 16 }
    jsgt r4, 2, lbb_19801                           if (r4 as i64) > (2 as i32 as i64) { pc += 41 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jeq r4, 0, lbb_19853                            if r4 == (0 as i32 as i64 as u64) { pc += 91 }
    jeq r4, 1, lbb_19846                            if r4 == (1 as i32 as i64 as u64) { pc += 83 }
    jeq r4, 2, lbb_19765                            if r4 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19820                                    if true { pc += 55 }
lbb_19765:
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 86 }
lbb_19767:
    jsgt r4, 18, lbb_19782                          if (r4 as i64) > (18 as i32 as i64) { pc += 14 }
    jsgt r4, 15, lbb_19807                          if (r4 as i64) > (15 as i32 as i64) { pc += 38 }
    jeq r4, 13, lbb_19836                           if r4 == (13 as i32 as i64 as u64) { pc += 66 }
    jeq r4, 14, lbb_19848                           if r4 == (14 as i32 as i64 as u64) { pc += 77 }
    jeq r4, 15, lbb_19773                           if r4 == (15 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19820                                    if true { pc += 47 }
lbb_19773:
    mov64 r3, 15                                    r3 = 15 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 78 }
lbb_19775:
    jsgt r4, 8, lbb_19789                           if (r4 as i64) > (8 as i32 as i64) { pc += 13 }
    jeq r4, 6, lbb_19832                            if r4 == (6 as i32 as i64 as u64) { pc += 55 }
    jeq r4, 7, lbb_19842                            if r4 == (7 as i32 as i64 as u64) { pc += 64 }
    jeq r4, 8, lbb_19780                            if r4 == (8 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19820                                    if true { pc += 40 }
lbb_19780:
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 71 }
lbb_19782:
    jsgt r4, 21, lbb_19795                          if (r4 as i64) > (21 as i32 as i64) { pc += 12 }
    jeq r4, 19, lbb_19834                           if r4 == (19 as i32 as i64 as u64) { pc += 50 }
    jeq r4, 20, lbb_19844                           if r4 == (20 as i32 as i64 as u64) { pc += 59 }
    jeq r4, 21, lbb_19787                           if r4 == (21 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19820                                    if true { pc += 33 }
lbb_19787:
    mov64 r3, 21                                    r3 = 21 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 64 }
lbb_19789:
    jsgt r4, 10, lbb_19813                          if (r4 as i64) > (10 as i32 as i64) { pc += 23 }
    jeq r4, 9, lbb_19822                            if r4 == (9 as i32 as i64 as u64) { pc += 31 }
    jeq r4, 10, lbb_19793                           if r4 == (10 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19820                                    if true { pc += 27 }
lbb_19793:
    mov64 r3, 10                                    r3 = 10 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 58 }
lbb_19795:
    jsgt r4, 23, lbb_19818                          if (r4 as i64) > (23 as i32 as i64) { pc += 22 }
    jeq r4, 22, lbb_19824                           if r4 == (22 as i32 as i64 as u64) { pc += 27 }
    jeq r4, 23, lbb_19799                           if r4 == (23 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19820                                    if true { pc += 21 }
lbb_19799:
    mov64 r3, 23                                    r3 = 23 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 52 }
lbb_19801:
    jeq r4, 3, lbb_19838                            if r4 == (3 as i32 as i64 as u64) { pc += 36 }
    jeq r4, 4, lbb_19850                            if r4 == (4 as i32 as i64 as u64) { pc += 47 }
    jeq r4, 5, lbb_19805                            if r4 == (5 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19820                                    if true { pc += 15 }
lbb_19805:
    mov64 r3, 5                                     r3 = 5 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 46 }
lbb_19807:
    jeq r4, 16, lbb_19840                           if r4 == (16 as i32 as i64 as u64) { pc += 32 }
    jeq r4, 17, lbb_19852                           if r4 == (17 as i32 as i64 as u64) { pc += 43 }
    jeq r4, 18, lbb_19811                           if r4 == (18 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19820                                    if true { pc += 9 }
lbb_19811:
    mov64 r3, 18                                    r3 = 18 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 40 }
lbb_19813:
    jeq r4, 11, lbb_19826                           if r4 == (11 as i32 as i64 as u64) { pc += 12 }
    jeq r4, 12, lbb_19816                           if r4 == (12 as i32 as i64 as u64) { pc += 1 }
    ja lbb_19820                                    if true { pc += 4 }
lbb_19816:
    mov64 r3, 12                                    r3 = 12 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 35 }
lbb_19818:
    jeq r4, 24, lbb_19828                           if r4 == (24 as i32 as i64 as u64) { pc += 9 }
    jeq r4, 25, lbb_19830                           if r4 == (25 as i32 as i64 as u64) { pc += 10 }
lbb_19820:
    mov64 r5, r2                                    r5 = r2
    ja lbb_19853                                    if true { pc += 31 }
lbb_19822:
    mov64 r3, 9                                     r3 = 9 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 29 }
lbb_19824:
    mov64 r3, 22                                    r3 = 22 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 27 }
lbb_19826:
    mov64 r3, 11                                    r3 = 11 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 25 }
lbb_19828:
    mov64 r3, 24                                    r3 = 24 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 23 }
lbb_19830:
    mov64 r3, 25                                    r3 = 25 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 21 }
lbb_19832:
    mov64 r3, 6                                     r3 = 6 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 19 }
lbb_19834:
    mov64 r3, 19                                    r3 = 19 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 17 }
lbb_19836:
    mov64 r3, 13                                    r3 = 13 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 15 }
lbb_19838:
    mov64 r3, 3                                     r3 = 3 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 13 }
lbb_19840:
    mov64 r3, 16                                    r3 = 16 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 11 }
lbb_19842:
    mov64 r3, 7                                     r3 = 7 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 9 }
lbb_19844:
    mov64 r3, 20                                    r3 = 20 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 7 }
lbb_19846:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 5 }
lbb_19848:
    mov64 r3, 14                                    r3 = 14 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 3 }
lbb_19850:
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
    ja lbb_19853                                    if true { pc += 1 }
lbb_19852:
    mov64 r3, 17                                    r3 = 17 as i32 as i64 as u64
lbb_19853:
    stxw [r1+0x4], r5                       
    stxw [r1+0x0], r3                       
    exit                                    

function_19856:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    syscall [invalid]                       
    jne r0, 0, lbb_19873                            if r0 != (0 as i32 as i64 as u64) { pc += 12 }
    ldxdw r1, [r10-0x8]                     
    stxdw [r6+0x28], r1                     
    ldxdw r1, [r10-0x10]                    
    stxdw [r6+0x20], r1                     
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x20]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x8], r1                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_19882                                    if true { pc += 9 }
lbb_19873:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, r0                                    r2 = r0
    call function_19748                     
    ldxw r1, [r10-0x30]                     
    ldxw r2, [r10-0x2c]                     
    stxw [r6+0x8], r2                       
    stxw [r6+0x4], r1                       
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_19882:
    stxw [r6+0x0], r1                       
    exit                                    

function_19884:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    syscall [invalid]                       
    jne r0, 0, lbb_19897                            if r0 != (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r1, [r10-0x8]                     
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x10]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x8], r1                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_19906                                    if true { pc += 9 }
lbb_19897:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r0                                    r2 = r0
    call function_19748                     
    ldxw r1, [r10-0x20]                     
    ldxw r2, [r10-0x1c]                     
    stxw [r6+0x8], r2                       
    stxw [r6+0x4], r1                       
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_19906:
    stxw [r6+0x0], r1                       
    exit                                    

function_19908:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    exit                                    

function_19910:
    mov64 r7, r3                                    r7 = r3
    mov64 r6, r2                                    r6 = r2
    mov64 r8, r1                                    r8 = r1
    mov64 r1, 24                                    r1 = 24 as i32 as i64 as u64
    mov64 r2, 8                                     r2 = 8 as i32 as i64 as u64
    call function_6742                      
    jne r0, 0, lbb_19920                            if r0 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 8                                     r1 = 8 as i32 as i64 as u64
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    call function_20094                     
lbb_19920:
    stxb [r0+0x10], r8                      
    stxdw [r0+0x8], r7                      
    stxdw [r0+0x0], r6                      
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    exit                                    

function_19925:
    call function_19931                     

function_19926:
    call function_19929                     

function_19927:
    syscall [invalid]                       
    exit                                    

function_19929:
    call custom_panic                       
    syscall [invalid]                       

function_19931:
    syscall [invalid]                       

function_19932:
    lddw r1, 0x100033b02 --> b"Error: memory allocation failed, out of memory"        r1 load str located at 4295179010
    mov64 r2, 46                                    r2 = 46 as i32 as i64 as u64
    call function_19927                     
    call function_19925                     

function_19937:
    call function_19932                     
    mov64 r3, r2                                    r3 = r2
    lddw r2, 0x1000351d0 --> b"\x00\x00\x00\x00`p\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x0…        r2 load str located at 4295184848
    call function_21025                     
    exit                                    

function_19943:
    exit                                    

function_19944:
    ldxdw r2, [r1+0x0]                      
    jeq r2, 0, lbb_19949                            if r2 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r1, [r1+0x8]                      
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    call function_6768                      
lbb_19949:
    exit                                    

function_19950:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x100033b30 --> b"Error"               r2 load str located at 4295179056
    mov64 r3, 5                                     r3 = 5 as i32 as i64 as u64
    call function_21539                     
    exit                                    

function_19956:
    lddw r1, 0x100035200 --> b"\x00\x00\x00\x005;\x03\x00\x11\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295184896
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    lddw r2, 0x100035210 --> b"\x00\x00\x00\x00F;\x03\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x19\x00\x00\x0…        r2 load str located at 4295184912
    call function_20640                     

function_19968:
    mov64 r6, r1                                    r6 = r1
    mov64 r4, r2                                    r4 = r2
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r4, r2, lbb_19975                           if r4 < r2 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_19975:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_20004                            if r3 != (0 as i32 as i64 as u64) { pc += 27 }
    ldxdw r1, [r6+0x0]                      
    mov64 r7, r1                                    r7 = r1
    lsh64 r7, 1                                     r7 <<= 1   ///  r7 = r7.wrapping_shl(1)
    jgt r7, r4, lbb_19982                           if r7 > r4 { pc += 1 }
    mov64 r7, r4                                    r7 = r4
lbb_19982:
    jgt r7, 8, lbb_19984                            if r7 > (8 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 8                                     r7 = 8 as i32 as i64 as u64
lbb_19984:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r2, r7                                    r2 = r7
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    rsh64 r2, 63                                    r2 >>= 63   ///  r2 = r2.wrapping_shr(63)
    jeq r1, 0, lbb_19993                            if r1 == (0 as i32 as i64 as u64) { pc += 4 }
    ldxdw r3, [r6+0x8]                      
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r3                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_19993:
    stxdw [r10-0x10], r3                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -24                                   r4 += -24   ///  r4 = r4.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r3, r7                                    r3 = r7
    call function_20049                     
    ldxdw r1, [r10-0x30]                    
    jeq r1, 0, lbb_20005                            if r1 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r2, [r10-0x20]                    
    ldxdw r1, [r10-0x28]                    
lbb_20004:
    call function_20091                     
lbb_20005:
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x0], r7                      
    stxdw [r6+0x8], r1                      
    exit                                    

function_20009:
    mov64 r6, r1                                    r6 = r1
    ldxdw r3, [r6+0x0]                      
    mov64 r4, r3                                    r4 = r3
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r4, 0, lbb_20017                            if r4 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_20017:
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_20044                            if r5 != (0 as i32 as i64 as u64) { pc += 25 }
    mov64 r7, r3                                    r7 = r3
    lsh64 r7, 1                                     r7 <<= 1   ///  r7 = r7.wrapping_shl(1)
    jgt r7, r4, lbb_20023                           if r7 > r4 { pc += 1 }
    mov64 r7, r4                                    r7 = r4
lbb_20023:
    jgt r7, 8, lbb_20025                            if r7 > (8 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 8                                     r7 = 8 as i32 as i64 as u64
lbb_20025:
    mov64 r2, r7                                    r2 = r7
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    rsh64 r2, 63                                    r2 >>= 63   ///  r2 = r2.wrapping_shr(63)
    jeq r3, 0, lbb_20033                            if r3 == (0 as i32 as i64 as u64) { pc += 4 }
    ldxdw r1, [r6+0x8]                      
    stxdw [r10-0x8], r3                     
    stxdw [r10-0x18], r1                    
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_20033:
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -24                                   r4 += -24   ///  r4 = r4.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r3, r7                                    r3 = r7
    call function_20049                     
    ldxdw r1, [r10-0x30]                    
    jeq r1, 0, lbb_20045                            if r1 == (0 as i32 as i64 as u64) { pc += 3 }
    ldxdw r2, [r10-0x20]                    
    ldxdw r1, [r10-0x28]                    
lbb_20044:
    call function_20091                     
lbb_20045:
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x0], r7                      
    stxdw [r6+0x8], r1                      
    exit                                    

function_20049:
    mov64 r7, r3                                    r7 = r3
    mov64 r6, r1                                    r6 = r1
    jeq r2, 0, lbb_20065                            if r2 == (0 as i32 as i64 as u64) { pc += 13 }
    ldxdw r1, [r4+0x8]                      
    jeq r1, 0, lbb_20078                            if r1 == (0 as i32 as i64 as u64) { pc += 24 }
    ldxdw r2, [r4+0x10]                     
    jne r2, 0, lbb_20067                            if r2 != (0 as i32 as i64 as u64) { pc += 11 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r7, 0, lbb_20086                            if r7 == (0 as i32 as i64 as u64) { pc += 27 }
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_6742                      
    mov64 r1, r7                                    r1 = r7
    jeq r0, 0, lbb_20074                            if r0 == (0 as i32 as i64 as u64) { pc += 10 }
    ja lbb_20086                                    if true { pc += 21 }
lbb_20065:
    stdw [r6+0x8], 0                        
    ja lbb_20076                                    if true { pc += 9 }
lbb_20067:
    ldxdw r1, [r4+0x0]                      
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r4, r7                                    r4 = r7
    call function_6769                      
    mov64 r1, r7                                    r1 = r7
    jeq r0, 0, lbb_20074                            if r0 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_20086                                    if true { pc += 12 }
lbb_20074:
    stxdw [r6+0x10], r7                     
    stdw [r6+0x8], 1                        
lbb_20076:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ja lbb_20089                                    if true { pc += 11 }
lbb_20078:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r7, 0, lbb_20086                            if r7 == (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_6742                      
    mov64 r1, r7                                    r1 = r7
    jeq r0, 0, lbb_20074                            if r0 == (0 as i32 as i64 as u64) { pc += -12 }
lbb_20086:
    stxdw [r6+0x10], r1                     
    stxdw [r6+0x8], r0                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_20089:
    stxdw [r6+0x0], r1                      
    exit                                    

function_20091:
    jne r1, 0, lbb_20093                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    call function_19956                     
lbb_20093:
    call function_20094                     

function_20094:
    mov64 r3, r1                                    r3 = r1
    mov64 r1, r2                                    r1 = r2
    mov64 r2, r3                                    r2 = r3
    call function_6811                      
    ldxdw r3, [r2+0x10]                     
    stxdw [r1+0x8], r3                      
    ldxdw r2, [r2+0x8]                      
    stxdw [r1+0x0], r2                      
    exit                                    

function_20103:
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r1+0x10]                     
    ldxdw r1, [r1+0x8]                      
    call function_21842                     
    exit                                    

function_20108:
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r1+0x10]                     
    ldxdw r1, [r1+0x8]                      
    call function_21608                     
    exit                                    

function_20113:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    ldxdw r2, [r7+0x8]                      
    jeq r2, 0, lbb_20141                            if r2 == (0 as i32 as i64 as u64) { pc += 24 }
    ldxdw r1, [r7+0x0]                      
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r3, r1                                    r3 = r1
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
lbb_20121:
    ldxdw r8, [r3+0x0]                      
    add64 r8, r4                                    r8 += r4   ///  r8 = r8.wrapping_add(r4)
    add64 r3, 16                                    r3 += 16   ///  r3 = r3.wrapping_add(16 as i32 as i64 as u64)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r4, r8                                    r4 = r8
    jne r2, 0, lbb_20121                            if r2 != (0 as i32 as i64 as u64) { pc += -6 }
    ldxdw r2, [r7+0x18]                     
    jeq r2, 0, lbb_20144                            if r2 == (0 as i32 as i64 as u64) { pc += 15 }
    ldxdw r3, [r1+0x8]                      
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jeq r3, 0, lbb_20134                            if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_20134:
    jlt r8, 16, lbb_20136                           if r8 < (16 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_20136:
    jslt r8, 0, lbb_20141                           if (r8 as i64) < (0 as i32 as i64) { pc += 4 }
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    lsh64 r8, 1                                     r8 <<= 1   ///  r8 = r8.wrapping_shl(1)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jeq r1, 0, lbb_20144                            if r1 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_20141:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_20155                                    if true { pc += 11 }
lbb_20144:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_20155                            if r8 == (0 as i32 as i64 as u64) { pc += 8 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jslt r8, 0, lbb_20182                           if (r8 as i64) < (0 as i32 as i64) { pc += 33 }
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    mov64 r1, r8                                    r1 = r8
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_6742                      
    jeq r0, 0, lbb_20182                            if r0 == (0 as i32 as i64 as u64) { pc += 28 }
    mov64 r1, r8                                    r1 = r8
lbb_20155:
    stxdw [r10-0x18], r0                    
    stxdw [r10-0x20], r1                    
    stdw [r10-0x10], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    lddw r2, 0x1000351d0 --> b"\x00\x00\x00\x00`p\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x0…        r2 load str located at 4295184848
    mov64 r3, r7                                    r3 = r7
    call function_21025                     
    jne r0, 0, lbb_20172                            if r0 != (0 as i32 as i64 as u64) { pc += 7 }
    ldxdw r1, [r10-0x10]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x18]                    
    stxdw [r6+0x8], r1                      
    ldxdw r1, [r10-0x20]                    
    stxdw [r6+0x0], r1                      
    exit                                    
lbb_20172:
    mov64 r3, r10                                   r3 = r10
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    lddw r1, 0x100033b62 --> b"a formatting trait implementation returned an error"        r1 load str located at 4295179106
    mov64 r2, 51                                    r2 = 51 as i32 as i64 as u64
    lddw r4, 0x100035228 --> b"\x00\x00\x00\x00Xp\x02\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x0…        r4 load str located at 4295184936
    lddw r5, 0x100035248 --> b"\x00\x00\x00\x00\x95;\x03\x00\x18\x00\x00\x00\x00\x00\x00\x00y\x02\x00\x0…        r5 load str located at 4295184968
    call function_20704                     
lbb_20182:
    mov64 r1, r9                                    r1 = r9
    mov64 r2, r8                                    r2 = r8
    call function_20091                     

function_20185:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r2                                    r1 = r2
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jlt r1, 128, lbb_20211                          if r1 < (128 as i32 as i64 as u64) { pc += 21 }
    stw [r10-0x4], 0                        
    jlt r1, 2048, lbb_20224                         if r1 < (2048 as i32 as i64 as u64) { pc += 32 }
    mov64 r1, r2                                    r1 = r2
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jlt r1, 65536, lbb_20197                        if r1 < (65536 as i32 as i64 as u64) { pc += 1 }
    ja lbb_20233                                    if true { pc += 36 }
lbb_20197:
    mov64 r1, r2                                    r1 = r2
    and64 r1, 63                                    r1 &= 63   ///  r1 = r1.and(63)
    or64 r1, 128                                    r1 |= 128   ///  r1 = r1.or(128)
    stxb [r10-0x2], r1                      
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 12                                    r1 >>= 12   ///  r1 = r1.wrapping_shr(12)
    or64 r1, 224                                    r1 |= 224   ///  r1 = r1.or(224)
    stxb [r10-0x4], r1                      
    rsh64 r2, 6                                     r2 >>= 6   ///  r2 = r2.wrapping_shr(6)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    or64 r2, 128                                    r2 |= 128   ///  r2 = r2.or(128)
    stxb [r10-0x3], r2                      
    mov64 r7, 3                                     r7 = 3 as i32 as i64 as u64
    ja lbb_20252                                    if true { pc += 41 }
lbb_20211:
    ldxdw r7, [r6+0x10]                     
    ldxdw r1, [r6+0x0]                      
    jne r7, r1, lbb_20218                           if r7 != r1 { pc += 4 }
    mov64 r1, r6                                    r1 = r6
    mov64 r8, r2                                    r8 = r2
    call function_20009                     
    mov64 r2, r8                                    r2 = r8
lbb_20218:
    ldxdw r1, [r6+0x8]                      
    add64 r1, r7                                    r1 += r7   ///  r1 = r1.wrapping_add(r7)
    stxb [r1+0x0], r2                       
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r6+0x10], r7                     
    ja lbb_20269                                    if true { pc += 45 }
lbb_20224:
    mov64 r1, r2                                    r1 = r2
    and64 r1, 63                                    r1 &= 63   ///  r1 = r1.and(63)
    or64 r1, 128                                    r1 |= 128   ///  r1 = r1.or(128)
    stxb [r10-0x3], r1                      
    rsh64 r2, 6                                     r2 >>= 6   ///  r2 = r2.wrapping_shr(6)
    or64 r2, 192                                    r2 |= 192   ///  r2 = r2.or(192)
    stxb [r10-0x4], r2                      
    mov64 r7, 2                                     r7 = 2 as i32 as i64 as u64
    ja lbb_20252                                    if true { pc += 19 }
lbb_20233:
    mov64 r1, r2                                    r1 = r2
    and64 r1, 63                                    r1 &= 63   ///  r1 = r1.and(63)
    or64 r1, 128                                    r1 |= 128   ///  r1 = r1.or(128)
    stxb [r10-0x1], r1                      
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 6                                     r1 >>= 6   ///  r1 = r1.wrapping_shr(6)
    and64 r1, 63                                    r1 &= 63   ///  r1 = r1.and(63)
    or64 r1, 128                                    r1 |= 128   ///  r1 = r1.or(128)
    stxb [r10-0x2], r1                      
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 12                                    r1 >>= 12   ///  r1 = r1.wrapping_shr(12)
    and64 r1, 63                                    r1 &= 63   ///  r1 = r1.and(63)
    or64 r1, 128                                    r1 |= 128   ///  r1 = r1.or(128)
    stxb [r10-0x3], r1                      
    rsh64 r2, 18                                    r2 >>= 18   ///  r2 = r2.wrapping_shr(18)
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    or64 r2, 240                                    r2 |= 240   ///  r2 = r2.or(240)
    stxb [r10-0x4], r2                      
    mov64 r7, 4                                     r7 = 4 as i32 as i64 as u64
lbb_20252:
    ldxdw r8, [r6+0x10]                     
    ldxdw r1, [r6+0x0]                      
    sub64 r1, r8                                    r1 -= r8   ///  r1 = r1.wrapping_sub(r8)
    jge r1, r7, lbb_20261                           if r1 >= r7 { pc += 5 }
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    call function_19968                     
    ldxdw r8, [r6+0x10]                     
lbb_20261:
    ldxdw r1, [r6+0x8]                      
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -4                                    r2 += -4   ///  r2 = r2.wrapping_add(-4 as i32 as i64 as u64)
    mov64 r3, r7                                    r3 = r7
    call function_23152                     
    add64 r8, r7                                    r8 += r7   ///  r8 = r8.wrapping_add(r7)
    stxdw [r6+0x10], r8                     
lbb_20269:
    exit                                    

function_20270:
    mov64 r6, r3                                    r6 = r3
    mov64 r7, r1                                    r7 = r1
    ldxdw r8, [r7+0x10]                     
    ldxdw r1, [r7+0x0]                      
    sub64 r1, r8                                    r1 -= r8   ///  r1 = r1.wrapping_sub(r8)
    jge r1, r6, lbb_20283                           if r1 >= r6 { pc += 7 }
    mov64 r1, r7                                    r1 = r7
    mov64 r9, r2                                    r9 = r2
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r6                                    r3 = r6
    call function_19968                     
    mov64 r2, r9                                    r2 = r9
    ldxdw r8, [r7+0x10]                     
lbb_20283:
    ldxdw r1, [r7+0x8]                      
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    mov64 r3, r6                                    r3 = r6
    call function_23152                     
    add64 r8, r6                                    r8 += r6   ///  r8 = r8.wrapping_add(r6)
    stxdw [r7+0x10], r8                     
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    exit                                    

function_20291:
    call function_20185                     
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    exit                                    

function_20294:
    exit                                    

function_20295:
    exit                                    

function_20296:
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x100033bad --> b"out of range integral type conversion attempted"        r2 load str located at 4295179181
    mov64 r3, 47                                    r3 = 47 as i32 as i64 as u64
    call function_21373                     
    exit                                    

function_20302:
    mov64 r6, r2                                    r6 = r2
    mov64 r7, r1                                    r7 = r1
    call function_22811                     
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_20320                            if r0 != (0 as i32 as i64 as u64) { pc += 13 }
    ldxdw r1, [r6+0x20]                     
    ldxdw r2, [r6+0x28]                     
    ldxdw r4, [r2+0x18]                     
    lddw r2, 0x100033c14 --> b".."                  r2 load str located at 4295179284
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    callx r4                                
    jne r0, 0, lbb_20320                            if r0 != (0 as i32 as i64 as u64) { pc += 5 }
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r6                                    r2 = r6
    call function_22811                     
    mov64 r8, r0                                    r8 = r0
lbb_20320:
    mov64 r0, r8                                    r0 = r8
    exit                                    

function_20322:
    lddw r2, 0xa3f5295a8e9e84c1                     r2 load str located at -6632349407316638527
    stxdw [r1+0x8], r2                      
    lddw r2, 0xca6c34d708d27ee8                     r2 load str located at -3860652682392731928
    stxdw [r1+0x0], r2                      
    exit                                    

function_20329:
    mov64 r4, r2                                    r4 = r2
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    jsgt r4, 12, lbb_20341                          if (r4 as i64) > (12 as i32 as i64) { pc += 8 }
    jeq r4, 0, lbb_20469                            if r4 == (0 as i32 as i64 as u64) { pc += 135 }
    jeq r4, 9, lbb_20493                            if r4 == (9 as i32 as i64 as u64) { pc += 158 }
    jeq r4, 10, lbb_20337                           if r4 == (10 as i32 as i64 as u64) { pc += 1 }
    ja lbb_20353                                    if true { pc += 16 }
lbb_20337:
    sth [r1+0xa], 512                       
    stdw [r1+0x2], 0                        
    sth [r1+0x0], 28252                     
    ja lbb_20628                                    if true { pc += 287 }
lbb_20341:
    jsgt r4, 38, lbb_20351                          if (r4 as i64) > (38 as i32 as i64) { pc += 9 }
    jeq r4, 13, lbb_20473                           if r4 == (13 as i32 as i64 as u64) { pc += 130 }
    jeq r4, 34, lbb_20345                           if r4 == (34 as i32 as i64 as u64) { pc += 1 }
    ja lbb_20353                                    if true { pc += 8 }
lbb_20345:
    and64 r3, 65536                                 r3 &= 65536   ///  r3 = r3.and(65536)
    jeq r3, 0, lbb_20480                            if r3 == (0 as i32 as i64 as u64) { pc += 133 }
    sth [r1+0xa], 512                       
    stdw [r1+0x2], 0                        
    sth [r1+0x0], 8796                      
    ja lbb_20628                                    if true { pc += 277 }
lbb_20351:
    jeq r4, 39, lbb_20477                           if r4 == (39 as i32 as i64 as u64) { pc += 125 }
    jeq r4, 92, lbb_20489                           if r4 == (92 as i32 as i64 as u64) { pc += 136 }
lbb_20353:
    mov64 r4, r2                                    r4 = r2
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    jlt r4, 768, lbb_20480                          if r4 < (768 as i32 as i64 as u64) { pc += 123 }
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_20360                            if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_20480                                    if true { pc += 120 }
lbb_20360:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r2                                    r1 = r2
    mov64 r7, r2                                    r7 = r2
    call function_22997                     
    mov64 r2, r7                                    r2 = r7
    mov64 r1, r6                                    r1 = r6
    jeq r0, 0, lbb_20480                            if r0 == (0 as i32 as i64 as u64) { pc += 113 }
    lddw r3, 0xfffffffc                             r3 load str located at 4294967292
    mov64 r4, r2                                    r4 = r2
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    mov64 r3, r2                                    r3 = r2
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    lddw r4, 0xfffffff8                             r4 load str located at 4294967288
    mov64 r5, r3                                    r5 = r3
    and64 r5, r4                                    r5 &= r4   ///  r5 = r5.and(r4)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    or64 r3, r5                                     r3 |= r5   ///  r3 = r3.or(r5)
    lddw r4, 0xffffffe0                             r4 load str located at 4294967264
    mov64 r5, r3                                    r5 = r3
    and64 r5, r4                                    r5 &= r4   ///  r5 = r5.and(r4)
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    or64 r3, r5                                     r3 |= r5   ///  r3 = r3.or(r5)
    mov64 r4, r2                                    r4 = r2
    rsh64 r4, 12                                    r4 >>= 12   ///  r4 = r4.wrapping_shr(12)
    and64 r4, 15                                    r4 &= 15   ///  r4 = r4.and(15)
    mov64 r6, r2                                    r6 = r2
    rsh64 r6, 16                                    r6 >>= 16   ///  r6 = r6.wrapping_shr(16)
    and64 r6, 15                                    r6 &= 15   ///  r6 = r6.and(15)
    mov64 r7, r2                                    r7 = r2
    rsh64 r7, 4                                     r7 >>= 4   ///  r7 = r7.wrapping_shr(4)
    and64 r7, 15                                    r7 &= 15   ///  r7 = r7.and(15)
    lddw r0, 0x100033ab0 --> b"0123456789abcdefsrc/error.rsbacktrace capture fail"        r0 load str located at 4295178928
    lddw r5, 0x100033ab0 --> b"0123456789abcdefsrc/error.rsbacktrace capture fail"        r5 load str located at 4295178928
    add64 r5, r7                                    r5 += r7   ///  r5 = r5.wrapping_add(r7)
    mov64 r8, r2                                    r8 = r2
    rsh64 r8, 8                                     r8 >>= 8   ///  r8 = r8.wrapping_shr(8)
    and64 r8, 15                                    r8 &= 15   ///  r8 = r8.and(15)
    mov64 r9, r2                                    r9 = r2
    rsh64 r9, 20                                    r9 >>= 20   ///  r9 = r9.wrapping_shr(20)
    and64 r9, 15                                    r9 &= 15   ///  r9 = r9.and(15)
    lddw r7, 0x100033ab0 --> b"0123456789abcdefsrc/error.rsbacktrace capture fail"        r7 load str located at 4295178928
    add64 r7, r9                                    r7 += r9   ///  r7 = r7.wrapping_add(r9)
    lddw r9, 0x100033ab0 --> b"0123456789abcdefsrc/error.rsbacktrace capture fail"        r9 load str located at 4295178928
    add64 r9, r6                                    r9 += r6   ///  r9 = r9.wrapping_add(r6)
    lddw r6, 0x100033ab0 --> b"0123456789abcdefsrc/error.rsbacktrace capture fail"        r6 load str located at 4295178928
    add64 r6, r4                                    r6 += r4   ///  r6 = r6.wrapping_add(r4)
    lddw r4, 0x100033ab0 --> b"0123456789abcdefsrc/error.rsbacktrace capture fail"        r4 load str located at 4295178928
    add64 r4, r8                                    r4 += r8   ///  r4 = r4.wrapping_add(r8)
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    ldxb r2, [r0+0x0]                       
    stxb [r10-0x2], r2                      
    ldxb r2, [r5+0x0]                       
    stxb [r10-0x3], r2                      
    ldxb r2, [r4+0x0]                       
    stxb [r10-0x4], r2                      
    ldxb r2, [r6+0x0]                       
    stxb [r10-0x5], r2                      
    ldxb r2, [r9+0x0]                       
    stxb [r10-0x6], r2                      
    ldxb r2, [r7+0x0]                       
    stxb [r10-0x7], r2                      
    lddw r2, 0xfffffe00                             r2 load str located at 4294966784
    mov64 r4, r3                                    r4 = r3
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    rsh64 r4, 8                                     r4 >>= 8   ///  r4 = r4.wrapping_shr(8)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    lddw r2, 0xfffe0000                             r2 load str located at 4294836224
    mov64 r4, r3                                    r4 = r3
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    rsh64 r4, 16                                    r4 >>= 16   ///  r4 = r4.wrapping_shr(16)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    mov64 r4, r3                                    r4 = r3
    and64 r4, -2                                    r4 &= -2   ///  r4 = r4.and(-2)
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    and64 r3, 1431655765                            r3 &= 1431655765   ///  r3 = r3.and(1431655765)
    sub64 r4, r3                                    r4 -= r3   ///  r4 = r4.wrapping_sub(r3)
    mov64 r2, r4                                    r2 = r4
    and64 r2, 858993459                             r2 &= 858993459   ///  r2 = r2.and(858993459)
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    and64 r4, 858993459                             r4 &= 858993459   ///  r4 = r4.and(858993459)
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    stb [r10-0x8], 0                        
    sth [r10-0xa], 0                        
    stb [r10-0x1], 125                      
    and64 r2, 252645135                             r2 &= 252645135   ///  r2 = r2.and(252645135)
    mul64 r2, 16843009                              r2 *= 16843009   ///  r2 = r2.wrapping_mul(16843009 as u64)
    rsh64 r2, 26                                    r2 >>= 26   ///  r2 = r2.wrapping_shr(26)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    add64 r2, -2                                    r2 += -2   ///  r2 = r2.wrapping_add(-2 as i32 as i64 as u64)
    jlt r2, 11, lbb_20617                           if r2 < (11 as i32 as i64 as u64) { pc += 150 }
    mov64 r1, r2                                    r1 = r2
    ja lbb_20597                                    if true { pc += 128 }
lbb_20469:
    sth [r1+0xa], 512                       
    stdw [r1+0x2], 0                        
    sth [r1+0x0], 12380                     
    ja lbb_20628                                    if true { pc += 155 }
lbb_20473:
    sth [r1+0xa], 512                       
    stdw [r1+0x2], 0                        
    sth [r1+0x0], 29276                     
    ja lbb_20628                                    if true { pc += 151 }
lbb_20477:
    and64 r3, 256                                   r3 &= 256   ///  r3 = r3.and(256)
    jeq r3, 0, lbb_20480                            if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_20613                                    if true { pc += 133 }
lbb_20480:
    mov64 r6, r1                                    r6 = r1
    mov64 r7, r2                                    r7 = r2
    mov64 r1, r2                                    r1 = r2
    call function_22603                     
    jne r0, 0, lbb_20486                            if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_20497                                    if true { pc += 11 }
lbb_20486:
    stxw [r6+0x4], r7                       
    stb [r6+0x0], 128                       
    ja lbb_20628                                    if true { pc += 139 }
lbb_20489:
    sth [r1+0xa], 512                       
    stdw [r1+0x2], 0                        
    sth [r1+0x0], 23644                     
    ja lbb_20628                                    if true { pc += 135 }
lbb_20493:
    sth [r1+0xa], 512                       
    stdw [r1+0x2], 0                        
    sth [r1+0x0], 29788                     
    ja lbb_20628                                    if true { pc += 131 }
lbb_20497:
    lddw r1, 0xfffffffc                             r1 load str located at 4294967292
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r2                                    r3 = r2
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    mov64 r1, r2                                    r1 = r2
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    lddw r3, 0xfffffff8                             r3 load str located at 4294967288
    mov64 r4, r1                                    r4 = r1
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    or64 r1, r4                                     r1 |= r4   ///  r1 = r1.or(r4)
    lddw r3, 0xffffffe0                             r3 load str located at 4294967264
    mov64 r4, r1                                    r4 = r1
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    or64 r1, r4                                     r1 |= r4   ///  r1 = r1.or(r4)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 12                                    r3 >>= 12   ///  r3 = r3.wrapping_shr(12)
    and64 r3, 15                                    r3 &= 15   ///  r3 = r3.and(15)
    mov64 r0, r2                                    r0 = r2
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    and64 r0, 15                                    r0 &= 15   ///  r0 = r0.and(15)
    rsh64 r7, 4                                     r7 >>= 4   ///  r7 = r7.wrapping_shr(4)
    and64 r7, 15                                    r7 &= 15   ///  r7 = r7.and(15)
    lddw r5, 0x100033ab0 --> b"0123456789abcdefsrc/error.rsbacktrace capture fail"        r5 load str located at 4295178928
    lddw r4, 0x100033ab0 --> b"0123456789abcdefsrc/error.rsbacktrace capture fail"        r4 load str located at 4295178928
    add64 r4, r7                                    r4 += r7   ///  r4 = r4.wrapping_add(r7)
    mov64 r8, r2                                    r8 = r2
    rsh64 r8, 8                                     r8 >>= 8   ///  r8 = r8.wrapping_shr(8)
    and64 r8, 15                                    r8 &= 15   ///  r8 = r8.and(15)
    mov64 r9, r2                                    r9 = r2
    rsh64 r9, 20                                    r9 >>= 20   ///  r9 = r9.wrapping_shr(20)
    and64 r9, 15                                    r9 &= 15   ///  r9 = r9.and(15)
    lddw r7, 0x100033ab0 --> b"0123456789abcdefsrc/error.rsbacktrace capture fail"        r7 load str located at 4295178928
    add64 r7, r9                                    r7 += r9   ///  r7 = r7.wrapping_add(r9)
    lddw r9, 0x100033ab0 --> b"0123456789abcdefsrc/error.rsbacktrace capture fail"        r9 load str located at 4295178928
    add64 r9, r0                                    r9 += r0   ///  r9 = r9.wrapping_add(r0)
    lddw r0, 0x100033ab0 --> b"0123456789abcdefsrc/error.rsbacktrace capture fail"        r0 load str located at 4295178928
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    lddw r3, 0x100033ab0 --> b"0123456789abcdefsrc/error.rsbacktrace capture fail"        r3 load str located at 4295178928
    add64 r3, r8                                    r3 += r8   ///  r3 = r3.wrapping_add(r8)
    and64 r2, 15                                    r2 &= 15   ///  r2 = r2.and(15)
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    ldxb r2, [r5+0x0]                       
    stxb [r10-0x2], r2                      
    ldxb r2, [r4+0x0]                       
    stxb [r10-0x3], r2                      
    ldxb r2, [r3+0x0]                       
    stxb [r10-0x4], r2                      
    ldxb r2, [r0+0x0]                       
    stxb [r10-0x5], r2                      
    ldxb r2, [r9+0x0]                       
    stxb [r10-0x6], r2                      
    ldxb r2, [r7+0x0]                       
    stxb [r10-0x7], r2                      
    lddw r2, 0xfffffe00                             r2 load str located at 4294966784
    mov64 r3, r1                                    r3 = r1
    and64 r3, r2                                    r3 &= r2   ///  r3 = r3.and(r2)
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    lddw r2, 0xfffe0000                             r2 load str located at 4294836224
    mov64 r3, r1                                    r3 = r1
    and64 r3, r2                                    r3 &= r2   ///  r3 = r3.and(r2)
    rsh64 r3, 16                                    r3 >>= 16   ///  r3 = r3.wrapping_shr(16)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    xor64 r1, -1                                    r1 ^= -1   ///  r1 = r1.xor(-1)
    mov64 r2, r1                                    r2 = r1
    and64 r2, -2                                    r2 &= -2   ///  r2 = r2.and(-2)
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
    and64 r1, 1431655765                            r1 &= 1431655765   ///  r1 = r1.and(1431655765)
    sub64 r2, r1                                    r2 -= r1   ///  r2 = r2.wrapping_sub(r1)
    mov64 r1, r2                                    r1 = r2
    and64 r1, 858993459                             r1 &= 858993459   ///  r1 = r1.and(858993459)
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    and64 r2, 858993459                             r2 &= 858993459   ///  r2 = r2.and(858993459)
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 4                                     r2 >>= 4   ///  r2 = r2.wrapping_shr(4)
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    stb [r10-0x8], 0                        
    sth [r10-0xa], 0                        
    stb [r10-0x1], 125                      
    and64 r1, 252645135                             r1 &= 252645135   ///  r1 = r1.and(252645135)
    mul64 r1, 16843009                              r1 *= 16843009   ///  r1 = r1.wrapping_mul(16843009 as u64)
    rsh64 r1, 26                                    r1 >>= 26   ///  r1 = r1.wrapping_shr(26)
    and64 r1, 63                                    r1 &= 63   ///  r1 = r1.and(63)
    add64 r1, -2                                    r1 += -2   ///  r1 = r1.wrapping_add(-2 as i32 as i64 as u64)
    jlt r1, 11, lbb_20601                           if r1 < (11 as i32 as i64 as u64) { pc += 4 }
lbb_20597:
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    lddw r3, 0x1000354e0 --> b"\x00\x00\x00\x00\xddC\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x008\x00\x00\x0…        r3 load str located at 4295185632
    call function_21995                     
lbb_20601:
    mov64 r2, r10                                   r2 = r10
    add64 r2, -10                                   r2 += -10   ///  r2 = r2.wrapping_add(-10 as i32 as i64 as u64)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    stb [r2+0x2], 123                       
    sth [r2+0x0], 30044                     
    ldxh r2, [r10-0x2]                      
    stxh [r6+0x8], r2                       
    ldxdw r2, [r10-0xa]                     
    stxdw [r6+0x0], r2                      
    stxb [r6+0xa], r1                       
    stb [r6+0xb], 10                        
    ja lbb_20628                                    if true { pc += 15 }
lbb_20613:
    sth [r1+0xa], 512                       
    stdw [r1+0x2], 0                        
    sth [r1+0x0], 10076                     
    ja lbb_20628                                    if true { pc += 11 }
lbb_20617:
    mov64 r3, r10                                   r3 = r10
    add64 r3, -10                                   r3 += -10   ///  r3 = r3.wrapping_add(-10 as i32 as i64 as u64)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    stb [r3+0x2], 123                       
    sth [r3+0x0], 30044                     
    ldxh r3, [r10-0x2]                      
    stxh [r1+0x8], r3                       
    ldxdw r3, [r10-0xa]                     
    stxdw [r1+0x0], r3                      
    stxb [r1+0xa], r2                       
    stb [r1+0xb], 10                        
lbb_20628:
    exit                                    

function_20629:
    mov64 r3, r1                                    r3 = r1
    lddw r1, 0x100033c16 --> b"called `Option::unwrap()` on a `None` value"        r1 load str located at 4295179286
    mov64 r2, 43                                    r2 = 43 as i32 as i64 as u64
    call function_20650                     

function_20634:
    stxdw [r10-0x8], r2                     
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_20663                     

function_20640:
    stxdw [r10-0x10], r2                    
    stxdw [r10-0x18], r1                    
    lddw r1, 0x100035270 --> b"\x00\x00\x00\x00X{\x02\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x0…        r1 load str located at 4295185008
    stxdw [r10-0x20], r1                    
    sth [r10-0x8], 1                        
    stdw [r10-0x28], 1                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    call function_19926                     

function_20650:
    mov64 r4, r10                                   r4 = r10
    add64 r4, -16                                   r4 += -16   ///  r4 = r4.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x40], r4                    
    stxdw [r10-0x8], r2                     
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 0                      
    stdw [r10-0x30], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_20640                     

function_20663:
    lddw r3, 0x100033aa0 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00012345678…        r3 load str located at 4295178912
    stxdw [r10-0x40], r3                    
    mov64 r3, r10                                   r3 = r10
    add64 r3, -16                                   r3 += -16   ///  r3 = r3.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r3                    
    lddw r3, 0x10002cf98 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x13\x08\x00\x00\x00\x00\x00y\x12\x00\x00\x…        r3 load str located at 4295151512
    stxdw [r10-0x8], r3                     
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    call function_20640                     

function_20679:
    stxdw [r10-0x58], r2                    
    stxdw [r10-0x60], r1                    
    lddw r1, 0x100035290 --> b"\x00\x00\x00\x00\xa83\x03\x00 \x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295185040
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x10002cf40 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295151424
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_20640                     

function_20704:
    stxdw [r10-0x68], r2                    
    stxdw [r10-0x70], r1                    
    stxdw [r10-0x58], r4                    
    stxdw [r10-0x60], r3                    
    lddw r1, 0x1000352b0 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295185072
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    lddw r1, 0x10002cf68 --> b"y\x13\x00\x00\x00\x00\x00\x00y\x11\x08\x00\x00\x00\x00\x00y\x14\x18\x00\x…        r1 load str located at 4295151464
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x10002cf98 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x13\x08\x00\x00\x00\x00\x00y\x12\x00\x00\x…        r1 load str located at 4295151512
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r5                                    r2 = r5
    call function_20640                     
    mov64 r7, r3                                    r7 = r3
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    stxdw [r10-0x18], r2                    
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    stxdw [r10-0x40], r2                    
    ldxdw r2, [r1+0x8]                      
    stxdw [r10-0x28], r2                    
    ldxdw r2, [r1+0x0]                      
    stxdw [r10-0x30], r2                    
    ldxdw r1, [r1+0x10]                     
    stxdw [r10-0x20], r1                    
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x8], r1                     
    ja lbb_20766                                    if true { pc += 19 }
lbb_20747:
    ldxdw r2, [r10-0x18]                    
    ldxdw r3, [r10-0x8]                     
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    sub64 r9, r3                                    r9 -= r3   ///  r9 = r9.wrapping_sub(r3)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    ldxdw r3, [r10-0x20]                    
    stxb [r3+0x0], r1                       
    ldxdw r1, [r10-0x28]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r10-0x30]                    
    mov64 r3, r9                                    r3 = r9
    callx r4                                
    mov64 r1, r0                                    r1 = r0
    neg64 r0                                        r0 = -r0   ///  r0 = (r0 as i64).wrapping_neg() as u64
    jne r1, 0, lbb_20892                            if r1 != (0 as i32 as i64 as u64) { pc += 130 }
    ldxdw r1, [r10-0x10]                    
    xor64 r1, 1                                     r1 ^= 1   ///  r1 = r1.xor(1)
    stxdw [r10-0x8], r8                     
    jne r1, 0, lbb_20892                            if r1 != (0 as i32 as i64 as u64) { pc += 126 }
lbb_20766:
    jgt r6, r7, lbb_20863                           if r6 > r7 { pc += 96 }
    mov64 r1, r6                                    r1 = r6
lbb_20768:
    ldxdw r2, [r10-0x18]                    
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r3, r7                                    r3 = r7
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    jlt r3, 16, lbb_20833                           if r3 < (16 as i32 as i64 as u64) { pc += 60 }
    mov64 r6, r2                                    r6 = r2
    add64 r6, 7                                     r6 += 7   ///  r6 = r6.wrapping_add(7 as i32 as i64 as u64)
    and64 r6, -8                                    r6 &= -8   ///  r6 = r6.and(-8)
    mov64 r4, r6                                    r4 = r6
    sub64 r4, r2                                    r4 -= r2   ///  r4 = r4.wrapping_sub(r2)
    jeq r4, 0, lbb_20786                            if r4 == (0 as i32 as i64 as u64) { pc += 7 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_20780:
    mov64 r0, r2                                    r0 = r2
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    ldxb r0, [r0+0x0]                       
    jeq r0, 10, lbb_20846                           if r0 == (10 as i32 as i64 as u64) { pc += 62 }
    add64 r5, 1                                     r5 += 1   ///  r5 = r5.wrapping_add(1 as i32 as i64 as u64)
    jlt r5, r4, lbb_20780                           if r5 < r4 { pc += -6 }
lbb_20786:
    stxdw [r10-0x38], r1                    
    mov64 r1, r3                                    r1 = r3
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    jgt r4, r1, lbb_20819                           if r4 > r1 { pc += 28 }
    mov64 r0, 8                                     r0 = 8 as i32 as i64 as u64
    add64 r0, r6                                    r0 += r6   ///  r0 = r0.wrapping_add(r6)
    ja lbb_20798                                    if true { pc += 4 }
lbb_20794:
    add64 r0, 16                                    r0 += 16   ///  r0 = r0.wrapping_add(16 as i32 as i64 as u64)
    add64 r4, 16                                    r4 += 16   ///  r4 = r4.wrapping_add(16 as i32 as i64 as u64)
    ldxdw r1, [r10-0x10]                    
    jgt r4, r1, lbb_20819                           if r4 > r1 { pc += 21 }
lbb_20798:
    ldxdw r6, [r0-0x8]                      
    mov64 r8, r6                                    r8 = r6
    lddw r1, 0xa0a0a0a0a0a0a0a                      r1 load str located at 723401728380766730
    xor64 r8, r1                                    r8 ^= r1   ///  r8 = r8.xor(r1)
    lddw r5, 0xfefefefefefefeff                     r5 load str located at -72340172838076673
    add64 r8, r5                                    r8 += r5   ///  r8 = r8.wrapping_add(r5)
    xor64 r6, -1                                    r6 ^= -1   ///  r6 = r6.xor(-1)
    and64 r8, r6                                    r8 &= r6   ///  r8 = r8.and(r6)
    ldxdw r6, [r0+0x0]                      
    mov64 r9, r6                                    r9 = r6
    xor64 r9, r1                                    r9 ^= r1   ///  r9 = r9.xor(r1)
    add64 r9, r5                                    r9 += r5   ///  r9 = r9.wrapping_add(r5)
    xor64 r6, -1                                    r6 ^= -1   ///  r6 = r6.xor(-1)
    and64 r9, r6                                    r9 &= r6   ///  r9 = r9.and(r6)
    or64 r9, r8                                     r9 |= r8   ///  r9 = r9.or(r8)
    lddw r1, 0x8080808080808080                     r1 load str located at -9187201950435737472
    and64 r9, r1                                    r9 &= r1   ///  r9 = r9.and(r1)
    jeq r9, 0, lbb_20794                            if r9 == (0 as i32 as i64 as u64) { pc += -25 }
lbb_20819:
    mov64 r6, r7                                    r6 = r7
    ldxdw r1, [r10-0x38]                    
    jeq r3, r4, lbb_20863                           if r3 == r4 { pc += 41 }
    sub64 r3, r4                                    r3 -= r4   ///  r3 = r3.wrapping_sub(r4)
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_20825:
    mov64 r0, r2                                    r0 = r2
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    ldxb r0, [r0+0x0]                       
    jeq r0, 10, lbb_20844                           if r0 == (10 as i32 as i64 as u64) { pc += 15 }
    add64 r5, 1                                     r5 += 1   ///  r5 = r5.wrapping_add(1 as i32 as i64 as u64)
    mov64 r6, r7                                    r6 = r7
    jlt r5, r3, lbb_20825                           if r5 < r3 { pc += -7 }
    ja lbb_20863                                    if true { pc += 30 }
lbb_20833:
    mov64 r6, r7                                    r6 = r7
    jeq r7, r1, lbb_20863                           if r7 == r1 { pc += 28 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_20836:
    mov64 r4, r2                                    r4 = r2
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    ldxb r4, [r4+0x0]                       
    jeq r4, 10, lbb_20846                           if r4 == (10 as i32 as i64 as u64) { pc += 6 }
    add64 r5, 1                                     r5 += 1   ///  r5 = r5.wrapping_add(1 as i32 as i64 as u64)
    mov64 r6, r7                                    r6 = r7
    jlt r5, r3, lbb_20836                           if r5 < r3 { pc += -7 }
    ja lbb_20863                                    if true { pc += 19 }
lbb_20844:
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r5, r4                                    r5 = r4
lbb_20846:
    mov64 r2, r1                                    r2 = r1
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    mov64 r6, r2                                    r6 = r2
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    jlt r2, r7, lbb_20854                           if r2 < r7 { pc += 3 }
lbb_20851:
    mov64 r1, r6                                    r1 = r6
    jgt r6, r7, lbb_20863                           if r6 > r7 { pc += 10 }
    ja lbb_20768                                    if true { pc += -86 }
lbb_20854:
    ldxdw r2, [r10-0x18]                    
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ldxb r1, [r2+0x0]                       
    mov64 r8, r6                                    r8 = r6
    mov64 r9, r6                                    r9 = r6
    jeq r1, 10, lbb_20868                           if r1 == (10 as i32 as i64 as u64) { pc += 6 }
    ja lbb_20851                                    if true { pc += -12 }
lbb_20863:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x8]                     
    mov64 r8, r1                                    r8 = r1
    mov64 r9, r7                                    r9 = r7
    jeq r1, r7, lbb_20892                           if r1 == r7 { pc += 24 }
lbb_20868:
    stxdw [r10-0x10], r0                    
    ldxdw r1, [r10-0x20]                    
    ldxb r1, [r1+0x0]                       
    jeq r1, 0, lbb_20882                            if r1 == (0 as i32 as i64 as u64) { pc += 10 }
    ldxdw r1, [r10-0x28]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r10-0x30]                    
    lddw r2, 0x100033af7 --> b"    "                r2 load str located at 4295178999
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
    callx r4                                
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_20892                            if r1 != (0 as i32 as i64 as u64) { pc += 10 }
lbb_20882:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0x8]                     
    jeq r9, r2, lbb_20747                           if r9 == r2 { pc += -138 }
    ldxdw r1, [r10-0x40]                    
    add64 r1, r9                                    r1 += r9   ///  r1 = r1.wrapping_add(r9)
    ldxb r2, [r1+0x0]                       
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r2, 10, lbb_20747                           if r2 == (10 as i32 as i64 as u64) { pc += -143 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_20747                                    if true { pc += -145 }
lbb_20892:
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    exit                                    

function_20894:
    ldxdw r7, [r1+0x8]                      
    ldxdw r6, [r1+0x0]                      
    ldxdw r8, [r1+0x10]                     
    ldxb r1, [r8+0x0]                       
    jne r1, 0, lbb_20910                            if r1 != (0 as i32 as i64 as u64) { pc += 11 }
lbb_20899:
    mov64 r3, r2                                    r3 = r2
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r3, 10, lbb_20905                           if r3 == (10 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_20905:
    stxb [r8+0x0], r1                       
    ldxdw r3, [r7+0x20]                     
    mov64 r1, r6                                    r1 = r6
    callx r3                                
    ja lbb_20921                                    if true { pc += 11 }
lbb_20910:
    ldxdw r4, [r7+0x18]                     
    mov64 r1, r6                                    r1 = r6
    mov64 r9, r2                                    r9 = r2
    lddw r2, 0x100033af7 --> b"    "                r2 load str located at 4295178999
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
    callx r4                                
    mov64 r2, r9                                    r2 = r9
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_20899                            if r1 == (0 as i32 as i64 as u64) { pc += -22 }
lbb_20921:
    exit                                    

function_20922:
    mov64 r6, r1                                    r6 = r1
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ldxdw r9, [r6+0x0]                      
    ldxb r1, [r6+0x10]                      
    jne r1, 0, lbb_21015                            if r1 != (0 as i32 as i64 as u64) { pc += 88 }
    ldxdw r8, [r6+0x8]                      
    ldxw r1, [r8+0x34]                      
    mov64 r4, r1                                    r4 = r1
    and64 r4, 4                                     r4 &= 4   ///  r4 = r4.and(4)
    jne r4, 0, lbb_20954                            if r4 != (0 as i32 as i64 as u64) { pc += 22 }
    lddw r4, 0x100033c59 --> b"((\x0a,library/core/src/fmt/num.rs0x00010203040506070"        r4 load str located at 4295179353
    jeq r9, 0, lbb_20937                            if r9 == (0 as i32 as i64 as u64) { pc += 2 }
    lddw r4, 0x100033c55 --> b", ,\x0a((\x0a,library/core/src/fmt/num.rs0x0001020304050"        r4 load str located at 4295179349
lbb_20937:
    stxdw [r10-0x68], r3                    
    stxdw [r10-0x70], r2                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jeq r9, 0, lbb_20942                            if r9 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
lbb_20942:
    ldxdw r1, [r8+0x20]                     
    ldxdw r2, [r8+0x28]                     
    ldxdw r5, [r2+0x18]                     
    mov64 r2, r4                                    r2 = r4
    callx r5                                
    ldxdw r1, [r10-0x70]                    
    ldxdw r2, [r10-0x68]                    
    jne r0, 0, lbb_21015                            if r0 != (0 as i32 as i64 as u64) { pc += 65 }
    ldxdw r3, [r2+0x18]                     
    mov64 r2, r8                                    r2 = r8
    callx r3                                
    ja lbb_21014                                    if true { pc += 60 }
lbb_20954:
    jeq r9, 0, lbb_20956                            if r9 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_20969                                    if true { pc += 13 }
lbb_20956:
    ldxdw r1, [r8+0x20]                     
    ldxdw r4, [r8+0x28]                     
    ldxdw r4, [r4+0x18]                     
    stxdw [r10-0x70], r2                    
    lddw r2, 0x100033c5a --> b"(\x0a,library/core/src/fmt/num.rs0x000102030405060708"        r2 load str located at 4295179354
    stxdw [r10-0x68], r3                    
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    callx r4                                
    ldxdw r3, [r10-0x68]                    
    ldxdw r2, [r10-0x70]                    
    jne r0, 0, lbb_21015                            if r0 != (0 as i32 as i64 as u64) { pc += 47 }
    ldxw r1, [r8+0x34]                      
lbb_20969:
    stb [r10-0x41], 1                       
    ldxdw r4, [r8+0x20]                     
    ldxdw r5, [r8+0x28]                     
    mov64 r0, r10                                   r0 = r10
    add64 r0, -65                                   r0 += -65   ///  r0 = r0.wrapping_add(-65 as i32 as i64 as u64)
    stxdw [r10-0x50], r0                    
    stxdw [r10-0x58], r5                    
    stxdw [r10-0x60], r4                    
    ldxdw r4, [r8+0x0]                      
    stxdw [r10-0x68], r4                    
    ldxdw r4, [r8+0x8]                      
    stxdw [r10-0x70], r4                    
    ldxdw r0, [r8+0x10]                     
    ldxdw r5, [r8+0x18]                     
    ldxw r4, [r8+0x30]                      
    ldxb r8, [r8+0x38]                      
    stxb [r10-0x8], r8                      
    stxw [r10-0x10], r4                     
    stxw [r10-0xc], r1                      
    lddw r1, 0x1000352d0 --> b"\x00\x00\x00\x00P{\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x0…        r1 load str located at 4295185104
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stxdw [r10-0x28], r5                    
    stxdw [r10-0x30], r0                    
    ldxdw r1, [r10-0x70]                    
    stxdw [r10-0x38], r1                    
    ldxdw r1, [r10-0x68]                    
    stxdw [r10-0x40], r1                    
    ldxdw r4, [r3+0x18]                     
    mov64 r3, r10                                   r3 = r10
    add64 r3, -64                                   r3 += -64   ///  r3 = r3.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r1, r2                                    r1 = r2
    mov64 r2, r3                                    r2 = r3
    callx r4                                
    jne r0, 0, lbb_21015                            if r0 != (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r1, [r10-0x18]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r10-0x20]                    
    lddw r2, 0x100033c57 --> b",\x0a"               r2 load str located at 4295179351
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    callx r4                                
lbb_21014:
    mov64 r7, r0                                    r7 = r0
lbb_21015:
    stxb [r6+0x10], r7                      
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r6+0x0], r9                      
    mov64 r0, r6                                    r0 = r6
    exit                                    

function_21020:
    mov64 r3, r2                                    r3 = r2
    lddw r2, 0x1000352d0 --> b"\x00\x00\x00\x00P{\x02\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x0…        r2 load str located at 4295185104
    call function_21025                     
    exit                                    

function_21025:
    stxdw [r10-0x18], r2                    
    stxdw [r10-0x20], r1                    
    stb [r10-0x8], 3                        
    stdw [r10-0x10], 32                     
    stdw [r10-0x30], 0                      
    stdw [r10-0x40], 0                      
    ldxdw r8, [r3+0x20]                     
    stxdw [r10-0x50], r3                    
    jne r8, 0, lbb_21064                            if r8 != (0 as i32 as i64 as u64) { pc += 30 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r6, [r3+0x18]                     
    jeq r6, 0, lbb_21138                            if r6 == (0 as i32 as i64 as u64) { pc += 101 }
    ldxdw r1, [r10-0x50]                    
    ldxdw r8, [r1+0x10]                     
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    lsh64 r6, 4                                     r6 <<= 4   ///  r6 = r6.wrapping_shl(4)
    ldxdw r9, [r1+0x0]                      
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_21061                                    if true { pc += 17 }
lbb_21044:
    ldxdw r1, [r10-0x18]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r2, [r9-0x8]                      
    ldxdw r1, [r10-0x20]                    
    callx r4                                
    jne r0, 0, lbb_21155                            if r0 != (0 as i32 as i64 as u64) { pc += 105 }
lbb_21050:
    ldxdw r1, [r8-0x8]                      
    ldxdw r3, [r8+0x0]                      
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    callx r3                                
    jne r0, 0, lbb_21155                            if r0 != (0 as i32 as i64 as u64) { pc += 99 }
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    add64 r8, 16                                    r8 += 16   ///  r8 = r8.wrapping_add(16 as i32 as i64 as u64)
    add64 r9, 16                                    r9 += 16   ///  r9 = r9.wrapping_add(16 as i32 as i64 as u64)
    add64 r6, -16                                   r6 += -16   ///  r6 = r6.wrapping_add(-16 as i32 as i64 as u64)
    jeq r6, 0, lbb_21138                            if r6 == (0 as i32 as i64 as u64) { pc += 77 }
lbb_21061:
    ldxdw r3, [r9+0x0]                      
    jeq r3, 0, lbb_21050                            if r3 == (0 as i32 as i64 as u64) { pc += -13 }
    ja lbb_21044                                    if true { pc += -20 }
lbb_21064:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r9, [r3+0x28]                     
    jeq r9, 0, lbb_21138                            if r9 == (0 as i32 as i64 as u64) { pc += 71 }
    add64 r8, 24                                    r8 += 24   ///  r8 = r8.wrapping_add(24 as i32 as i64 as u64)
    mul64 r9, 56                                    r9 *= 56   ///  r9 = r9.wrapping_mul(56 as u64)
    ldxdw r1, [r10-0x50]                    
    ldxdw r2, [r1+0x10]                     
    stxdw [r10-0x48], r2                    
    ldxdw r6, [r1+0x0]                      
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_21092                                    if true { pc += 17 }
lbb_21075:
    stxdw [r10-0x28], r2                    
    stxdw [r10-0x30], r1                    
    ldxdw r1, [r8+0x8]                      
    lsh64 r1, 4                                     r1 <<= 4   ///  r1 = r1.wrapping_shl(4)
    ldxdw r2, [r10-0x48]                    
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxdw r1, [r2+0x0]                      
    ldxdw r3, [r2+0x8]                      
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    callx r3                                
    jne r0, 0, lbb_21155                            if r0 != (0 as i32 as i64 as u64) { pc += 68 }
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    add64 r8, 56                                    r8 += 56   ///  r8 = r8.wrapping_add(56 as i32 as i64 as u64)
    add64 r6, 16                                    r6 += 16   ///  r6 = r6.wrapping_add(16 as i32 as i64 as u64)
    add64 r9, -56                                   r9 += -56   ///  r9 = r9.wrapping_add(-56 as i32 as i64 as u64)
    jeq r9, 0, lbb_21138                            if r9 == (0 as i32 as i64 as u64) { pc += 46 }
lbb_21092:
    ldxdw r3, [r6+0x0]                      
    jeq r3, 0, lbb_21100                            if r3 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r1, [r10-0x18]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r2, [r6-0x8]                      
    ldxdw r1, [r10-0x20]                    
    callx r4                                
    jne r0, 0, lbb_21155                            if r0 != (0 as i32 as i64 as u64) { pc += 55 }
lbb_21100:
    ldxw r1, [r8+0x10]                      
    stxw [r10-0x10], r1                     
    ldxb r1, [r8+0x18]                      
    stxb [r10-0x8], r1                      
    ldxw r1, [r8+0x14]                      
    stxw [r10-0xc], r1                      
    ldxdw r1, [r8+0x0]                      
    ldxdw r3, [r8-0x8]                      
    jeq r3, 0, lbb_21120                            if r3 == (0 as i32 as i64 as u64) { pc += 11 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jeq r3, 1, lbb_21112                            if r3 == (1 as i32 as i64 as u64) { pc += 1 }
    ja lbb_21121                                    if true { pc += 9 }
lbb_21112:
    lsh64 r1, 4                                     r1 <<= 4   ///  r1 = r1.wrapping_shl(4)
    ldxdw r3, [r10-0x48]                    
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    ldxdw r4, [r3+0x8]                      
    jne r4, 0, lbb_21121                            if r4 != (0 as i32 as i64 as u64) { pc += 4 }
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r1, [r3+0x0]                      
    ja lbb_21121                                    if true { pc += 1 }
lbb_21120:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_21121:
    stxdw [r10-0x38], r1                    
    stxdw [r10-0x40], r2                    
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r3, [r8-0x18]                     
    jeq r3, 2, lbb_21075                            if r3 == (2 as i32 as i64 as u64) { pc += -51 }
    ldxdw r2, [r8-0x10]                     
    jeq r3, 1, lbb_21130                            if r3 == (1 as i32 as i64 as u64) { pc += 2 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ja lbb_21075                                    if true { pc += -55 }
lbb_21130:
    lsh64 r2, 4                                     r2 <<= 4   ///  r2 = r2.wrapping_shl(4)
    ldxdw r3, [r10-0x48]                    
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    ldxdw r4, [r3+0x8]                      
    jne r4, 0, lbb_21075                            if r4 != (0 as i32 as i64 as u64) { pc += -60 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r2, [r3+0x0]                      
    ja lbb_21075                                    if true { pc += -63 }
lbb_21138:
    ldxdw r1, [r10-0x50]                    
    ldxdw r1, [r1+0x8]                      
    jlt r7, r1, lbb_21142                           if r7 < r1 { pc += 1 }
    ja lbb_21153                                    if true { pc += 11 }
lbb_21142:
    lsh64 r7, 4                                     r7 <<= 4   ///  r7 = r7.wrapping_shl(4)
    ldxdw r1, [r10-0x50]                    
    ldxdw r1, [r1+0x0]                      
    add64 r1, r7                                    r1 += r7   ///  r1 = r1.wrapping_add(r7)
    ldxdw r3, [r1+0x8]                      
    ldxdw r2, [r1+0x0]                      
    ldxdw r1, [r10-0x18]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r10-0x20]                    
    callx r4                                
    jne r0, 0, lbb_21155                            if r0 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_21153:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_21156                                    if true { pc += 1 }
lbb_21155:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_21156:
    exit                                    

function_21157:
    mov64 r0, r4                                    r0 = r4
    mov64 r8, r3                                    r8 = r3
    ldxdw r7, [r5-0xff8]                    
    stxdw [r10-0x18], r7                    
    jne r2, 0, lbb_21166                            if r2 != (0 as i32 as i64 as u64) { pc += 4 }
    mov64 r3, 45                                    r3 = 45 as i32 as i64 as u64
    ldxw r9, [r1+0x34]                      
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    ja lbb_21171                                    if true { pc += 5 }
lbb_21166:
    mov64 r3, 1114112                               r3 = 1114112 as i32 as i64 as u64
    ldxw r9, [r1+0x34]                      
    mov64 r2, r9                                    r2 = r9
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_21231                            if r2 != (0 as i32 as i64 as u64) { pc += 60 }
lbb_21171:
    ldxdw r2, [r5-0x1000]                   
    stxdw [r10-0x20], r2                    
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r2, r9                                    r2 = r9
    and64 r2, 4                                     r2 &= 4   ///  r2 = r2.and(4)
    jeq r2, 0, lbb_21178                            if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_21205                                    if true { pc += 27 }
lbb_21178:
    ldxdw r2, [r1+0x0]                      
    jne r2, 0, lbb_21189                            if r2 != (0 as i32 as i64 as u64) { pc += 9 }
    ldxdw r8, [r1+0x28]                     
    ldxdw r7, [r1+0x20]                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r8                                    r2 = r8
    mov64 r5, r0                                    r5 = r0
    call function_21350                     
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_21228                            if r0 != (0 as i32 as i64 as u64) { pc += 40 }
    ja lbb_21222                                    if true { pc += 33 }
lbb_21189:
    ldxdw r8, [r1+0x8]                      
    jgt r8, r7, lbb_21192                           if r8 > r7 { pc += 1 }
    ja lbb_21214                                    if true { pc += 22 }
lbb_21192:
    and64 r9, 8                                     r9 &= 8   ///  r9 = r9.and(8)
    jeq r9, 0, lbb_21195                            if r9 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_21258                                    if true { pc += 63 }
lbb_21195:
    sub64 r8, r7                                    r8 -= r7   ///  r8 = r8.wrapping_sub(r7)
    ldxb r9, [r1+0x38]                      
    stxdw [r10-0x28], r0                    
    stxdw [r10-0x30], r3                    
    stxdw [r10-0x38], r4                    
    jsgt r9, 1, lbb_21284                           if (r9 as i64) > (1 as i32 as i64) { pc += 83 }
    jeq r9, 0, lbb_21303                            if r9 == (0 as i32 as i64 as u64) { pc += 101 }
lbb_21202:
    mov64 r9, r8                                    r9 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ja lbb_21303                                    if true { pc += 98 }
lbb_21205:
    stxdw [r10-0x30], r3                    
    stxdw [r10-0x28], r0                    
    jlt r0, 32, lbb_21235                           if r0 < (32 as i32 as i64 as u64) { pc += 27 }
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r0                                    r2 = r0
    call function_22073                     
    mov64 r1, r6                                    r1 = r6
    ja lbb_21245                                    if true { pc += 31 }
lbb_21214:
    ldxdw r8, [r1+0x28]                     
    ldxdw r7, [r1+0x20]                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r8                                    r2 = r8
    mov64 r5, r0                                    r5 = r0
    call function_21350                     
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_21228                            if r0 != (0 as i32 as i64 as u64) { pc += 6 }
lbb_21222:
    ldxdw r4, [r8+0x18]                     
    mov64 r1, r7                                    r1 = r7
    ldxdw r2, [r10-0x20]                    
    ldxdw r3, [r10-0x18]                    
    callx r4                                
    mov64 r6, r0                                    r6 = r0
lbb_21228:
    and64 r6, 1                                     r6 &= 1   ///  r6 = r6.and(1)
    mov64 r0, r6                                    r0 = r6
    exit                                    
lbb_21231:
    mov64 r3, 43                                    r3 = 43 as i32 as i64 as u64
    ldxdw r7, [r10-0x18]                    
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    ja lbb_21171                                    if true { pc += -64 }
lbb_21235:
    mov64 r2, r0                                    r2 = r0
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r2, 0, lbb_21245                            if r2 == (0 as i32 as i64 as u64) { pc += 7 }
    mov64 r2, r8                                    r2 = r8
    ldxdw r3, [r10-0x28]                    
    ja lbb_21251                                    if true { pc += 10 }
lbb_21241:
    add64 r0, r4                                    r0 += r4   ///  r0 = r0.wrapping_add(r4)
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    jne r3, 0, lbb_21251                            if r3 != (0 as i32 as i64 as u64) { pc += 6 }
lbb_21245:
    add64 r0, r7                                    r0 += r7   ///  r0 = r0.wrapping_add(r7)
    mov64 r7, r0                                    r7 = r0
    ldxdw r0, [r10-0x28]                    
    mov64 r4, r8                                    r4 = r8
    ldxdw r3, [r10-0x30]                    
    ja lbb_21178                                    if true { pc += -73 }
lbb_21251:
    ldxb r5, [r2+0x0]                       
    lsh64 r5, 56                                    r5 <<= 56   ///  r5 = r5.wrapping_shl(56)
    arsh64 r5, 56                                   r5 >>= 56 (signed)   ///  r5 = (r5 as i64).wrapping_shr(56)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jsgt r5, -65, lbb_21241                         if (r5 as i64) > (-65 as i32 as i64) { pc += -15 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ja lbb_21241                                    if true { pc += -17 }
lbb_21258:
    ldxw r2, [r1+0x30]                      
    stxdw [r10-0x40], r2                    
    stw [r1+0x30], 48                       
    ldxb r2, [r1+0x38]                      
    stxdw [r10-0x48], r2                    
    stb [r1+0x38], 1                        
    ldxdw r2, [r1+0x20]                     
    stxdw [r10-0x10], r1                    
    ldxdw r9, [r1+0x28]                     
    stxdw [r10-0x8], r2                     
    mov64 r1, r2                                    r1 = r2
    mov64 r2, r9                                    r2 = r9
    mov64 r5, r0                                    r5 = r0
    call function_21350                     
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_21228                            if r0 != (0 as i32 as i64 as u64) { pc += -46 }
    sub64 r8, r7                                    r8 -= r7   ///  r8 = r8.wrapping_sub(r7)
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
lbb_21276:
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    jeq r8, 0, lbb_21286                            if r8 == (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r3, [r9+0x20]                     
    ldxdw r1, [r10-0x8]                     
    mov64 r2, 48                                    r2 = 48 as i32 as i64 as u64
    callx r3                                
    jne r0, 0, lbb_21228                            if r0 != (0 as i32 as i64 as u64) { pc += -55 }
    ja lbb_21276                                    if true { pc += -8 }
lbb_21284:
    jeq r9, 2, lbb_21299                            if r9 == (2 as i32 as i64 as u64) { pc += 14 }
    ja lbb_21202                                    if true { pc += -84 }
lbb_21286:
    ldxdw r4, [r9+0x18]                     
    ldxdw r1, [r10-0x8]                     
    ldxdw r2, [r10-0x20]                    
    ldxdw r3, [r10-0x18]                    
    callx r4                                
    jne r0, 0, lbb_21228                            if r0 != (0 as i32 as i64 as u64) { pc += -64 }
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x48]                    
    stxb [r1+0x38], r2                      
    ldxdw r2, [r10-0x40]                    
    stxw [r1+0x30], r2                      
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ja lbb_21228                                    if true { pc += -71 }
lbb_21299:
    mov64 r9, r8                                    r9 = r8
    rsh64 r9, 1                                     r9 >>= 1   ///  r9 = r9.wrapping_shr(1)
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
    rsh64 r8, 1                                     r8 >>= 1   ///  r8 = r8.wrapping_shr(1)
lbb_21303:
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    ldxw r2, [r1+0x30]                      
    stxdw [r10-0x10], r2                    
    ldxdw r2, [r1+0x28]                     
    stxdw [r10-0x8], r2                     
    ldxdw r7, [r1+0x20]                     
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
lbb_21310:
    add64 r9, -1                                    r9 += -1   ///  r9 = r9.wrapping_add(-1 as i32 as i64 as u64)
    jeq r9, 0, lbb_21319                            if r9 == (0 as i32 as i64 as u64) { pc += 7 }
    ldxdw r1, [r10-0x8]                     
    ldxdw r3, [r1+0x20]                     
    mov64 r1, r7                                    r1 = r7
    ldxdw r2, [r10-0x10]                    
    callx r3                                
    jne r0, 0, lbb_21228                            if r0 != (0 as i32 as i64 as u64) { pc += -90 }
    ja lbb_21310                                    if true { pc += -9 }
lbb_21319:
    mov64 r1, r7                                    r1 = r7
    ldxdw r2, [r10-0x8]                     
    ldxdw r3, [r10-0x30]                    
    ldxdw r4, [r10-0x38]                    
    ldxdw r5, [r10-0x28]                    
    call function_21350                     
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_21228                            if r0 != (0 as i32 as i64 as u64) { pc += -99 }
    ldxdw r1, [r10-0x8]                     
    ldxdw r4, [r1+0x18]                     
    mov64 r1, r7                                    r1 = r7
    ldxdw r2, [r10-0x20]                    
    ldxdw r3, [r10-0x18]                    
    callx r4                                
    jne r0, 0, lbb_21228                            if r0 != (0 as i32 as i64 as u64) { pc += -106 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_21335:
    mov64 r1, r8                                    r1 = r8
    jeq r8, r6, lbb_21346                           if r8 == r6 { pc += 9 }
    ldxdw r1, [r10-0x8]                     
    ldxdw r3, [r1+0x20]                     
    mov64 r1, r7                                    r1 = r7
    ldxdw r2, [r10-0x10]                    
    callx r3                                
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    jeq r0, 0, lbb_21335                            if r0 == (0 as i32 as i64 as u64) { pc += -9 }
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r6                                    r1 = r6
lbb_21346:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jlt r1, r8, lbb_21228                           if r1 < r8 { pc += -120 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ja lbb_21228                                    if true { pc += -122 }

function_21350:
    mov64 r6, r5                                    r6 = r5
    mov64 r7, r4                                    r7 = r4
    mov64 r8, r2                                    r8 = r2
    mov64 r2, r3                                    r2 = r3
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jeq r2, 1114112, lbb_21365                      if r2 == (1114112 as i32 as i64 as u64) { pc += 8 }
    ldxdw r4, [r8+0x20]                     
    mov64 r9, r1                                    r9 = r1
    mov64 r2, r3                                    r2 = r3
    callx r4                                
    mov64 r1, r9                                    r1 = r9
    mov64 r2, r0                                    r2 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_21367                            if r2 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_21365:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jne r7, 0, lbb_21368                            if r7 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_21367:
    exit                                    
lbb_21368:
    ldxdw r4, [r8+0x18]                     
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r6                                    r3 = r6
    callx r4                                
    ja lbb_21367                                    if true { pc += -6 }

function_21373:
    ldxdw r5, [r1+0x10]                     
    ldxdw r0, [r1+0x0]                      
    mov64 r4, r0                                    r4 = r0
    or64 r4, r5                                     r4 |= r5   ///  r4 = r4.or(r5)
    jeq r4, 0, lbb_21426                            if r4 == (0 as i32 as i64 as u64) { pc += 48 }
    stxdw [r10-0x18], r0                    
    stxdw [r10-0x8], r2                     
    stxdw [r10-0x10], r3                    
    jeq r5, 0, lbb_21439                            if r5 == (0 as i32 as i64 as u64) { pc += 57 }
    ldxdw r6, [r10-0x8]                     
    mov64 r3, r6                                    r3 = r6
    ldxdw r4, [r10-0x10]                    
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r5, [r1+0x18]                     
    jeq r5, 0, lbb_21414                            if r5 == (0 as i32 as i64 as u64) { pc += 25 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r10-0x8]                     
lbb_21391:
    mov64 r8, r6                                    r8 = r6
    mov64 r7, r4                                    r7 = r4
    jeq r8, r3, lbb_21439                           if r8 == r3 { pc += 45 }
    mov64 r6, r8                                    r6 = r8
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    ldxb r4, [r8+0x0]                       
    mov64 r9, r4                                    r9 = r4
    lsh64 r9, 56                                    r9 <<= 56   ///  r9 = r9.wrapping_shl(56)
    arsh64 r9, 56                                   r9 >>= 56 (signed)   ///  r9 = (r9 as i64).wrapping_shr(56)
    jsgt r9, -1, lbb_21409                          if (r9 as i64) > (-1 as i32 as i64) { pc += 8 }
    mov64 r6, r8                                    r6 = r8
    add64 r6, 2                                     r6 += 2   ///  r6 = r6.wrapping_add(2 as i32 as i64 as u64)
    jlt r4, 224, lbb_21409                          if r4 < (224 as i32 as i64 as u64) { pc += 5 }
    mov64 r6, r8                                    r6 = r8
    add64 r6, 3                                     r6 += 3   ///  r6 = r6.wrapping_add(3 as i32 as i64 as u64)
    jlt r4, 240, lbb_21409                          if r4 < (240 as i32 as i64 as u64) { pc += 2 }
    mov64 r6, r8                                    r6 = r8
    add64 r6, 4                                     r6 += 4   ///  r6 = r6.wrapping_add(4 as i32 as i64 as u64)
lbb_21409:
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    mov64 r4, r6                                    r4 = r6
    sub64 r4, r8                                    r4 -= r8   ///  r4 = r4.wrapping_sub(r8)
    add64 r4, r7                                    r4 += r7   ///  r4 = r4.wrapping_add(r7)
    jlt r0, r5, lbb_21391                           if r0 < r5 { pc += -23 }
lbb_21414:
    jeq r6, r3, lbb_21439                           if r6 == r3 { pc += 24 }
    ldxb r3, [r6+0x0]                       
    mov64 r5, r3                                    r5 = r3
    lsh64 r5, 56                                    r5 <<= 56   ///  r5 = r5.wrapping_shl(56)
    arsh64 r5, 56                                   r5 >>= 56 (signed)   ///  r5 = (r5 as i64).wrapping_shr(56)
    jsgt r5, -1, lbb_21421                          if (r5 as i64) > (-1 as i32 as i64) { pc += 1 }
    jlt r3, 224, lbb_21421                          if r3 < (224 as i32 as i64 as u64) { pc += 0 }
lbb_21421:
    jeq r4, 0, lbb_21438                            if r4 == (0 as i32 as i64 as u64) { pc += 16 }
    ldxdw r3, [r10-0x10]                    
    jlt r4, r3, lbb_21432                           if r4 < r3 { pc += 8 }
    jeq r4, r3, lbb_21438                           if r4 == r3 { pc += 13 }
    ja lbb_21439                                    if true { pc += 13 }
lbb_21426:
    ldxdw r5, [r1+0x20]                     
    ldxdw r1, [r1+0x28]                     
    ldxdw r4, [r1+0x18]                     
    mov64 r1, r5                                    r1 = r5
    callx r4                                
    ja lbb_21537                                    if true { pc += 105 }
lbb_21432:
    ldxdw r3, [r10-0x8]                     
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    ldxb r3, [r3+0x0]                       
    lsh64 r3, 56                                    r3 <<= 56   ///  r3 = r3.wrapping_shl(56)
    arsh64 r3, 56                                   r3 >>= 56 (signed)   ///  r3 = (r3 as i64).wrapping_shr(56)
    jslt r3, -64, lbb_21439                         if (r3 as i64) < (-64 as i32 as i64) { pc += 1 }
lbb_21438:
    stxdw [r10-0x10], r4                    
lbb_21439:
    ldxdw r2, [r10-0x18]                    
    jne r2, 0, lbb_21449                            if r2 != (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r2, [r1+0x20]                     
    ldxdw r1, [r1+0x28]                     
    ldxdw r4, [r1+0x18]                     
    mov64 r1, r2                                    r1 = r2
    ldxdw r2, [r10-0x8]                     
    ldxdw r3, [r10-0x10]                    
    callx r4                                
    ja lbb_21537                                    if true { pc += 88 }
lbb_21449:
    ldxdw r9, [r1+0x8]                      
    ldxdw r7, [r10-0x10]                    
    ldxdw r8, [r10-0x8]                     
    jlt r7, 32, lbb_21459                           if r7 < (32 as i32 as i64 as u64) { pc += 6 }
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r7                                    r2 = r7
    call function_22073                     
    mov64 r1, r6                                    r1 = r6
    ja lbb_21468                                    if true { pc += 9 }
lbb_21459:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r7, 0, lbb_21468                            if r7 == (0 as i32 as i64 as u64) { pc += 7 }
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    ja lbb_21477                                    if true { pc += 13 }
lbb_21464:
    add64 r0, r4                                    r0 += r4   ///  r0 = r0.wrapping_add(r4)
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    jne r3, 0, lbb_21477                            if r3 != (0 as i32 as i64 as u64) { pc += 9 }
lbb_21468:
    jle r9, r0, lbb_21484                           if r9 <= r0 { pc += 15 }
    sub64 r9, r0                                    r9 -= r0   ///  r9 = r9.wrapping_sub(r0)
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxb r2, [r1+0x38]                      
    jsgt r2, 1, lbb_21492                           if (r2 as i64) > (1 as i32 as i64) { pc += 19 }
    jeq r2, 0, lbb_21498                            if r2 == (0 as i32 as i64 as u64) { pc += 24 }
    mov64 r7, r9                                    r7 = r9
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ja lbb_21498                                    if true { pc += 21 }
lbb_21477:
    ldxb r5, [r2+0x0]                       
    lsh64 r5, 56                                    r5 <<= 56   ///  r5 = r5.wrapping_shl(56)
    arsh64 r5, 56                                   r5 >>= 56 (signed)   ///  r5 = (r5 as i64).wrapping_shr(56)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jsgt r5, -65, lbb_21464                         if (r5 as i64) > (-65 as i32 as i64) { pc += -18 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ja lbb_21464                                    if true { pc += -20 }
lbb_21484:
    ldxdw r2, [r1+0x20]                     
    ldxdw r1, [r1+0x28]                     
    ldxdw r4, [r1+0x18]                     
    mov64 r1, r2                                    r1 = r2
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    callx r4                                
    ja lbb_21537                                    if true { pc += 45 }
lbb_21492:
    jeq r2, 2, lbb_21494                            if r2 == (2 as i32 as i64 as u64) { pc += 1 }
    ja lbb_21498                                    if true { pc += 4 }
lbb_21494:
    mov64 r7, r9                                    r7 = r9
    rsh64 r7, 1                                     r7 >>= 1   ///  r7 = r7.wrapping_shr(1)
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    rsh64 r9, 1                                     r9 >>= 1   ///  r9 = r9.wrapping_shr(1)
lbb_21498:
    stxdw [r10-0x18], r9                    
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    ldxw r8, [r1+0x30]                      
    ldxdw r6, [r1+0x28]                     
    ldxdw r9, [r1+0x20]                     
lbb_21503:
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    jeq r7, 0, lbb_21513                            if r7 == (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r3, [r6+0x20]                     
    mov64 r1, r9                                    r1 = r9
    mov64 r2, r8                                    r2 = r8
    callx r3                                
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_21537                            if r1 != (0 as i32 as i64 as u64) { pc += 25 }
    ja lbb_21503                                    if true { pc += -10 }
lbb_21513:
    ldxdw r4, [r6+0x18]                     
    mov64 r1, r9                                    r1 = r9
    ldxdw r2, [r10-0x8]                     
    ldxdw r3, [r10-0x10]                    
    callx r4                                
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_21537                            if r1 != (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_21522:
    ldxdw r2, [r10-0x18]                    
    mov64 r1, r2                                    r1 = r2
    jeq r2, r7, lbb_21533                           if r2 == r7 { pc += 8 }
    ldxdw r3, [r6+0x20]                     
    mov64 r1, r9                                    r1 = r9
    mov64 r2, r8                                    r2 = r8
    callx r3                                
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    jeq r0, 0, lbb_21522                            if r0 == (0 as i32 as i64 as u64) { pc += -9 }
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r7                                    r1 = r7
lbb_21533:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x18]                    
    jlt r1, r2, lbb_21537                           if r1 < r2 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_21537:
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    exit                                    

function_21539:
    ldxdw r4, [r1+0x20]                     
    ldxdw r1, [r1+0x28]                     
    ldxdw r5, [r1+0x18]                     
    mov64 r1, r4                                    r1 = r4
    callx r5                                
    exit                                    

function_21545:
    mov64 r6, r5                                    r6 = r5
    mov64 r7, r4                                    r7 = r4
    mov64 r9, r3                                    r9 = r3
    mov64 r8, r1                                    r8 = r1
    ldxdw r1, [r8+0x28]                     
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r8+0x20]                     
    callx r4                                
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r9, 0, lbb_21556                            if r9 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_21556:
    stxb [r10-0x8], r0                      
    stxdw [r10-0x10], r8                    
    stxb [r10-0x7], r1                      
    stdw [r10-0x18], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r6                                    r3 = r6
    call function_20922                     
    ldxb r2, [r10-0x8]                      
    ldxdw r1, [r10-0x18]                    
    jne r1, 0, lbb_21573                            if r1 != (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, r2                                    r1 = r2
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_21600                            if r1 != (0 as i32 as i64 as u64) { pc += 29 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_21600                                    if true { pc += 27 }
lbb_21573:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_21600                            if r2 != (0 as i32 as i64 as u64) { pc += 25 }
    ldxdw r6, [r10-0x10]                    
    jne r1, 1, lbb_21579                            if r1 != (1 as i32 as i64 as u64) { pc += 2 }
    ldxb r1, [r10-0x7]                      
    jne r1, 0, lbb_21587                            if r1 != (0 as i32 as i64 as u64) { pc += 8 }
lbb_21579:
    ldxdw r1, [r6+0x20]                     
    ldxdw r2, [r6+0x28]                     
    ldxdw r4, [r2+0x18]                     
    lddw r2, 0x100033c13 --> b")"                   r2 load str located at 4295179283
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    callx r4                                
    ja lbb_21600                                    if true { pc += 13 }
lbb_21587:
    ldxw r1, [r6+0x34]                      
    and64 r1, 4                                     r1 &= 4   ///  r1 = r1.and(4)
    jne r1, 0, lbb_21579                            if r1 != (0 as i32 as i64 as u64) { pc += -11 }
    ldxdw r1, [r6+0x20]                     
    ldxdw r2, [r6+0x28]                     
    ldxdw r4, [r2+0x18]                     
    lddw r2, 0x100033c5c --> b","                   r2 load str located at 4295179356
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    callx r4                                
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_21579                            if r1 == (0 as i32 as i64 as u64) { pc += -21 }
lbb_21600:
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    exit                                    

function_21602:
    ldxdw r3, [r1+0x20]                     
    ldxdw r1, [r1+0x28]                     
    ldxdw r4, [r1+0x20]                     
    mov64 r1, r3                                    r1 = r3
    callx r4                                
    exit                                    

function_21608:
    mov64 r6, r2                                    r6 = r2
    mov64 r8, r1                                    r8 = r1
    ldxdw r7, [r3+0x20]                     
    ldxdw r1, [r3+0x28]                     
    stxdw [r10-0x50], r1                    
    ldxdw r3, [r1+0x20]                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 34                                    r2 = 34 as i32 as i64 as u64
    stxdw [r10-0x28], r3                    
    callx r3                                
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_21827                            if r0 != (0 as i32 as i64 as u64) { pc += 207 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jeq r6, 0, lbb_21813                            if r6 == (0 as i32 as i64 as u64) { pc += 191 }
    mov64 r1, r8                                    r1 = r8
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    stxdw [r10-0x48], r1                    
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x58], r1                    
    mov64 r5, r8                                    r5 = r8
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    stxdw [r10-0x60], r6                    
    stxdw [r10-0x68], r8                    
    ja lbb_21639                                    if true { pc += 7 }
lbb_21632:
    add64 r1, r9                                    r1 += r9   ///  r1 = r1.wrapping_add(r9)
    stxdw [r10-0x58], r1                    
lbb_21634:
    sub64 r9, r3                                    r9 -= r3   ///  r9 = r9.wrapping_sub(r3)
    ldxdw r5, [r10-0x30]                    
    add64 r9, r5                                    r9 += r5   ///  r9 = r9.wrapping_add(r5)
    ldxdw r1, [r10-0x48]                    
    jeq r5, r1, lbb_21805                           if r5 == r1 { pc += 166 }
lbb_21639:
    ldxb r0, [r5+0x0]                       
    mov64 r1, r0                                    r1 = r0
    lsh64 r1, 56                                    r1 <<= 56   ///  r1 = r1.wrapping_shl(56)
    arsh64 r1, 56                                   r1 >>= 56 (signed)   ///  r1 = (r1 as i64).wrapping_shr(56)
    jsgt r1, -1, lbb_21680                          if (r1 as i64) > (-1 as i32 as i64) { pc += 36 }
    mov64 r1, r5                                    r1 = r5
    add64 r1, 2                                     r1 += 2   ///  r1 = r1.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    ldxb r1, [r5+0x1]                       
    and64 r1, 63                                    r1 &= 63   ///  r1 = r1.and(63)
    mov64 r3, r0                                    r3 = r0
    and64 r3, 31                                    r3 &= 31   ///  r3 = r3.and(31)
    mov64 r2, r3                                    r2 = r3
    lsh64 r2, 6                                     r2 <<= 6   ///  r2 = r2.wrapping_shl(6)
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    jgt r0, 223, lbb_21656                          if r0 > (223 as i32 as i64 as u64) { pc += 1 }
    ja lbb_21684                                    if true { pc += 28 }
lbb_21656:
    lsh64 r1, 6                                     r1 <<= 6   ///  r1 = r1.wrapping_shl(6)
    ldxb r4, [r5+0x2]                       
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    or64 r1, r4                                     r1 |= r4   ///  r1 = r1.or(r4)
    mov64 r4, r5                                    r4 = r5
    add64 r4, 3                                     r4 += 3   ///  r4 = r4.wrapping_add(3 as i32 as i64 as u64)
    stxdw [r10-0x30], r4                    
    mov64 r4, r3                                    r4 = r3
    lsh64 r4, 12                                    r4 <<= 12   ///  r4 = r4.wrapping_shl(12)
    mov64 r2, r1                                    r2 = r1
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    jlt r0, 240, lbb_21684                          if r0 < (240 as i32 as i64 as u64) { pc += 16 }
    lsh64 r1, 6                                     r1 <<= 6   ///  r1 = r1.wrapping_shl(6)
    ldxb r2, [r5+0x3]                       
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    lsh64 r3, 18                                    r3 <<= 18   ///  r3 = r3.wrapping_shl(18)
    and64 r3, 1835008                               r3 &= 1835008   ///  r3 = r3.and(1835008)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    mov64 r2, r5                                    r2 = r5
    add64 r2, 4                                     r2 += 4   ///  r2 = r2.wrapping_add(4 as i32 as i64 as u64)
    stxdw [r10-0x30], r2                    
    mov64 r2, r1                                    r2 = r1
    ja lbb_21684                                    if true { pc += 4 }
lbb_21680:
    mov64 r1, r5                                    r1 = r5
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    mov64 r2, r0                                    r2 = r0
lbb_21684:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -28                                   r1 += -28   ///  r1 = r1.wrapping_add(-28 as i32 as i64 as u64)
    stxdw [r10-0x40], r2                    
    mov64 r3, 65537                                 r3 = 65537 as i32 as i64 as u64
    stxdw [r10-0x38], r5                    
    call function_20329                     
    ldxdw r3, [r10-0x38]                    
    ldxb r1, [r10-0x1c]                     
    jeq r1, 128, lbb_21634                          if r1 == (128 as i32 as i64 as u64) { pc += -59 }
    ldxb r1, [r10-0x12]                     
    ldxb r2, [r10-0x11]                     
    sub64 r2, r1                                    r2 -= r1   ///  r2 = r2.wrapping_sub(r1)
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jeq r2, 1, lbb_21634                            if r2 == (1 as i32 as i64 as u64) { pc += -64 }
    ldxdw r4, [r10-0x58]                    
    jlt r9, r4, lbb_21755                           if r9 < r4 { pc += 55 }
    jeq r4, 0, lbb_21710                            if r4 == (0 as i32 as i64 as u64) { pc += 9 }
    jlt r4, r6, lbb_21704                           if r4 < r6 { pc += 2 }
    jeq r4, r6, lbb_21710                           if r4 == r6 { pc += 7 }
    ja lbb_21755                                    if true { pc += 51 }
lbb_21704:
    mov64 r1, r8                                    r1 = r8
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    ldxb r1, [r1+0x0]                       
    lsh64 r1, 56                                    r1 <<= 56   ///  r1 = r1.wrapping_shl(56)
    arsh64 r1, 56                                   r1 >>= 56 (signed)   ///  r1 = (r1 as i64).wrapping_shr(56)
    jslt r1, -64, lbb_21755                         if (r1 as i64) < (-64 as i32 as i64) { pc += 45 }
lbb_21710:
    jeq r9, 0, lbb_21714                            if r9 == (0 as i32 as i64 as u64) { pc += 3 }
    jlt r9, r6, lbb_21749                           if r9 < r6 { pc += 37 }
    jeq r9, r6, lbb_21714                           if r9 == r6 { pc += 1 }
    ja lbb_21755                                    if true { pc += 41 }
lbb_21714:
    mov64 r2, r8                                    r2 = r8
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    mov64 r3, r9                                    r3 = r9
    sub64 r3, r4                                    r3 -= r4   ///  r3 = r3.wrapping_sub(r4)
    ldxdw r1, [r10-0x50]                    
    ldxdw r4, [r1+0x18]                     
    mov64 r1, r7                                    r1 = r7
    callx r4                                
    jne r0, 0, lbb_21803                            if r0 != (0 as i32 as i64 as u64) { pc += 80 }
    mov64 r8, r7                                    r8 = r7
    ldxw r1, [r10-0x14]                     
    stxw [r10-0x8], r1                      
    ldxdw r1, [r10-0x1c]                    
    stxdw [r10-0x10], r1                    
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    jne r1, 128, lbb_21732                          if r1 != (128 as i32 as i64 as u64) { pc += 2 }
    mov64 r7, 128                                   r7 = 128 as i32 as i64 as u64
    ja lbb_21781                                    if true { pc += 49 }
lbb_21732:
    ldxb r7, [r10-0x5]                      
    ldxb r1, [r10-0x6]                      
lbb_21734:
    jge r1, r7, lbb_21790                           if r1 >= r7 { pc += 55 }
    mov64 r6, r1                                    r6 = r1
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    stxb [r10-0x6], r6                      
    jgt r1, 9, lbb_21769                            if r1 > (9 as i32 as i64 as u64) { pc += 30 }
    mov64 r2, r10                                   r2 = r10
    add64 r2, -16                                   r2 += -16   ///  r2 = r2.wrapping_add(-16 as i32 as i64 as u64)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxb r2, [r2+0x0]                       
    mov64 r1, r8                                    r1 = r8
    ldxdw r3, [r10-0x28]                    
    callx r3                                
    mov64 r1, r6                                    r1 = r6
    jne r0, 0, lbb_21803                            if r0 != (0 as i32 as i64 as u64) { pc += 55 }
    ja lbb_21734                                    if true { pc += -15 }
lbb_21749:
    mov64 r1, r8                                    r1 = r8
    add64 r1, r9                                    r1 += r9   ///  r1 = r1.wrapping_add(r9)
    ldxb r1, [r1+0x0]                       
    lsh64 r1, 56                                    r1 <<= 56   ///  r1 = r1.wrapping_shl(56)
    arsh64 r1, 56                                   r1 >>= 56 (signed)   ///  r1 = (r1 as i64).wrapping_shr(56)
    jsgt r1, -65, lbb_21714                         if (r1 as i64) > (-65 as i32 as i64) { pc += -41 }
lbb_21755:
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r4                                    r3 = r4
    mov64 r4, r9                                    r4 = r9
    lddw r5, 0x100035330 --> b"\x00\x00\x00\x00\xf8;\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00_\x09\x00\x0…        r5 load str located at 4295185200
    call function_22257                     
lbb_21762:
    ldxb r1, [r10-0x6]                      
    ldxb r2, [r10-0x5]                      
    jge r1, r2, lbb_21790                           if r1 >= r2 { pc += 25 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    stxb [r10-0x6], r2                      
    jlt r1, 10, lbb_21773                           if r1 < (10 as i32 as i64 as u64) { pc += 4 }
lbb_21769:
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    lddw r3, 0x1000354f8 --> b"\x00\x00\x00\x00\xddC\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00f\x00\x00\x0…        r3 load str located at 4295185656
    call function_20679                     
lbb_21773:
    mov64 r2, r10                                   r2 = r10
    add64 r2, -16                                   r2 += -16   ///  r2 = r2.wrapping_add(-16 as i32 as i64 as u64)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxb r2, [r2+0x0]                       
lbb_21777:
    mov64 r1, r8                                    r1 = r8
    ldxdw r3, [r10-0x28]                    
    callx r3                                
    jne r0, 0, lbb_21803                            if r0 != (0 as i32 as i64 as u64) { pc += 22 }
lbb_21781:
    mov64 r1, r7                                    r1 = r7
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    jeq r1, 128, lbb_21785                          if r1 == (128 as i32 as i64 as u64) { pc += 1 }
    ja lbb_21762                                    if true { pc += -23 }
lbb_21785:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxw r2, [r10-0xc]                      
    stw [r10-0x8], 0                        
    stdw [r10-0x10], 0                      
    ja lbb_21777                                    if true { pc += -13 }
lbb_21790:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r6, [r10-0x60]                    
    mov64 r7, r8                                    r7 = r8
    ldxdw r8, [r10-0x68]                    
    ldxdw r3, [r10-0x38]                    
    ldxdw r2, [r10-0x40]                    
    jlt r2, 128, lbb_21632                          if r2 < (128 as i32 as i64 as u64) { pc += -165 }
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    jlt r2, 2048, lbb_21632                         if r2 < (2048 as i32 as i64 as u64) { pc += -167 }
    mov64 r1, 3                                     r1 = 3 as i32 as i64 as u64
    jlt r2, 65536, lbb_21632                        if r2 < (65536 as i32 as i64 as u64) { pc += -169 }
    mov64 r1, 4                                     r1 = 4 as i32 as i64 as u64
    ja lbb_21632                                    if true { pc += -171 }
lbb_21803:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    ja lbb_21827                                    if true { pc += 22 }
lbb_21805:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0x58]                    
    jeq r3, 0, lbb_21813                            if r3 == (0 as i32 as i64 as u64) { pc += 4 }
    jlt r3, r6, lbb_21829                           if r3 < r6 { pc += 19 }
    mov64 r1, r3                                    r1 = r3
    jeq r3, r6, lbb_21813                           if r3 == r6 { pc += 1 }
    ja lbb_21836                                    if true { pc += 23 }
lbb_21813:
    add64 r8, r1                                    r8 += r1   ///  r8 = r8.wrapping_add(r1)
    sub64 r6, r1                                    r6 -= r1   ///  r6 = r6.wrapping_sub(r1)
    ldxdw r1, [r10-0x50]                    
    ldxdw r4, [r1+0x18]                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r6                                    r3 = r6
    callx r4                                
    jne r0, 0, lbb_21827                            if r0 != (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, r7                                    r1 = r7
    mov64 r2, 34                                    r2 = 34 as i32 as i64 as u64
    ldxdw r3, [r10-0x28]                    
    callx r3                                
    mov64 r9, r0                                    r9 = r0
lbb_21827:
    mov64 r0, r9                                    r0 = r9
    exit                                    
lbb_21829:
    mov64 r1, r8                                    r1 = r8
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    ldxb r2, [r1+0x0]                       
    lsh64 r2, 56                                    r2 <<= 56   ///  r2 = r2.wrapping_shl(56)
    arsh64 r2, 56                                   r2 >>= 56 (signed)   ///  r2 = (r2 as i64).wrapping_shr(56)
    mov64 r1, r3                                    r1 = r3
    jsgt r2, -65, lbb_21813                         if (r2 as i64) > (-65 as i32 as i64) { pc += -23 }
lbb_21836:
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r6                                    r2 = r6
    mov64 r4, r6                                    r4 = r6
    lddw r5, 0x100035318 --> b"\x00\x00\x00\x00\xf8;\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00f\x09\x00\x0…        r5 load str located at 4295185176
    call function_22257                     

function_21842:
    mov64 r4, r2                                    r4 = r2
    mov64 r2, r1                                    r2 = r1
    mov64 r1, r3                                    r1 = r3
    mov64 r3, r4                                    r3 = r4
    call function_21373                     
    exit                                    

function_21848:
    mov64 r8, r1                                    r8 = r1
    ldxdw r6, [r2+0x20]                     
    ldxdw r1, [r2+0x28]                     
    ldxdw r9, [r1+0x20]                     
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 39                                    r2 = 39 as i32 as i64 as u64
    callx r9                                
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_21918                            if r0 != (0 as i32 as i64 as u64) { pc += 61 }
    ldxw r2, [r8+0x0]                       
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r3, 257                                   r3 = 257 as i32 as i64 as u64
    call function_20329                     
    ldxb r1, [r10-0x10]                     
    jne r1, 128, lbb_21866                          if r1 != (128 as i32 as i64 as u64) { pc += 2 }
    mov64 r8, 128                                   r8 = 128 as i32 as i64 as u64
    ja lbb_21900                                    if true { pc += 34 }
lbb_21866:
    ldxb r8, [r10-0x5]                      
    ldxb r1, [r10-0x6]                      
lbb_21868:
    jge r1, r8, lbb_21914                           if r1 >= r8 { pc += 45 }
    mov64 r7, r1                                    r7 = r1
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    stxb [r10-0x6], r7                      
    jgt r1, 9, lbb_21889                            if r1 > (9 as i32 as i64 as u64) { pc += 16 }
    mov64 r2, r10                                   r2 = r10
    add64 r2, -16                                   r2 += -16   ///  r2 = r2.wrapping_add(-16 as i32 as i64 as u64)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxb r2, [r2+0x0]                       
    mov64 r1, r6                                    r1 = r6
    callx r9                                
    mov64 r1, r7                                    r1 = r7
    jne r0, 0, lbb_21912                            if r0 != (0 as i32 as i64 as u64) { pc += 31 }
    ja lbb_21868                                    if true { pc += -14 }
lbb_21882:
    ldxb r1, [r10-0x6]                      
    ldxb r2, [r10-0x5]                      
    jge r1, r2, lbb_21914                           if r1 >= r2 { pc += 29 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    stxb [r10-0x6], r2                      
    jlt r1, 10, lbb_21893                           if r1 < (10 as i32 as i64 as u64) { pc += 4 }
lbb_21889:
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    lddw r3, 0x1000354f8 --> b"\x00\x00\x00\x00\xddC\x03\x00\x1a\x00\x00\x00\x00\x00\x00\x00f\x00\x00\x0…        r3 load str located at 4295185656
    call function_20679                     
lbb_21893:
    mov64 r2, r10                                   r2 = r10
    add64 r2, -16                                   r2 += -16   ///  r2 = r2.wrapping_add(-16 as i32 as i64 as u64)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxb r2, [r2+0x0]                       
    mov64 r1, r6                                    r1 = r6
    callx r9                                
    jne r0, 0, lbb_21912                            if r0 != (0 as i32 as i64 as u64) { pc += 12 }
lbb_21900:
    mov64 r1, r8                                    r1 = r8
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    jeq r1, 128, lbb_21904                          if r1 == (128 as i32 as i64 as u64) { pc += 1 }
    ja lbb_21882                                    if true { pc += -22 }
lbb_21904:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxw r2, [r10-0xc]                      
    stw [r10-0x8], 0                        
    stdw [r10-0x10], 0                      
    mov64 r1, r6                                    r1 = r6
    callx r9                                
    jne r0, 0, lbb_21912                            if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_21900                                    if true { pc += -12 }
lbb_21912:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ja lbb_21918                                    if true { pc += 4 }
lbb_21914:
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 39                                    r2 = 39 as i32 as i64 as u64
    callx r9                                
    mov64 r1, r0                                    r1 = r0
lbb_21918:
    mov64 r0, r1                                    r0 = r1
    exit                                    

function_21920:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r0, r3                                    r0 = r3
    add64 r0, 7                                     r0 += 7   ///  r0 = r0.wrapping_add(7 as i32 as i64 as u64)
    and64 r0, -8                                    r0 &= -8   ///  r0 = r0.and(-8)
    jeq r0, r3, lbb_21940                           if r0 == r3 { pc += 15 }
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
    jlt r0, r4, lbb_21928                           if r0 < r4 { pc += 1 }
    mov64 r0, r4                                    r0 = r4
lbb_21928:
    jeq r0, 0, lbb_21940                            if r0 == (0 as i32 as i64 as u64) { pc += 11 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
lbb_21931:
    mov64 r5, r3                                    r5 = r3
    add64 r5, r6                                    r5 += r6   ///  r5 = r5.wrapping_add(r6)
    ldxb r5, [r5+0x0]                       
    mov64 r8, r2                                    r8 = r2
    and64 r8, 255                                   r8 &= 255   ///  r8 = r8.and(255)
    jeq r5, r8, lbb_21992                           if r5 == r8 { pc += 55 }
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    mov64 r5, r0                                    r5 = r0
    jlt r6, r0, lbb_21931                           if r6 < r0 { pc += -9 }
lbb_21940:
    stxdw [r10-0x8], r1                     
    mov64 r0, r4                                    r0 = r4
    add64 r0, -16                                   r0 += -16   ///  r0 = r0.wrapping_add(-16 as i32 as i64 as u64)
    jgt r5, r0, lbb_21973                           if r5 > r0 { pc += 29 }
    mov64 r6, r2                                    r6 = r2
    and64 r6, 255                                   r6 &= 255   ///  r6 = r6.and(255)
    lddw r7, 0x101010101010101                      r7 load str located at 72340172838076673
    mul64 r6, r7                                    r6 *= r7   ///  r6 = r6.wrapping_mul(r7)
    lddw r7, 0xfefefefefefefeff                     r7 load str located at -72340172838076673
    ja lbb_21954                                    if true { pc += 2 }
lbb_21952:
    add64 r5, 16                                    r5 += 16   ///  r5 = r5.wrapping_add(16 as i32 as i64 as u64)
    jgt r5, r0, lbb_21973                           if r5 > r0 { pc += 19 }
lbb_21954:
    mov64 r9, r3                                    r9 = r3
    add64 r9, r5                                    r9 += r5   ///  r9 = r9.wrapping_add(r5)
    ldxdw r8, [r9+0x0]                      
    xor64 r8, r6                                    r8 ^= r6   ///  r8 = r8.xor(r6)
    mov64 r1, r8                                    r1 = r8
    add64 r1, r7                                    r1 += r7   ///  r1 = r1.wrapping_add(r7)
    xor64 r8, -1                                    r8 ^= -1   ///  r8 = r8.xor(-1)
    and64 r1, r8                                    r1 &= r8   ///  r1 = r1.and(r8)
    ldxdw r8, [r9+0x8]                      
    xor64 r8, r6                                    r8 ^= r6   ///  r8 = r8.xor(r6)
    mov64 r9, r8                                    r9 = r8
    add64 r9, r7                                    r9 += r7   ///  r9 = r9.wrapping_add(r7)
    xor64 r8, -1                                    r8 ^= -1   ///  r8 = r8.xor(-1)
    and64 r9, r8                                    r9 &= r8   ///  r9 = r9.and(r8)
    or64 r9, r1                                     r9 |= r1   ///  r9 = r9.or(r1)
    lddw r1, 0x8080808080808080                     r1 load str located at -9187201950435737472
    and64 r9, r1                                    r9 &= r1   ///  r9 = r9.and(r1)
    jeq r9, 0, lbb_21952                            if r9 == (0 as i32 as i64 as u64) { pc += -21 }
lbb_21973:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x8]                     
    jeq r4, r5, lbb_21992                           if r4 == r5 { pc += 16 }
    sub64 r4, r5                                    r4 -= r5   ///  r4 = r4.wrapping_sub(r5)
    add64 r3, r5                                    r3 += r5   ///  r3 = r3.wrapping_add(r5)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_21979:
    mov64 r6, r3                                    r6 = r3
    add64 r6, r0                                    r6 += r0   ///  r6 = r6.wrapping_add(r0)
    ldxb r7, [r6+0x0]                       
    mov64 r6, r2                                    r6 = r2
    and64 r6, 255                                   r6 &= 255   ///  r6 = r6.and(255)
    jeq r7, r6, lbb_21989                           if r7 == r6 { pc += 4 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    jlt r0, r4, lbb_21979                           if r0 < r4 { pc += -9 }
    ja lbb_21992                                    if true { pc += 3 }
lbb_21989:
    add64 r5, r0                                    r5 += r0   ///  r5 = r5.wrapping_add(r0)
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r6, r5                                    r6 = r5
lbb_21992:
    stxdw [r1+0x8], r6                      
    stxdw [r1+0x0], r7                      
    exit                                    

function_21995:
    call function_21996                     

function_21996:
    stxdw [r10-0x58], r2                    
    stxdw [r10-0x60], r1                    
    lddw r1, 0x100035348 --> b"\x00\x00\x00\x00B=\x03\x00\x12\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295185224
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x10002cf40 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295151424
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_20640                     

function_22021:
    call function_22022                     

function_22022:
    stxdw [r10-0x58], r2                    
    stxdw [r10-0x60], r1                    
    lddw r1, 0x100035368 --> b"\x00\x00\x00\x00\x90:\x03\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295185256
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x10002cf40 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295151424
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_20640                     

function_22047:
    call function_22048                     

function_22048:
    stxdw [r10-0x58], r2                    
    stxdw [r10-0x60], r1                    
    lddw r1, 0x100035388 --> b"\x00\x00\x00\x00v=\x03\x00\x16\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295185288
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x10002cf40 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295151424
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_20640                     

function_22073:
    mov64 r7, r1                                    r7 = r1
    add64 r7, 7                                     r7 += 7   ///  r7 = r7.wrapping_add(7 as i32 as i64 as u64)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
    mov64 r3, r7                                    r3 = r7
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    jlt r2, r3, lbb_22212                           if r2 < r3 { pc += 133 }
    mov64 r5, r2                                    r5 = r2
    sub64 r5, r3                                    r5 -= r3   ///  r5 = r5.wrapping_sub(r3)
    jlt r5, 8, lbb_22212                            if r5 < (8 as i32 as i64 as u64) { pc += 130 }
    stxdw [r10-0x8], r3                     
    mov64 r2, r5                                    r2 = r5
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jeq r7, r1, lbb_22095                           if r7 == r1 { pc += 7 }
    mov64 r6, r1                                    r6 = r1
    sub64 r6, r7                                    r6 -= r7   ///  r6 = r6.wrapping_sub(r7)
    mov64 r7, r1                                    r7 = r1
    ja lbb_22104                                    if true { pc += 12 }
lbb_22092:
    add64 r3, r9                                    r3 += r9   ///  r3 = r3.wrapping_add(r9)
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    jne r8, 1, lbb_22104                            if r8 != (1 as i32 as i64 as u64) { pc += 9 }
lbb_22095:
    ldxdw r4, [r10-0x8]                     
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    jeq r2, 0, lbb_22119                            if r2 == (0 as i32 as i64 as u64) { pc += 21 }
    mov64 r0, r5                                    r0 = r5
    and64 r0, -8                                    r0 &= -8   ///  r0 = r0.and(-8)
    mov64 r4, r1                                    r4 = r1
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_22124                                    if true { pc += 20 }
lbb_22104:
    ldxb r4, [r7+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jsgt r4, -65, lbb_22111                         if (r4 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_22111:
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    jeq r6, 0, lbb_22092                            if r6 == (0 as i32 as i64 as u64) { pc += -21 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ja lbb_22092                                    if true { pc += -23 }
lbb_22115:
    add64 r0, r6                                    r0 += r6   ///  r0 = r0.wrapping_add(r6)
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    jne r2, 0, lbb_22124                            if r2 != (0 as i32 as i64 as u64) { pc += 5 }
lbb_22119:
    rsh64 r5, 3                                     r5 >>= 3   ///  r5 = r5.wrapping_shr(3)
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    lddw r9, 0x101010101010101                      r9 load str located at 72340172838076673
    ja lbb_22196                                    if true { pc += 72 }
lbb_22124:
    ldxb r7, [r4+0x0]                       
    lsh64 r7, 56                                    r7 <<= 56   ///  r7 = r7.wrapping_shl(56)
    arsh64 r7, 56                                   r7 >>= 56 (signed)   ///  r7 = (r7 as i64).wrapping_shr(56)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jsgt r7, -65, lbb_22115                         if (r7 as i64) > (-65 as i32 as i64) { pc += -14 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ja lbb_22115                                    if true { pc += -16 }
lbb_22131:
    ldxdw r7, [r2+0x0]                      
    mov64 r6, r7                                    r6 = r7
    rsh64 r6, 6                                     r6 >>= 6   ///  r6 = r6.wrapping_shr(6)
    xor64 r7, -1                                    r7 ^= -1   ///  r7 = r7.xor(-1)
    rsh64 r7, 7                                     r7 >>= 7   ///  r7 = r7.wrapping_shr(7)
    or64 r7, r6                                     r7 |= r6   ///  r7 = r7.or(r6)
    and64 r7, r9                                    r7 &= r9   ///  r7 = r7.and(r9)
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    ldxdw r4, [r2+0x8]                      
    mov64 r6, r4                                    r6 = r4
    rsh64 r6, 6                                     r6 >>= 6   ///  r6 = r6.wrapping_shr(6)
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    rsh64 r4, 7                                     r4 >>= 7   ///  r4 = r4.wrapping_shr(7)
    or64 r4, r6                                     r4 |= r6   ///  r4 = r4.or(r6)
    and64 r4, r9                                    r4 &= r9   ///  r4 = r4.and(r9)
    add64 r4, r7                                    r4 += r7   ///  r4 = r4.wrapping_add(r7)
    ldxdw r7, [r2+0x10]                     
    mov64 r6, r7                                    r6 = r7
    rsh64 r6, 6                                     r6 >>= 6   ///  r6 = r6.wrapping_shr(6)
    xor64 r7, -1                                    r7 ^= -1   ///  r7 = r7.xor(-1)
    rsh64 r7, 7                                     r7 >>= 7   ///  r7 = r7.wrapping_shr(7)
    or64 r7, r6                                     r7 |= r6   ///  r7 = r7.or(r6)
    and64 r7, r9                                    r7 &= r9   ///  r7 = r7.and(r9)
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    ldxdw r4, [r2+0x18]                     
    mov64 r6, r4                                    r6 = r4
    rsh64 r6, 6                                     r6 >>= 6   ///  r6 = r6.wrapping_shr(6)
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    rsh64 r4, 7                                     r4 >>= 7   ///  r4 = r4.wrapping_shr(7)
    or64 r4, r6                                     r4 |= r6   ///  r4 = r4.or(r6)
    and64 r4, r9                                    r4 &= r9   ///  r4 = r4.and(r9)
    add64 r4, r7                                    r4 += r7   ///  r4 = r4.wrapping_add(r7)
    add64 r2, 32                                    r2 += 32   ///  r2 = r2.wrapping_add(32 as i32 as i64 as u64)
    jne r2, r1, lbb_22131                           if r2 != r1 { pc += -34 }
lbb_22165:
    mov64 r1, r8                                    r1 = r8
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    ldxdw r7, [r10-0x8]                     
    mov64 r2, r7                                    r2 = r7
    and64 r2, 3                                     r2 &= 3   ///  r2 = r2.and(3)
    mov64 r5, r3                                    r5 = r3
    sub64 r5, r7                                    r5 -= r7   ///  r5 = r5.wrapping_sub(r7)
    mov64 r6, r4                                    r6 = r4
    stxdw [r10-0x10], r8                    
    mov64 r8, r3                                    r8 = r3
    lddw r3, 0xff00ff00ff00ff                       r3 load str located at 71777214294589695
    and64 r6, r3                                    r6 &= r3   ///  r6 = r6.and(r3)
    rsh64 r4, 8                                     r4 >>= 8   ///  r4 = r4.wrapping_shr(8)
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    lddw r6, 0x1000100010001                        r6 load str located at 281479271743489
    mul64 r4, r6                                    r4 *= r6   ///  r4 = r4.wrapping_mul(r6)
    rsh64 r4, 48                                    r4 >>= 48   ///  r4 = r4.wrapping_shr(48)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    mov64 r0, r4                                    r0 = r4
    jeq r2, 0, lbb_22196                            if r2 == (0 as i32 as i64 as u64) { pc += 8 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r10-0x10]                    
    jeq r2, 0, lbb_22244                            if r2 == (0 as i32 as i64 as u64) { pc += 53 }
    and64 r7, 252                                   r7 &= 252   ///  r7 = r7.and(252)
    lsh64 r7, 3                                     r7 <<= 3   ///  r7 = r7.wrapping_shl(3)
    jlt r8, 192, lbb_22225                          if r8 < (192 as i32 as i64 as u64) { pc += 31 }
    mov64 r8, 192                                   r8 = 192 as i32 as i64 as u64
    ja lbb_22225                                    if true { pc += 29 }
lbb_22196:
    mov64 r3, r5                                    r3 = r5
    mov64 r8, r1                                    r8 = r1
    jeq r3, 0, lbb_22256                            if r3 == (0 as i32 as i64 as u64) { pc += 57 }
    mov64 r5, r3                                    r5 = r3
    jlt r3, 192, lbb_22202                          if r3 < (192 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 192                                   r5 = 192 as i32 as i64 as u64
lbb_22202:
    stxdw [r10-0x8], r5                     
    lsh64 r5, 3                                     r5 <<= 3   ///  r5 = r5.wrapping_shl(3)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jlt r3, 4, lbb_22165                            if r3 < (4 as i32 as i64 as u64) { pc += -41 }
    mov64 r2, r5                                    r2 = r5
    and64 r2, 2016                                  r2 &= 2016   ///  r2 = r2.and(2016)
    mov64 r1, r8                                    r1 = r8
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, r8                                    r2 = r8
    ja lbb_22131                                    if true { pc += -81 }
lbb_22212:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r2, 0, lbb_22256                            if r2 == (0 as i32 as i64 as u64) { pc += 42 }
lbb_22214:
    ldxb r4, [r1+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jsgt r4, -65, lbb_22220                         if (r4 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_22220:
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    jeq r2, 0, lbb_22256                            if r2 == (0 as i32 as i64 as u64) { pc += 32 }
    ja lbb_22214                                    if true { pc += -11 }
lbb_22225:
    add64 r6, r7                                    r6 += r7   ///  r6 = r6.wrapping_add(r7)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    and64 r8, 3                                     r8 &= 3   ///  r8 = r8.and(3)
    lsh64 r8, 3                                     r8 <<= 3   ///  r8 = r8.wrapping_shl(3)
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
lbb_22231:
    ldxdw r0, [r6+0x0]                      
    mov64 r5, r0                                    r5 = r0
    rsh64 r5, 6                                     r5 >>= 6   ///  r5 = r5.wrapping_shr(6)
    xor64 r0, -1                                    r0 ^= -1   ///  r0 = r0.xor(-1)
    rsh64 r0, 7                                     r0 >>= 7   ///  r0 = r0.wrapping_shr(7)
    or64 r0, r5                                     r0 |= r5   ///  r0 = r0.or(r5)
    and64 r0, r1                                    r0 &= r1   ///  r0 = r0.and(r1)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    add64 r8, -8                                    r8 += -8   ///  r8 = r8.wrapping_add(-8 as i32 as i64 as u64)
    mov64 r2, r0                                    r2 = r0
    jeq r8, 0, lbb_22244                            if r8 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_22231                                    if true { pc += -13 }
lbb_22244:
    lddw r1, 0xff00ff00ff00ff                       r1 load str located at 71777214294589695
    mov64 r2, r0                                    r2 = r0
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    and64 r0, r1                                    r0 &= r1   ///  r0 = r0.and(r1)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    lddw r1, 0x1000100010001                        r1 load str located at 281479271743489
    mul64 r0, r1                                    r0 *= r1   ///  r0 = r0.wrapping_mul(r1)
    rsh64 r0, 48                                    r0 >>= 48   ///  r0 = r0.wrapping_shr(48)
    add64 r0, r4                                    r0 += r4   ///  r0 = r0.wrapping_add(r4)
lbb_22256:
    exit                                    

function_22257:
    call function_22258                     

function_22258:
    stxdw [r10-0xc8], r4                    
    stxdw [r10-0xd0], r3                    
    jlt r2, 257, lbb_22286                          if r2 < (257 as i32 as i64 as u64) { pc += 25 }
    mov64 r0, 256                                   r0 = 256 as i32 as i64 as u64
    ldxb r6, [r1+0x100]                     
    lsh64 r6, 56                                    r6 <<= 56   ///  r6 = r6.wrapping_shl(56)
    arsh64 r6, 56                                   r6 >>= 56 (signed)   ///  r6 = (r6 as i64).wrapping_shr(56)
    jsgt r6, -65, lbb_22277                         if (r6 as i64) > (-65 as i32 as i64) { pc += 11 }
    mov64 r0, 255                                   r0 = 255 as i32 as i64 as u64
    ldxb r6, [r1+0xff]                      
    lsh64 r6, 56                                    r6 <<= 56   ///  r6 = r6.wrapping_shl(56)
    arsh64 r6, 56                                   r6 >>= 56 (signed)   ///  r6 = (r6 as i64).wrapping_shr(56)
    jsgt r6, -65, lbb_22277                         if (r6 as i64) > (-65 as i32 as i64) { pc += 6 }
    mov64 r0, 254                                   r0 = 254 as i32 as i64 as u64
    ldxb r6, [r1+0xfe]                      
    lsh64 r6, 56                                    r6 <<= 56   ///  r6 = r6.wrapping_shl(56)
    arsh64 r6, 56                                   r6 >>= 56 (signed)   ///  r6 = (r6 as i64).wrapping_shr(56)
    jsgt r6, -65, lbb_22277                         if (r6 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r0, 253                                   r0 = 253 as i32 as i64 as u64
lbb_22277:
    mov64 r6, r1                                    r6 = r1
    add64 r6, r0                                    r6 += r0   ///  r6 = r6.wrapping_add(r0)
    ldxb r6, [r6+0x0]                       
    lsh64 r6, 56                                    r6 <<= 56   ///  r6 = r6.wrapping_shl(56)
    arsh64 r6, 56                                   r6 >>= 56 (signed)   ///  r6 = (r6 as i64).wrapping_shr(56)
    jsgt r6, -65, lbb_22287                         if (r6 as i64) > (-65 as i32 as i64) { pc += 4 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r0                                    r4 = r0
    call function_22257                     
lbb_22286:
    mov64 r0, r2                                    r0 = r2
lbb_22287:
    lddw r6, 0x100033d99 --> b"[...]begin <= end (`byte index  is not a char boun"        r6 load str located at 4295179673
    jlt r0, r2, lbb_22291                           if r0 < r2 { pc += 1 }
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
lbb_22291:
    mov64 r7, 5                                     r7 = 5 as i32 as i64 as u64
    jlt r0, r2, lbb_22294                           if r0 < r2 { pc += 1 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_22294:
    stxdw [r10-0xb8], r0                    
    stxdw [r10-0xc0], r1                    
    stxdw [r10-0xa8], r7                    
    stxdw [r10-0xb0], r6                    
    jgt r3, r2, lbb_22334                           if r3 > r2 { pc += 35 }
    jgt r4, r2, lbb_22334                           if r4 > r2 { pc += 34 }
    jle r3, r4, lbb_22366                           if r3 <= r4 { pc += 65 }
    lddw r1, 0x1000353a8 --> b"\x00\x00\x00\x00\x9e=\x03\x00\x0e\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295185320
    stxdw [r10-0x80], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    stxdw [r10-0x70], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x10002cf98 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x13\x08\x00\x00\x00\x00\x00y\x12\x00\x00\x…        r1 load str located at 4295151512
    stxdw [r10-0x18], r1                    
    stxdw [r10-0x28], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -192                                  r1 += -192   ///  r1 = r1.wrapping_add(-192 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -200                                  r1 += -200   ///  r1 = r1.wrapping_add(-200 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    lddw r1, 0x10002cf40 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295151424
    stxdw [r10-0x38], r1                    
    stxdw [r10-0x48], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    stxdw [r10-0x50], r1                    
    stdw [r10-0x60], 0                      
    stdw [r10-0x78], 4                      
    stdw [r10-0x68], 4                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r2, r5                                    r2 = r5
    call function_20640                     
lbb_22334:
    jgt r3, r2, lbb_22336                           if r3 > r2 { pc += 1 }
    mov64 r3, r4                                    r3 = r4
lbb_22336:
    stxdw [r10-0x90], r3                    
    lddw r1, 0x100035438 --> b"\x00\x00\x00\x00\xad=\x03\x00\x0b\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295185464
    stxdw [r10-0x80], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    stxdw [r10-0x70], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x10002cf98 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x13\x08\x00\x00\x00\x00\x00y\x12\x00\x00\x…        r1 load str located at 4295151512
    stxdw [r10-0x28], r1                    
    stxdw [r10-0x38], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -192                                  r1 += -192   ///  r1 = r1.wrapping_add(-192 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    lddw r1, 0x10002cf40 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295151424
    stxdw [r10-0x48], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    stxdw [r10-0x50], r1                    
    stdw [r10-0x60], 0                      
    stdw [r10-0x78], 3                      
    stdw [r10-0x68], 3                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r2, r5                                    r2 = r5
    call function_20640                     
lbb_22366:
    jeq r3, 0, lbb_22374                            if r3 == (0 as i32 as i64 as u64) { pc += 7 }
    jge r3, r2, lbb_22374                           if r3 >= r2 { pc += 6 }
    mov64 r0, r1                                    r0 = r1
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    ldxb r0, [r0+0x0]                       
    lsh64 r0, 56                                    r0 <<= 56   ///  r0 = r0.wrapping_shl(56)
    arsh64 r0, 56                                   r0 >>= 56 (signed)   ///  r0 = (r0 as i64).wrapping_shr(56)
    jslt r0, -64, lbb_22375                         if (r0 as i64) < (-64 as i32 as i64) { pc += 1 }
lbb_22374:
    mov64 r3, r4                                    r3 = r4
lbb_22375:
    stxdw [r10-0xa0], r3                    
    mov64 r0, r3                                    r0 = r3
    mov64 r3, r2                                    r3 = r2
    mov64 r4, r0                                    r4 = r0
    jge r0, r2, lbb_22413                           if r0 >= r2 { pc += 33 }
    mov64 r3, r4                                    r3 = r4
    mov64 r0, r3                                    r0 = r3
    add64 r0, -3                                    r0 += -3   ///  r0 = r0.wrapping_add(-3 as i32 as i64 as u64)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jgt r0, r3, lbb_22387                           if r0 > r3 { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_22387:
    jne r6, 0, lbb_22389                            if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r0                                    r4 = r0
lbb_22389:
    mov64 r0, r3                                    r0 = r3
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    jle r4, r0, lbb_22397                           if r4 <= r0 { pc += 5 }
    mov64 r1, r4                                    r1 = r4
    mov64 r2, r0                                    r2 = r0
    lddw r3, 0x100035468 --> b"\x00\x00\x00\x00\xfa=\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00\x05\x01\x00…        r3 load str located at 4295185512
    call function_22047                     
lbb_22397:
    mov64 r6, r1                                    r6 = r1
    add64 r6, r4                                    r6 += r4   ///  r6 = r6.wrapping_add(r4)
    mov64 r7, r3                                    r7 = r3
    mov64 r3, r1                                    r3 = r1
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
    sub64 r3, r6                                    r3 -= r6   ///  r3 = r3.wrapping_sub(r6)
    mov64 r0, r1                                    r0 = r1
    add64 r0, r7                                    r0 += r7   ///  r0 = r0.wrapping_add(r7)
lbb_22405:
    jeq r3, 0, lbb_22412                            if r3 == (0 as i32 as i64 as u64) { pc += 6 }
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    ldxb r6, [r0+0x0]                       
    add64 r0, -1                                    r0 += -1   ///  r0 = r0.wrapping_add(-1 as i32 as i64 as u64)
    lsh64 r6, 56                                    r6 <<= 56   ///  r6 = r6.wrapping_shl(56)
    arsh64 r6, 56                                   r6 >>= 56 (signed)   ///  r6 = (r6 as i64).wrapping_shr(56)
    jslt r6, -64, lbb_22405                         if (r6 as i64) < (-64 as i32 as i64) { pc += -7 }
lbb_22412:
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
lbb_22413:
    jeq r3, 0, lbb_22417                            if r3 == (0 as i32 as i64 as u64) { pc += 3 }
    jlt r3, r2, lbb_22453                           if r3 < r2 { pc += 38 }
    jeq r3, r2, lbb_22417                           if r3 == r2 { pc += 1 }
    ja lbb_22459                                    if true { pc += 42 }
lbb_22417:
    jeq r3, r2, lbb_22451                           if r3 == r2 { pc += 33 }
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    ldxb r0, [r1+0x0]                       
    mov64 r2, r0                                    r2 = r0
    lsh64 r2, 56                                    r2 <<= 56   ///  r2 = r2.wrapping_shl(56)
    arsh64 r2, 56                                   r2 >>= 56 (signed)   ///  r2 = (r2 as i64).wrapping_shr(56)
    jsgt r2, -1, lbb_22461                          if (r2 as i64) > (-1 as i32 as i64) { pc += 37 }
    ldxb r2, [r1+0x1]                       
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    mov64 r4, r0                                    r4 = r0
    and64 r4, 31                                    r4 &= 31   ///  r4 = r4.and(31)
    mov64 r6, r4                                    r6 = r4
    lsh64 r6, 6                                     r6 <<= 6   ///  r6 = r6.wrapping_shl(6)
    or64 r6, r2                                     r6 |= r2   ///  r6 = r6.or(r2)
    jgt r0, 223, lbb_22433                          if r0 > (223 as i32 as i64 as u64) { pc += 1 }
    ja lbb_22462                                    if true { pc += 29 }
lbb_22433:
    lsh64 r2, 6                                     r2 <<= 6   ///  r2 = r2.wrapping_shl(6)
    ldxb r6, [r1+0x2]                       
    and64 r6, 63                                    r6 &= 63   ///  r6 = r6.and(63)
    or64 r2, r6                                     r2 |= r6   ///  r2 = r2.or(r6)
    mov64 r7, r4                                    r7 = r4
    lsh64 r7, 12                                    r7 <<= 12   ///  r7 = r7.wrapping_shl(12)
    mov64 r6, r2                                    r6 = r2
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    jlt r0, 240, lbb_22462                          if r0 < (240 as i32 as i64 as u64) { pc += 20 }
    lsh64 r2, 6                                     r2 <<= 6   ///  r2 = r2.wrapping_shl(6)
    ldxb r1, [r1+0x3]                       
    and64 r1, 63                                    r1 &= 63   ///  r1 = r1.and(63)
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    lsh64 r4, 18                                    r4 <<= 18   ///  r4 = r4.wrapping_shl(18)
    and64 r4, 1835008                               r4 &= 1835008   ///  r4 = r4.and(1835008)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    mov64 r6, r2                                    r6 = r2
    ja lbb_22462                                    if true { pc += 11 }
lbb_22451:
    mov64 r1, r5                                    r1 = r5
    call function_20629                     
lbb_22453:
    mov64 r4, r1                                    r4 = r1
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxb r4, [r4+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    jsgt r4, -65, lbb_22417                         if (r4 as i64) > (-65 as i32 as i64) { pc += -42 }
lbb_22459:
    mov64 r4, r2                                    r4 = r2
    call function_22257                     
lbb_22461:
    mov64 r6, r0                                    r6 = r0
lbb_22462:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxw [r10-0x94], r6                     
    jlt r6, 128, lbb_22470                          if r6 < (128 as i32 as i64 as u64) { pc += 5 }
    mov64 r1, 2                                     r1 = 2 as i32 as i64 as u64
    jlt r6, 2048, lbb_22470                         if r6 < (2048 as i32 as i64 as u64) { pc += 3 }
    mov64 r1, 3                                     r1 = 3 as i32 as i64 as u64
    jlt r6, 65536, lbb_22470                        if r6 < (65536 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 4                                     r1 = 4 as i32 as i64 as u64
lbb_22470:
    stxdw [r10-0x90], r3                    
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    stxdw [r10-0x88], r1                    
    lddw r1, 0x1000353e8 --> b"\x00\x00\x00\x00\xad=\x03\x00\x0b\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295185384
    stxdw [r10-0x80], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    stxdw [r10-0x70], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x10002cf98 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x13\x08\x00\x00\x00\x00\x00y\x12\x00\x00\x…        r1 load str located at 4295151512
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -192                                  r1 += -192   ///  r1 = r1.wrapping_add(-192 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100027b90 --> b"\xbf&\x00\x00\x00\x00\x00\x00\xbf\x17\x00\x00\x00\x00\x00\x00\x85\x10\x00…        r1 load str located at 4295130000
    stxdw [r10-0x28], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x10002abe0 --> b"\xbf\x18\x00\x00\x00\x00\x00\x00y& \x00\x00\x00\x00\x00y!(\x00\x00\x00\x0…        r1 load str located at 4295142368
    stxdw [r10-0x38], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -148                                  r1 += -148   ///  r1 = r1.wrapping_add(-148 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    lddw r1, 0x10002cf40 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295151424
    stxdw [r10-0x48], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -160                                  r1 += -160   ///  r1 = r1.wrapping_add(-160 as i32 as i64 as u64)
    stxdw [r10-0x50], r1                    
    stdw [r10-0x60], 0                      
    stdw [r10-0x78], 5                      
    stdw [r10-0x68], 5                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r2, r5                                    r2 = r5
    call function_20640                     

function_22514:
    mov64 r9, r1                                    r9 = r1
    ldxdw r1, [r5-0xff0]                    
    stxdw [r10-0x10], r1                    
    ldxdw r6, [r5-0xff8]                    
    jeq r3, 0, lbb_22560                            if r3 == (0 as i32 as i64 as u64) { pc += 41 }
    lsh64 r3, 1                                     r3 <<= 1   ///  r3 = r3.wrapping_shl(1)
    mov64 r1, r2                                    r1 = r2
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    stxdw [r10-0x8], r1                     
    ldxdw r8, [r5-0x1000]                   
    mov64 r3, r9                                    r3 = r9
    and64 r3, 65280                                 r3 &= 65280   ///  r3 = r3.and(65280)
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    stxdw [r10-0x20], r4                    
    stxdw [r10-0x18], r8                    
    ja lbb_22535                                    if true { pc += 4 }
lbb_22531:
    jgt r1, r3, lbb_22560                           if r1 > r3 { pc += 28 }
    mov64 r0, r5                                    r0 = r5
    ldxdw r1, [r10-0x8]                     
    jeq r2, r1, lbb_22560                           if r2 == r1 { pc += 25 }
lbb_22535:
    ldxb r7, [r2+0x1]                       
    mov64 r5, r0                                    r5 = r0
    add64 r5, r7                                    r5 += r7   ///  r5 = r5.wrapping_add(r7)
    ldxb r1, [r2+0x0]                       
    add64 r2, 2                                     r2 += 2   ///  r2 = r2.wrapping_add(2 as i32 as i64 as u64)
    jeq r1, r3, lbb_22542                           if r1 == r3 { pc += 1 }
    ja lbb_22531                                    if true { pc += -11 }
lbb_22542:
    jgt r0, r5, lbb_22598                           if r0 > r5 { pc += 55 }
    jgt r5, r8, lbb_22593                           if r5 > r8 { pc += 49 }
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
lbb_22545:
    jeq r7, 0, lbb_22554                            if r7 == (0 as i32 as i64 as u64) { pc += 8 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    ldxb r8, [r4+0x0]                       
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    jeq r8, r1, lbb_22588                           if r8 == r1 { pc += 35 }
    ja lbb_22545                                    if true { pc += -9 }
lbb_22554:
    mov64 r0, r5                                    r0 = r5
    ldxdw r4, [r10-0x20]                    
    ldxdw r8, [r10-0x18]                    
    ldxdw r1, [r10-0x8]                     
    jeq r2, r1, lbb_22560                           if r2 == r1 { pc += 1 }
    ja lbb_22535                                    if true { pc += -25 }
lbb_22560:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ldxdw r1, [r10-0x10]                    
    jeq r1, 0, lbb_22588                            if r1 == (0 as i32 as i64 as u64) { pc += 25 }
    mov64 r2, r6                                    r2 = r6
    ldxdw r1, [r10-0x10]                    
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    and64 r9, 65535                                 r9 &= 65535   ///  r9 = r9.and(65535)
lbb_22567:
    mov64 r4, r6                                    r4 = r6
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    ldxb r3, [r6+0x0]                       
    mov64 r5, r3                                    r5 = r3
    lsh64 r5, 56                                    r5 <<= 56   ///  r5 = r5.wrapping_shl(56)
    arsh64 r5, 56                                   r5 >>= 56 (signed)   ///  r5 = (r5 as i64).wrapping_shr(56)
    jslt r5, 0, lbb_22576                           if (r5 as i64) < (0 as i32 as i64) { pc += 2 }
    mov64 r6, r4                                    r6 = r4
    ja lbb_22582                                    if true { pc += 6 }
lbb_22576:
    jeq r4, r2, lbb_22590                           if r4 == r2 { pc += 13 }
    and64 r3, 127                                   r3 &= 127   ///  r3 = r3.and(127)
    lsh64 r3, 8                                     r3 <<= 8   ///  r3 = r3.wrapping_shl(8)
    ldxb r4, [r6+0x1]                       
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    add64 r6, 2                                     r6 += 2   ///  r6 = r6.wrapping_add(2 as i32 as i64 as u64)
lbb_22582:
    sub64 r9, r3                                    r9 -= r3   ///  r9 = r9.wrapping_sub(r3)
    lsh64 r9, 32                                    r9 <<= 32   ///  r9 = r9.wrapping_shl(32)
    arsh64 r9, 32                                   r9 >>= 32 (signed)   ///  r9 = (r9 as i64).wrapping_shr(32)
    jslt r9, 0, lbb_22588                           if (r9 as i64) < (0 as i32 as i64) { pc += 2 }
    xor64 r0, 1                                     r0 ^= 1   ///  r0 = r0.xor(1)
    jne r6, r2, lbb_22567                           if r6 != r2 { pc += -21 }
lbb_22588:
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    exit                                    
lbb_22590:
    lddw r1, 0x100035480 --> b"\x00\x00\x00\x00\x15>\x03\x00%\x00\x00\x00\x00\x00\x00\x00\x1a\x00\x00\x0…        r1 load str located at 4295185536
    call function_20629                     
lbb_22593:
    mov64 r1, r5                                    r1 = r5
    ldxdw r2, [r10-0x18]                    
    lddw r3, 0x100035498 --> b"\x00\x00\x00\x00\x15>\x03\x00%\x00\x00\x00\x00\x00\x00\x00\x0a\x00\x00\x0…        r3 load str located at 4295185560
    call function_22021                     
lbb_22598:
    mov64 r1, r0                                    r1 = r0
    mov64 r2, r5                                    r2 = r5
    lddw r3, 0x100035498 --> b"\x00\x00\x00\x00\x15>\x03\x00%\x00\x00\x00\x00\x00\x00\x00\x0a\x00\x00\x0…        r3 load str located at 4295185560
    call function_22047                     

function_22603:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 32, lbb_22640                           if r2 < (32 as i32 as i64 as u64) { pc += 32 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r2, 127, lbb_22640                          if r2 < (127 as i32 as i64 as u64) { pc += 30 }
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 65536, lbb_22628                        if r2 < (65536 as i32 as i64 as u64) { pc += 14 }
    jlt r2, 131072, lbb_22616                       if r2 < (131072 as i32 as i64 as u64) { pc += 1 }
    ja lbb_22642                                    if true { pc += 26 }
lbb_22616:
    lddw r2, 0x100033f56 --> b"^"{\x05\x03\x04-\x03f\x03\x01/.\x80\x82\x1d\x031\x0f\x1c\x04$\x09\x1e\x05…        r2 load str located at 4295180118
    stxdw [r10-0xff8], r2                   
    stdw [r10-0xff0], 450                   
    stdw [r10-0x1000], 196                  
    mov64 r5, r10                                   r5 = r10
    lddw r2, 0x100033e3a --> b"\x00\x06\x01\x01\x03\x01\x04\x02\x05\x07\x07\x02\x08\x08\x09\x02\x0a\x05\…        r2 load str located at 4295179834
    mov64 r3, 44                                    r3 = 44 as i32 as i64 as u64
    lddw r4, 0x100033e92 --> b"\x0c';>NO\x8f\x9e\x9e\x9f{\x8b\x93\x96\xa2\xb2\xba\x86\xb1\x06\x07\x096=>…        r4 load str located at 4295179922
    ja lbb_22639                                    if true { pc += 11 }
lbb_22628:
    lddw r2, 0x100034288 --> b"\x00 _"\x82\xdf\x04\x82D\x08\x1b\x04\x06\x11\x81\xac\x0e\x80\xab\x05\x1f\…        r2 load str located at 4295180936
    stxdw [r10-0xff8], r2                   
    stdw [r10-0xff0], 301                   
    stdw [r10-0x1000], 288                  
    mov64 r5, r10                                   r5 = r10
    lddw r2, 0x100034118 --> b"\x00\x01\x03\x05\x05\x06\x06\x02\x07\x06\x08\x07\x09\x11\x0a\x1c\x0b\x19\…        r2 load str located at 4295180568
    mov64 r3, 40                                    r3 = 40 as i32 as i64 as u64
    lddw r4, 0x100034168 --> b"\xadxy\x8b\x8d\xa20WX\x8b\x8c\x90\x1c\xdd\x0e\x0fKL\xfb\xfc./?\]_\xe2\x84…        r4 load str located at 4295180648
lbb_22639:
    call function_22514                     
lbb_22640:
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    exit                                    
lbb_22642:
    mov64 r3, r1                                    r3 = r1
    add64 r3, -191457                               r3 += -191457   ///  r3 = r3.wrapping_add(-191457 as i32 as i64 as u64)
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r3, 15, lbb_22651                           if r3 < (15 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_22651:
    mov64 r5, r1                                    r5 = r1
    add64 r5, -192094                               r5 += -192094   ///  r5 = r5.wrapping_add(-192094 as i32 as i64 as u64)
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r5, 2466, lbb_22658                         if r5 < (2466 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_22658:
    lsh64 r4, 2                                     r4 <<= 2   ///  r4 = r4.wrapping_shl(2)
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    mov64 r6, r1                                    r6 = r1
    add64 r6, -177978                               r6 += -177978   ///  r6 = r6.wrapping_add(-177978 as i32 as i64 as u64)
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r6, 6, lbb_22667                            if r6 < (6 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_22667:
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    mov64 r4, r1                                    r4 = r1
    add64 r4, -183970                               r4 += -183970   ///  r4 = r4.wrapping_add(-183970 as i32 as i64 as u64)
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    jlt r4, 14, lbb_22674                           if r4 < (14 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_22674:
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    or64 r5, r2                                     r5 |= r2   ///  r5 = r5.or(r2)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    jne r5, 0, lbb_22640                            if r5 != (0 as i32 as i64 as u64) { pc += -38 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -195102                               r2 += -195102   ///  r2 = r2.wrapping_add(-195102 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 1506, lbb_22640                         if r2 < (1506 as i32 as i64 as u64) { pc += -43 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -201547                               r2 += -201547   ///  r2 = r2.wrapping_add(-201547 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 5, lbb_22640                            if r2 < (5 as i32 as i64 as u64) { pc += -48 }
    mov64 r2, r1                                    r2 = r1
    add64 r2, -205744                               r2 += -205744   ///  r2 = r2.wrapping_add(-205744 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jlt r2, 712016, lbb_22640                       if r2 < (712016 as i32 as i64 as u64) { pc += -53 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, -32                                   r2 &= -32   ///  r2 = r2.and(-32)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jeq r2, 173792, lbb_22640                       if r2 == (173792 as i32 as i64 as u64) { pc += -58 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, -2                                    r2 &= -2   ///  r2 = r2.and(-2)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jeq r2, 178206, lbb_22640                       if r2 == (178206 as i32 as i64 as u64) { pc += -63 }
    add64 r1, -1114112                              r1 += -1114112   ///  r1 = r1.wrapping_add(-1114112 as i32 as i64 as u64)
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    lddw r2, 0xfffd01f0                             r2 load str located at 4294771184
    jlt r1, r2, lbb_22640                           if r1 < r2 { pc += -70 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_22640                                    if true { pc += -72 }

function_22712:
    mov64 r2, r1                                    r2 = r1
    lddw r1, 0x100035260 --> b"\x00\x00\x00\x00\xdc;\x03\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295184992
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_20640                     

function_22723:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxb r1, [r1+0x0]                       
    ja lbb_22736                                    if true { pc += 10 }
lbb_22726:
    mov64 r5, r10                                   r5 = r10
    add64 r5, -128                                  r5 += -128   ///  r5 = r5.wrapping_add(-128 as i32 as i64 as u64)
    add64 r5, r3                                    r5 += r3   ///  r5 = r5.wrapping_add(r3)
    stxb [r5+0x7f], r4                      
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r4, r1                                    r4 = r1
    and64 r4, 255                                   r4 &= 255   ///  r4 = r4.and(255)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    jlt r4, 16, lbb_22744                           if r4 < (16 as i32 as i64 as u64) { pc += 8 }
lbb_22736:
    mov64 r5, r1                                    r5 = r1
    and64 r5, 15                                    r5 &= 15   ///  r5 = r5.and(15)
    mov64 r4, r5                                    r4 = r5
    or64 r4, 48                                     r4 |= 48   ///  r4 = r4.or(48)
    jlt r5, 10, lbb_22726                           if r5 < (10 as i32 as i64 as u64) { pc += -15 }
    add64 r5, 87                                    r5 += 87   ///  r5 = r5.wrapping_add(87 as i32 as i64 as u64)
    mov64 r4, r5                                    r4 = r5
    ja lbb_22726                                    if true { pc += -18 }
lbb_22744:
    mov64 r1, r3                                    r1 = r3
    add64 r1, 128                                   r1 += 128   ///  r1 = r1.wrapping_add(128 as i32 as i64 as u64)
    jlt r1, 129, lbb_22751                          if r1 < (129 as i32 as i64 as u64) { pc += 4 }
    mov64 r2, 128                                   r2 = 128 as i32 as i64 as u64
    lddw r3, 0x100035300 --> b"\x00\x00\x00\x00]<\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00i\x00\x00\x00\x…        r3 load str located at 4295185152
    call function_21995                     
lbb_22751:
    mov64 r1, r3                                    r1 = r3
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    add64 r1, 128                                   r1 += 128   ///  r1 = r1.wrapping_add(128 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r2                                    r1 = r2
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x100033c78 --> b"0x"                  r3 load str located at 4295179384
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    call function_21157                     
    exit                                    

function_22767:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxb r1, [r1+0x0]                       
    ja lbb_22780                                    if true { pc += 10 }
lbb_22770:
    mov64 r5, r10                                   r5 = r10
    add64 r5, -128                                  r5 += -128   ///  r5 = r5.wrapping_add(-128 as i32 as i64 as u64)
    add64 r5, r3                                    r5 += r3   ///  r5 = r5.wrapping_add(r3)
    stxb [r5+0x7f], r4                      
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r4, r1                                    r4 = r1
    and64 r4, 255                                   r4 &= 255   ///  r4 = r4.and(255)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    jlt r4, 16, lbb_22788                           if r4 < (16 as i32 as i64 as u64) { pc += 8 }
lbb_22780:
    mov64 r5, r1                                    r5 = r1
    and64 r5, 15                                    r5 &= 15   ///  r5 = r5.and(15)
    mov64 r4, r5                                    r4 = r5
    or64 r4, 48                                     r4 |= 48   ///  r4 = r4.or(48)
    jlt r5, 10, lbb_22770                           if r5 < (10 as i32 as i64 as u64) { pc += -15 }
    add64 r5, 55                                    r5 += 55   ///  r5 = r5.wrapping_add(55 as i32 as i64 as u64)
    mov64 r4, r5                                    r4 = r5
    ja lbb_22770                                    if true { pc += -18 }
lbb_22788:
    mov64 r1, r3                                    r1 = r3
    add64 r1, 128                                   r1 += 128   ///  r1 = r1.wrapping_add(128 as i32 as i64 as u64)
    jlt r1, 129, lbb_22795                          if r1 < (129 as i32 as i64 as u64) { pc += 4 }
    mov64 r2, 128                                   r2 = 128 as i32 as i64 as u64
    lddw r3, 0x100035300 --> b"\x00\x00\x00\x00]<\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00i\x00\x00\x00\x…        r3 load str located at 4295185152
    call function_21995                     
lbb_22795:
    mov64 r1, r3                                    r1 = r3
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    add64 r1, 128                                   r1 += 128   ///  r1 = r1.wrapping_add(128 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r2                                    r1 = r2
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x100033c78 --> b"0x"                  r3 load str located at 4295179384
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    call function_21157                     
    exit                                    

function_22811:
    mov64 r3, r2                                    r3 = r2
    ldxw r2, [r3+0x34]                      
    mov64 r4, r2                                    r4 = r2
    and64 r4, 16                                    r4 &= 16   ///  r4 = r4.and(16)
    jne r4, 0, lbb_22823                            if r4 != (0 as i32 as i64 as u64) { pc += 7 }
    and64 r2, 32                                    r2 &= 32   ///  r2 = r2.and(32)
    jeq r2, 0, lbb_22819                            if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_22843                                    if true { pc += 24 }
lbb_22819:
    ldxdw r1, [r1+0x0]                      
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_22890                     
    ja lbb_22889                                    if true { pc += 66 }
lbb_22823:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r4, [r1+0x0]                      
    ja lbb_22834                                    if true { pc += 8 }
lbb_22826:
    mov64 r5, r10                                   r5 = r10
    add64 r5, -128                                  r5 += -128   ///  r5 = r5.wrapping_add(-128 as i32 as i64 as u64)
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    stxb [r5+0x7f], r4                      
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r4, r1                                    r4 = r1
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    jlt r1, 16, lbb_22867                           if r1 < (16 as i32 as i64 as u64) { pc += 33 }
lbb_22834:
    mov64 r1, r4                                    r1 = r4
    mov64 r5, r1                                    r5 = r1
    and64 r5, 15                                    r5 &= 15   ///  r5 = r5.and(15)
    mov64 r4, r5                                    r4 = r5
    or64 r4, 48                                     r4 |= 48   ///  r4 = r4.or(48)
    jlt r5, 10, lbb_22826                           if r5 < (10 as i32 as i64 as u64) { pc += -14 }
    add64 r5, 87                                    r5 += 87   ///  r5 = r5.wrapping_add(87 as i32 as i64 as u64)
    mov64 r4, r5                                    r4 = r5
    ja lbb_22826                                    if true { pc += -17 }
lbb_22843:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r4, [r1+0x0]                      
    ja lbb_22854                                    if true { pc += 8 }
lbb_22846:
    mov64 r5, r10                                   r5 = r10
    add64 r5, -128                                  r5 += -128   ///  r5 = r5.wrapping_add(-128 as i32 as i64 as u64)
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    stxb [r5+0x7f], r4                      
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r4, r1                                    r4 = r1
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    jlt r1, 16, lbb_22863                           if r1 < (16 as i32 as i64 as u64) { pc += 9 }
lbb_22854:
    mov64 r1, r4                                    r1 = r4
    mov64 r5, r1                                    r5 = r1
    and64 r5, 15                                    r5 &= 15   ///  r5 = r5.and(15)
    mov64 r4, r5                                    r4 = r5
    or64 r4, 48                                     r4 |= 48   ///  r4 = r4.or(48)
    jlt r5, 10, lbb_22846                           if r5 < (10 as i32 as i64 as u64) { pc += -14 }
    add64 r5, 55                                    r5 += 55   ///  r5 = r5.wrapping_add(55 as i32 as i64 as u64)
    mov64 r4, r5                                    r4 = r5
    ja lbb_22846                                    if true { pc += -17 }
lbb_22863:
    mov64 r1, r2                                    r1 = r2
    add64 r1, 128                                   r1 += 128   ///  r1 = r1.wrapping_add(128 as i32 as i64 as u64)
    jlt r1, 129, lbb_22874                          if r1 < (129 as i32 as i64 as u64) { pc += 8 }
    ja lbb_22870                                    if true { pc += 3 }
lbb_22867:
    mov64 r1, r2                                    r1 = r2
    add64 r1, 128                                   r1 += 128   ///  r1 = r1.wrapping_add(128 as i32 as i64 as u64)
    jlt r1, 129, lbb_22874                          if r1 < (129 as i32 as i64 as u64) { pc += 4 }
lbb_22870:
    mov64 r2, 128                                   r2 = 128 as i32 as i64 as u64
    lddw r3, 0x100035300 --> b"\x00\x00\x00\x00]<\x03\x00\x1b\x00\x00\x00\x00\x00\x00\x00i\x00\x00\x00\x…        r3 load str located at 4295185152
    call function_21995                     
lbb_22874:
    mov64 r1, r2                                    r1 = r2
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    add64 r1, 128                                   r1 += 128   ///  r1 = r1.wrapping_add(128 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r3                                    r1 = r3
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x100033c78 --> b"0x"                  r3 load str located at 4295179384
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    call function_21157                     
lbb_22889:
    exit                                    

function_22890:
    mov64 r4, 39                                    r4 = 39 as i32 as i64 as u64
    jlt r1, 10000, lbb_22924                        if r1 < (10000 as i32 as i64 as u64) { pc += 32 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_22893:
    mov64 r5, r1                                    r5 = r1
    div64 r1, 10000                                 r1 /= 10000   ///  r1 = r1 / (10000 as u64)
    mov64 r6, r1                                    r6 = r1
    mul64 r6, 10000                                 r6 *= 10000   ///  r6 = r6.wrapping_mul(10000 as u64)
    mov64 r0, r5                                    r0 = r5
    sub64 r0, r6                                    r0 -= r6   ///  r0 = r0.wrapping_sub(r6)
    mov64 r6, r0                                    r6 = r0
    and64 r6, 65535                                 r6 &= 65535   ///  r6 = r6.and(65535)
    div64 r6, 100                                   r6 /= 100   ///  r6 = r6 / (100 as u64)
    mov64 r7, r6                                    r7 = r6
    mul64 r7, 100                                   r7 *= 100   ///  r7 = r7.wrapping_mul(100 as u64)
    sub64 r0, r7                                    r0 -= r7   ///  r0 = r0.wrapping_sub(r7)
    mov64 r7, r10                                   r7 = r10
    add64 r7, -39                                   r7 += -39   ///  r7 = r7.wrapping_add(-39 as i32 as i64 as u64)
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    lsh64 r6, 1                                     r6 <<= 1   ///  r6 = r6.wrapping_shl(1)
    lddw r8, 0x100033c7a --> b"00010203040506070809101112131415161718192021222324"        r8 load str located at 4295179386
    add64 r8, r6                                    r8 += r6   ///  r8 = r8.wrapping_add(r6)
    ldxh r6, [r8+0x0]                       
    stxh [r7+0x23], r6                      
    lsh64 r0, 1                                     r0 <<= 1   ///  r0 = r0.wrapping_shl(1)
    and64 r0, 65534                                 r0 &= 65534   ///  r0 = r0.and(65534)
    lddw r6, 0x100033c7a --> b"00010203040506070809101112131415161718192021222324"        r6 load str located at 4295179386
    add64 r6, r0                                    r6 += r0   ///  r6 = r6.wrapping_add(r0)
    ldxh r0, [r6+0x0]                       
    stxh [r7+0x25], r0                      
    add64 r4, -4                                    r4 += -4   ///  r4 = r4.wrapping_add(-4 as i32 as i64 as u64)
    jgt r5, 99999999, lbb_22893                     if r5 > (99999999 as i32 as i64 as u64) { pc += -30 }
    add64 r4, 39                                    r4 += 39   ///  r4 = r4.wrapping_add(39 as i32 as i64 as u64)
lbb_22924:
    jgt r1, 99, lbb_22926                           if r1 > (99 as i32 as i64 as u64) { pc += 1 }
    ja lbb_22944                                    if true { pc += 18 }
lbb_22926:
    mov64 r5, r1                                    r5 = r1
    and64 r5, 65535                                 r5 &= 65535   ///  r5 = r5.and(65535)
    div64 r5, 100                                   r5 /= 100   ///  r5 = r5 / (100 as u64)
    mov64 r0, r5                                    r0 = r5
    mul64 r0, 100                                   r0 *= 100   ///  r0 = r0.wrapping_mul(100 as u64)
    sub64 r1, r0                                    r1 -= r0   ///  r1 = r1.wrapping_sub(r0)
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    and64 r1, 65534                                 r1 &= 65534   ///  r1 = r1.and(65534)
    lddw r0, 0x100033c7a --> b"00010203040506070809101112131415161718192021222324"        r0 load str located at 4295179386
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -39                                   r1 += -39   ///  r1 = r1.wrapping_add(-39 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    ldxh r0, [r0+0x0]                       
    stxh [r1+0x0], r0                       
    mov64 r1, r5                                    r1 = r5
lbb_22944:
    jlt r1, 10, lbb_22956                           if r1 < (10 as i32 as i64 as u64) { pc += 11 }
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    lddw r5, 0x100033c7a --> b"00010203040506070809101112131415161718192021222324"        r5 load str located at 4295179386
    add64 r5, r1                                    r5 += r1   ///  r5 = r5.wrapping_add(r1)
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -39                                   r1 += -39   ///  r1 = r1.wrapping_add(-39 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    ldxh r5, [r5+0x0]                       
    stxh [r1+0x0], r5                       
    ja lbb_22962                                    if true { pc += 6 }
lbb_22956:
    add64 r4, -1                                    r4 += -1   ///  r4 = r4.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -39                                   r5 += -39   ///  r5 = r5.wrapping_add(-39 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    or64 r1, 48                                     r1 |= 48   ///  r1 = r1.or(48)
    stxb [r5+0x0], r1                       
lbb_22962:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -39                                   r1 += -39   ///  r1 = r1.wrapping_add(-39 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    stxdw [r10-0x1000], r1                  
    mov64 r1, 39                                    r1 = 39 as i32 as i64 as u64
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    stxdw [r10-0xff8], r1                   
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r3                                    r1 = r3
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_21157                     
    exit                                    

function_22975:
    mov64 r3, r2                                    r3 = r2
    ldxb r1, [r1+0x0]                       
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_22890                     
    exit                                    

function_22980:
    mov64 r3, r2                                    r3 = r2
    ldxdw r1, [r1+0x0]                      
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_22890                     
    exit                                    

function_22985:
    ldxdw r3, [r1+0x0]                      
    ldxdw r1, [r1+0x8]                      
    ldxdw r4, [r1+0x18]                     
    mov64 r1, r3                                    r1 = r3
    callx r4                                
    exit                                    

function_22991:
    mov64 r4, r2                                    r4 = r2
    ldxdw r3, [r1+0x8]                      
    ldxdw r2, [r1+0x0]                      
    mov64 r1, r4                                    r1 = r4
    call function_21373                     
    exit                                    

function_22997:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 33                                    r3 = 33 as i32 as i64 as u64
    mov64 r4, r1                                    r4 = r1
    lsh64 r4, 11                                    r4 <<= 11   ///  r4 = r4.wrapping_shl(11)
    mov64 r5, 33                                    r5 = 33 as i32 as i64 as u64
    ja lbb_23008                                    if true { pc += 5 }
lbb_23003:
    mov64 r5, r3                                    r5 = r3
    sub64 r3, r2                                    r3 -= r2   ///  r3 = r3.wrapping_sub(r2)
    mov64 r0, r2                                    r0 = r2
    jgt r5, r2, lbb_23008                           if r5 > r2 { pc += 1 }
    ja lbb_23032                                    if true { pc += 24 }
lbb_23008:
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
    mov64 r2, r3                                    r2 = r3
    lsh64 r2, 2                                     r2 <<= 2   ///  r2 = r2.wrapping_shl(2)
    lddw r6, 0x1000343f8 --> b"\x00\x03\x00\x00\x83\x04 \x00\x91\x05`\x00]\x13\xa0\x00\x12\x17 \x1f\x0c …        r6 load str located at 4295181304
    add64 r6, r2                                    r6 += r2   ///  r6 = r6.wrapping_add(r2)
    ldxw r6, [r6+0x0]                       
    lsh64 r6, 11                                    r6 <<= 11   ///  r6 = r6.wrapping_shl(11)
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    mov64 r7, r4                                    r7 = r4
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    jeq r6, r7, lbb_23030                           if r6 == r7 { pc += 7 }
    mov64 r2, r3                                    r2 = r3
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    jlt r6, r7, lbb_23027                           if r6 < r7 { pc += 1 }
    mov64 r2, r0                                    r2 = r0
lbb_23027:
    jgt r6, r7, lbb_23003                           if r6 > r7 { pc += -25 }
    mov64 r3, r5                                    r3 = r5
    ja lbb_23003                                    if true { pc += -27 }
lbb_23030:
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
lbb_23032:
    jgt r2, 32, lbb_23088                           if r2 > (32 as i32 as i64 as u64) { pc += 55 }
    mov64 r3, r2                                    r3 = r2
    lsh64 r3, 2                                     r3 <<= 2   ///  r3 = r3.wrapping_shl(2)
    lddw r4, 0x1000343f8 --> b"\x00\x03\x00\x00\x83\x04 \x00\x91\x05`\x00]\x13\xa0\x00\x12\x17 \x1f\x0c …        r4 load str located at 4295181304
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    mov64 r3, 727                                   r3 = 727 as i32 as i64 as u64
    ldxw r0, [r4+0x0]                       
    jeq r2, 32, lbb_23045                           if r2 == (32 as i32 as i64 as u64) { pc += 4 }
    add64 r4, 4                                     r4 += 4   ///  r4 = r4.wrapping_add(4 as i32 as i64 as u64)
    jeq r4, 0, lbb_23045                            if r4 == (0 as i32 as i64 as u64) { pc += 2 }
    ldxw r3, [r4+0x0]                       
    rsh64 r3, 21                                    r3 >>= 21   ///  r3 = r3.wrapping_shr(21)
lbb_23045:
    rsh64 r0, 21                                    r0 >>= 21   ///  r0 = r0.wrapping_shr(21)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jne r2, 0, lbb_23076                            if r2 != (0 as i32 as i64 as u64) { pc += 28 }
lbb_23048:
    mov64 r2, r0                                    r2 = r0
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    jeq r3, 0, lbb_23074                            if r3 == (0 as i32 as i64 as u64) { pc += 22 }
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    lddw r5, 0x10003447c --> b"\x00p\x00\x07\x00-\x01\x01\x01\x02\x01\x02\x01\x01H\x0b0\x15\x10\x01e\x07…        r5 load str located at 4295181436
    add64 r5, r0                                    r5 += r0   ///  r5 = r5.wrapping_add(r0)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_23060:
    mov64 r2, r0                                    r2 = r0
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    jgt r2, 726, lbb_23083                          if r2 > (726 as i32 as i64 as u64) { pc += 20 }
    mov64 r2, r5                                    r2 = r5
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    ldxb r2, [r2+0x0]                       
    add64 r6, r2                                    r6 += r2   ///  r6 = r6.wrapping_add(r2)
    mov64 r2, r6                                    r2 = r6
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jgt r2, r1, lbb_23073                           if r2 > r1 { pc += 2 }
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    jlt r4, r3, lbb_23060                           if r4 < r3 { pc += -13 }
lbb_23073:
    add64 r0, r4                                    r0 += r4   ///  r0 = r0.wrapping_add(r4)
lbb_23074:
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    exit                                    
lbb_23076:
    lsh64 r2, 2                                     r2 <<= 2   ///  r2 = r2.wrapping_shl(2)
    lddw r4, 0x1000343f8 --> b"\x00\x03\x00\x00\x83\x04 \x00\x91\x05`\x00]\x13\xa0\x00\x12\x17 \x1f\x0c …        r4 load str located at 4295181304
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    ldxw r4, [r2-0x4]                       
    and64 r4, 2097151                               r4 &= 2097151   ///  r4 = r4.and(2097151)
    ja lbb_23048                                    if true { pc += -35 }
lbb_23083:
    mov64 r1, r2                                    r1 = r2
    mov64 r2, 727                                   r2 = 727 as i32 as i64 as u64
    lddw r3, 0x1000354c8 --> b"\x00\x00\x00\x00\xb5C\x03\x00(\x00\x00\x00\x00\x00\x00\x00\\x00\x00\x00\x…        r3 load str located at 4295185608
    call function_20679                     
lbb_23088:
    mov64 r1, r2                                    r1 = r2
    mov64 r2, 33                                    r2 = 33 as i32 as i64 as u64
    lddw r3, 0x1000354b0 --> b"\x00\x00\x00\x00\xb5C\x03\x00(\x00\x00\x00\x00\x00\x00\x00P\x00\x00\x00(\…        r3 load str located at 4295185584
    call function_20679                     

function_23093:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, r2                                    r5 = r2
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jlt r5, 1065353216, lbb_23125                   if r5 < (1065353216 as i32 as i64 as u64) { pc += 26 }
    jlt r5, 2139095040, lbb_23107                   if r5 < (2139095040 as i32 as i64 as u64) { pc += 7 }
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mov64 r3, -1                                    r3 = -1 as i32 as i64 as u64
    jeq r2, 2139095040, lbb_23105                   if r2 == (2139095040 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_23105:
    mov64 r4, r3                                    r4 = r3
    ja lbb_23125                                    if true { pc += 18 }
lbb_23107:
    mov64 r3, r2                                    r3 = r2
    lsh64 r3, 40                                    r3 <<= 40   ///  r3 = r3.wrapping_shl(40)
    lddw r4, 0x8000000000000000                     r4 load str located at -9223372036854775808
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    rsh64 r2, 23                                    r2 >>= 23   ///  r2 = r2.wrapping_shr(23)
    mov64 r4, 126                                   r4 = 126 as i32 as i64 as u64
    sub64 r4, r2                                    r4 -= r2   ///  r4 = r4.wrapping_sub(r2)
    and64 r4, 127                                   r4 &= 127   ///  r4 = r4.and(127)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -16                                   r2 += -16   ///  r2 = r2.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r2                                    r1 = r2
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    call function_23325                     
    mov64 r1, r6                                    r1 = r6
    ldxdw r3, [r10-0x8]                     
    ldxdw r4, [r10-0x10]                    
lbb_23125:
    stxdw [r1+0x8], r3                      
    stxdw [r1+0x0], r4                      
    exit                                    

function_23128:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    lddw r2, 0x3ff0000000000000                     r2 load str located at 4607182418800017408
    jlt r1, r2, lbb_23151                           if r1 < r2 { pc += 19 }
    lddw r2, 0x43f0000000000000                     r2 load str located at 4895412794951729152
    jlt r1, r2, lbb_23141                           if r1 < r2 { pc += 6 }
    mov64 r0, -1                                    r0 = -1 as i32 as i64 as u64
    lddw r2, 0x7ff0000000000001                     r2 load str located at 9218868437227405313
    jlt r1, r2, lbb_23151                           if r1 < r2 { pc += 12 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_23151                                    if true { pc += 10 }
lbb_23141:
    mov64 r0, r1                                    r0 = r1
    lsh64 r0, 11                                    r0 <<= 11   ///  r0 = r0.wrapping_shl(11)
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    rsh64 r1, 52                                    r1 >>= 52   ///  r1 = r1.wrapping_shr(52)
    mov64 r2, 62                                    r2 = 62 as i32 as i64 as u64
    sub64 r2, r1                                    r2 -= r1   ///  r2 = r2.wrapping_sub(r1)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    rsh64 r0, r2                                    r0 >>= r2   ///  r0 = r0.wrapping_shr(r2 as u32)
lbb_23151:
    exit                                    

function_23152:
    mov64 r6, r1                                    r6 = r1
    syscall [invalid]                       
    mov64 r0, r6                                    r0 = r6
    exit                                    

function_23156:
    mov64 r6, r1                                    r6 = r1
    syscall [invalid]                       
    mov64 r0, r6                                    r0 = r6
    exit                                    

function_23160:
    mov64 r6, r1                                    r6 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    syscall [invalid]                       
    mov64 r0, r6                                    r0 = r6
    exit                                    

function_23165:
    stw [r10-0x4], 0                        
    mov64 r4, r10                                   r4 = r10
    add64 r4, -4                                    r4 += -4   ///  r4 = r4.wrapping_add(-4 as i32 as i64 as u64)
    syscall [invalid]                       
    ldxw r0, [r10-0x4]                      
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    exit                                    

function_23173:
    mov64 r6, r2                                    r6 = r2
    mov64 r8, r1                                    r8 = r1
    mov64 r7, r8                                    r7 = r8
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    arsh64 r7, 32                                   r7 >>= 32 (signed)   ///  r7 = (r7 as i64).wrapping_shr(32)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r1, r7                                    r1 = r7
    call function_23430                     
    mov64 r9, r6                                    r9 = r6
    jslt r0, 0, lbb_23185                           if (r0 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r9, r8                                    r9 = r8
lbb_23185:
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r7                                    r2 = r7
    call function_25947                     
    jne r0, 0, lbb_23190                            if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r9                                    r6 = r9
lbb_23190:
    mov64 r0, r6                                    r0 = r6
    exit                                    

function_23192:
    mov64 r7, r2                                    r7 = r2
    mov64 r6, r1                                    r6 = r1
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r8, r7                                    r8 = r7
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    arsh64 r8, 32                                   r8 >>= 32 (signed)   ///  r8 = (r8 as i64).wrapping_shr(32)
    mov64 r2, r8                                    r2 = r8
    call function_23430                     
    mov64 r9, r6                                    r9 = r6
    jslt r0, 0, lbb_23204                           if (r0 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r9, r7                                    r9 = r7
lbb_23204:
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r8                                    r2 = r8
    call function_25947                     
    jne r0, 0, lbb_23209                            if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r9                                    r6 = r9
lbb_23209:
    mov64 r0, r6                                    r0 = r6
    exit                                    

function_23211:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_23273                            if r1 == (0 as i32 as i64 as u64) { pc += 60 }
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    mov64 r2, r1                                    r2 = r1
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 16                                    r3 >>= 16   ///  r3 = r3.wrapping_shr(16)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    lddw r3, 0x5555555555555555                     r3 load str located at 6148914691236517205
    mov64 r4, r2                                    r4 = r2
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    sub64 r2, r4                                    r2 -= r4   ///  r2 = r2.wrapping_sub(r4)
    lddw r4, 0x3333333333333333                     r4 load str located at 3689348814741910323
    mov64 r3, r2                                    r3 = r2
    and64 r3, r4                                    r3 &= r4   ///  r3 = r3.and(r4)
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    and64 r2, r4                                    r2 &= r4   ///  r2 = r2.and(r4)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    mov64 r2, r3                                    r2 = r3
    rsh64 r2, 4                                     r2 >>= 4   ///  r2 = r2.wrapping_shr(4)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    lddw r2, 0xf0f0f0f0f0f0f0f                      r2 load str located at 1085102592571150095
    and64 r3, r2                                    r3 &= r2   ///  r3 = r3.and(r2)
    lddw r2, 0x101010101010101                      r2 load str located at 72340172838076673
    mul64 r3, r2                                    r3 *= r2   ///  r3 = r3.wrapping_mul(r2)
    rsh64 r3, 56                                    r3 >>= 56   ///  r3 = r3.wrapping_shr(56)
    lsh64 r1, r3                                    r1 <<= r3   ///  r1 = r1.wrapping_shl(r3 as u32)
    lsh64 r3, 52                                    r3 <<= 52   ///  r3 = r3.wrapping_shl(52)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 11                                    r2 >>= 11   ///  r2 = r2.wrapping_shr(11)
    mov64 r0, r2                                    r0 = r2
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    lsh64 r1, 53                                    r1 <<= 53   ///  r1 = r1.wrapping_shl(53)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 63                                    r3 >>= 63   ///  r3 = r3.wrapping_shr(63)
    and64 r3, r2                                    r3 &= r2   ///  r3 = r3.and(r2)
    sub64 r1, r3                                    r1 -= r3   ///  r1 = r1.wrapping_sub(r3)
    rsh64 r1, 63                                    r1 >>= 63   ///  r1 = r1.wrapping_shr(63)
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
    lddw r1, 0x43d0000000000000                     r1 load str located at 4886405595696988160
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
lbb_23273:
    exit                                    

function_23274:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_23324                            if r1 == (0 as i32 as i64 as u64) { pc += 48 }
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    mov64 r2, r1                                    r2 = r1
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    lddw r3, 0xffffff00                             r3 load str located at 4294967040
    mov64 r4, r2                                    r4 = r2
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r4, 8                                     r4 >>= 8   ///  r4 = r4.wrapping_shr(8)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    lddw r3, 0xffff0000                             r3 load str located at 4294901760
    mov64 r4, r2                                    r4 = r2
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r4, 16                                    r4 >>= 16   ///  r4 = r4.wrapping_shr(16)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    and64 r3, 1431655765                            r3 &= 1431655765   ///  r3 = r3.and(1431655765)
    sub64 r2, r3                                    r2 -= r3   ///  r2 = r2.wrapping_sub(r3)
    mov64 r3, r2                                    r3 = r2
    and64 r3, 858993459                             r3 &= 858993459   ///  r3 = r3.and(858993459)
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    and64 r2, 858993459                             r2 &= 858993459   ///  r2 = r2.and(858993459)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    mov64 r2, r3                                    r2 = r3
    rsh64 r2, 4                                     r2 >>= 4   ///  r2 = r2.wrapping_shr(4)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    and64 r3, 252645135                             r3 &= 252645135   ///  r3 = r3.and(252645135)
    mul64 r3, 16843009                              r3 *= 16843009   ///  r3 = r3.wrapping_mul(16843009 as u64)
    lddw r2, 0xff000000                             r2 load str located at 4278190080
    and64 r3, r2                                    r3 &= r2   ///  r3 = r3.and(r2)
    rsh64 r3, 24                                    r3 >>= 24   ///  r3 = r3.wrapping_shr(24)
    mov64 r2, r3                                    r2 = r3
    add64 r2, 21                                    r2 += 21   ///  r2 = r2.wrapping_add(21 as i32 as i64 as u64)
    lsh64 r1, r2                                    r1 <<= r2   ///  r1 = r1.wrapping_shl(r2 as u32)
    mov64 r0, 1053                                  r0 = 1053 as i32 as i64 as u64
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
    lsh64 r0, 52                                    r0 <<= 52   ///  r0 = r0.wrapping_shl(52)
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
lbb_23324:
    exit                                    

function_23325:
    mov64 r5, r4                                    r5 = r4
    and64 r5, 64                                    r5 &= 64   ///  r5 = r5.and(64)
    jne r5, 0, lbb_23340                            if r5 != (0 as i32 as i64 as u64) { pc += 12 }
    jeq r4, 0, lbb_23344                            if r4 == (0 as i32 as i64 as u64) { pc += 15 }
    mov64 r5, r4                                    r5 = r4
    and64 r5, 63                                    r5 &= 63   ///  r5 = r5.and(63)
    rsh64 r2, r5                                    r2 >>= r5   ///  r2 = r2.wrapping_shr(r5 as u32)
    neg64 r4                                        r4 = -r4   ///  r4 = (r4 as i64).wrapping_neg() as u64
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    mov64 r0, r3                                    r0 = r3
    lsh64 r0, r4                                    r0 <<= r4   ///  r0 = r0.wrapping_shl(r4 as u32)
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    rsh64 r3, r5                                    r3 >>= r5   ///  r3 = r3.wrapping_shr(r5 as u32)
    mov64 r2, r0                                    r2 = r0
    ja lbb_23344                                    if true { pc += 4 }
lbb_23340:
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    rsh64 r3, r4                                    r3 >>= r4   ///  r3 = r3.wrapping_shr(r4 as u32)
    mov64 r2, r3                                    r2 = r3
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_23344:
    stxdw [r1+0x8], r3                      
    stxdw [r1+0x0], r2                      
    exit                                    

function_23347:
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    lddw r4, 0x80000000                             r4 load str located at 2147483648
    mov64 r5, r1                                    r5 = r1
    arsh64 r5, 63                                   r5 >>= 63 (signed)   ///  r5 = (r5 as i64).wrapping_shr(63)
    mov64 r3, r1                                    r3 = r1
    xor64 r3, r5                                    r3 ^= r5   ///  r3 = r3.xor(r5)
    sub64 r3, r5                                    r3 -= r5   ///  r3 = r3.wrapping_sub(r5)
    mov64 r5, 64                                    r5 = 64 as i32 as i64 as u64
    jeq r3, 0, lbb_23401                            if r3 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r5, r3                                    r5 = r3
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    mov64 r0, r3                                    r0 = r3
    or64 r0, r5                                     r0 |= r5   ///  r0 = r0.or(r5)
    mov64 r5, r0                                    r5 = r0
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    or64 r0, r5                                     r0 |= r5   ///  r0 = r0.or(r5)
    mov64 r5, r0                                    r5 = r0
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    or64 r0, r5                                     r0 |= r5   ///  r0 = r0.or(r5)
    mov64 r5, r0                                    r5 = r0
    rsh64 r5, 8                                     r5 >>= 8   ///  r5 = r5.wrapping_shr(8)
    or64 r0, r5                                     r0 |= r5   ///  r0 = r0.or(r5)
    mov64 r5, r0                                    r5 = r0
    rsh64 r5, 16                                    r5 >>= 16   ///  r5 = r5.wrapping_shr(16)
    or64 r0, r5                                     r0 |= r5   ///  r0 = r0.or(r5)
    mov64 r5, r0                                    r5 = r0
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    or64 r0, r5                                     r0 |= r5   ///  r0 = r0.or(r5)
    xor64 r0, -1                                    r0 ^= -1   ///  r0 = r0.xor(-1)
    lddw r5, 0x5555555555555555                     r5 load str located at 6148914691236517205
    mov64 r6, r0                                    r6 = r0
    rsh64 r6, 1                                     r6 >>= 1   ///  r6 = r6.wrapping_shr(1)
    and64 r6, r5                                    r6 &= r5   ///  r6 = r6.and(r5)
    sub64 r0, r6                                    r0 -= r6   ///  r0 = r0.wrapping_sub(r6)
    lddw r6, 0x3333333333333333                     r6 load str located at 3689348814741910323
    mov64 r5, r0                                    r5 = r0
    and64 r5, r6                                    r5 &= r6   ///  r5 = r5.and(r6)
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    and64 r0, r6                                    r0 &= r6   ///  r0 = r0.and(r6)
    add64 r5, r0                                    r5 += r0   ///  r5 = r5.wrapping_add(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    add64 r5, r0                                    r5 += r0   ///  r5 = r5.wrapping_add(r0)
    lddw r0, 0xf0f0f0f0f0f0f0f                      r0 load str located at 1085102592571150095
    and64 r5, r0                                    r5 &= r0   ///  r5 = r5.and(r0)
    lddw r0, 0x101010101010101                      r0 load str located at 72340172838076673
    mul64 r5, r0                                    r5 *= r0   ///  r5 = r5.wrapping_mul(r0)
    rsh64 r5, 56                                    r5 >>= 56   ///  r5 = r5.wrapping_shr(56)
lbb_23401:
    and64 r2, r4                                    r2 &= r4   ///  r2 = r2.and(r4)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_23408                            if r1 == (0 as i32 as i64 as u64) { pc += 4 }
    mov64 r1, r5                                    r1 = r5
    lsh64 r1, 23                                    r1 <<= 23   ///  r1 = r1.wrapping_shl(23)
    mov64 r0, 1585446912                            r0 = 1585446912 as i32 as i64 as u64
    sub64 r0, r1                                    r0 -= r1   ///  r0 = r0.wrapping_sub(r1)
lbb_23408:
    and64 r5, 63                                    r5 &= 63   ///  r5 = r5.and(63)
    lsh64 r3, r5                                    r3 <<= r5   ///  r3 = r3.wrapping_shl(r5 as u32)
    mov64 r4, r3                                    r4 = r3
    and64 r4, 65535                                 r4 &= 65535   ///  r4 = r4.and(65535)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 8                                     r1 >>= 8   ///  r1 = r1.wrapping_shr(8)
    or64 r1, r4                                     r1 |= r4   ///  r1 = r1.or(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 39                                    r4 >>= 39   ///  r4 = r4.wrapping_shr(39)
    rsh64 r3, 40                                    r3 >>= 40   ///  r3 = r3.wrapping_shr(40)
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    lddw r3, 0x80000000                             r3 load str located at 2147483648
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    rsh64 r1, 31                                    r1 >>= 31   ///  r1 = r1.wrapping_shr(31)
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    exit                                    

function_23430:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r3, r1                                    r3 = r1
    and64 r3, 2147483647                            r3 &= 2147483647   ///  r3 = r3.and(2147483647)
    jgt r3, 2139095040, lbb_23460                   if r3 > (2139095040 as i32 as i64 as u64) { pc += 26 }
    mov64 r4, r2                                    r4 = r2
    and64 r4, 2147483647                            r4 &= 2147483647   ///  r4 = r4.and(2147483647)
    jgt r4, 2139095040, lbb_23460                   if r4 > (2139095040 as i32 as i64 as u64) { pc += 23 }
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r4, 0, lbb_23460                            if r4 == (0 as i32 as i64 as u64) { pc += 20 }
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r3, r2                                    r3 = r2
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    jsgt r3, -1, lbb_23453                          if (r3 as i64) > (-1 as i32 as i64) { pc += 6 }
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    jsgt r1, r2, lbb_23460                          if (r1 as i64) > (r2 as i64) { pc += 10 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_23459                           if r1 == r2 { pc += 7 }
    ja lbb_23460                                    if true { pc += 7 }
lbb_23453:
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    jslt r1, r2, lbb_23460                          if (r1 as i64) < (r2 as i64) { pc += 4 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_23459                           if r1 == r2 { pc += 1 }
    ja lbb_23460                                    if true { pc += 1 }
lbb_23459:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_23460:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    exit                                    

function_23463:
    call function_23673                     
    exit                                    

function_23465:
    mov64 r0, r1                                    r0 = r1
    mov64 r3, r2                                    r3 = r2
    and64 r3, 8388607                               r3 &= 8388607   ///  r3 = r3.and(8388607)
    mov64 r5, r0                                    r5 = r0
    and64 r5, 8388607                               r5 &= 8388607   ///  r5 = r5.and(8388607)
    mov64 r1, r2                                    r1 = r2
    xor64 r1, r0                                    r1 ^= r0   ///  r1 = r1.xor(r0)
    and64 r1, -2147483648                           r1 &= -2147483648   ///  r1 = r1.and(-2147483648)
    mov64 r4, r2                                    r4 = r2
    rsh64 r4, 23                                    r4 >>= 23   ///  r4 = r4.wrapping_shr(23)
    and64 r4, 255                                   r4 &= 255   ///  r4 = r4.and(255)
    mov64 r6, r0                                    r6 = r0
    rsh64 r6, 23                                    r6 >>= 23   ///  r6 = r6.wrapping_shr(23)
    and64 r6, 255                                   r6 &= 255   ///  r6 = r6.and(255)
    mov64 r7, r6                                    r7 = r6
    add64 r7, -255                                  r7 += -255   ///  r7 = r7.wrapping_add(-255 as i32 as i64 as u64)
    jlt r7, -254, lbb_23513                         if r7 < (-254 as i32 as i64 as u64) { pc += 31 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r8, r4                                    r8 = r4
    add64 r8, -255                                  r8 += -255   ///  r8 = r8.wrapping_add(-255 as i32 as i64 as u64)
    jlt r8, -254, lbb_23513                         if r8 < (-254 as i32 as i64 as u64) { pc += 27 }
lbb_23486:
    lsh64 r3, 8                                     r3 <<= 8   ///  r3 = r3.wrapping_shl(8)
    lddw r2, 0x80000000                             r2 load str located at 2147483648
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    add64 r4, r7                                    r4 += r7   ///  r4 = r4.wrapping_add(r7)
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    or64 r5, 8388608                                r5 |= 8388608   ///  r5 = r5.or(8388608)
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    mul64 r3, r5                                    r3 *= r5   ///  r3 = r3.wrapping_mul(r5)
    mov64 r2, r3                                    r2 = r3
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mov64 r5, r2                                    r5 = r2
    and64 r5, 8388608                               r5 &= 8388608   ///  r5 = r5.and(8388608)
    jeq r5, 0, lbb_23504                            if r5 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_23523                                    if true { pc += 19 }
lbb_23504:
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    mov64 r5, r3                                    r5 = r3
    rsh64 r5, 31                                    r5 >>= 31   ///  r5 = r5.wrapping_shr(31)
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    or64 r2, r5                                     r2 |= r5   ///  r2 = r2.or(r5)
    lsh64 r3, 1                                     r3 <<= 1   ///  r3 = r3.wrapping_shl(1)
    add64 r4, -127                                  r4 += -127   ///  r4 = r4.wrapping_add(-127 as i32 as i64 as u64)
    jsgt r4, 254, lbb_23575                         if (r4 as i64) > (254 as i32 as i64) { pc += 63 }
    ja lbb_23525                                    if true { pc += 12 }
lbb_23513:
    mov64 r9, r0                                    r9 = r0
    and64 r9, 2147483647                            r9 &= 2147483647   ///  r9 = r9.and(2147483647)
    jgt r9, 2139095040, lbb_23530                   if r9 > (2139095040 as i32 as i64 as u64) { pc += 14 }
    mov64 r8, r2                                    r8 = r2
    and64 r8, 2147483647                            r8 &= 2147483647   ///  r8 = r8.and(2147483647)
    jgt r8, 2139095040, lbb_23520                   if r8 > (2139095040 as i32 as i64 as u64) { pc += 1 }
    ja lbb_23532                                    if true { pc += 12 }
lbb_23520:
    or64 r2, 4194304                                r2 |= 4194304   ///  r2 = r2.or(4194304)
    mov64 r0, r2                                    r0 = r2
    ja lbb_23577                                    if true { pc += 54 }
lbb_23523:
    add64 r4, -126                                  r4 += -126   ///  r4 = r4.wrapping_add(-126 as i32 as i64 as u64)
    jsgt r4, 254, lbb_23575                         if (r4 as i64) > (254 as i32 as i64) { pc += 50 }
lbb_23525:
    jslt r4, 1, lbb_23537                           if (r4 as i64) < (1 as i32 as i64) { pc += 11 }
    and64 r2, 8388607                               r2 &= 8388607   ///  r2 = r2.and(8388607)
    lsh64 r4, 23                                    r4 <<= 23   ///  r4 = r4.wrapping_shl(23)
    or64 r4, r2                                     r4 |= r2   ///  r4 = r4.or(r2)
    ja lbb_23557                                    if true { pc += 27 }
lbb_23530:
    or64 r0, 4194304                                r0 |= 4194304   ///  r0 = r0.or(4194304)
    ja lbb_23577                                    if true { pc += 45 }
lbb_23532:
    jeq r9, 2139095040, lbb_23534                   if r9 == (2139095040 as i32 as i64 as u64) { pc += 1 }
    ja lbb_23571                                    if true { pc += 37 }
lbb_23534:
    mov64 r0, 2143289344                            r0 = 2143289344 as i32 as i64 as u64
    jeq r8, 0, lbb_23577                            if r8 == (0 as i32 as i64 as u64) { pc += 41 }
    ja lbb_23575                                    if true { pc += 38 }
lbb_23537:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    sub64 r5, r4                                    r5 -= r4   ///  r5 = r5.wrapping_sub(r4)
    jgt r5, 31, lbb_23576                           if r5 > (31 as i32 as i64 as u64) { pc += 36 }
    add64 r4, 31                                    r4 += 31   ///  r4 = r4.wrapping_add(31 as i32 as i64 as u64)
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    mov64 r0, r2                                    r0 = r2
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    lsh64 r0, r4                                    r0 <<= r4   ///  r0 = r0.wrapping_shl(r4 as u32)
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    rsh64 r3, r5                                    r3 >>= r5   ///  r3 = r3.wrapping_shr(r5 as u32)
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    rsh64 r2, r5                                    r2 >>= r5   ///  r2 = r2.wrapping_shr(r5 as u32)
    mov64 r3, r0                                    r3 = r0
    mov64 r4, r2                                    r4 = r2
lbb_23557:
    mov64 r0, r4                                    r0 = r4
    or64 r0, r1                                     r0 |= r1   ///  r0 = r0.or(r1)
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    lddw r1, 0x80000000                             r1 load str located at 2147483648
    jgt r3, r1, lbb_23565                           if r3 > r1 { pc += 1 }
    ja lbb_23567                                    if true { pc += 2 }
lbb_23565:
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    ja lbb_23577                                    if true { pc += 10 }
lbb_23567:
    jne r3, r1, lbb_23577                           if r3 != r1 { pc += 9 }
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    add64 r0, r4                                    r0 += r4   ///  r0 = r0.wrapping_add(r4)
    ja lbb_23577                                    if true { pc += 6 }
lbb_23571:
    jeq r8, 2139095040, lbb_23573                   if r8 == (2139095040 as i32 as i64 as u64) { pc += 1 }
    ja lbb_23578                                    if true { pc += 5 }
lbb_23573:
    mov64 r0, 2143289344                            r0 = 2143289344 as i32 as i64 as u64
    jeq r9, 0, lbb_23577                            if r9 == (0 as i32 as i64 as u64) { pc += 2 }
lbb_23575:
    or64 r1, 2139095040                             r1 |= 2139095040   ///  r1 = r1.or(2139095040)
lbb_23576:
    mov64 r0, r1                                    r0 = r1
lbb_23577:
    exit                                    
lbb_23578:
    jeq r9, 0, lbb_23576                            if r9 == (0 as i32 as i64 as u64) { pc += -3 }
    jeq r8, 0, lbb_23576                            if r8 == (0 as i32 as i64 as u64) { pc += -4 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jlt r9, 8388608, lbb_23583                      if r9 < (8388608 as i32 as i64 as u64) { pc += 1 }
    ja lbb_23627                                    if true { pc += 44 }
lbb_23583:
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    jeq r5, 0, lbb_23622                            if r5 == (0 as i32 as i64 as u64) { pc += 37 }
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    mov64 r2, r5                                    r2 = r5
    or64 r2, r0                                     r2 |= r0   ///  r2 = r2.or(r0)
    mov64 r0, r2                                    r0 = r2
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r2, r0                                     r2 |= r0   ///  r2 = r2.or(r0)
    mov64 r0, r2                                    r0 = r2
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r2, r0                                     r2 |= r0   ///  r2 = r2.or(r0)
    mov64 r7, r2                                    r7 = r2
    and64 r7, 8388352                               r7 &= 8388352   ///  r7 = r7.and(8388352)
    rsh64 r7, 8                                     r7 >>= 8   ///  r7 = r7.wrapping_shr(8)
    mov64 r0, r2                                    r0 = r2
    or64 r0, r7                                     r0 |= r7   ///  r0 = r0.or(r7)
    and64 r2, 8323072                               r2 &= 8323072   ///  r2 = r2.and(8323072)
    rsh64 r2, 16                                    r2 >>= 16   ///  r2 = r2.wrapping_shr(16)
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    xor64 r0, -1                                    r0 ^= -1   ///  r0 = r0.xor(-1)
    mov64 r2, r0                                    r2 = r0
    rsh64 r2, 1                                     r2 >>= 1   ///  r2 = r2.wrapping_shr(1)
    and64 r2, 1431655765                            r2 &= 1431655765   ///  r2 = r2.and(1431655765)
    sub64 r0, r2                                    r0 -= r2   ///  r0 = r0.wrapping_sub(r2)
    mov64 r2, r0                                    r2 = r0
    and64 r2, 858993459                             r2 &= 858993459   ///  r2 = r2.and(858993459)
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    and64 r0, 858993459                             r0 &= 858993459   ///  r0 = r0.and(858993459)
    add64 r2, r0                                    r2 += r0   ///  r2 = r2.wrapping_add(r0)
    mov64 r0, r2                                    r0 = r2
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    add64 r2, r0                                    r2 += r0   ///  r2 = r2.wrapping_add(r0)
    and64 r2, 252645135                             r2 &= 252645135   ///  r2 = r2.and(252645135)
    mul64 r2, 16843009                              r2 *= 16843009   ///  r2 = r2.wrapping_mul(16843009 as u64)
    lddw r0, 0xff000000                             r0 load str located at 4278190080
    and64 r2, r0                                    r2 &= r0   ///  r2 = r2.and(r0)
    rsh64 r2, 24                                    r2 >>= 24   ///  r2 = r2.wrapping_shr(24)
lbb_23622:
    mov64 r7, 9                                     r7 = 9 as i32 as i64 as u64
    sub64 r7, r2                                    r7 -= r2   ///  r7 = r7.wrapping_sub(r2)
    add64 r2, 24                                    r2 += 24   ///  r2 = r2.wrapping_add(24 as i32 as i64 as u64)
    and64 r2, 31                                    r2 &= 31   ///  r2 = r2.and(31)
    lsh64 r5, r2                                    r5 <<= r2   ///  r5 = r5.wrapping_shl(r2 as u32)
lbb_23627:
    jgt r8, 8388607, lbb_23486                      if r8 > (8388607 as i32 as i64 as u64) { pc += -142 }
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    jeq r3, 0, lbb_23667                            if r3 == (0 as i32 as i64 as u64) { pc += 37 }
    mov64 r0, r3                                    r0 = r3
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    mov64 r2, r3                                    r2 = r3
    or64 r2, r0                                     r2 |= r0   ///  r2 = r2.or(r0)
    mov64 r0, r2                                    r0 = r2
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r2, r0                                     r2 |= r0   ///  r2 = r2.or(r0)
    mov64 r0, r2                                    r0 = r2
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r2, r0                                     r2 |= r0   ///  r2 = r2.or(r0)
    mov64 r8, r2                                    r8 = r2
    and64 r8, 8388352                               r8 &= 8388352   ///  r8 = r8.and(8388352)
    rsh64 r8, 8                                     r8 >>= 8   ///  r8 = r8.wrapping_shr(8)
    mov64 r0, r2                                    r0 = r2
    or64 r0, r8                                     r0 |= r8   ///  r0 = r0.or(r8)
    and64 r2, 8323072                               r2 &= 8323072   ///  r2 = r2.and(8323072)
    rsh64 r2, 16                                    r2 >>= 16   ///  r2 = r2.wrapping_shr(16)
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    xor64 r0, -1                                    r0 ^= -1   ///  r0 = r0.xor(-1)
    mov64 r2, r0                                    r2 = r0
    rsh64 r2, 1                                     r2 >>= 1   ///  r2 = r2.wrapping_shr(1)
    and64 r2, 1431655765                            r2 &= 1431655765   ///  r2 = r2.and(1431655765)
    sub64 r0, r2                                    r0 -= r2   ///  r0 = r0.wrapping_sub(r2)
    mov64 r2, r0                                    r2 = r0
    and64 r2, 858993459                             r2 &= 858993459   ///  r2 = r2.and(858993459)
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    and64 r0, 858993459                             r0 &= 858993459   ///  r0 = r0.and(858993459)
    add64 r2, r0                                    r2 += r0   ///  r2 = r2.wrapping_add(r0)
    mov64 r0, r2                                    r0 = r2
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    add64 r2, r0                                    r2 += r0   ///  r2 = r2.wrapping_add(r0)
    and64 r2, 252645135                             r2 &= 252645135   ///  r2 = r2.and(252645135)
    mul64 r2, 16843009                              r2 *= 16843009   ///  r2 = r2.wrapping_mul(16843009 as u64)
    lddw r0, 0xff000000                             r0 load str located at 4278190080
    and64 r2, r0                                    r2 &= r0   ///  r2 = r2.and(r0)
    rsh64 r2, 24                                    r2 >>= 24   ///  r2 = r2.wrapping_shr(24)
lbb_23667:
    sub64 r7, r2                                    r7 -= r2   ///  r7 = r7.wrapping_sub(r2)
    add64 r2, 24                                    r2 += 24   ///  r2 = r2.wrapping_add(24 as i32 as i64 as u64)
    and64 r2, 31                                    r2 &= 31   ///  r2 = r2.and(31)
    lsh64 r3, r2                                    r3 <<= r2   ///  r3 = r3.wrapping_shl(r2 as u32)
    add64 r7, 9                                     r7 += 9   ///  r7 = r7.wrapping_add(9 as i32 as i64 as u64)
    ja lbb_23486                                    if true { pc += -187 }

function_23673:
    mov64 r3, r2                                    r3 = r2
    mov64 r0, r1                                    r0 = r1
    mov64 r6, r3                                    r6 = r3
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    and64 r6, r1                                    r6 &= r1   ///  r6 = r6.and(r1)
    lddw r1, 0xfffffffffffff                        r1 load str located at 4503599627370495
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    mov64 r4, r0                                    r4 = r0
    and64 r4, r1                                    r4 &= r1   ///  r4 = r4.and(r1)
    mov64 r7, r3                                    r7 = r3
    rsh64 r7, 52                                    r7 >>= 52   ///  r7 = r7.wrapping_shr(52)
    and64 r7, 2047                                  r7 &= 2047   ///  r7 = r7.and(2047)
    mov64 r8, r0                                    r8 = r0
    rsh64 r8, 52                                    r8 >>= 52   ///  r8 = r8.wrapping_shr(52)
    and64 r8, 2047                                  r8 &= 2047   ///  r8 = r8.and(2047)
    mov64 r1, r8                                    r1 = r8
    add64 r1, -2047                                 r1 += -2047   ///  r1 = r1.wrapping_add(-2047 as i32 as i64 as u64)
    jlt r1, -2046, lbb_23733                        if r1 < (-2046 as i32 as i64 as u64) { pc += 39 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r1, r7                                    r1 = r7
    add64 r1, -2047                                 r1 += -2047   ///  r1 = r1.wrapping_add(-2047 as i32 as i64 as u64)
    jlt r1, -2046, lbb_23733                        if r1 < (-2046 as i32 as i64 as u64) { pc += 35 }
lbb_23698:
    stxdw [r10-0x18], r6                    
    lsh64 r2, 11                                    r2 <<= 11   ///  r2 = r2.wrapping_shl(11)
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    lddw r6, 0x10000000000000                       r6 load str located at 4503599627370496
    or64 r4, r6                                     r4 |= r6   ///  r4 = r4.or(r6)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_25903                     
    add64 r7, r8                                    r7 += r8   ///  r7 = r7.wrapping_add(r8)
    add64 r7, r9                                    r7 += r9   ///  r7 = r7.wrapping_add(r9)
    ldxdw r2, [r10-0x8]                     
    mov64 r3, r2                                    r3 = r2
    and64 r3, r6                                    r3 &= r6   ///  r3 = r3.and(r6)
    ldxdw r1, [r10-0x10]                    
    jeq r3, 0, lbb_23719                            if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_23749                                    if true { pc += 30 }
lbb_23719:
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 63                                    r3 >>= 63   ///  r3 = r3.wrapping_shr(63)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    add64 r7, -1023                                 r7 += -1023   ///  r7 = r7.wrapping_add(-1023 as i32 as i64 as u64)
    ldxdw r5, [r10-0x18]                    
    jsgt r7, 2046, lbb_23728                        if (r7 as i64) > (2046 as i32 as i64) { pc += 1 }
    ja lbb_23752                                    if true { pc += 24 }
lbb_23728:
    lddw r1, 0x7ff0000000000000                     r1 load str located at 9218868437227405312
    or64 r5, r1                                     r5 |= r1   ///  r5 = r5.or(r1)
    mov64 r0, r5                                    r0 = r5
    ja lbb_23810                                    if true { pc += 77 }
lbb_23733:
    lddw r9, 0x7fffffffffffffff                     r9 load str located at 9223372036854775807
    mov64 r5, r0                                    r5 = r0
    and64 r5, r9                                    r5 &= r9   ///  r5 = r5.and(r9)
    lddw r1, 0x7ff0000000000000                     r1 load str located at 9218868437227405312
    jgt r5, r1, lbb_23759                           if r5 > r1 { pc += 19 }
    mov64 r0, r3                                    r0 = r3
    and64 r0, r9                                    r0 &= r9   ///  r0 = r0.and(r9)
    jgt r0, r1, lbb_23744                           if r0 > r1 { pc += 1 }
    ja lbb_23763                                    if true { pc += 19 }
lbb_23744:
    lddw r1, 0x8000000000000                        r1 load str located at 2251799813685248
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r0, r3                                    r0 = r3
    ja lbb_23810                                    if true { pc += 61 }
lbb_23749:
    add64 r7, -1022                                 r7 += -1022   ///  r7 = r7.wrapping_add(-1022 as i32 as i64 as u64)
    ldxdw r5, [r10-0x18]                    
    jsgt r7, 2046, lbb_23728                        if (r7 as i64) > (2046 as i32 as i64) { pc += -24 }
lbb_23752:
    jslt r7, 1, lbb_23772                           if (r7 as i64) < (1 as i32 as i64) { pc += 19 }
    lddw r3, 0xfffffffffffff                        r3 load str located at 4503599627370495
    and64 r2, r3                                    r2 &= r3   ///  r2 = r2.and(r3)
    lsh64 r7, 52                                    r7 <<= 52   ///  r7 = r7.wrapping_shl(52)
    or64 r7, r2                                     r7 |= r2   ///  r7 = r7.or(r2)
    ja lbb_23799                                    if true { pc += 40 }
lbb_23759:
    lddw r1, 0x8000000000000                        r1 load str located at 2251799813685248
    or64 r0, r1                                     r0 |= r1   ///  r0 = r0.or(r1)
    ja lbb_23810                                    if true { pc += 47 }
lbb_23763:
    jeq r5, r1, lbb_23765                           if r5 == r1 { pc += 1 }
    ja lbb_23778                                    if true { pc += 13 }
lbb_23765:
    mov64 r1, r0                                    r1 = r0
    lddw r0, 0x7ff8000000000000                     r0 load str located at 9221120237041090560
    jeq r1, 0, lbb_23810                            if r1 == (0 as i32 as i64 as u64) { pc += 41 }
    lddw r1, 0x7ff0000000000000                     r1 load str located at 9218868437227405312
    ja lbb_23783                                    if true { pc += 11 }
lbb_23772:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    sub64 r3, r7                                    r3 -= r7   ///  r3 = r3.wrapping_sub(r7)
    jgt r3, 63, lbb_23776                           if r3 > (63 as i32 as i64 as u64) { pc += 1 }
    ja lbb_23786                                    if true { pc += 10 }
lbb_23776:
    mov64 r0, r5                                    r0 = r5
    ja lbb_23810                                    if true { pc += 32 }
lbb_23778:
    jeq r0, r1, lbb_23780                           if r0 == r1 { pc += 1 }
    ja lbb_23811                                    if true { pc += 31 }
lbb_23780:
    lddw r0, 0x7ff8000000000000                     r0 load str located at 9221120237041090560
    jeq r5, 0, lbb_23810                            if r5 == (0 as i32 as i64 as u64) { pc += 27 }
lbb_23783:
    or64 r6, r1                                     r6 |= r1   ///  r6 = r6.or(r1)
lbb_23784:
    mov64 r0, r6                                    r0 = r6
    ja lbb_23810                                    if true { pc += 24 }
lbb_23786:
    add64 r7, 63                                    r7 += 63   ///  r7 = r7.wrapping_add(63 as i32 as i64 as u64)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r4, r2                                    r4 = r2
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    lsh64 r4, r7                                    r4 <<= r7   ///  r4 = r4.wrapping_shl(r7 as u32)
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    rsh64 r1, r3                                    r1 >>= r3   ///  r1 = r1.wrapping_shr(r3 as u32)
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    rsh64 r2, r3                                    r2 >>= r3   ///  r2 = r2.wrapping_shr(r3 as u32)
    mov64 r1, r4                                    r1 = r4
    mov64 r7, r2                                    r7 = r2
lbb_23799:
    mov64 r0, r7                                    r0 = r7
    or64 r0, r5                                     r0 |= r5   ///  r0 = r0.or(r5)
    lddw r2, 0x8000000000000000                     r2 load str located at -9223372036854775808
    jgt r1, r2, lbb_23805                           if r1 > r2 { pc += 1 }
    ja lbb_23807                                    if true { pc += 2 }
lbb_23805:
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    ja lbb_23810                                    if true { pc += 3 }
lbb_23807:
    jne r1, r2, lbb_23810                           if r1 != r2 { pc += 2 }
    and64 r7, 1                                     r7 &= 1   ///  r7 = r7.and(1)
    add64 r0, r7                                    r0 += r7   ///  r0 = r0.wrapping_add(r7)
lbb_23810:
    exit                                    
lbb_23811:
    jeq r5, 0, lbb_23784                            if r5 == (0 as i32 as i64 as u64) { pc += -28 }
    jeq r0, 0, lbb_23784                            if r0 == (0 as i32 as i64 as u64) { pc += -29 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    lddw r1, 0x10000000000000                       r1 load str located at 4503599627370496
    jlt r5, r1, lbb_23818                           if r5 < r1 { pc += 1 }
    ja lbb_23868                                    if true { pc += 50 }
lbb_23818:
    mov64 r5, 64                                    r5 = 64 as i32 as i64 as u64
    jeq r4, 0, lbb_23863                            if r4 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
    mov64 r3, r4                                    r3 = r4
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 8                                     r1 >>= 8   ///  r1 = r1.wrapping_shr(8)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 16                                    r1 >>= 16   ///  r1 = r1.wrapping_shr(16)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    lddw r1, 0x5555555555555555                     r1 load str located at 6148914691236517205
    mov64 r5, r3                                    r5 = r3
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    and64 r5, r1                                    r5 &= r1   ///  r5 = r5.and(r1)
    sub64 r3, r5                                    r3 -= r5   ///  r3 = r3.wrapping_sub(r5)
    lddw r1, 0x3333333333333333                     r1 load str located at 3689348814741910323
    mov64 r5, r3                                    r5 = r3
    and64 r5, r1                                    r5 &= r1   ///  r5 = r5.and(r1)
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    add64 r5, r3                                    r5 += r3   ///  r5 = r5.wrapping_add(r3)
    mov64 r1, r5                                    r1 = r5
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    add64 r5, r1                                    r5 += r1   ///  r5 = r5.wrapping_add(r1)
    lddw r1, 0xf0f0f0f0f0f0f0f                      r1 load str located at 1085102592571150095
    and64 r5, r1                                    r5 &= r1   ///  r5 = r5.and(r1)
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
    mul64 r5, r1                                    r5 *= r1   ///  r5 = r5.wrapping_mul(r1)
    rsh64 r5, 56                                    r5 >>= 56   ///  r5 = r5.wrapping_shr(56)
lbb_23863:
    mov64 r9, 12                                    r9 = 12 as i32 as i64 as u64
    sub64 r9, r5                                    r9 -= r5   ///  r9 = r9.wrapping_sub(r5)
    add64 r5, 53                                    r5 += 53   ///  r5 = r5.wrapping_add(53 as i32 as i64 as u64)
    and64 r5, 63                                    r5 &= 63   ///  r5 = r5.and(63)
    lsh64 r4, r5                                    r4 <<= r5   ///  r4 = r4.wrapping_shl(r5 as u32)
lbb_23868:
    lddw r1, 0xfffffffffffff                        r1 load str located at 4503599627370495
    jgt r0, r1, lbb_23698                           if r0 > r1 { pc += -173 }
    mov64 r3, 64                                    r3 = 64 as i32 as i64 as u64
    jeq r2, 0, lbb_23916                            if r2 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    mov64 r1, r2                                    r1 = r2
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 16                                    r3 >>= 16   ///  r3 = r3.wrapping_shr(16)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    xor64 r1, -1                                    r1 ^= -1   ///  r1 = r1.xor(-1)
    lddw r3, 0x5555555555555555                     r3 load str located at 6148914691236517205
    mov64 r5, r1                                    r5 = r1
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    and64 r5, r3                                    r5 &= r3   ///  r5 = r5.and(r3)
    sub64 r1, r5                                    r1 -= r5   ///  r1 = r1.wrapping_sub(r5)
    lddw r5, 0x3333333333333333                     r5 load str located at 3689348814741910323
    mov64 r3, r1                                    r3 = r1
    and64 r3, r5                                    r3 &= r5   ///  r3 = r3.and(r5)
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    and64 r1, r5                                    r1 &= r5   ///  r1 = r1.and(r5)
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    lddw r1, 0xf0f0f0f0f0f0f0f                      r1 load str located at 1085102592571150095
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
    mul64 r3, r1                                    r3 *= r1   ///  r3 = r3.wrapping_mul(r1)
    rsh64 r3, 56                                    r3 >>= 56   ///  r3 = r3.wrapping_shr(56)
lbb_23916:
    sub64 r9, r3                                    r9 -= r3   ///  r9 = r9.wrapping_sub(r3)
    add64 r3, 53                                    r3 += 53   ///  r3 = r3.wrapping_add(53 as i32 as i64 as u64)
    and64 r3, 63                                    r3 &= 63   ///  r3 = r3.and(63)
    lsh64 r2, r3                                    r2 <<= r3   ///  r2 = r2.wrapping_shl(r3 as u32)
    add64 r9, 12                                    r9 += 12   ///  r9 = r9.wrapping_add(12 as i32 as i64 as u64)
    ja lbb_23698                                    if true { pc += -224 }

function_23922:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r3, r1                                    r3 = r1
    and64 r3, 2147483647                            r3 &= 2147483647   ///  r3 = r3.and(2147483647)
    jgt r3, 2139095040, lbb_23952                   if r3 > (2139095040 as i32 as i64 as u64) { pc += 26 }
    mov64 r4, r2                                    r4 = r2
    and64 r4, 2147483647                            r4 &= 2147483647   ///  r4 = r4.and(2147483647)
    jgt r4, 2139095040, lbb_23952                   if r4 > (2139095040 as i32 as i64 as u64) { pc += 23 }
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r4, 0, lbb_23952                            if r4 == (0 as i32 as i64 as u64) { pc += 20 }
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r3, r2                                    r3 = r2
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    jsgt r3, -1, lbb_23945                          if (r3 as i64) > (-1 as i32 as i64) { pc += 6 }
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    jsgt r1, r2, lbb_23952                          if (r1 as i64) > (r2 as i64) { pc += 10 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_23951                           if r1 == r2 { pc += 7 }
    ja lbb_23952                                    if true { pc += 7 }
lbb_23945:
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    jslt r1, r2, lbb_23952                          if (r1 as i64) < (r2 as i64) { pc += 4 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_23951                           if r1 == r2 { pc += 1 }
    ja lbb_23952                                    if true { pc += 1 }
lbb_23951:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_23952:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    exit                                    

function_23955:
    mov64 r0, r1                                    r0 = r1
    mov64 r3, r2                                    r3 = r2
    xor64 r3, r0                                    r3 ^= r0   ///  r3 = r3.xor(r0)
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    stxdw [r10-0x18], r3                    
    lddw r3, 0xfffffffffffff                        r3 load str located at 4503599627370495
    mov64 r1, r2                                    r1 = r2
    and64 r1, r3                                    r1 &= r3   ///  r1 = r1.and(r3)
    mov64 r8, r0                                    r8 = r0
    and64 r8, r3                                    r8 &= r3   ///  r8 = r8.and(r3)
    mov64 r7, r2                                    r7 = r2
    rsh64 r7, 52                                    r7 >>= 52   ///  r7 = r7.wrapping_shr(52)
    and64 r7, 2047                                  r7 &= 2047   ///  r7 = r7.and(2047)
    mov64 r3, r0                                    r3 = r0
    rsh64 r3, 52                                    r3 >>= 52   ///  r3 = r3.wrapping_shr(52)
    and64 r3, 2047                                  r3 &= 2047   ///  r3 = r3.and(2047)
    stxdw [r10-0x20], r3                    
    add64 r3, -2047                                 r3 += -2047   ///  r3 = r3.wrapping_add(-2047 as i32 as i64 as u64)
    jlt r3, -2046, lbb_24084                        if r3 < (-2046 as i32 as i64 as u64) { pc += 107 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r3, r7                                    r3 = r7
    add64 r3, -2047                                 r3 += -2047   ///  r3 = r3.wrapping_add(-2047 as i32 as i64 as u64)
    jlt r3, -2046, lbb_24084                        if r3 < (-2046 as i32 as i64 as u64) { pc += 103 }
lbb_23981:
    lddw r2, 0x10000000000000                       r2 load str located at 4503599627370496
    mov64 r6, r8                                    r6 = r8
    or64 r6, r2                                     r6 |= r2   ///  r6 = r6.or(r2)
    mov64 r3, r1                                    r3 = r1
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    stxdw [r10-0x28], r3                    
    rsh64 r3, 21                                    r3 >>= 21   ///  r3 = r3.wrapping_shr(21)
    mov64 r4, 1963258675                            r4 = 1963258675 as i32 as i64 as u64
    sub64 r4, r3                                    r4 -= r3   ///  r4 = r4.wrapping_sub(r3)
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    mov64 r2, r4                                    r2 = r4
    mul64 r2, r3                                    r2 *= r3   ///  r2 = r2.wrapping_mul(r3)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mul64 r2, r4                                    r2 *= r4   ///  r2 = r2.wrapping_mul(r4)
    rsh64 r2, 31                                    r2 >>= 31   ///  r2 = r2.wrapping_shr(31)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mov64 r4, r2                                    r4 = r2
    mul64 r4, r3                                    r4 *= r3   ///  r4 = r4.wrapping_mul(r3)
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    neg64 r4                                        r4 = -r4   ///  r4 = (r4 as i64).wrapping_neg() as u64
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    mul64 r4, r2                                    r4 *= r2   ///  r4 = r4.wrapping_mul(r2)
    rsh64 r4, 31                                    r4 >>= 31   ///  r4 = r4.wrapping_shr(31)
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    mov64 r2, r4                                    r2 = r4
    mul64 r2, r3                                    r2 *= r3   ///  r2 = r2.wrapping_mul(r3)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mul64 r2, r4                                    r2 *= r4   ///  r2 = r2.wrapping_mul(r4)
    lsh64 r1, 11                                    r1 <<= 11   ///  r1 = r1.wrapping_shl(11)
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    rsh64 r2, 31                                    r2 >>= 31   ///  r2 = r2.wrapping_shr(31)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mov64 r4, r2                                    r4 = r2
    mul64 r4, r3                                    r4 *= r3   ///  r4 = r4.wrapping_mul(r3)
    mov64 r3, r2                                    r3 = r2
    mul64 r3, r1                                    r3 *= r1   ///  r3 = r3.wrapping_mul(r1)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    mov64 r3, r1                                    r3 = r1
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    mul64 r3, r2                                    r3 *= r2   ///  r3 = r3.wrapping_mul(r2)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mul64 r2, r1                                    r2 *= r1   ///  r2 = r2.wrapping_mul(r1)
    lsh64 r2, 1                                     r2 <<= 1   ///  r2 = r2.wrapping_shl(1)
    rsh64 r3, 31                                    r3 >>= 31   ///  r3 = r3.wrapping_shr(31)
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    add64 r2, -225                                  r2 += -225   ///  r2 = r2.wrapping_add(-225 as i32 as i64 as u64)
    stxdw [r10-0x30], r6                    
    lsh64 r6, 1                                     r6 <<= 1   ///  r6 = r6.wrapping_shl(1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_25903                     
    ldxdw r2, [r10-0x20]                    
    sub64 r2, r7                                    r2 -= r7   ///  r2 = r2.wrapping_sub(r7)
    add64 r2, r9                                    r2 += r9   ///  r2 = r2.wrapping_add(r9)
    ldxdw r0, [r10-0x8]                     
    lddw r1, 0x20000000000000                       r1 load str located at 9007199254740992
    jlt r0, r1, lbb_24063                           if r0 < r1 { pc += 1 }
    ja lbb_24100                                    if true { pc += 37 }
lbb_24063:
    ldxdw r3, [r10-0x28]                    
    mov64 r1, r3                                    r1 = r3
    mul64 r1, r0                                    r1 *= r0   ///  r1 = r1.wrapping_mul(r0)
    lsh64 r8, 53                                    r8 <<= 53   ///  r8 = r8.wrapping_shl(53)
    sub64 r8, r1                                    r8 -= r1   ///  r8 = r8.wrapping_sub(r1)
    add64 r2, 1022                                  r2 += 1022   ///  r2 = r2.wrapping_add(1022 as i32 as i64 as u64)
    jsgt r2, 2046, lbb_24110                        if (r2 as i64) > (2046 as i32 as i64) { pc += 40 }
lbb_24070:
    jsgt r2, 0, lbb_24117                           if (r2 as i64) > (0 as i32 as i64) { pc += 46 }
    jslt r2, -52, lbb_24259                         if (r2 as i64) < (-52 as i32 as i64) { pc += 187 }
    mov64 r1, r2                                    r1 = r2
    add64 r1, 52                                    r1 += 52   ///  r1 = r1.wrapping_add(52 as i32 as i64 as u64)
    lsh64 r6, r1                                    r6 <<= r1   ///  r6 = r6.wrapping_shl(r1 as u32)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    rsh64 r0, r1                                    r0 >>= r1   ///  r0 = r0.wrapping_shr(r1 as u32)
    mov64 r1, r3                                    r1 = r3
    mul64 r1, r0                                    r1 *= r0   ///  r1 = r1.wrapping_mul(r0)
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    sub64 r6, r1                                    r6 -= r1   ///  r6 = r6.wrapping_sub(r1)
    mov64 r8, r6                                    r8 = r6
    ja lbb_24124                                    if true { pc += 40 }
lbb_24084:
    lddw r5, 0x7fffffffffffffff                     r5 load str located at 9223372036854775807
    mov64 r4, r0                                    r4 = r0
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    lddw r6, 0x7ff0000000000000                     r6 load str located at 9218868437227405312
    jgt r4, r6, lbb_24114                           if r4 > r6 { pc += 23 }
    mov64 r3, r2                                    r3 = r2
    and64 r3, r5                                    r3 &= r5   ///  r3 = r3.and(r5)
    jgt r3, r6, lbb_24095                           if r3 > r6 { pc += 1 }
    ja lbb_24134                                    if true { pc += 39 }
lbb_24095:
    lddw r1, 0x8000000000000                        r1 load str located at 2251799813685248
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    mov64 r0, r2                                    r0 = r2
    ja lbb_24133                                    if true { pc += 33 }
lbb_24100:
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    mov64 r1, r0                                    r1 = r0
    ldxdw r3, [r10-0x28]                    
    mul64 r1, r3                                    r1 *= r3   ///  r1 = r1.wrapping_mul(r3)
    lsh64 r8, 52                                    r8 <<= 52   ///  r8 = r8.wrapping_shl(52)
    sub64 r8, r1                                    r8 -= r1   ///  r8 = r8.wrapping_sub(r1)
    add64 r2, 1023                                  r2 += 1023   ///  r2 = r2.wrapping_add(1023 as i32 as i64 as u64)
    ldxdw r6, [r10-0x30]                    
    jsgt r2, 2046, lbb_24110                        if (r2 as i64) > (2046 as i32 as i64) { pc += 1 }
    ja lbb_24070                                    if true { pc += -40 }
lbb_24110:
    lddw r1, 0x7ff0000000000000                     r1 load str located at 9218868437227405312
    ldxdw r0, [r10-0x18]                    
    ja lbb_24132                                    if true { pc += 18 }
lbb_24114:
    lddw r1, 0x8000000000000                        r1 load str located at 2251799813685248
    ja lbb_24132                                    if true { pc += 15 }
lbb_24117:
    lddw r1, 0xfffffffffffff                        r1 load str located at 4503599627370495
    and64 r0, r1                                    r0 &= r1   ///  r0 = r0.and(r1)
    lsh64 r2, 52                                    r2 <<= 52   ///  r2 = r2.wrapping_shl(52)
    or64 r2, r0                                     r2 |= r0   ///  r2 = r2.or(r0)
    lsh64 r8, 1                                     r8 <<= 1   ///  r8 = r8.wrapping_shl(1)
    mov64 r0, r2                                    r0 = r2
lbb_24124:
    mov64 r2, r0                                    r2 = r0
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    add64 r2, r8                                    r2 += r8   ///  r2 = r2.wrapping_add(r8)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jgt r2, r3, lbb_24130                           if r2 > r3 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_24130:
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
    ldxdw r1, [r10-0x18]                    
lbb_24132:
    or64 r0, r1                                     r0 |= r1   ///  r0 = r0.or(r1)
lbb_24133:
    exit                                    
lbb_24134:
    lddw r2, 0x7ff0000000000000                     r2 load str located at 9218868437227405312
    jeq r4, r2, lbb_24138                           if r4 == r2 { pc += 1 }
    ja lbb_24142                                    if true { pc += 4 }
lbb_24138:
    lddw r0, 0x7ff8000000000000                     r0 load str located at 9221120237041090560
    jeq r3, r2, lbb_24133                           if r3 == r2 { pc += -8 }
    ja lbb_24110                                    if true { pc += -32 }
lbb_24142:
    jeq r3, r2, lbb_24259                           if r3 == r2 { pc += 116 }
    jeq r4, 0, lbb_24256                            if r4 == (0 as i32 as i64 as u64) { pc += 112 }
    jeq r3, 0, lbb_24110                            if r3 == (0 as i32 as i64 as u64) { pc += -35 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    lddw r2, 0x10000000000000                       r2 load str located at 4503599627370496
    jlt r4, r2, lbb_24150                           if r4 < r2 { pc += 1 }
    ja lbb_24200                                    if true { pc += 50 }
lbb_24150:
    mov64 r4, 64                                    r4 = 64 as i32 as i64 as u64
    jeq r8, 0, lbb_24195                            if r8 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r4, r8                                    r4 = r8
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    mov64 r2, r8                                    r2 = r8
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    mov64 r4, r2                                    r4 = r2
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    mov64 r4, r2                                    r4 = r2
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    mov64 r4, r2                                    r4 = r2
    rsh64 r4, 8                                     r4 >>= 8   ///  r4 = r4.wrapping_shr(8)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    mov64 r4, r2                                    r4 = r2
    rsh64 r4, 16                                    r4 >>= 16   ///  r4 = r4.wrapping_shr(16)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    mov64 r4, r2                                    r4 = r2
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    lddw r4, 0x5555555555555555                     r4 load str located at 6148914691236517205
    mov64 r5, r2                                    r5 = r2
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    and64 r5, r4                                    r5 &= r4   ///  r5 = r5.and(r4)
    sub64 r2, r5                                    r2 -= r5   ///  r2 = r2.wrapping_sub(r5)
    lddw r5, 0x3333333333333333                     r5 load str located at 3689348814741910323
    mov64 r4, r2                                    r4 = r2
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    and64 r2, r5                                    r2 &= r5   ///  r2 = r2.and(r5)
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    mov64 r2, r4                                    r2 = r4
    rsh64 r2, 4                                     r2 >>= 4   ///  r2 = r2.wrapping_shr(4)
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    lddw r2, 0xf0f0f0f0f0f0f0f                      r2 load str located at 1085102592571150095
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    lddw r2, 0x101010101010101                      r2 load str located at 72340172838076673
    mul64 r4, r2                                    r4 *= r2   ///  r4 = r4.wrapping_mul(r2)
    rsh64 r4, 56                                    r4 >>= 56   ///  r4 = r4.wrapping_shr(56)
lbb_24195:
    mov64 r9, 12                                    r9 = 12 as i32 as i64 as u64
    sub64 r9, r4                                    r9 -= r4   ///  r9 = r9.wrapping_sub(r4)
    add64 r4, 53                                    r4 += 53   ///  r4 = r4.wrapping_add(53 as i32 as i64 as u64)
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    lsh64 r8, r4                                    r8 <<= r4   ///  r8 = r8.wrapping_shl(r4 as u32)
lbb_24200:
    lddw r2, 0xfffffffffffff                        r2 load str located at 4503599627370495
    jgt r3, r2, lbb_23981                           if r3 > r2 { pc += -222 }
    mov64 r3, 64                                    r3 = 64 as i32 as i64 as u64
    jeq r1, 0, lbb_24248                            if r1 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    mov64 r2, r1                                    r2 = r1
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 16                                    r3 >>= 16   ///  r3 = r3.wrapping_shr(16)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    lddw r3, 0x5555555555555555                     r3 load str located at 6148914691236517205
    mov64 r4, r2                                    r4 = r2
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    sub64 r2, r4                                    r2 -= r4   ///  r2 = r2.wrapping_sub(r4)
    lddw r4, 0x3333333333333333                     r4 load str located at 3689348814741910323
    mov64 r3, r2                                    r3 = r2
    and64 r3, r4                                    r3 &= r4   ///  r3 = r3.and(r4)
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    and64 r2, r4                                    r2 &= r4   ///  r2 = r2.and(r4)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    mov64 r2, r3                                    r2 = r3
    rsh64 r2, 4                                     r2 >>= 4   ///  r2 = r2.wrapping_shr(4)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    lddw r2, 0xf0f0f0f0f0f0f0f                      r2 load str located at 1085102592571150095
    and64 r3, r2                                    r3 &= r2   ///  r3 = r3.and(r2)
    lddw r2, 0x101010101010101                      r2 load str located at 72340172838076673
    mul64 r3, r2                                    r3 *= r2   ///  r3 = r3.wrapping_mul(r2)
    rsh64 r3, 56                                    r3 >>= 56   ///  r3 = r3.wrapping_shr(56)
lbb_24248:
    mov64 r2, r3                                    r2 = r3
    add64 r2, 53                                    r2 += 53   ///  r2 = r2.wrapping_add(53 as i32 as i64 as u64)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    lsh64 r1, r2                                    r1 <<= r2   ///  r1 = r1.wrapping_shl(r2 as u32)
    add64 r3, r9                                    r3 += r9   ///  r3 = r3.wrapping_add(r9)
    add64 r3, -12                                   r3 += -12   ///  r3 = r3.wrapping_add(-12 as i32 as i64 as u64)
    mov64 r9, r3                                    r9 = r3
    ja lbb_23981                                    if true { pc += -275 }
lbb_24256:
    lddw r0, 0x7ff8000000000000                     r0 load str located at 9221120237041090560
    jeq r3, 0, lbb_24133                            if r3 == (0 as i32 as i64 as u64) { pc += -126 }
lbb_24259:
    ldxdw r0, [r10-0x18]                    
    ja lbb_24133                                    if true { pc += -128 }

function_24261:
    call function_24263                     
    exit                                    

function_24263:
    mov64 r4, r2                                    r4 = r2
    and64 r4, 2147483647                            r4 &= 2147483647   ///  r4 = r4.and(2147483647)
    mov64 r5, r1                                    r5 = r1
    and64 r5, 2147483647                            r5 &= 2147483647   ///  r5 = r5.and(2147483647)
    mov64 r0, r5                                    r0 = r5
    add64 r0, -2139095040                           r0 += -2139095040   ///  r0 = r0.wrapping_add(-2139095040 as i32 as i64 as u64)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    lddw r3, 0x80800001                             r3 load str located at 2155872257
    jlt r0, r3, lbb_24418                           if r0 < r3 { pc += 144 }
    mov64 r0, r4                                    r0 = r4
    add64 r0, -2139095040                           r0 += -2139095040   ///  r0 = r0.wrapping_add(-2139095040 as i32 as i64 as u64)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    jlt r0, r3, lbb_24418                           if r0 < r3 { pc += 139 }
lbb_24279:
    mov64 r3, r2                                    r3 = r2
    jgt r4, r5, lbb_24282                           if r4 > r5 { pc += 1 }
    mov64 r3, r1                                    r3 = r1
lbb_24282:
    mov64 r0, r1                                    r0 = r1
    jgt r4, r5, lbb_24285                           if r4 > r5 { pc += 1 }
    mov64 r0, r2                                    r0 = r2
lbb_24285:
    mov64 r4, r3                                    r4 = r3
    and64 r4, 8388607                               r4 &= 8388607   ///  r4 = r4.and(8388607)
    mov64 r6, r0                                    r6 = r0
    rsh64 r6, 23                                    r6 >>= 23   ///  r6 = r6.wrapping_shr(23)
    and64 r6, 255                                   r6 &= 255   ///  r6 = r6.and(255)
    mov64 r5, r3                                    r5 = r3
    rsh64 r5, 23                                    r5 >>= 23   ///  r5 = r5.wrapping_shr(23)
    and64 r5, 255                                   r5 &= 255   ///  r5 = r5.and(255)
    jeq r5, 0, lbb_24295                            if r5 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_24339                                    if true { pc += 44 }
lbb_24295:
    mov64 r7, 32                                    r7 = 32 as i32 as i64 as u64
    jeq r4, 0, lbb_24334                            if r4 == (0 as i32 as i64 as u64) { pc += 37 }
    mov64 r5, r4                                    r5 = r4
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    mov64 r7, r4                                    r7 = r4
    or64 r7, r5                                     r7 |= r5   ///  r7 = r7.or(r5)
    mov64 r5, r7                                    r5 = r7
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    or64 r7, r5                                     r7 |= r5   ///  r7 = r7.or(r5)
    mov64 r5, r7                                    r5 = r7
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    or64 r7, r5                                     r7 |= r5   ///  r7 = r7.or(r5)
    mov64 r8, r7                                    r8 = r7
    and64 r8, 8388352                               r8 &= 8388352   ///  r8 = r8.and(8388352)
    rsh64 r8, 8                                     r8 >>= 8   ///  r8 = r8.wrapping_shr(8)
    mov64 r5, r7                                    r5 = r7
    or64 r5, r8                                     r5 |= r8   ///  r5 = r5.or(r8)
    and64 r7, 8323072                               r7 &= 8323072   ///  r7 = r7.and(8323072)
    rsh64 r7, 16                                    r7 >>= 16   ///  r7 = r7.wrapping_shr(16)
    or64 r5, r7                                     r5 |= r7   ///  r5 = r5.or(r7)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    mov64 r7, r5                                    r7 = r5
    rsh64 r7, 1                                     r7 >>= 1   ///  r7 = r7.wrapping_shr(1)
    and64 r7, 1431655765                            r7 &= 1431655765   ///  r7 = r7.and(1431655765)
    sub64 r5, r7                                    r5 -= r7   ///  r5 = r5.wrapping_sub(r7)
    mov64 r7, r5                                    r7 = r5
    and64 r7, 858993459                             r7 &= 858993459   ///  r7 = r7.and(858993459)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, 858993459                             r5 &= 858993459   ///  r5 = r5.and(858993459)
    add64 r7, r5                                    r7 += r5   ///  r7 = r7.wrapping_add(r5)
    mov64 r5, r7                                    r5 = r7
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    add64 r7, r5                                    r7 += r5   ///  r7 = r7.wrapping_add(r5)
    and64 r7, 252645135                             r7 &= 252645135   ///  r7 = r7.and(252645135)
    mul64 r7, 16843009                              r7 *= 16843009   ///  r7 = r7.wrapping_mul(16843009 as u64)
    lddw r5, 0xff000000                             r5 load str located at 4278190080
    and64 r7, r5                                    r7 &= r5   ///  r7 = r7.and(r5)
    rsh64 r7, 24                                    r7 >>= 24   ///  r7 = r7.wrapping_shr(24)
lbb_24334:
    mov64 r5, 9                                     r5 = 9 as i32 as i64 as u64
    sub64 r5, r7                                    r5 -= r7   ///  r5 = r5.wrapping_sub(r7)
    add64 r7, 24                                    r7 += 24   ///  r7 = r7.wrapping_add(24 as i32 as i64 as u64)
    and64 r7, 31                                    r7 &= 31   ///  r7 = r7.and(31)
    lsh64 r4, r7                                    r4 <<= r7   ///  r4 = r4.wrapping_shl(r7 as u32)
lbb_24339:
    and64 r0, 8388607                               r0 &= 8388607   ///  r0 = r0.and(8388607)
    jne r6, 0, lbb_24385                            if r6 != (0 as i32 as i64 as u64) { pc += 44 }
    mov64 r7, 32                                    r7 = 32 as i32 as i64 as u64
    jeq r0, 0, lbb_24380                            if r0 == (0 as i32 as i64 as u64) { pc += 37 }
    mov64 r6, r0                                    r6 = r0
    rsh64 r6, 1                                     r6 >>= 1   ///  r6 = r6.wrapping_shr(1)
    mov64 r7, r0                                    r7 = r0
    or64 r7, r6                                     r7 |= r6   ///  r7 = r7.or(r6)
    mov64 r6, r7                                    r6 = r7
    rsh64 r6, 2                                     r6 >>= 2   ///  r6 = r6.wrapping_shr(2)
    or64 r7, r6                                     r7 |= r6   ///  r7 = r7.or(r6)
    mov64 r6, r7                                    r6 = r7
    rsh64 r6, 4                                     r6 >>= 4   ///  r6 = r6.wrapping_shr(4)
    or64 r7, r6                                     r7 |= r6   ///  r7 = r7.or(r6)
    mov64 r8, r7                                    r8 = r7
    and64 r8, 8388352                               r8 &= 8388352   ///  r8 = r8.and(8388352)
    rsh64 r8, 8                                     r8 >>= 8   ///  r8 = r8.wrapping_shr(8)
    mov64 r6, r7                                    r6 = r7
    or64 r6, r8                                     r6 |= r8   ///  r6 = r6.or(r8)
    and64 r7, 8323072                               r7 &= 8323072   ///  r7 = r7.and(8323072)
    rsh64 r7, 16                                    r7 >>= 16   ///  r7 = r7.wrapping_shr(16)
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    xor64 r6, -1                                    r6 ^= -1   ///  r6 = r6.xor(-1)
    mov64 r7, r6                                    r7 = r6
    rsh64 r7, 1                                     r7 >>= 1   ///  r7 = r7.wrapping_shr(1)
    and64 r7, 1431655765                            r7 &= 1431655765   ///  r7 = r7.and(1431655765)
    sub64 r6, r7                                    r6 -= r7   ///  r6 = r6.wrapping_sub(r7)
    mov64 r7, r6                                    r7 = r6
    and64 r7, 858993459                             r7 &= 858993459   ///  r7 = r7.and(858993459)
    rsh64 r6, 2                                     r6 >>= 2   ///  r6 = r6.wrapping_shr(2)
    and64 r6, 858993459                             r6 &= 858993459   ///  r6 = r6.and(858993459)
    add64 r7, r6                                    r7 += r6   ///  r7 = r7.wrapping_add(r6)
    mov64 r6, r7                                    r6 = r7
    rsh64 r6, 4                                     r6 >>= 4   ///  r6 = r6.wrapping_shr(4)
    add64 r7, r6                                    r7 += r6   ///  r7 = r7.wrapping_add(r6)
    and64 r7, 252645135                             r7 &= 252645135   ///  r7 = r7.and(252645135)
    mul64 r7, 16843009                              r7 *= 16843009   ///  r7 = r7.wrapping_mul(16843009 as u64)
    lddw r6, 0xff000000                             r6 load str located at 4278190080
    and64 r7, r6                                    r7 &= r6   ///  r7 = r7.and(r6)
    rsh64 r7, 24                                    r7 >>= 24   ///  r7 = r7.wrapping_shr(24)
lbb_24380:
    mov64 r6, 9                                     r6 = 9 as i32 as i64 as u64
    sub64 r6, r7                                    r6 -= r7   ///  r6 = r6.wrapping_sub(r7)
    add64 r7, 24                                    r7 += 24   ///  r7 = r7.wrapping_add(24 as i32 as i64 as u64)
    and64 r7, 31                                    r7 &= 31   ///  r7 = r7.and(31)
    lsh64 r0, r7                                    r0 <<= r7   ///  r0 = r0.wrapping_shl(r7 as u32)
lbb_24385:
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    lsh64 r0, 3                                     r0 <<= 3   ///  r0 = r0.wrapping_shl(3)
    or64 r0, 67108864                               r0 |= 67108864   ///  r0 = r0.or(67108864)
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    mov64 r7, r0                                    r7 = r0
    jeq r5, r6, lbb_24397                           if r5 == r6 { pc += 4 }
    mov64 r1, r5                                    r1 = r5
    sub64 r1, r6                                    r1 -= r6   ///  r1 = r1.wrapping_sub(r6)
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jlt r1, 32, lbb_24526                           if r1 < (32 as i32 as i64 as u64) { pc += 129 }
lbb_24397:
    and64 r3, -2147483648                           r3 &= -2147483648   ///  r3 = r3.and(-2147483648)
    or64 r4, 67108864                               r4 |= 67108864   ///  r4 = r4.or(67108864)
    jsgt r2, -1, lbb_24401                          if (r2 as i64) > (-1 as i32 as i64) { pc += 1 }
    ja lbb_24427                                    if true { pc += 26 }
lbb_24401:
    mov64 r1, r7                                    r1 = r7
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    mov64 r2, r1                                    r2 = r1
    and64 r2, 134217728                             r2 &= 134217728   ///  r2 = r2.and(134217728)
    jeq r2, 0, lbb_24413                            if r2 == (0 as i32 as i64 as u64) { pc += 7 }
    lddw r2, 0xfffffffe                             r2 load str located at 4294967294
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
    and64 r7, 1                                     r7 &= 1   ///  r7 = r7.and(1)
    or64 r1, r7                                     r1 |= r7   ///  r1 = r1.or(r7)
    add64 r5, 1                                     r5 += 1   ///  r5 = r5.wrapping_add(1 as i32 as i64 as u64)
lbb_24413:
    jsgt r5, 254, lbb_24415                         if (r5 as i64) > (254 as i32 as i64) { pc += 1 }
    ja lbb_24491                                    if true { pc += 76 }
lbb_24415:
    or64 r3, 2139095040                             r3 |= 2139095040   ///  r3 = r3.or(2139095040)
    mov64 r0, r3                                    r0 = r3
    ja lbb_24546                                    if true { pc += 128 }
lbb_24418:
    jgt r5, 2139095040, lbb_24424                   if r5 > (2139095040 as i32 as i64 as u64) { pc += 5 }
    jgt r4, 2139095040, lbb_24421                   if r4 > (2139095040 as i32 as i64 as u64) { pc += 1 }
    ja lbb_24547                                    if true { pc += 126 }
lbb_24421:
    or64 r4, 4194304                                r4 |= 4194304   ///  r4 = r4.or(4194304)
    mov64 r0, r4                                    r0 = r4
    ja lbb_24546                                    if true { pc += 122 }
lbb_24424:
    or64 r5, 4194304                                r5 |= 4194304   ///  r5 = r5.or(4194304)
    mov64 r0, r5                                    r0 = r5
    ja lbb_24546                                    if true { pc += 119 }
lbb_24427:
    sub64 r4, r7                                    r4 -= r7   ///  r4 = r4.wrapping_sub(r7)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r2, r4                                    r2 = r4
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jeq r2, 0, lbb_24546                            if r2 == (0 as i32 as i64 as u64) { pc += 113 }
    mov64 r1, r4                                    r1 = r4
    jgt r2, 67108863, lbb_24413                     if r2 > (67108863 as i32 as i64 as u64) { pc += -22 }
    lddw r1, 0xfffffffe                             r1 load str located at 4294967294
    mov64 r2, r4                                    r2 = r4
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    rsh64 r2, 1                                     r2 >>= 1   ///  r2 = r2.wrapping_shr(1)
    mov64 r1, r4                                    r1 = r4
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    lddw r2, 0xfffffffc                             r2 load str located at 4294967292
    mov64 r0, r1                                    r0 = r1
    and64 r0, r2                                    r0 &= r2   ///  r0 = r0.and(r2)
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r1, r0                                     r1 |= r0   ///  r1 = r1.or(r0)
    lddw r2, 0xfffffff0                             r2 load str located at 4294967280
    mov64 r0, r1                                    r0 = r1
    and64 r0, r2                                    r0 &= r2   ///  r0 = r0.and(r2)
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r1, r0                                     r1 |= r0   ///  r1 = r1.or(r0)
    lddw r2, 0xffffff00                             r2 load str located at 4294967040
    mov64 r0, r1                                    r0 = r1
    and64 r0, r2                                    r0 &= r2   ///  r0 = r0.and(r2)
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    or64 r1, r0                                     r1 |= r0   ///  r1 = r1.or(r0)
    lddw r2, 0xffff0000                             r2 load str located at 4294901760
    mov64 r0, r1                                    r0 = r1
    and64 r0, r2                                    r0 &= r2   ///  r0 = r0.and(r2)
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    or64 r1, r0                                     r1 |= r0   ///  r1 = r1.or(r0)
    xor64 r1, -1                                    r1 ^= -1   ///  r1 = r1.xor(-1)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 1                                     r2 >>= 1   ///  r2 = r2.wrapping_shr(1)
    and64 r2, 1431655765                            r2 &= 1431655765   ///  r2 = r2.and(1431655765)
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r2, r1                                    r2 = r1
    and64 r2, 858993459                             r2 &= 858993459   ///  r2 = r2.and(858993459)
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    and64 r1, 858993459                             r1 &= 858993459   ///  r1 = r1.and(858993459)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    and64 r2, 252645135                             r2 &= 252645135   ///  r2 = r2.and(252645135)
    mul64 r2, 16843009                              r2 *= 16843009   ///  r2 = r2.wrapping_mul(16843009 as u64)
    lddw r1, 0xff000000                             r1 load str located at 4278190080
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    rsh64 r2, 24                                    r2 >>= 24   ///  r2 = r2.wrapping_shr(24)
    add64 r2, -5                                    r2 += -5   ///  r2 = r2.wrapping_add(-5 as i32 as i64 as u64)
    sub64 r5, r2                                    r5 -= r2   ///  r5 = r5.wrapping_sub(r2)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    lsh64 r4, r2                                    r4 <<= r2   ///  r4 = r4.wrapping_shl(r2 as u32)
    mov64 r1, r4                                    r1 = r4
lbb_24491:
    jslt r5, 1, lbb_24493                           if (r5 as i64) < (1 as i32 as i64) { pc += 1 }
    ja lbb_24511                                    if true { pc += 18 }
lbb_24493:
    mov64 r2, r5                                    r2 = r5
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    and64 r2, 31                                    r2 &= 31   ///  r2 = r2.and(31)
    mov64 r0, r1                                    r0 = r1
    lsh64 r0, r2                                    r0 <<= r2   ///  r0 = r0.wrapping_shl(r2 as u32)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_24504                            if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_24504:
    sub64 r2, r5                                    r2 -= r5   ///  r2 = r2.wrapping_sub(r5)
    and64 r2, 31                                    r2 &= 31   ///  r2 = r2.and(31)
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    rsh64 r1, r2                                    r1 >>= r2   ///  r1 = r1.wrapping_shr(r2 as u32)
    or64 r1, r4                                     r1 |= r4   ///  r1 = r1.or(r4)
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_24511:
    lsh64 r5, 23                                    r5 <<= 23   ///  r5 = r5.wrapping_shl(23)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 3                                     r2 >>= 3   ///  r2 = r2.wrapping_shr(3)
    mov64 r0, r2                                    r0 = r2
    and64 r0, 8388607                               r0 &= 8388607   ///  r0 = r0.and(8388607)
    or64 r0, r5                                     r0 |= r5   ///  r0 = r0.or(r5)
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jgt r1, 4, lbb_24545                            if r1 > (4 as i32 as i64 as u64) { pc += 25 }
    jeq r1, 4, lbb_24522                            if r1 == (4 as i32 as i64 as u64) { pc += 1 }
    ja lbb_24546                                    if true { pc += 24 }
lbb_24522:
    and64 r2, 536870911                             r2 &= 536870911   ///  r2 = r2.and(536870911)
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    ja lbb_24546                                    if true { pc += 20 }
lbb_24526:
    mov64 r6, 32                                    r6 = 32 as i32 as i64 as u64
    sub64 r6, r1                                    r6 -= r1   ///  r6 = r6.wrapping_sub(r1)
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    mov64 r7, r0                                    r7 = r0
    lsh64 r7, r6                                    r7 <<= r6   ///  r7 = r7.wrapping_shl(r6 as u32)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jne r7, 0, lbb_24537                            if r7 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_24537:
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    rsh64 r0, r1                                    r0 >>= r1   ///  r0 = r0.wrapping_shr(r1 as u32)
    or64 r0, r6                                     r0 |= r6   ///  r0 = r0.or(r6)
    mov64 r7, r0                                    r7 = r0
    ja lbb_24397                                    if true { pc += -148 }
lbb_24545:
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
lbb_24546:
    exit                                    
lbb_24547:
    jeq r5, 2139095040, lbb_24549                   if r5 == (2139095040 as i32 as i64 as u64) { pc += 1 }
    ja lbb_24559                                    if true { pc += 10 }
lbb_24549:
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    lddw r3, 0x80000000                             r3 load str located at 2147483648
    mov64 r0, r1                                    r0 = r1
    jeq r2, r3, lbb_24557                           if r2 == r3 { pc += 1 }
    ja lbb_24546                                    if true { pc += -11 }
lbb_24557:
    mov64 r0, 2143289344                            r0 = 2143289344 as i32 as i64 as u64
    ja lbb_24546                                    if true { pc += -13 }
lbb_24559:
    mov64 r0, r2                                    r0 = r2
    jeq r4, 2139095040, lbb_24546                   if r4 == (2139095040 as i32 as i64 as u64) { pc += -15 }
    jeq r5, 0, lbb_24565                            if r5 == (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r0, r1                                    r0 = r1
    jeq r4, 0, lbb_24546                            if r4 == (0 as i32 as i64 as u64) { pc += -18 }
    ja lbb_24279                                    if true { pc += -286 }
lbb_24565:
    mov64 r0, r2                                    r0 = r2
    jeq r4, 0, lbb_24568                            if r4 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_24546                                    if true { pc += -22 }
lbb_24568:
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    mov64 r0, r2                                    r0 = r2
    ja lbb_24546                                    if true { pc += -25 }

function_24571:
    lddw r3, 0x7fffffffffffffff                     r3 load str located at 9223372036854775807
    mov64 r4, r2                                    r4 = r2
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    mov64 r5, r1                                    r5 = r1
    and64 r5, r3                                    r5 &= r3   ///  r5 = r5.and(r3)
    lddw r3, 0x8010000000000000                     r3 load str located at -9218868437227405312
    mov64 r6, r5                                    r6 = r5
    add64 r6, r3                                    r6 += r3   ///  r6 = r6.wrapping_add(r3)
    lddw r0, 0x8010000000000001                     r0 load str located at -9218868437227405311
    jlt r6, r0, lbb_24743                           if r6 < r0 { pc += 159 }
    mov64 r6, r4                                    r6 = r4
    add64 r6, r3                                    r6 += r3   ///  r6 = r6.wrapping_add(r3)
    jlt r6, r0, lbb_24743                           if r6 < r0 { pc += 156 }
lbb_24587:
    mov64 r3, r2                                    r3 = r2
    jgt r4, r5, lbb_24590                           if r4 > r5 { pc += 1 }
    mov64 r3, r1                                    r3 = r1
lbb_24590:
    mov64 r0, r1                                    r0 = r1
    jgt r4, r5, lbb_24593                           if r4 > r5 { pc += 1 }
    mov64 r0, r2                                    r0 = r2
lbb_24593:
    lddw r7, 0xfffffffffffff                        r7 load str located at 4503599627370495
    mov64 r5, r3                                    r5 = r3
    and64 r5, r7                                    r5 &= r7   ///  r5 = r5.and(r7)
    mov64 r6, r0                                    r6 = r0
    rsh64 r6, 52                                    r6 >>= 52   ///  r6 = r6.wrapping_shr(52)
    and64 r6, 2047                                  r6 &= 2047   ///  r6 = r6.and(2047)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 52                                    r4 >>= 52   ///  r4 = r4.wrapping_shr(52)
    and64 r4, 2047                                  r4 &= 2047   ///  r4 = r4.and(2047)
    jeq r4, 0, lbb_24605                            if r4 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_24655                                    if true { pc += 50 }
lbb_24605:
    mov64 r8, 64                                    r8 = 64 as i32 as i64 as u64
    jeq r5, 0, lbb_24650                            if r5 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r8, r5                                    r8 = r5
    rsh64 r8, 1                                     r8 >>= 1   ///  r8 = r8.wrapping_shr(1)
    mov64 r4, r5                                    r4 = r5
    or64 r4, r8                                     r4 |= r8   ///  r4 = r4.or(r8)
    mov64 r8, r4                                    r8 = r4
    rsh64 r8, 2                                     r8 >>= 2   ///  r8 = r8.wrapping_shr(2)
    or64 r4, r8                                     r4 |= r8   ///  r4 = r4.or(r8)
    mov64 r8, r4                                    r8 = r4
    rsh64 r8, 4                                     r8 >>= 4   ///  r8 = r8.wrapping_shr(4)
    or64 r4, r8                                     r4 |= r8   ///  r4 = r4.or(r8)
    mov64 r8, r4                                    r8 = r4
    rsh64 r8, 8                                     r8 >>= 8   ///  r8 = r8.wrapping_shr(8)
    or64 r4, r8                                     r4 |= r8   ///  r4 = r4.or(r8)
    mov64 r8, r4                                    r8 = r4
    rsh64 r8, 16                                    r8 >>= 16   ///  r8 = r8.wrapping_shr(16)
    or64 r4, r8                                     r4 |= r8   ///  r4 = r4.or(r8)
    mov64 r8, r4                                    r8 = r4
    rsh64 r8, 32                                    r8 >>= 32   ///  r8 = r8.wrapping_shr(32)
    or64 r4, r8                                     r4 |= r8   ///  r4 = r4.or(r8)
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    lddw r8, 0x5555555555555555                     r8 load str located at 6148914691236517205
    mov64 r9, r4                                    r9 = r4
    rsh64 r9, 1                                     r9 >>= 1   ///  r9 = r9.wrapping_shr(1)
    and64 r9, r8                                    r9 &= r8   ///  r9 = r9.and(r8)
    sub64 r4, r9                                    r4 -= r9   ///  r4 = r4.wrapping_sub(r9)
    lddw r9, 0x3333333333333333                     r9 load str located at 3689348814741910323
    mov64 r8, r4                                    r8 = r4
    and64 r8, r9                                    r8 &= r9   ///  r8 = r8.and(r9)
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    and64 r4, r9                                    r4 &= r9   ///  r4 = r4.and(r9)
    add64 r8, r4                                    r8 += r4   ///  r8 = r8.wrapping_add(r4)
    mov64 r4, r8                                    r4 = r8
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    add64 r8, r4                                    r8 += r4   ///  r8 = r8.wrapping_add(r4)
    lddw r4, 0xf0f0f0f0f0f0f0f                      r4 load str located at 1085102592571150095
    and64 r8, r4                                    r8 &= r4   ///  r8 = r8.and(r4)
    lddw r4, 0x101010101010101                      r4 load str located at 72340172838076673
    mul64 r8, r4                                    r8 *= r4   ///  r8 = r8.wrapping_mul(r4)
    rsh64 r8, 56                                    r8 >>= 56   ///  r8 = r8.wrapping_shr(56)
lbb_24650:
    mov64 r4, 12                                    r4 = 12 as i32 as i64 as u64
    sub64 r4, r8                                    r4 -= r8   ///  r4 = r4.wrapping_sub(r8)
    add64 r8, 53                                    r8 += 53   ///  r8 = r8.wrapping_add(53 as i32 as i64 as u64)
    and64 r8, 63                                    r8 &= 63   ///  r8 = r8.and(63)
    lsh64 r5, r8                                    r5 <<= r8   ///  r5 = r5.wrapping_shl(r8 as u32)
lbb_24655:
    and64 r0, r7                                    r0 &= r7   ///  r0 = r0.and(r7)
    jne r6, 0, lbb_24707                            if r6 != (0 as i32 as i64 as u64) { pc += 50 }
    mov64 r7, 64                                    r7 = 64 as i32 as i64 as u64
    jeq r0, 0, lbb_24702                            if r0 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r7, r0                                    r7 = r0
    rsh64 r7, 1                                     r7 >>= 1   ///  r7 = r7.wrapping_shr(1)
    mov64 r6, r0                                    r6 = r0
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    mov64 r7, r6                                    r7 = r6
    rsh64 r7, 2                                     r7 >>= 2   ///  r7 = r7.wrapping_shr(2)
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    mov64 r7, r6                                    r7 = r6
    rsh64 r7, 4                                     r7 >>= 4   ///  r7 = r7.wrapping_shr(4)
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    mov64 r7, r6                                    r7 = r6
    rsh64 r7, 8                                     r7 >>= 8   ///  r7 = r7.wrapping_shr(8)
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    mov64 r7, r6                                    r7 = r6
    rsh64 r7, 16                                    r7 >>= 16   ///  r7 = r7.wrapping_shr(16)
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    mov64 r7, r6                                    r7 = r6
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    xor64 r6, -1                                    r6 ^= -1   ///  r6 = r6.xor(-1)
    lddw r7, 0x5555555555555555                     r7 load str located at 6148914691236517205
    mov64 r8, r6                                    r8 = r6
    rsh64 r8, 1                                     r8 >>= 1   ///  r8 = r8.wrapping_shr(1)
    and64 r8, r7                                    r8 &= r7   ///  r8 = r8.and(r7)
    sub64 r6, r8                                    r6 -= r8   ///  r6 = r6.wrapping_sub(r8)
    lddw r8, 0x3333333333333333                     r8 load str located at 3689348814741910323
    mov64 r7, r6                                    r7 = r6
    and64 r7, r8                                    r7 &= r8   ///  r7 = r7.and(r8)
    rsh64 r6, 2                                     r6 >>= 2   ///  r6 = r6.wrapping_shr(2)
    and64 r6, r8                                    r6 &= r8   ///  r6 = r6.and(r8)
    add64 r7, r6                                    r7 += r6   ///  r7 = r7.wrapping_add(r6)
    mov64 r6, r7                                    r6 = r7
    rsh64 r6, 4                                     r6 >>= 4   ///  r6 = r6.wrapping_shr(4)
    add64 r7, r6                                    r7 += r6   ///  r7 = r7.wrapping_add(r6)
    lddw r6, 0xf0f0f0f0f0f0f0f                      r6 load str located at 1085102592571150095
    and64 r7, r6                                    r7 &= r6   ///  r7 = r7.and(r6)
    lddw r6, 0x101010101010101                      r6 load str located at 72340172838076673
    mul64 r7, r6                                    r7 *= r6   ///  r7 = r7.wrapping_mul(r6)
    rsh64 r7, 56                                    r7 >>= 56   ///  r7 = r7.wrapping_shr(56)
lbb_24702:
    mov64 r6, 12                                    r6 = 12 as i32 as i64 as u64
    sub64 r6, r7                                    r6 -= r7   ///  r6 = r6.wrapping_sub(r7)
    add64 r7, 53                                    r7 += 53   ///  r7 = r7.wrapping_add(53 as i32 as i64 as u64)
    and64 r7, 63                                    r7 &= 63   ///  r7 = r7.and(63)
    lsh64 r0, r7                                    r0 <<= r7   ///  r0 = r0.wrapping_shl(r7 as u32)
lbb_24707:
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    lddw r1, 0x8000000000000000                     r1 load str located at -9223372036854775808
    lsh64 r5, 3                                     r5 <<= 3   ///  r5 = r5.wrapping_shl(3)
    lsh64 r0, 3                                     r0 <<= 3   ///  r0 = r0.wrapping_shl(3)
    lddw r7, 0x80000000000000                       r7 load str located at 36028797018963968
    or64 r0, r7                                     r0 |= r7   ///  r0 = r0.or(r7)
    mov64 r8, r0                                    r8 = r0
    jeq r4, r6, lbb_24721                           if r4 == r6 { pc += 4 }
    mov64 r9, r4                                    r9 = r4
    sub64 r9, r6                                    r9 -= r6   ///  r9 = r9.wrapping_sub(r6)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jlt r9, 64, lbb_24846                           if r9 < (64 as i32 as i64 as u64) { pc += 125 }
lbb_24721:
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    or64 r5, r7                                     r5 |= r7   ///  r5 = r5.or(r7)
    jsgt r2, -1, lbb_24725                          if (r2 as i64) > (-1 as i32 as i64) { pc += 1 }
    ja lbb_24757                                    if true { pc += 32 }
lbb_24725:
    mov64 r1, r8                                    r1 = r8
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    lddw r2, 0x100000000000000                      r2 load str located at 72057594037927936
    mov64 r5, r1                                    r5 = r1
    and64 r5, r2                                    r5 &= r2   ///  r5 = r5.and(r2)
    jeq r5, 0, lbb_24736                            if r5 == (0 as i32 as i64 as u64) { pc += 4 }
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
    or64 r1, r8                                     r1 |= r8   ///  r1 = r1.or(r8)
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
lbb_24736:
    jsgt r4, 2046, lbb_24738                        if (r4 as i64) > (2046 as i32 as i64) { pc += 1 }
    ja lbb_24813                                    if true { pc += 75 }
lbb_24738:
    lddw r1, 0x7ff0000000000000                     r1 load str located at 9218868437227405312
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    mov64 r0, r3                                    r0 = r3
    ja lbb_24862                                    if true { pc += 119 }
lbb_24743:
    lddw r3, 0x7ff0000000000000                     r3 load str located at 9218868437227405312
    jgt r5, r3, lbb_24752                           if r5 > r3 { pc += 6 }
    jgt r4, r3, lbb_24748                           if r4 > r3 { pc += 1 }
    ja lbb_24863                                    if true { pc += 115 }
lbb_24748:
    lddw r1, 0x8000000000000                        r1 load str located at 2251799813685248
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    ja lbb_24861                                    if true { pc += 109 }
lbb_24752:
    lddw r1, 0x8000000000000                        r1 load str located at 2251799813685248
    or64 r5, r1                                     r5 |= r1   ///  r5 = r5.or(r1)
    mov64 r0, r5                                    r0 = r5
    ja lbb_24862                                    if true { pc += 105 }
lbb_24757:
    sub64 r5, r8                                    r5 -= r8   ///  r5 = r5.wrapping_sub(r8)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r5, 0, lbb_24862                            if r5 == (0 as i32 as i64 as u64) { pc += 102 }
    lddw r2, 0x7fffffffffffff                       r2 load str located at 36028797018963967
    mov64 r1, r5                                    r1 = r5
    jgt r5, r2, lbb_24736                           if r5 > r2 { pc += -28 }
    mov64 r2, r5                                    r2 = r5
    rsh64 r2, 1                                     r2 >>= 1   ///  r2 = r2.wrapping_shr(1)
    mov64 r1, r5                                    r1 = r5
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 4                                     r2 >>= 4   ///  r2 = r2.wrapping_shr(4)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 8                                     r2 >>= 8   ///  r2 = r2.wrapping_shr(8)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 16                                    r2 >>= 16   ///  r2 = r2.wrapping_shr(16)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    xor64 r1, -1                                    r1 ^= -1   ///  r1 = r1.xor(-1)
    lddw r2, 0x5555555555555555                     r2 load str located at 6148914691236517205
    mov64 r0, r1                                    r0 = r1
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r2                                    r0 &= r2   ///  r0 = r0.and(r2)
    sub64 r1, r0                                    r1 -= r0   ///  r1 = r1.wrapping_sub(r0)
    lddw r0, 0x3333333333333333                     r0 load str located at 3689348814741910323
    mov64 r2, r1                                    r2 = r1
    and64 r2, r0                                    r2 &= r0   ///  r2 = r2.and(r0)
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    and64 r1, r0                                    r1 &= r0   ///  r1 = r1.and(r0)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    lddw r1, 0xf0f0f0f0f0f0f0f                      r1 load str located at 1085102592571150095
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
    mul64 r2, r1                                    r2 *= r1   ///  r2 = r2.wrapping_mul(r1)
    rsh64 r2, 56                                    r2 >>= 56   ///  r2 = r2.wrapping_shr(56)
    add64 r2, -8                                    r2 += -8   ///  r2 = r2.wrapping_add(-8 as i32 as i64 as u64)
    sub64 r4, r2                                    r4 -= r2   ///  r4 = r4.wrapping_sub(r2)
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    lsh64 r5, r2                                    r5 <<= r2   ///  r5 = r5.wrapping_shl(r2 as u32)
    mov64 r1, r5                                    r1 = r5
lbb_24813:
    jslt r4, 1, lbb_24815                           if (r4 as i64) < (1 as i32 as i64) { pc += 1 }
    ja lbb_24829                                    if true { pc += 14 }
lbb_24815:
    mov64 r2, r4                                    r2 = r4
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    mov64 r0, r1                                    r0 = r1
    lsh64 r0, r2                                    r0 <<= r2   ///  r0 = r0.wrapping_shl(r2 as u32)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_24824                            if r0 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_24824:
    sub64 r2, r4                                    r2 -= r4   ///  r2 = r2.wrapping_sub(r4)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    rsh64 r1, r2                                    r1 >>= r2   ///  r1 = r1.wrapping_shr(r2 as u32)
    or64 r1, r5                                     r1 |= r5   ///  r1 = r1.or(r5)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_24829:
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 3                                     r2 >>= 3   ///  r2 = r2.wrapping_shr(3)
    lddw r5, 0xfffffffffffff                        r5 load str located at 4503599627370495
    mov64 r0, r2                                    r0 = r2
    and64 r0, r5                                    r0 &= r5   ///  r0 = r0.and(r5)
    lsh64 r4, 52                                    r4 <<= 52   ///  r4 = r4.wrapping_shl(52)
    or64 r4, r0                                     r4 |= r0   ///  r4 = r4.or(r0)
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jgt r1, 4, lbb_24860                            if r1 > (4 as i32 as i64 as u64) { pc += 20 }
    mov64 r0, r4                                    r0 = r4
    jeq r1, 4, lbb_24843                            if r1 == (4 as i32 as i64 as u64) { pc += 1 }
    ja lbb_24862                                    if true { pc += 19 }
lbb_24843:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    ja lbb_24861                                    if true { pc += 15 }
lbb_24846:
    mov64 r6, r9                                    r6 = r9
    neg64 r6                                        r6 = -r6   ///  r6 = (r6 as i64).wrapping_neg() as u64
    and64 r6, 63                                    r6 &= 63   ///  r6 = r6.and(63)
    mov64 r8, r0                                    r8 = r0
    lsh64 r8, r6                                    r8 <<= r6   ///  r8 = r8.wrapping_shl(r6 as u32)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jne r8, 0, lbb_24854                            if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_24854:
    lsh64 r9, 32                                    r9 <<= 32   ///  r9 = r9.wrapping_shl(32)
    rsh64 r9, 32                                    r9 >>= 32   ///  r9 = r9.wrapping_shr(32)
    rsh64 r0, r9                                    r0 >>= r9   ///  r0 = r0.wrapping_shr(r9 as u32)
    or64 r0, r6                                     r0 |= r6   ///  r0 = r0.or(r6)
    mov64 r8, r0                                    r8 = r0
    ja lbb_24721                                    if true { pc += -139 }
lbb_24860:
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
lbb_24861:
    mov64 r0, r4                                    r0 = r4
lbb_24862:
    exit                                    
lbb_24863:
    jeq r5, r3, lbb_24865                           if r5 == r3 { pc += 1 }
    ja lbb_24874                                    if true { pc += 9 }
lbb_24865:
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    mov64 r0, r1                                    r0 = r1
    jeq r2, r3, lbb_24871                           if r2 == r3 { pc += 1 }
    ja lbb_24862                                    if true { pc += -9 }
lbb_24871:
    lddw r0, 0x7ff8000000000000                     r0 load str located at 9221120237041090560
    ja lbb_24862                                    if true { pc += -12 }
lbb_24874:
    mov64 r0, r2                                    r0 = r2
    jeq r4, r3, lbb_24862                           if r4 == r3 { pc += -14 }
    jeq r5, 0, lbb_24880                            if r5 == (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r0, r1                                    r0 = r1
    jeq r4, 0, lbb_24862                            if r4 == (0 as i32 as i64 as u64) { pc += -17 }
    ja lbb_24587                                    if true { pc += -293 }
lbb_24880:
    mov64 r0, r2                                    r0 = r2
    jeq r4, 0, lbb_24883                            if r4 == (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_24862                                    if true { pc += -21 }
lbb_24883:
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    mov64 r0, r2                                    r0 = r2
    ja lbb_24862                                    if true { pc += -24 }

function_24886:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_24949                            if r1 == (0 as i32 as i64 as u64) { pc += 61 }
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 1                                     r2 >>= 1   ///  r2 = r2.wrapping_shr(1)
    mov64 r3, r1                                    r3 = r1
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    mov64 r2, r3                                    r2 = r3
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    mov64 r2, r3                                    r2 = r3
    rsh64 r2, 4                                     r2 >>= 4   ///  r2 = r2.wrapping_shr(4)
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    lddw r2, 0xffffff00                             r2 load str located at 4294967040
    mov64 r4, r3                                    r4 = r3
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    rsh64 r4, 8                                     r4 >>= 8   ///  r4 = r4.wrapping_shr(8)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    lddw r4, 0xffff0000                             r4 load str located at 4294901760
    mov64 r5, r3                                    r5 = r3
    and64 r5, r4                                    r5 &= r4   ///  r5 = r5.and(r4)
    rsh64 r5, 16                                    r5 >>= 16   ///  r5 = r5.wrapping_shr(16)
    or64 r3, r5                                     r3 |= r5   ///  r3 = r3.or(r5)
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    and64 r4, 1431655765                            r4 &= 1431655765   ///  r4 = r4.and(1431655765)
    sub64 r3, r4                                    r3 -= r4   ///  r3 = r3.wrapping_sub(r4)
    mov64 r4, r3                                    r4 = r3
    and64 r4, 858993459                             r4 &= 858993459   ///  r4 = r4.and(858993459)
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    and64 r3, 858993459                             r3 &= 858993459   ///  r3 = r3.and(858993459)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    and64 r4, 252645135                             r4 &= 252645135   ///  r4 = r4.and(252645135)
    mul64 r4, 16843009                              r4 *= 16843009   ///  r4 = r4.wrapping_mul(16843009 as u64)
    lddw r3, 0xff000000                             r3 load str located at 4278190080
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r4, 24                                    r4 >>= 24   ///  r4 = r4.wrapping_shr(24)
    lsh64 r1, r4                                    r1 <<= r4   ///  r1 = r1.wrapping_shl(r4 as u32)
    lsh64 r4, 23                                    r4 <<= 23   ///  r4 = r4.wrapping_shl(23)
    mov64 r3, r1                                    r3 = r1
    and64 r3, r2                                    r3 &= r2   ///  r3 = r3.and(r2)
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    mov64 r0, r3                                    r0 = r3
    sub64 r0, r4                                    r0 -= r4   ///  r0 = r0.wrapping_sub(r4)
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 7                                     r2 >>= 7   ///  r2 = r2.wrapping_shr(7)
    and64 r2, r3                                    r2 &= r3   ///  r2 = r2.and(r3)
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    lsh64 r1, 24                                    r1 <<= 24   ///  r1 = r1.wrapping_shl(24)
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    lddw r2, 0x80000000                             r2 load str located at 2147483648
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    rsh64 r1, 31                                    r1 >>= 31   ///  r1 = r1.wrapping_shr(31)
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
    add64 r0, 1317011456                            r0 += 1317011456   ///  r0 = r0.wrapping_add(1317011456 as i32 as i64 as u64)
lbb_24949:
    exit                                    

function_24950:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    call function_24959                     
    ldxdw r1, [r10-0x20]                    
    ldxdw r2, [r10-0x18]                    
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_24959:
    stxdw [r10-0xc0], r4                    
    mov64 r0, r3                                    r0 = r3
    stxdw [r10-0xb8], r2                    
    stxdw [r10-0xd0], r1                    
    mov64 r2, r5                                    r2 = r5
    rsh64 r2, 1                                     r2 >>= 1   ///  r2 = r2.wrapping_shr(1)
    mov64 r4, r5                                    r4 = r5
    or64 r4, r2                                     r4 |= r2   ///  r4 = r4.or(r2)
    mov64 r2, r4                                    r2 = r4
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    or64 r4, r2                                     r4 |= r2   ///  r4 = r4.or(r2)
    mov64 r2, r4                                    r2 = r4
    rsh64 r2, 4                                     r2 >>= 4   ///  r2 = r2.wrapping_shr(4)
    or64 r4, r2                                     r4 |= r2   ///  r4 = r4.or(r2)
    mov64 r2, r4                                    r2 = r4
    rsh64 r2, 8                                     r2 >>= 8   ///  r2 = r2.wrapping_shr(8)
    or64 r4, r2                                     r4 |= r2   ///  r4 = r4.or(r2)
    mov64 r2, r4                                    r2 = r4
    rsh64 r2, 16                                    r2 >>= 16   ///  r2 = r2.wrapping_shr(16)
    or64 r4, r2                                     r4 |= r2   ///  r4 = r4.or(r2)
    mov64 r2, r0                                    r2 = r0
    rsh64 r2, 1                                     r2 >>= 1   ///  r2 = r2.wrapping_shr(1)
    mov64 r7, r0                                    r7 = r0
    or64 r7, r2                                     r7 |= r2   ///  r7 = r7.or(r2)
    mov64 r2, r7                                    r2 = r7
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    or64 r7, r2                                     r7 |= r2   ///  r7 = r7.or(r2)
    mov64 r2, r4                                    r2 = r4
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    or64 r4, r2                                     r4 |= r2   ///  r4 = r4.or(r2)
    mov64 r2, r7                                    r2 = r7
    rsh64 r2, 4                                     r2 >>= 4   ///  r2 = r2.wrapping_shr(4)
    or64 r7, r2                                     r7 |= r2   ///  r7 = r7.or(r2)
    mov64 r2, r7                                    r2 = r7
    rsh64 r2, 8                                     r2 >>= 8   ///  r2 = r2.wrapping_shr(8)
    or64 r7, r2                                     r7 |= r2   ///  r7 = r7.or(r2)
    lddw r2, 0x5555555555555555                     r2 load str located at 6148914691236517205
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    and64 r3, r2                                    r3 &= r2   ///  r3 = r3.and(r2)
    sub64 r4, r3                                    r4 -= r3   ///  r4 = r4.wrapping_sub(r3)
    mov64 r3, r7                                    r3 = r7
    rsh64 r3, 16                                    r3 >>= 16   ///  r3 = r3.wrapping_shr(16)
    or64 r7, r3                                     r7 |= r3   ///  r7 = r7.or(r3)
    mov64 r3, r7                                    r3 = r7
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    or64 r7, r3                                     r7 |= r3   ///  r7 = r7.or(r3)
    lddw r3, 0x3333333333333333                     r3 load str located at 3689348814741910323
    mov64 r6, r4                                    r6 = r4
    and64 r6, r3                                    r6 &= r3   ///  r6 = r6.and(r3)
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    add64 r6, r4                                    r6 += r4   ///  r6 = r6.wrapping_add(r4)
    xor64 r7, -1                                    r7 ^= -1   ///  r7 = r7.xor(-1)
    mov64 r4, r7                                    r4 = r7
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    sub64 r7, r4                                    r7 -= r4   ///  r7 = r7.wrapping_sub(r4)
    mov64 r4, r6                                    r4 = r6
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    add64 r6, r4                                    r6 += r4   ///  r6 = r6.wrapping_add(r4)
    mov64 r9, r7                                    r9 = r7
    and64 r9, r3                                    r9 &= r3   ///  r9 = r9.and(r3)
    rsh64 r7, 2                                     r7 >>= 2   ///  r7 = r7.wrapping_shr(2)
    and64 r7, r3                                    r7 &= r3   ///  r7 = r7.and(r3)
    add64 r9, r7                                    r9 += r7   ///  r9 = r9.wrapping_add(r7)
    mov64 r4, r9                                    r4 = r9
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    add64 r9, r4                                    r9 += r4   ///  r9 = r9.wrapping_add(r4)
    lddw r1, 0xf0f0f0f0f0f0f0f                      r1 load str located at 1085102592571150095
    and64 r9, r1                                    r9 &= r1   ///  r9 = r9.and(r1)
    and64 r6, r1                                    r6 &= r1   ///  r6 = r6.and(r1)
    lddw r4, 0x101010101010101                      r4 load str located at 72340172838076673
    mul64 r6, r4                                    r6 *= r4   ///  r6 = r6.wrapping_mul(r4)
    mul64 r9, r4                                    r9 *= r4   ///  r9 = r9.wrapping_mul(r4)
    rsh64 r9, 56                                    r9 >>= 56   ///  r9 = r9.wrapping_shr(56)
    stxdw [r10-0xc8], r5                    
    jne r0, 0, lbb_25082                            if r0 != (0 as i32 as i64 as u64) { pc += 40 }
    ldxdw r9, [r10-0xb8]                    
    mov64 r7, r9                                    r7 = r9
    rsh64 r7, 1                                     r7 >>= 1   ///  r7 = r7.wrapping_shr(1)
    mov64 r8, r0                                    r8 = r0
    mov64 r0, r9                                    r0 = r9
    or64 r0, r7                                     r0 |= r7   ///  r0 = r0.or(r7)
    mov64 r7, r0                                    r7 = r0
    rsh64 r7, 2                                     r7 >>= 2   ///  r7 = r7.wrapping_shr(2)
    or64 r0, r7                                     r0 |= r7   ///  r0 = r0.or(r7)
    mov64 r7, r0                                    r7 = r0
    rsh64 r7, 4                                     r7 >>= 4   ///  r7 = r7.wrapping_shr(4)
    or64 r0, r7                                     r0 |= r7   ///  r0 = r0.or(r7)
    mov64 r7, r0                                    r7 = r0
    rsh64 r7, 8                                     r7 >>= 8   ///  r7 = r7.wrapping_shr(8)
    or64 r0, r7                                     r0 |= r7   ///  r0 = r0.or(r7)
    mov64 r7, r0                                    r7 = r0
    rsh64 r7, 16                                    r7 >>= 16   ///  r7 = r7.wrapping_shr(16)
    or64 r0, r7                                     r0 |= r7   ///  r0 = r0.or(r7)
    mov64 r7, r0                                    r7 = r0
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    or64 r0, r7                                     r0 |= r7   ///  r0 = r0.or(r7)
    xor64 r0, -1                                    r0 ^= -1   ///  r0 = r0.xor(-1)
    mov64 r7, r0                                    r7 = r0
    rsh64 r7, 1                                     r7 >>= 1   ///  r7 = r7.wrapping_shr(1)
    and64 r7, r2                                    r7 &= r2   ///  r7 = r7.and(r2)
    sub64 r0, r7                                    r0 -= r7   ///  r0 = r0.wrapping_sub(r7)
    ldxdw r5, [r10-0xc8]                    
    mov64 r9, r0                                    r9 = r0
    and64 r9, r3                                    r9 &= r3   ///  r9 = r9.and(r3)
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    and64 r0, r3                                    r0 &= r3   ///  r0 = r0.and(r3)
    add64 r9, r0                                    r9 += r0   ///  r9 = r9.wrapping_add(r0)
    mov64 r0, r9                                    r0 = r9
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    add64 r9, r0                                    r9 += r0   ///  r9 = r9.wrapping_add(r0)
    mov64 r0, r8                                    r0 = r8
    and64 r9, r1                                    r9 &= r1   ///  r9 = r9.and(r1)
    mul64 r9, r4                                    r9 *= r4   ///  r9 = r9.wrapping_mul(r4)
    rsh64 r9, 56                                    r9 >>= 56   ///  r9 = r9.wrapping_shr(56)
    add64 r9, 64                                    r9 += 64   ///  r9 = r9.wrapping_add(64 as i32 as i64 as u64)
lbb_25082:
    rsh64 r6, 56                                    r6 >>= 56   ///  r6 = r6.wrapping_shr(56)
    jne r5, 0, lbb_25123                            if r5 != (0 as i32 as i64 as u64) { pc += 39 }
    mov64 r7, r0                                    r7 = r0
    ldxdw r6, [r10-0xc0]                    
    mov64 r0, r6                                    r0 = r6
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    or64 r6, r0                                     r6 |= r0   ///  r6 = r6.or(r0)
    mov64 r0, r6                                    r0 = r6
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r6, r0                                     r6 |= r0   ///  r6 = r6.or(r0)
    mov64 r0, r6                                    r0 = r6
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r6, r0                                     r6 |= r0   ///  r6 = r6.or(r0)
    mov64 r0, r6                                    r0 = r6
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    or64 r6, r0                                     r6 |= r0   ///  r6 = r6.or(r0)
    mov64 r0, r6                                    r0 = r6
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    or64 r6, r0                                     r6 |= r0   ///  r6 = r6.or(r0)
    mov64 r0, r6                                    r0 = r6
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    or64 r6, r0                                     r6 |= r0   ///  r6 = r6.or(r0)
    xor64 r6, -1                                    r6 ^= -1   ///  r6 = r6.xor(-1)
    mov64 r0, r6                                    r0 = r6
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r2                                    r0 &= r2   ///  r0 = r0.and(r2)
    sub64 r6, r0                                    r6 -= r0   ///  r6 = r6.wrapping_sub(r0)
    mov64 r0, r7                                    r0 = r7
    ldxdw r5, [r10-0xc8]                    
    mov64 r2, r6                                    r2 = r6
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    and64 r6, r3                                    r6 &= r3   ///  r6 = r6.and(r3)
    and64 r2, r3                                    r2 &= r3   ///  r2 = r2.and(r3)
    add64 r6, r2                                    r6 += r2   ///  r6 = r6.wrapping_add(r2)
    mov64 r2, r6                                    r2 = r6
    rsh64 r2, 4                                     r2 >>= 4   ///  r2 = r2.wrapping_shr(4)
    add64 r6, r2                                    r6 += r2   ///  r6 = r6.wrapping_add(r2)
    and64 r6, r1                                    r6 &= r1   ///  r6 = r6.and(r1)
    mul64 r6, r4                                    r6 *= r4   ///  r6 = r6.wrapping_mul(r4)
    rsh64 r6, 56                                    r6 >>= 56   ///  r6 = r6.wrapping_shr(56)
    add64 r6, 64                                    r6 += 64   ///  r6 = r6.wrapping_add(64 as i32 as i64 as u64)
lbb_25123:
    jle r6, r9, lbb_25140                           if r6 <= r9 { pc += 16 }
    mov64 r2, r9                                    r2 = r9
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    ldxdw r4, [r10-0xb8]                    
    ldxdw r1, [r10-0xd0]                    
    jgt r2, 63, lbb_25131                           if r2 > (63 as i32 as i64 as u64) { pc += 1 }
    ja lbb_25163                                    if true { pc += 32 }
lbb_25131:
    ldxdw r2, [r10-0xc0]                    
    mov64 r9, r4                                    r9 = r4
    jeq r2, 0, lbb_25136                            if r2 == (0 as i32 as i64 as u64) { pc += 2 }
    mov64 r8, r9                                    r8 = r9
    div64 r8, r2                                    r8 /= r2   ///  r8 = r8 / r2
lbb_25136:
    mod64 r9, r2                                    r9 %= r2   ///  r9 = r9 % r2
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ja lbb_25199                                    if true { pc += 59 }
lbb_25140:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    ldxdw r9, [r10-0xb8]                    
    ldxdw r4, [r10-0xc0]                    
    ldxdw r1, [r10-0xd0]                    
    jlt r9, r4, lbb_25148                           if r9 < r4 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_25148:
    jlt r0, r5, lbb_25150                           if r0 < r5 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_25150:
    jeq r0, r5, lbb_25152                           if r0 == r5 { pc += 1 }
    mov64 r3, r2                                    r3 = r2
lbb_25152:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jne r3, 0, lbb_25199                            if r3 != (0 as i32 as i64 as u64) { pc += 44 }
    sub64 r0, r5                                    r0 -= r5   ///  r0 = r0.wrapping_sub(r5)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r9, r4, lbb_25160                           if r9 < r4 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_25160:
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
    sub64 r9, r4                                    r9 -= r4   ///  r9 = r9.wrapping_sub(r4)
    ja lbb_25199                                    if true { pc += 36 }
lbb_25163:
    mov64 r2, r6                                    r2 = r6
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jgt r2, 95, lbb_25168                           if r2 > (95 as i32 as i64 as u64) { pc += 1 }
    ja lbb_25204                                    if true { pc += 36 }
lbb_25168:
    ldxdw r6, [r10-0xc0]                    
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    jeq r6, 0, lbb_25174                            if r6 == (0 as i32 as i64 as u64) { pc += 2 }
    mov64 r3, r0                                    r3 = r0
    div64 r3, r6                                    r3 /= r6   ///  r3 = r3 / r6
lbb_25174:
    mod64 r0, r6                                    r0 %= r6   ///  r0 = r0 % r6
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    mov64 r5, r4                                    r5 = r4
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    or64 r0, r4                                     r0 |= r4   ///  r0 = r0.or(r4)
    div64 r0, r6                                    r0 /= r6   ///  r0 = r0 / r6
    mov64 r2, r0                                    r2 = r0
    mul64 r2, r6                                    r2 *= r6   ///  r2 = r2.wrapping_mul(r6)
    sub64 r4, r2                                    r4 -= r2   ///  r4 = r4.wrapping_sub(r2)
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    or64 r4, r5                                     r4 |= r5   ///  r4 = r4.or(r5)
    mov64 r2, r4                                    r2 = r4
    div64 r2, r6                                    r2 /= r6   ///  r2 = r2 / r6
    mov64 r8, r0                                    r8 = r0
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    or64 r8, r2                                     r8 |= r2   ///  r8 = r8.or(r2)
    mul64 r2, r6                                    r2 *= r6   ///  r2 = r2.wrapping_mul(r6)
    sub64 r4, r2                                    r4 -= r2   ///  r4 = r4.wrapping_sub(r2)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r2, r0                                    r2 = r0
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_25198:
    mov64 r9, r4                                    r9 = r4
lbb_25199:
    stxdw [r1+0x10], r9                     
    stxdw [r1+0x0], r8                      
    stxdw [r1+0x18], r0                     
    stxdw [r1+0x8], r2                      
    exit                                    
lbb_25204:
    stxdw [r10-0xd8], r0                    
    mov64 r1, r6                                    r1 = r6
    sub64 r1, r9                                    r1 -= r9   ///  r1 = r1.wrapping_sub(r9)
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jlt r1, 32, lbb_25211                           if r1 < (32 as i32 as i64 as u64) { pc += 1 }
    ja lbb_25280                                    if true { pc += 69 }
lbb_25211:
    mov64 r3, r5                                    r3 = r5
    mov64 r7, 64                                    r7 = 64 as i32 as i64 as u64
    sub64 r7, r9                                    r7 -= r9   ///  r7 = r7.wrapping_sub(r9)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    arsh64 r7, 32                                   r7 >>= 32 (signed)   ///  r7 = (r7 as i64).wrapping_shr(32)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    ldxdw r9, [r10-0xc0]                    
    mov64 r2, r9                                    r2 = r9
    mov64 r4, r7                                    r4 = r7
    call function_23325                     
    ldxdw r6, [r10-0x80]                    
    jeq r6, 0, lbb_25232                            if r6 == (0 as i32 as i64 as u64) { pc += 8 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    ldxdw r2, [r10-0xb8]                    
    ldxdw r3, [r10-0xd8]                    
    mov64 r4, r7                                    r4 = r7
    call function_23325                     
    ldxdw r8, [r10-0x90]                    
    div64 r8, r6                                    r8 /= r6   ///  r8 = r8 / r6
lbb_25232:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -160                                  r1 += -160   ///  r1 = r1.wrapping_add(-160 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r8                                    r4 = r8
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_25903                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    ldxdw r7, [r10-0xc8]                    
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r8                                    r4 = r8
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_25903                     
    ldxdw r2, [r10-0x98]                    
    ldxdw r1, [r10-0xb0]                    
    mov64 r4, r2                                    r4 = r2
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r4, r2, lbb_25254                           if r4 < r2 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_25254:
    ldxdw r2, [r10-0xa8]                    
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxdw r3, [r10-0xa0]                    
    ldxdw r1, [r10-0xd0]                    
    ldxdw r0, [r10-0xd8]                    
    jne r2, 0, lbb_25497                            if r2 != (0 as i32 as i64 as u64) { pc += 237 }
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r6, [r10-0xb8]                    
    jlt r6, r3, lbb_25265                           if r6 < r3 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_25265:
    jlt r0, r4, lbb_25267                           if r0 < r4 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_25267:
    jeq r0, r4, lbb_25269                           if r0 == r4 { pc += 1 }
    mov64 r2, r5                                    r2 = r5
lbb_25269:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_25497                            if r2 != (0 as i32 as i64 as u64) { pc += 226 }
    sub64 r0, r4                                    r0 -= r4   ///  r0 = r0.wrapping_sub(r4)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r9, [r10-0xb8]                    
    jlt r9, r3, lbb_25277                           if r9 < r3 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_25277:
    sub64 r0, r4                                    r0 -= r4   ///  r0 = r0.wrapping_sub(r4)
    sub64 r9, r3                                    r9 -= r3   ///  r9 = r9.wrapping_sub(r3)
    ja lbb_25199                                    if true { pc += -81 }
lbb_25280:
    mov64 r1, 96                                    r1 = 96 as i32 as i64 as u64
    sub64 r1, r6                                    r1 -= r6   ///  r1 = r1.wrapping_sub(r6)
    stxdw [r10-0xf8], r1                    
    mov64 r4, r1                                    r4 = r1
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    stxdw [r10-0xf0], r4                    
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    ldxdw r2, [r10-0xc0]                    
    mov64 r3, r5                                    r3 = r5
    call function_23325                     
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x10]                    
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x100], r1                   
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r0, [r10-0xd8]                    
    ldxdw r5, [r10-0xb8]                    
lbb_25301:
    stxdw [r10-0xe8], r4                    
    stxdw [r10-0xe0], r3                    
    mov64 r8, 64                                    r8 = 64 as i32 as i64 as u64
    sub64 r8, r9                                    r8 -= r9   ///  r8 = r8.wrapping_sub(r9)
    mov64 r7, r8                                    r7 = r8
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    mov64 r9, r7                                    r9 = r7
    arsh64 r9, 32                                   r9 >>= 32 (signed)   ///  r9 = (r9 as i64).wrapping_shr(32)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r5                                    r2 = r5
    stxdw [r10-0xb8], r2                    
    stxdw [r10-0xd8], r0                    
    mov64 r3, r0                                    r3 = r0
    mov64 r4, r9                                    r4 = r9
    call function_23325                     
    ldxdw r2, [r10-0x20]                    
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    ldxdw r1, [r10-0xf0]                    
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jge r7, r1, lbb_25335                           if r7 >= r1 { pc += 13 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r7, r2                                    r7 = r2
    ldxdw r2, [r10-0xc0]                    
    ldxdw r6, [r10-0xc8]                    
    mov64 r3, r6                                    r3 = r6
    mov64 r4, r9                                    r4 = r9
    call function_23325                     
    mov64 r2, r7                                    r2 = r7
    ldxdw r1, [r10-0x60]                    
    jeq r1, 0, lbb_25516                            if r1 == (0 as i32 as i64 as u64) { pc += 183 }
    div64 r2, r1                                    r2 /= r1   ///  r2 = r2 / r1
    ja lbb_25516                                    if true { pc += 181 }
lbb_25335:
    ldxdw r1, [r10-0x100]                   
    div64 r2, r1                                    r2 /= r1   ///  r2 = r2 / r1
    ldxdw r1, [r10-0xf8]                    
    sub64 r8, r1                                    r8 -= r1   ///  r8 = r8.wrapping_sub(r1)
    and64 r8, 127                                   r8 &= 127   ///  r8 = r8.and(127)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r7, r2                                    r7 = r2
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r8                                    r4 = r8
    call function_26120                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0xc0]                    
    ldxdw r7, [r10-0xc8]                    
    mov64 r5, r7                                    r5 = r7
    call function_25903                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    ldxdw r2, [r10-0x40]                    
    ldxdw r3, [r10-0x38]                    
    mov64 r4, r8                                    r4 = r8
    call function_26120                     
    ldxdw r3, [r10-0x30]                    
    mov64 r8, r3                                    r8 = r3
    ldxdw r1, [r10-0xe8]                    
    add64 r8, r1                                    r8 += r1   ///  r8 = r8.wrapping_add(r1)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r8, r3, lbb_25368                           if r8 < r3 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_25368:
    ldxdw r4, [r10-0x50]                    
    ldxdw r0, [r10-0xd8]                    
    ldxdw r5, [r10-0xb8]                    
    jlt r5, r4, lbb_25373                           if r5 < r4 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_25373:
    ldxdw r3, [r10-0x48]                    
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
    sub64 r0, r2                                    r0 -= r2   ///  r0 = r0.wrapping_sub(r2)
    mov64 r2, r0                                    r2 = r0
    rsh64 r2, 1                                     r2 >>= 1   ///  r2 = r2.wrapping_shr(1)
    mov64 r3, r0                                    r3 = r0
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    mov64 r2, r3                                    r2 = r3
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    ldxdw r2, [r10-0x28]                    
    sub64 r5, r4                                    r5 -= r4   ///  r5 = r5.wrapping_sub(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 8                                     r4 >>= 8   ///  r4 = r4.wrapping_shr(8)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 16                                    r4 >>= 16   ///  r4 = r4.wrapping_shr(16)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    lddw r9, 0x5555555555555555                     r9 load str located at 6148914691236517205
    and64 r4, r9                                    r4 &= r9   ///  r4 = r4.and(r9)
    sub64 r3, r4                                    r3 -= r4   ///  r3 = r3.wrapping_sub(r4)
    mov64 r9, r3                                    r9 = r3
    lddw r4, 0x3333333333333333                     r4 load str located at 3689348814741910323
    and64 r9, r4                                    r9 &= r4   ///  r9 = r9.and(r4)
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    and64 r3, r4                                    r3 &= r4   ///  r3 = r3.and(r4)
    add64 r9, r3                                    r9 += r3   ///  r9 = r9.wrapping_add(r3)
    mov64 r3, r9                                    r3 = r9
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r9, r3                                    r9 += r3   ///  r9 = r9.wrapping_add(r3)
    lddw r3, 0xf0f0f0f0f0f0f0f                      r3 load str located at 1085102592571150095
    and64 r9, r3                                    r9 &= r3   ///  r9 = r9.and(r3)
    lddw r3, 0x101010101010101                      r3 load str located at 72340172838076673
    mul64 r9, r3                                    r9 *= r3   ///  r9 = r9.wrapping_mul(r3)
    rsh64 r9, 56                                    r9 >>= 56   ///  r9 = r9.wrapping_shr(56)
    jne r0, 0, lbb_25466                            if r0 != (0 as i32 as i64 as u64) { pc += 44 }
    mov64 r3, r5                                    r3 = r5
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 8                                     r4 >>= 8   ///  r4 = r4.wrapping_shr(8)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 16                                    r4 >>= 16   ///  r4 = r4.wrapping_shr(16)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    lddw r9, 0x5555555555555555                     r9 load str located at 6148914691236517205
    and64 r4, r9                                    r4 &= r9   ///  r4 = r4.and(r9)
    sub64 r3, r4                                    r3 -= r4   ///  r3 = r3.wrapping_sub(r4)
    mov64 r9, r3                                    r9 = r3
    lddw r4, 0x3333333333333333                     r4 load str located at 3689348814741910323
    and64 r9, r4                                    r9 &= r4   ///  r9 = r9.and(r4)
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    and64 r3, r4                                    r3 &= r4   ///  r3 = r3.and(r4)
    add64 r9, r3                                    r9 += r3   ///  r9 = r9.wrapping_add(r3)
    mov64 r3, r9                                    r3 = r9
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r9, r3                                    r9 += r3   ///  r9 = r9.wrapping_add(r3)
    lddw r3, 0xf0f0f0f0f0f0f0f                      r3 load str located at 1085102592571150095
    and64 r9, r3                                    r9 &= r3   ///  r9 = r9.and(r3)
    lddw r3, 0x101010101010101                      r3 load str located at 72340172838076673
    mul64 r9, r3                                    r9 *= r3   ///  r9 = r9.wrapping_mul(r3)
    rsh64 r9, 56                                    r9 >>= 56   ///  r9 = r9.wrapping_shr(56)
    add64 r9, 64                                    r9 += 64   ///  r9 = r9.wrapping_add(64 as i32 as i64 as u64)
lbb_25466:
    ldxdw r3, [r10-0xe0]                    
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r1, r6                                    r1 = r6
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jgt r1, r9, lbb_25474                           if r1 > r9 { pc += 1 }
    ja lbb_25566                                    if true { pc += 92 }
lbb_25474:
    mov64 r1, r9                                    r1 = r9
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r4, r8                                    r4 = r8
    mov64 r3, r2                                    r3 = r2
    jgt r1, 63, lbb_25481                           if r1 > (63 as i32 as i64 as u64) { pc += 1 }
    ja lbb_25301                                    if true { pc += -180 }
lbb_25481:
    ldxdw r6, [r10-0xc0]                    
    mov64 r9, r5                                    r9 = r5
    jeq r6, 0, lbb_25486                            if r6 == (0 as i32 as i64 as u64) { pc += 2 }
    mov64 r1, r9                                    r1 = r9
    div64 r1, r6                                    r1 /= r6   ///  r1 = r1 / r6
lbb_25486:
    mov64 r3, r8                                    r3 = r8
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r1, [r10-0xd0]                    
    jlt r3, r8, lbb_25493                           if r3 < r8 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_25493:
    mod64 r9, r6                                    r9 %= r6   ///  r9 = r9 % r6
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    mov64 r8, r3                                    r8 = r3
    ja lbb_25199                                    if true { pc += -298 }
lbb_25497:
    add64 r7, r0                                    r7 += r0   ///  r7 = r7.wrapping_add(r0)
    mov64 r5, r9                                    r5 = r9
    ldxdw r2, [r10-0xb8]                    
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jlt r5, r9, lbb_25506                           if r5 < r9 { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_25506:
    add64 r7, r6                                    r7 += r6   ///  r7 = r7.wrapping_add(r6)
    sub64 r7, r4                                    r7 -= r4   ///  r7 = r7.wrapping_sub(r4)
    jlt r5, r3, lbb_25510                           if r5 < r3 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_25510:
    sub64 r7, r0                                    r7 -= r0   ///  r7 = r7.wrapping_sub(r0)
    sub64 r5, r3                                    r5 -= r3   ///  r5 = r5.wrapping_sub(r3)
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r9, r5                                    r9 = r5
    mov64 r0, r7                                    r0 = r7
    ja lbb_25199                                    if true { pc += -317 }
lbb_25516:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    mov64 r9, r2                                    r9 = r2
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r8, [r10-0xc0]                    
    mov64 r4, r8                                    r4 = r8
    mov64 r5, r6                                    r5 = r6
    call function_25903                     
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x70]                    
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r1, [r10-0xb8]                    
    jlt r1, r2, lbb_25530                           if r1 < r2 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_25530:
    ldxdw r3, [r10-0x68]                    
    ldxdw r1, [r10-0xd0]                    
    ldxdw r0, [r10-0xd8]                    
    jlt r0, r3, lbb_25535                           if r0 < r3 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_25535:
    jeq r0, r3, lbb_25537                           if r0 == r3 { pc += 1 }
    mov64 r4, r5                                    r4 = r5
lbb_25537:
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jne r4, 0, lbb_25540                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    ja lbb_25591                                    if true { pc += 51 }
lbb_25540:
    add64 r0, r6                                    r0 += r6   ///  r0 = r0.wrapping_add(r6)
    mov64 r6, r0                                    r6 = r0
    ldxdw r7, [r10-0xb8]                    
    mov64 r4, r7                                    r4 = r7
    add64 r4, r8                                    r4 += r8   ///  r4 = r4.wrapping_add(r8)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r4, r7, lbb_25549                           if r4 < r7 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_25549:
    add64 r6, r0                                    r6 += r0   ///  r6 = r6.wrapping_add(r0)
    ldxdw r7, [r10-0xe8]                    
    add64 r9, r7                                    r9 += r7   ///  r9 = r9.wrapping_add(r7)
    add64 r9, -1                                    r9 += -1   ///  r9 = r9.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r8, r9                                    r8 = r9
    jlt r9, r7, lbb_25557                           if r9 < r7 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_25557:
    sub64 r6, r3                                    r6 -= r3   ///  r6 = r6.wrapping_sub(r3)
    jlt r4, r2, lbb_25560                           if r4 < r2 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_25560:
    sub64 r6, r5                                    r6 -= r5   ///  r6 = r6.wrapping_sub(r5)
    sub64 r4, r2                                    r4 -= r2   ///  r4 = r4.wrapping_sub(r2)
    ldxdw r2, [r10-0xe0]                    
    add64 r2, r0                                    r2 += r0   ///  r2 = r2.wrapping_add(r0)
    mov64 r0, r6                                    r0 = r6
    ja lbb_25198                                    if true { pc += -368 }
lbb_25566:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    ldxdw r1, [r10-0xd0]                    
    ldxdw r6, [r10-0xc0]                    
    mov64 r9, r5                                    r9 = r5
    jlt r9, r6, lbb_25573                           if r9 < r6 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_25573:
    jlt r0, r7, lbb_25575                           if r0 < r7 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_25575:
    jeq r0, r7, lbb_25577                           if r0 == r7 { pc += 1 }
    mov64 r3, r4                                    r3 = r4
lbb_25577:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_25199                            if r3 != (0 as i32 as i64 as u64) { pc += -380 }
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r9, r6, lbb_25583                           if r9 < r6 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_25583:
    sub64 r0, r7                                    r0 -= r7   ///  r0 = r0.wrapping_sub(r7)
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
    jeq r8, 0, lbb_25587                            if r8 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_25587:
    sub64 r0, r4                                    r0 -= r4   ///  r0 = r0.wrapping_sub(r4)
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    sub64 r9, r6                                    r9 -= r6   ///  r9 = r9.wrapping_sub(r6)
    ja lbb_25199                                    if true { pc += -392 }
lbb_25591:
    ldxdw r6, [r10-0xe8]                    
    mov64 r8, r6                                    r8 = r6
    add64 r8, r9                                    r8 += r9   ///  r8 = r8.wrapping_add(r9)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r8, r6, lbb_25598                           if r8 < r6 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_25598:
    ldxdw r6, [r10-0xb8]                    
    jlt r6, r2, lbb_25601                           if r6 < r2 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_25601:
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
    sub64 r0, r5                                    r0 -= r5   ///  r0 = r0.wrapping_sub(r5)
    ldxdw r9, [r10-0xb8]                    
    sub64 r9, r2                                    r9 -= r2   ///  r9 = r9.wrapping_sub(r2)
    ldxdw r2, [r10-0xe0]                    
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    ja lbb_25199                                    if true { pc += -409 }

function_25608:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jlt r2, r3, lbb_25749                           if r2 < r3 { pc += 139 }
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    mov64 r6, r3                                    r6 = r3
    or64 r6, r4                                     r6 |= r4   ///  r6 = r6.or(r4)
    mov64 r4, r6                                    r4 = r6
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    or64 r6, r4                                     r6 |= r4   ///  r6 = r6.or(r4)
    mov64 r4, r6                                    r4 = r6
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    or64 r6, r4                                     r6 |= r4   ///  r6 = r6.or(r4)
    mov64 r4, r6                                    r4 = r6
    rsh64 r4, 8                                     r4 >>= 8   ///  r4 = r4.wrapping_shr(8)
    or64 r6, r4                                     r6 |= r4   ///  r6 = r6.or(r4)
    mov64 r4, r6                                    r4 = r6
    rsh64 r4, 16                                    r4 >>= 16   ///  r4 = r4.wrapping_shr(16)
    or64 r6, r4                                     r6 |= r4   ///  r6 = r6.or(r4)
    mov64 r4, r6                                    r4 = r6
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    or64 r6, r4                                     r6 |= r4   ///  r6 = r6.or(r4)
    xor64 r6, -1                                    r6 ^= -1   ///  r6 = r6.xor(-1)
    lddw r0, 0x5555555555555555                     r0 load str located at 6148914691236517205
    mov64 r4, r6                                    r4 = r6
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    and64 r4, r0                                    r4 &= r0   ///  r4 = r4.and(r0)
    sub64 r6, r4                                    r6 -= r4   ///  r6 = r6.wrapping_sub(r4)
    lddw r5, 0x3333333333333333                     r5 load str located at 3689348814741910323
    mov64 r4, r6                                    r4 = r6
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    rsh64 r6, 2                                     r6 >>= 2   ///  r6 = r6.wrapping_shr(2)
    and64 r6, r5                                    r6 &= r5   ///  r6 = r6.and(r5)
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    mov64 r6, r4                                    r6 = r4
    rsh64 r6, 4                                     r6 >>= 4   ///  r6 = r6.wrapping_shr(4)
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    lddw r6, 0xf0f0f0f0f0f0f0f                      r6 load str located at 1085102592571150095
    and64 r4, r6                                    r4 &= r6   ///  r4 = r4.and(r6)
    lddw r7, 0x101010101010101                      r7 load str located at 72340172838076673
    mul64 r4, r7                                    r4 *= r7   ///  r4 = r4.wrapping_mul(r7)
    mov64 r9, 64                                    r9 = 64 as i32 as i64 as u64
    rsh64 r4, 56                                    r4 >>= 56   ///  r4 = r4.wrapping_shr(56)
    jeq r2, 0, lbb_25690                            if r2 == (0 as i32 as i64 as u64) { pc += 35 }
    mov64 r9, r2                                    r9 = r2
    rsh64 r9, 1                                     r9 >>= 1   ///  r9 = r9.wrapping_shr(1)
    mov64 r8, r2                                    r8 = r2
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    mov64 r9, r8                                    r9 = r8
    rsh64 r9, 2                                     r9 >>= 2   ///  r9 = r9.wrapping_shr(2)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    mov64 r9, r8                                    r9 = r8
    rsh64 r9, 4                                     r9 >>= 4   ///  r9 = r9.wrapping_shr(4)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    mov64 r9, r8                                    r9 = r8
    rsh64 r9, 8                                     r9 >>= 8   ///  r9 = r9.wrapping_shr(8)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    mov64 r9, r8                                    r9 = r8
    rsh64 r9, 16                                    r9 >>= 16   ///  r9 = r9.wrapping_shr(16)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    mov64 r9, r8                                    r9 = r8
    rsh64 r9, 32                                    r9 >>= 32   ///  r9 = r9.wrapping_shr(32)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    xor64 r8, -1                                    r8 ^= -1   ///  r8 = r8.xor(-1)
    mov64 r9, r8                                    r9 = r8
    rsh64 r9, 1                                     r9 >>= 1   ///  r9 = r9.wrapping_shr(1)
    and64 r9, r0                                    r9 &= r0   ///  r9 = r9.and(r0)
    sub64 r8, r9                                    r8 -= r9   ///  r8 = r8.wrapping_sub(r9)
    mov64 r9, r8                                    r9 = r8
    and64 r9, r5                                    r9 &= r5   ///  r9 = r9.and(r5)
    rsh64 r8, 2                                     r8 >>= 2   ///  r8 = r8.wrapping_shr(2)
    and64 r8, r5                                    r8 &= r5   ///  r8 = r8.and(r5)
    add64 r9, r8                                    r9 += r8   ///  r9 = r9.wrapping_add(r8)
    mov64 r5, r9                                    r5 = r9
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    add64 r9, r5                                    r9 += r5   ///  r9 = r9.wrapping_add(r5)
    and64 r9, r6                                    r9 &= r6   ///  r9 = r9.and(r6)
    mul64 r9, r7                                    r9 *= r7   ///  r9 = r9.wrapping_mul(r7)
    rsh64 r9, 56                                    r9 >>= 56   ///  r9 = r9.wrapping_shr(56)
lbb_25690:
    sub64 r4, r9                                    r4 -= r9   ///  r4 = r4.wrapping_sub(r9)
    mov64 r5, r4                                    r5 = r4
    and64 r5, 63                                    r5 &= 63   ///  r5 = r5.and(63)
    mov64 r0, r3                                    r0 = r3
    lsh64 r0, r5                                    r0 <<= r5   ///  r0 = r0.wrapping_shl(r5 as u32)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jgt r0, r2, lbb_25699                           if r0 > r2 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_25699:
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    sub64 r4, r5                                    r4 -= r5   ///  r4 = r4.wrapping_sub(r5)
    mov64 r0, r4                                    r0 = r4
    and64 r0, 63                                    r0 &= 63   ///  r0 = r0.and(63)
    lsh64 r8, r0                                    r8 <<= r0   ///  r8 = r8.wrapping_shl(r0 as u32)
    mov64 r5, r3                                    r5 = r3
    lsh64 r5, r0                                    r5 <<= r0   ///  r5 = r5.wrapping_shl(r0 as u32)
    sub64 r2, r5                                    r2 -= r5   ///  r2 = r2.wrapping_sub(r5)
    jlt r2, r3, lbb_25749                           if r2 < r3 { pc += 40 }
    mov64 r0, r8                                    r0 = r8
    mov64 r6, r8                                    r6 = r8
    mov64 r7, r2                                    r7 = r2
    jsgt r5, -1, lbb_25730                          if (r5 as i64) > (-1 as i32 as i64) { pc += 17 }
    add64 r4, -1                                    r4 += -1   ///  r4 = r4.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r6, r4                                    r6 = r4
    and64 r6, 63                                    r6 &= 63   ///  r6 = r6.and(63)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    lsh64 r0, r6                                    r0 <<= r6   ///  r0 = r0.wrapping_shl(r6 as u32)
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    mov64 r7, r2                                    r7 = r2
    sub64 r7, r5                                    r7 -= r5   ///  r7 = r7.wrapping_sub(r5)
    mov64 r6, r0                                    r6 = r0
    jsgt r7, -1, lbb_25724                          if (r7 as i64) > (-1 as i32 as i64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_25724:
    jsgt r7, -1, lbb_25726                          if (r7 as i64) > (-1 as i32 as i64) { pc += 1 }
    mov64 r7, r2                                    r7 = r2
lbb_25726:
    or64 r6, r8                                     r6 |= r8   ///  r6 = r6.or(r8)
    mov64 r2, r7                                    r2 = r7
    mov64 r8, r6                                    r8 = r6
    jlt r7, r3, lbb_25749                           if r7 < r3 { pc += 19 }
lbb_25730:
    add64 r0, -1                                    r0 += -1   ///  r0 = r0.wrapping_add(-1 as i32 as i64 as u64)
    jeq r4, 0, lbb_25743                            if r4 == (0 as i32 as i64 as u64) { pc += 11 }
    mov64 r2, r4                                    r2 = r4
    ja lbb_25736                                    if true { pc += 2 }
lbb_25734:
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    jeq r2, 0, lbb_25743                            if r2 == (0 as i32 as i64 as u64) { pc += 7 }
lbb_25736:
    lsh64 r7, 1                                     r7 <<= 1   ///  r7 = r7.wrapping_shl(1)
    mov64 r3, r7                                    r3 = r7
    sub64 r3, r5                                    r3 -= r5   ///  r3 = r3.wrapping_sub(r5)
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    jslt r3, 0, lbb_25734                           if (r3 as i64) < (0 as i32 as i64) { pc += -7 }
    mov64 r7, r3                                    r7 = r3
    ja lbb_25734                                    if true { pc += -9 }
lbb_25743:
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    mov64 r2, r7                                    r2 = r7
    rsh64 r2, r4                                    r2 >>= r4   ///  r2 = r2.wrapping_shr(r4 as u32)
    and64 r7, r0                                    r7 &= r0   ///  r7 = r7.and(r0)
    or64 r7, r6                                     r7 |= r6   ///  r7 = r7.or(r6)
    mov64 r8, r7                                    r8 = r7
lbb_25749:
    stxdw [r1+0x8], r2                      
    stxdw [r1+0x0], r8                      
    exit                                    

function_25752:
    lddw r3, 0x7fffffffffffffff                     r3 load str located at 9223372036854775807
    mov64 r2, r1                                    r2 = r1
    and64 r2, r3                                    r2 &= r3   ///  r2 = r2.and(r3)
    lddw r3, 0xb810000000000000                     r3 load str located at -5183643171103440896
    mov64 r4, r2                                    r4 = r2
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    lddw r3, 0xc7f0000000000000                     r3 load str located at -4039728865751334912
    mov64 r5, r2                                    r5 = r2
    add64 r5, r3                                    r5 += r3   ///  r5 = r5.wrapping_add(r3)
    jlt r5, r4, lbb_25774                           if r5 < r4 { pc += 9 }
    lddw r3, 0x7ff0000000000000                     r3 load str located at 9218868437227405312
    jgt r2, r3, lbb_25769                           if r2 > r3 { pc += 1 }
    ja lbb_25782                                    if true { pc += 13 }
lbb_25769:
    rsh64 r2, 29                                    r2 >>= 29   ///  r2 = r2.wrapping_shr(29)
    and64 r2, 4194303                               r2 &= 4194303   ///  r2 = r2.and(4194303)
    or64 r2, 2143289344                             r2 |= 2143289344   ///  r2 = r2.or(2143289344)
    mov64 r0, r2                                    r0 = r2
    ja lbb_25826                                    if true { pc += 52 }
lbb_25774:
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 29                                    r3 >>= 29   ///  r3 = r3.wrapping_shr(29)
    and64 r2, 536870911                             r2 &= 536870911   ///  r2 = r2.and(536870911)
    jgt r2, 268435456, lbb_25779                    if r2 > (268435456 as i32 as i64 as u64) { pc += 1 }
    ja lbb_25820                                    if true { pc += 41 }
lbb_25779:
    add64 r3, 1073741825                            r3 += 1073741825   ///  r3 = r3.wrapping_add(1073741825 as i32 as i64 as u64)
    mov64 r0, r3                                    r0 = r3
    ja lbb_25826                                    if true { pc += 44 }
lbb_25782:
    mov64 r0, 2139095040                            r0 = 2139095040 as i32 as i64 as u64
    lddw r3, 0x47efffffffffffff                     r3 load str located at 5183643171103440895
    jgt r2, r3, lbb_25826                           if r2 > r3 { pc += 40 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    rsh64 r2, 52                                    r2 >>= 52   ///  r2 = r2.wrapping_shr(52)
    mov64 r3, 897                                   r3 = 897 as i32 as i64 as u64
    sub64 r3, r2                                    r3 -= r2   ///  r3 = r3.wrapping_sub(r2)
    jgt r3, 52, lbb_25826                           if r3 > (52 as i32 as i64 as u64) { pc += 35 }
    lddw r4, 0xfffffffffffff                        r4 load str located at 4503599627370495
    mov64 r0, r1                                    r0 = r1
    and64 r0, r4                                    r0 &= r4   ///  r0 = r0.and(r4)
    lddw r4, 0x10000000000000                       r4 load str located at 4503599627370496
    or64 r0, r4                                     r0 |= r4   ///  r0 = r0.or(r4)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    mov64 r4, r0                                    r4 = r0
    lsh64 r4, r2                                    r4 <<= r2   ///  r4 = r4.wrapping_shl(r2 as u32)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_25805                            if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_25805:
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    rsh64 r0, r3                                    r0 >>= r3   ///  r0 = r0.wrapping_shr(r3 as u32)
    mov64 r3, r0                                    r3 = r0
    and64 r3, 536870911                             r3 &= 536870911   ///  r3 = r3.and(536870911)
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    rsh64 r0, 29                                    r0 >>= 29   ///  r0 = r0.wrapping_shr(29)
    jgt r3, 268435456, lbb_25832                    if r3 > (268435456 as i32 as i64 as u64) { pc += 19 }
    jeq r3, 268435456, lbb_25815                    if r3 == (268435456 as i32 as i64 as u64) { pc += 1 }
    ja lbb_25826                                    if true { pc += 11 }
lbb_25815:
    mov64 r2, r0                                    r2 = r0
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    add64 r2, r0                                    r2 += r0   ///  r2 = r2.wrapping_add(r0)
    mov64 r0, r2                                    r0 = r2
    ja lbb_25826                                    if true { pc += 6 }
lbb_25820:
    mov64 r0, r3                                    r0 = r3
    add64 r0, 1073741824                            r0 += 1073741824   ///  r0 = r0.wrapping_add(1073741824 as i32 as i64 as u64)
    jeq r2, 268435456, lbb_25824                    if r2 == (268435456 as i32 as i64 as u64) { pc += 1 }
    ja lbb_25826                                    if true { pc += 2 }
lbb_25824:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
lbb_25826:
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    lddw r2, 0x80000000                             r2 load str located at 2147483648
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    or64 r0, r1                                     r0 |= r1   ///  r0 = r0.or(r1)
    exit                                    
lbb_25832:
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    ja lbb_25826                                    if true { pc += -8 }

function_25834:
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    mov64 r3, r1                                    r3 = r1
    and64 r3, 2147483647                            r3 &= 2147483647   ///  r3 = r3.and(2147483647)
    jgt r3, 2139095040, lbb_25865                   if r3 > (2139095040 as i32 as i64 as u64) { pc += 26 }
    mov64 r4, r2                                    r4 = r2
    and64 r4, 2147483647                            r4 &= 2147483647   ///  r4 = r4.and(2147483647)
    jgt r4, 2139095040, lbb_25865                   if r4 > (2139095040 as i32 as i64 as u64) { pc += 23 }
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r4, 0, lbb_25865                            if r4 == (0 as i32 as i64 as u64) { pc += 20 }
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r3, r2                                    r3 = r2
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    jsgt r3, -1, lbb_25858                          if (r3 as i64) > (-1 as i32 as i64) { pc += 6 }
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    jsgt r1, r2, lbb_25865                          if (r1 as i64) > (r2 as i64) { pc += 10 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_25864                           if r1 == r2 { pc += 7 }
    ja lbb_25865                                    if true { pc += 7 }
lbb_25858:
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    jslt r1, r2, lbb_25865                          if (r1 as i64) < (r2 as i64) { pc += 4 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_25864                           if r1 == r2 { pc += 1 }
    ja lbb_25865                                    if true { pc += 1 }
lbb_25864:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_25865:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    exit                                    

function_25868:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r3, r1                                    r3 = r1
    and64 r3, 2147483647                            r3 &= 2147483647   ///  r3 = r3.and(2147483647)
    jgt r3, 2139095040, lbb_25898                   if r3 > (2139095040 as i32 as i64 as u64) { pc += 26 }
    mov64 r4, r2                                    r4 = r2
    and64 r4, 2147483647                            r4 &= 2147483647   ///  r4 = r4.and(2147483647)
    jgt r4, 2139095040, lbb_25898                   if r4 > (2139095040 as i32 as i64 as u64) { pc += 23 }
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r4, 0, lbb_25898                            if r4 == (0 as i32 as i64 as u64) { pc += 20 }
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r3, r2                                    r3 = r2
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    jsgt r3, -1, lbb_25891                          if (r3 as i64) > (-1 as i32 as i64) { pc += 6 }
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    jsgt r1, r2, lbb_25898                          if (r1 as i64) > (r2 as i64) { pc += 10 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_25897                           if r1 == r2 { pc += 7 }
    ja lbb_25898                                    if true { pc += 7 }
lbb_25891:
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    jslt r1, r2, lbb_25898                          if (r1 as i64) < (r2 as i64) { pc += 4 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_25897                           if r1 == r2 { pc += 1 }
    ja lbb_25898                                    if true { pc += 1 }
lbb_25897:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_25898:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    exit                                    

function_25901:
    call function_23465                     
    exit                                    

function_25903:
    stxdw [r10-0x10], r3                    
    stxdw [r10-0x8], r1                     
    mov64 r1, r2                                    r1 = r2
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r7, r4                                    r7 = r4
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r6, r2                                    r6 = r2
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    mov64 r3, r7                                    r3 = r7
    mul64 r3, r1                                    r3 *= r1   ///  r3 = r3.wrapping_mul(r1)
    mul64 r7, r6                                    r7 *= r6   ///  r7 = r7.wrapping_mul(r6)
    mov64 r0, r4                                    r0 = r4
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r9, r0                                    r9 = r0
    mul64 r9, r1                                    r9 *= r1   ///  r9 = r9.wrapping_mul(r1)
    mov64 r1, r9                                    r1 = r9
    add64 r1, r7                                    r1 += r7   ///  r1 = r1.wrapping_add(r7)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jlt r1, r9, lbb_25925                           if r1 < r9 { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_25925:
    mov64 r9, r1                                    r9 = r1
    lsh64 r9, 32                                    r9 <<= 32   ///  r9 = r9.wrapping_shl(32)
    mov64 r7, r3                                    r7 = r3
    add64 r7, r9                                    r7 += r9   ///  r7 = r7.wrapping_add(r9)
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jlt r7, r3, lbb_25932                           if r7 < r3 { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_25932:
    ldxdw r3, [r10-0x8]                     
    stxdw [r3+0x0], r7                      
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    or64 r8, r1                                     r8 |= r1   ///  r8 = r8.or(r1)
    ldxdw r1, [r10-0x10]                    
    mul64 r4, r1                                    r4 *= r1   ///  r4 = r4.wrapping_mul(r1)
    mul64 r5, r2                                    r5 *= r2   ///  r5 = r5.wrapping_mul(r2)
    mul64 r0, r6                                    r0 *= r6   ///  r0 = r0.wrapping_mul(r6)
    add64 r0, r8                                    r0 += r8   ///  r0 = r0.wrapping_add(r8)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    add64 r0, r9                                    r0 += r9   ///  r0 = r0.wrapping_add(r9)
    add64 r5, r0                                    r5 += r0   ///  r5 = r5.wrapping_add(r0)
    stxdw [r3+0x8], r5                      
    exit                                    

function_25947:
    and64 r2, 2147483647                            r2 &= 2147483647   ///  r2 = r2.and(2147483647)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jgt r2, 2139095040, lbb_25952                   if r2 > (2139095040 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_25952:
    and64 r1, 2147483647                            r1 &= 2147483647   ///  r1 = r1.and(2147483647)
    jgt r1, 2139095040, lbb_25955                   if r1 > (2139095040 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_25955:
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    exit                                    

function_25957:
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    lddw r5, 0x7fffffffffffffff                     r5 load str located at 9223372036854775807
    mov64 r3, r1                                    r3 = r1
    and64 r3, r5                                    r3 &= r5   ///  r3 = r3.and(r5)
    lddw r6, 0x7ff0000000000000                     r6 load str located at 9218868437227405312
    jgt r3, r6, lbb_25988                           if r3 > r6 { pc += 22 }
    mov64 r4, r2                                    r4 = r2
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    jgt r4, r6, lbb_25988                           if r4 > r6 { pc += 19 }
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r4, 0, lbb_25988                            if r4 == (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r3, r2                                    r3 = r2
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    jsgt r3, -1, lbb_25981                          if (r3 as i64) > (-1 as i32 as i64) { pc += 6 }
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    jsgt r1, r2, lbb_25988                          if (r1 as i64) > (r2 as i64) { pc += 10 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_25987                           if r1 == r2 { pc += 7 }
    ja lbb_25988                                    if true { pc += 7 }
lbb_25981:
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    jslt r1, r2, lbb_25988                          if (r1 as i64) < (r2 as i64) { pc += 4 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_25987                           if r1 == r2 { pc += 1 }
    ja lbb_25988                                    if true { pc += 1 }
lbb_25987:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_25988:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    exit                                    

function_25991:
    mov64 r2, 64                                    r2 = 64 as i32 as i64 as u64
    jeq r1, 0, lbb_26036                            if r1 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 1                                     r2 >>= 1   ///  r2 = r2.wrapping_shr(1)
    mov64 r3, r1                                    r3 = r1
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    mov64 r2, r3                                    r2 = r3
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    mov64 r2, r3                                    r2 = r3
    rsh64 r2, 4                                     r2 >>= 4   ///  r2 = r2.wrapping_shr(4)
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    mov64 r2, r3                                    r2 = r3
    rsh64 r2, 8                                     r2 >>= 8   ///  r2 = r2.wrapping_shr(8)
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    mov64 r2, r3                                    r2 = r3
    rsh64 r2, 16                                    r2 >>= 16   ///  r2 = r2.wrapping_shr(16)
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    mov64 r2, r3                                    r2 = r3
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    lddw r2, 0x5555555555555555                     r2 load str located at 6148914691236517205
    mov64 r4, r3                                    r4 = r3
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    sub64 r3, r4                                    r3 -= r4   ///  r3 = r3.wrapping_sub(r4)
    lddw r4, 0x3333333333333333                     r4 load str located at 3689348814741910323
    mov64 r2, r3                                    r2 = r3
    and64 r2, r4                                    r2 &= r4   ///  r2 = r2.and(r4)
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    and64 r3, r4                                    r3 &= r4   ///  r3 = r3.and(r4)
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    lddw r3, 0xf0f0f0f0f0f0f0f                      r3 load str located at 1085102592571150095
    and64 r2, r3                                    r2 &= r3   ///  r2 = r2.and(r3)
    lddw r3, 0x101010101010101                      r3 load str located at 72340172838076673
    mul64 r2, r3                                    r2 *= r3   ///  r2 = r2.wrapping_mul(r3)
    rsh64 r2, 56                                    r2 >>= 56   ///  r2 = r2.wrapping_shr(56)
lbb_26036:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_26042                            if r1 == (0 as i32 as i64 as u64) { pc += 4 }
    mov64 r3, r2                                    r3 = r2
    lsh64 r3, 23                                    r3 <<= 23   ///  r3 = r3.wrapping_shl(23)
    mov64 r0, 1585446912                            r0 = 1585446912 as i32 as i64 as u64
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
lbb_26042:
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    lsh64 r1, r2                                    r1 <<= r2   ///  r1 = r1.wrapping_shl(r2 as u32)
    mov64 r3, r1                                    r3 = r1
    and64 r3, 65535                                 r3 &= 65535   ///  r3 = r3.and(65535)
    mov64 r2, r1                                    r2 = r1
    rsh64 r2, 8                                     r2 >>= 8   ///  r2 = r2.wrapping_shr(8)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 39                                    r3 >>= 39   ///  r3 = r3.wrapping_shr(39)
    rsh64 r1, 40                                    r1 >>= 40   ///  r1 = r1.wrapping_shr(40)
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
    xor64 r1, -1                                    r1 ^= -1   ///  r1 = r1.xor(-1)
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    sub64 r2, r3                                    r2 -= r3   ///  r2 = r2.wrapping_sub(r3)
    lddw r1, 0x80000000                             r1 load str located at 2147483648
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    rsh64 r2, 31                                    r2 >>= 31   ///  r2 = r2.wrapping_shr(31)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    exit                                    

function_26063:
    call function_23955                     
    exit                                    

function_26065:
    mov64 r6, r2                                    r6 = r2
    mov64 r7, r1                                    r7 = r1
    arsh64 r1, 63                                   r1 >>= 63 (signed)   ///  r1 = (r1 as i64).wrapping_shr(63)
    mov64 r2, r7                                    r2 = r7
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    sub64 r2, r1                                    r2 -= r1   ///  r2 = r2.wrapping_sub(r1)
    mov64 r1, r6                                    r1 = r6
    arsh64 r1, 63                                   r1 >>= 63 (signed)   ///  r1 = (r1 as i64).wrapping_shr(63)
    mov64 r3, r6                                    r3 = r6
    xor64 r3, r1                                    r3 ^= r1   ///  r3 = r3.xor(r1)
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_25608                     
    xor64 r6, r7                                    r6 ^= r7   ///  r6 = r6.xor(r7)
    ldxdw r1, [r10-0x10]                    
    mov64 r0, r1                                    r0 = r1
    neg64 r0                                        r0 = -r0   ///  r0 = (r0 as i64).wrapping_neg() as u64
    jslt r6, 0, lbb_26085                           if (r6 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r0, r1                                    r0 = r1
lbb_26085:
    exit                                    

function_26086:
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    lddw r5, 0x7fffffffffffffff                     r5 load str located at 9223372036854775807
    mov64 r3, r1                                    r3 = r1
    and64 r3, r5                                    r3 &= r5   ///  r3 = r3.and(r5)
    lddw r6, 0x7ff0000000000000                     r6 load str located at 9218868437227405312
    jgt r3, r6, lbb_26117                           if r3 > r6 { pc += 22 }
    mov64 r4, r2                                    r4 = r2
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    jgt r4, r6, lbb_26117                           if r4 > r6 { pc += 19 }
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r4, 0, lbb_26117                            if r4 == (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r3, r2                                    r3 = r2
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    jsgt r3, -1, lbb_26110                          if (r3 as i64) > (-1 as i32 as i64) { pc += 6 }
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    jsgt r1, r2, lbb_26117                          if (r1 as i64) > (r2 as i64) { pc += 10 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_26116                           if r1 == r2 { pc += 7 }
    ja lbb_26117                                    if true { pc += 7 }
lbb_26110:
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    jslt r1, r2, lbb_26117                          if (r1 as i64) < (r2 as i64) { pc += 4 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_26116                           if r1 == r2 { pc += 1 }
    ja lbb_26117                                    if true { pc += 1 }
lbb_26116:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_26117:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    exit                                    

function_26120:
    mov64 r5, r4                                    r5 = r4
    and64 r5, 64                                    r5 &= 64   ///  r5 = r5.and(64)
    jne r5, 0, lbb_26134                            if r5 != (0 as i32 as i64 as u64) { pc += 11 }
    jeq r4, 0, lbb_26138                            if r4 == (0 as i32 as i64 as u64) { pc += 14 }
    mov64 r5, r4                                    r5 = r4
    and64 r5, 63                                    r5 &= 63   ///  r5 = r5.and(63)
    lsh64 r3, r5                                    r3 <<= r5   ///  r3 = r3.wrapping_shl(r5 as u32)
    neg64 r4                                        r4 = -r4   ///  r4 = (r4 as i64).wrapping_neg() as u64
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    mov64 r0, r2                                    r0 = r2
    rsh64 r0, r4                                    r0 >>= r4   ///  r0 = r0.wrapping_shr(r4 as u32)
    or64 r3, r0                                     r3 |= r0   ///  r3 = r3.or(r0)
    lsh64 r2, r5                                    r2 <<= r5   ///  r2 = r2.wrapping_shl(r5 as u32)
    ja lbb_26138                                    if true { pc += 4 }
lbb_26134:
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    mov64 r3, r2                                    r3 = r2
    lsh64 r3, r4                                    r3 <<= r4   ///  r3 = r3.wrapping_shl(r4 as u32)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_26138:
    stxdw [r1+0x8], r3                      
    stxdw [r1+0x0], r2                      
    exit                                    

function_26141:
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    mov64 r3, r1                                    r3 = r1
    and64 r3, 2147483647                            r3 &= 2147483647   ///  r3 = r3.and(2147483647)
    jgt r3, 2139095040, lbb_26172                   if r3 > (2139095040 as i32 as i64 as u64) { pc += 26 }
    mov64 r4, r2                                    r4 = r2
    and64 r4, 2147483647                            r4 &= 2147483647   ///  r4 = r4.and(2147483647)
    jgt r4, 2139095040, lbb_26172                   if r4 > (2139095040 as i32 as i64 as u64) { pc += 23 }
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r4, 0, lbb_26172                            if r4 == (0 as i32 as i64 as u64) { pc += 20 }
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    arsh64 r2, 32                                   r2 >>= 32 (signed)   ///  r2 = (r2 as i64).wrapping_shr(32)
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    arsh64 r1, 32                                   r1 >>= 32 (signed)   ///  r1 = (r1 as i64).wrapping_shr(32)
    mov64 r3, r2                                    r3 = r2
    and64 r3, r1                                    r3 &= r1   ///  r3 = r3.and(r1)
    jsgt r3, -1, lbb_26165                          if (r3 as i64) > (-1 as i32 as i64) { pc += 6 }
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    jsgt r1, r2, lbb_26172                          if (r1 as i64) > (r2 as i64) { pc += 10 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_26171                           if r1 == r2 { pc += 7 }
    ja lbb_26172                                    if true { pc += 7 }
lbb_26165:
    lddw r0, 0xffffffff                             r0 load str located at 4294967295
    jslt r1, r2, lbb_26172                          if (r1 as i64) < (r2 as i64) { pc += 4 }
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r1, r2, lbb_26171                           if r1 == r2 { pc += 1 }
    ja lbb_26172                                    if true { pc += 1 }
lbb_26171:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_26172:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    arsh64 r0, 32                                   r0 >>= 32 (signed)   ///  r0 = (r0 as i64).wrapping_shr(32)
    exit                                    

function_26175:
    call function_24571                     
    exit                                    
