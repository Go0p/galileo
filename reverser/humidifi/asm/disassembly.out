function_0:
    ldxdw r1, [r1+0x0]                      
    ldxw r3, [r2+0x34]                      
    mov64 r4, r3                                    r4 = r3
    and64 r4, 16                                    r4 &= 16   ///  r4 = r4.and(16)
    jne r4, 0, lbb_9                                if r4 != (0 as i32 as i64 as u64) { pc += 4 }
    and64 r3, 32                                    r3 &= 32   ///  r3 = r3.and(32)
    jne r3, 0, lbb_11                               if r3 != (0 as i32 as i64 as u64) { pc += 4 }
    call function_16785                     
    ja lbb_12                                       if true { pc += 3 }
lbb_9:
    call function_16726                     
    ja lbb_12                                       if true { pc += 1 }
lbb_11:
    call function_16730                     
lbb_12:
    exit                                    

function_13:
    mov64 r3, r2                                    r3 = r2
    ldxdw r2, [r1+0x8]                      
    ldxdw r1, [r1+0x0]                      
    call function_16410                     
    exit                                    

function_18:
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
    ldxdw r7, [r1+0x48]                     
    ldxdw r2, [r1+0x50]                     
    stxdw [r10-0x10], r2                    
    jge r7, r2, lbb_119                             if r7 >= r2 { pc += 96 }
    mov64 r3, r7                                    r3 = r7
    mul64 r3, 56                                    r3 *= 56   ///  r3 = r3.wrapping_mul(56 as u64)
    ldxdw r4, [r1+0x38]                     
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxdw r3, [r1+0x20]                     
    mov64 r5, r7                                    r5 = r7
    add64 r5, r3                                    r5 += r3   ///  r5 = r5.wrapping_add(r3)
    mov64 r3, r5                                    r3 = r5
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    ldxdw r0, [r1+0x0]                      
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    lsh64 r5, 4                                     r5 <<= 4   ///  r5 = r5.wrapping_shl(4)
    stxdw [r10-0x18], r1                    
    ldxdw r3, [r1+0x10]                     
    add64 r5, r3                                    r5 += r3   ///  r5 = r5.wrapping_add(r3)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_61                                       if true { pc += 21 }
lbb_40:
    ldxdw r1, [r6+0x50]                     
    mov64 r7, r6                                    r7 = r6
    add64 r7, 40                                    r7 += 40   ///  r7 = r7.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r4+0x20], r7                     
    mov64 r7, r6                                    r7 = r6
    add64 r7, 88                                    r7 += 88   ///  r7 = r7.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r4+0x18], r7                     
    stxdw [r4+0x10], r1                     
    add64 r6, 72                                    r6 += 72   ///  r6 = r6.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r4+0x8], r6                      
    stxdw [r4+0x0], r2                      
    stxb [r4+0x32], r9                      
    stxb [r4+0x31], r3                      
    stxb [r4+0x30], r8                      
    add64 r5, 16                                    r5 += 16   ///  r5 = r5.wrapping_add(16 as i32 as i64 as u64)
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
    stdw [r4+0x28], 0                       
    add64 r4, 56                                    r4 += 56   ///  r4 = r4.wrapping_add(56 as i32 as i64 as u64)
    ldxdw r7, [r10-0x8]                     
    ldxdw r1, [r10-0x10]                    
    jge r7, r1, lbb_115                             if r7 >= r1 { pc += 54 }
lbb_61:
    ldxdw r3, [r0+0x0]                      
    ldxdw r6, [r3+0x0]                      
    mov64 r2, r6                                    r2 = r6
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r3, [r5-0x8]                      
    ldxdw r8, [r3+0x0]                      
    ldxdw r9, [r6+0x8]                      
    jne r9, r8, lbb_79                              if r9 != r8 { pc += 10 }
    ldxdw r8, [r3+0x8]                      
    ldxdw r9, [r2+0x8]                      
    jne r9, r8, lbb_79                              if r9 != r8 { pc += 7 }
    ldxdw r8, [r3+0x10]                     
    ldxdw r9, [r2+0x10]                     
    jne r9, r8, lbb_79                              if r9 != r8 { pc += 4 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r3, [r3+0x18]                     
    ldxdw r9, [r2+0x18]                     
    jeq r9, r3, lbb_80                              if r9 == r3 { pc += 1 }
lbb_79:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
lbb_80:
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    jne r8, 0, lbb_111                              if r8 != (0 as i32 as i64 as u64) { pc += 29 }
    ldxb r8, [r5+0x0]                       
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jne r8, 0, lbb_86                               if r8 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 119                                   r3 = 119 as i32 as i64 as u64
lbb_86:
    ldxb r8, [r6+0x0]                       
    or64 r3, r8                                     r3 |= r8   ///  r3 = r3.or(r8)
    and64 r3, 255                                   r3 &= 255   ///  r3 = r3.and(255)
    jne r3, 255, lbb_113                            if r3 != (255 as i32 as i64 as u64) { pc += 23 }
    ldxb r3, [r6+0x1]                       
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jeq r3, 0, lbb_101                              if r3 == (0 as i32 as i64 as u64) { pc += 7 }
    ldxb r1, [r6+0x2]                       
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_105                              if r1 == (0 as i32 as i64 as u64) { pc += 8 }
lbb_97:
    stxdw [r10-0x8], r7                     
    ldxb r1, [r6+0x3]                       
    jne r1, 0, lbb_40                               if r1 != (0 as i32 as i64 as u64) { pc += -60 }
    ja lbb_109                                      if true { pc += 8 }
lbb_101:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxb r1, [r6+0x2]                       
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_97                               if r1 != (0 as i32 as i64 as u64) { pc += -8 }
lbb_105:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    stxdw [r10-0x8], r7                     
    ldxb r1, [r6+0x3]                       
    jne r1, 0, lbb_40                               if r1 != (0 as i32 as i64 as u64) { pc += -69 }
lbb_109:
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ja lbb_40                                       if true { pc += -71 }
lbb_111:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ja lbb_117                                      if true { pc += 4 }
lbb_113:
    mov64 r0, 11                                    r0 = 11 as i32 as i64 as u64
    ja lbb_117                                      if true { pc += 2 }
lbb_115:
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
    ldxdw r7, [r10-0x10]                    
lbb_117:
    ldxdw r1, [r10-0x18]                    
    stxdw [r1+0x48], r7                     
lbb_119:
    exit                                    

function_120:
    mov64 r9, r5                                    r9 = r5
    mov64 r6, r4                                    r6 = r4
    stxdw [r10-0x58], r3                    
    mov64 r8, r2                                    r8 = r2
    stxdw [r10-0x48], r1                    
    mov64 r7, r3                                    r7 = r3
    arsh64 r7, 63                                   r7 >>= 63 (signed)   ///  r7 = (r7 as i64).wrapping_shr(63)
    mov64 r5, r6                                    r5 = r6
    arsh64 r5, 63                                   r5 >>= 63 (signed)   ///  r5 = (r5 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    mov64 r3, r7                                    r3 = r7
    stxdw [r10-0x50], r5                    
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r8                                    r4 = r8
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r2, r9                                    r2 = r9
    arsh64 r9, 63                                   r9 >>= 63 (signed)   ///  r9 = (r9 as i64).wrapping_shr(63)
    mov64 r4, r8                                    r4 = r8
    mov64 r6, r4                                    r6 = r4
    arsh64 r6, 63                                   r6 >>= 63 (signed)   ///  r6 = (r6 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r8, r2                                    r8 = r2
    mov64 r3, r9                                    r3 = r9
    mov64 r5, r6                                    r5 = r6
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r9                                    r3 = r9
    ldxdw r9, [r10-0x58]                    
    mov64 r4, r9                                    r4 = r9
    mov64 r5, r7                                    r5 = r7
    call function_17083                     
    and64 r6, r8                                    r6 &= r8   ///  r6 = r6.and(r8)
    ldxdw r3, [r10-0x8]                     
    add64 r3, r6                                    r3 += r6   ///  r3 = r3.wrapping_add(r6)
    ldxdw r1, [r10-0x28]                    
    ldxdw r2, [r10-0x40]                    
    mov64 r4, r2                                    r4 = r2
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    ldxdw r6, [r10-0x10]                    
    mov64 r1, r6                                    r1 = r6
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r1, r6, lbb_175                             if r1 < r6 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_175:
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r4, r2, lbb_179                             if r4 < r2 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_179:
    ldxdw r2, [r10-0x50]                    
    and64 r2, r9                                    r2 &= r9   ///  r2 = r2.and(r9)
    ldxdw r4, [r10-0x38]                    
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    ldxdw r2, [r10-0x20]                    
    mov64 r6, r4                                    r6 = r4
    add64 r6, r2                                    r6 += r2   ///  r6 = r6.wrapping_add(r2)
    mov64 r2, r6                                    r2 = r6
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r2, r6, lbb_192                             if r2 < r6 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_192:
    jlt r6, r4, lbb_194                             if r6 < r4 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_194:
    arsh64 r4, 63                                   r4 >>= 63 (signed)   ///  r4 = (r4 as i64).wrapping_shr(63)
    ldxdw r6, [r10-0x18]                    
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    mov64 r3, r4                                    r3 = r4
    lsh64 r3, 16                                    r3 <<= 16   ///  r3 = r3.wrapping_shl(16)
    mov64 r5, r2                                    r5 = r2
    rsh64 r5, 48                                    r5 >>= 48   ///  r5 = r5.wrapping_shr(48)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    arsh64 r4, 48                                   r4 >>= 48 (signed)   ///  r4 = (r4 as i64).wrapping_shr(48)
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    mov64 r3, r2                                    r3 = r2
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    xor64 r5, r3                                    r5 ^= r3   ///  r5 = r5.xor(r3)
    or64 r5, r4                                     r5 |= r4   ///  r5 = r5.or(r4)
    ldxdw r0, [r10-0x48]                    
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jne r5, 0, lbb_226                              if r5 != (0 as i32 as i64 as u64) { pc += 10 }
    ldxdw r3, [r10-0x30]                    
    rsh64 r3, 48                                    r3 >>= 48   ///  r3 = r3.wrapping_shr(48)
    mov64 r4, r1                                    r4 = r1
    lsh64 r4, 16                                    r4 <<= 16   ///  r4 = r4.wrapping_shl(16)
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    rsh64 r1, 48                                    r1 >>= 48   ///  r1 = r1.wrapping_shr(48)
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    stxdw [r0+0x8], r4                      
    stxdw [r0+0x10], r2                     
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_226:
    stxdw [r0+0x0], r3                      
    exit                                    

function_228:
    mov64 r7, r3                                    r7 = r3
    mov64 r6, r2                                    r6 = r2
    jslt r7, 0, lbb_435                             if (r7 as i64) < (0 as i32 as i64) { pc += 204 }
    mov64 r2, r6                                    r2 = r6
    or64 r2, r7                                     r2 |= r7   ///  r2 = r2.or(r7)
    jeq r2, 0, lbb_281                              if r2 == (0 as i32 as i64 as u64) { pc += 47 }
    stxdw [r10-0x30], r1                    
    lddw r4, 0x5555555555555555                     r4 load str located at 6148914691236517205
    lddw r3, 0x3333333333333333                     r3 load str located at 3689348814741910323
    lddw r2, 0xf0f0f0f0f0f0f0f                      r2 load str located at 1085102592571150095
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
    jne r7, 0, lbb_284                              if r7 != (0 as i32 as i64 as u64) { pc += 40 }
    mov64 r0, r6                                    r0 = r6
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    mov64 r5, r6                                    r5 = r6
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r4                                    r0 &= r4   ///  r0 = r0.and(r4)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    mov64 r4, r5                                    r4 = r5
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, r3                                    r5 &= r3   ///  r5 = r5.and(r3)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    mul64 r4, r1                                    r4 *= r1   ///  r4 = r4.wrapping_mul(r1)
    rsh64 r4, 56                                    r4 >>= 56   ///  r4 = r4.wrapping_shr(56)
    add64 r4, 64                                    r4 += 64   ///  r4 = r4.wrapping_add(64 as i32 as i64 as u64)
    ja lbb_319                                      if true { pc += 38 }
lbb_281:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ja lbb_432                                      if true { pc += 148 }
lbb_284:
    mov64 r0, r7                                    r0 = r7
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    mov64 r5, r7                                    r5 = r7
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r4                                    r0 &= r4   ///  r0 = r0.and(r4)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    mov64 r4, r5                                    r4 = r5
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, r3                                    r5 &= r3   ///  r5 = r5.and(r3)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    mul64 r4, r1                                    r4 *= r1   ///  r4 = r4.wrapping_mul(r1)
    rsh64 r4, 56                                    r4 >>= 56   ///  r4 = r4.wrapping_shr(56)
lbb_319:
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    and64 r4, 126                                   r4 &= 126   ///  r4 = r4.and(126)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_17074                     
    ldxdw r1, [r10-0x8]                     
    ldxdw r2, [r10-0x10]                    
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ja lbb_345                                      if true { pc += 14 }
lbb_331:
    sub64 r7, r6                                    r7 -= r6   ///  r7 = r7.wrapping_sub(r6)
    ldxdw r3, [r10-0x28]                    
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxdw r6, [r10-0x18]                    
    sub64 r6, r0                                    r6 -= r0   ///  r6 = r6.wrapping_sub(r0)
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    mov64 r3, r1                                    r3 = r1
    lsh64 r3, 62                                    r3 <<= 62   ///  r3 = r3.wrapping_shl(62)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    mov64 r9, r5                                    r9 = r5
    mov64 r3, r4                                    r3 = r4
    jeq r8, 0, lbb_426                              if r8 == (0 as i32 as i64 as u64) { pc += 81 }
lbb_345:
    mov64 r5, r2                                    r5 = r2
    add64 r5, r9                                    r5 += r9   ///  r5 = r5.wrapping_add(r9)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r5, r2, lbb_350                             if r5 < r2 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_350:
    stxdw [r10-0x18], r6                    
    mov64 r0, r1                                    r0 = r1
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    add64 r0, r4                                    r0 += r4   ///  r0 = r0.wrapping_add(r4)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jge r7, r0, lbb_394                             if r7 >= r0 { pc += 38 }
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    ldxdw r8, [r10-0x18]                    
    jge r8, r5, lbb_398                             if r8 >= r5 { pc += 39 }
lbb_359:
    jne r7, r0, lbb_400                             if r7 != r0 { pc += 40 }
lbb_360:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jeq r6, 0, lbb_403                              if r6 == (0 as i32 as i64 as u64) { pc += 41 }
lbb_362:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jeq r6, 0, lbb_406                              if r6 == (0 as i32 as i64 as u64) { pc += 42 }
lbb_364:
    stxdw [r10-0x20], r8                    
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r6, 0, lbb_410                              if r6 == (0 as i32 as i64 as u64) { pc += 43 }
lbb_367:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r6, 0, lbb_370                              if r6 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_369:
    mov64 r8, r2                                    r8 = r2
lbb_370:
    rsh64 r9, 1                                     r9 >>= 1   ///  r9 = r9.wrapping_shr(1)
    mov64 r5, r3                                    r5 = r3
    lsh64 r5, 63                                    r5 <<= 63   ///  r5 = r5.wrapping_shl(63)
    or64 r9, r5                                     r9 |= r5   ///  r9 = r9.or(r5)
    mov64 r5, r8                                    r5 = r8
    add64 r5, r9                                    r5 += r9   ///  r5 = r5.wrapping_add(r9)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jge r5, r8, lbb_414                             if r5 >= r8 { pc += 36 }
    stxdw [r10-0x28], r6                    
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    ldxdw r8, [r10-0x18]                    
    jge r8, r0, lbb_419                             if r8 >= r0 { pc += 37 }
lbb_382:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jle r2, 3, lbb_422                              if r2 <= (3 as i32 as i64 as u64) { pc += 38 }
lbb_384:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_387                              if r1 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_386:
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_387:
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxdw r3, [r10-0x20]                    
    sub64 r7, r3                                    r7 -= r3   ///  r7 = r7.wrapping_sub(r3)
    jeq r1, 0, lbb_331                              if r1 == (0 as i32 as i64 as u64) { pc += -61 }
    mov64 r8, r9                                    r8 = r9
    ja lbb_331                                      if true { pc += -63 }
lbb_394:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    ldxdw r8, [r10-0x18]                    
    jlt r8, r5, lbb_359                             if r8 < r5 { pc += -39 }
lbb_398:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jeq r7, r0, lbb_360                             if r7 == r0 { pc += -40 }
lbb_400:
    mov64 r6, r4                                    r6 = r4
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jne r6, 0, lbb_362                              if r6 != (0 as i32 as i64 as u64) { pc += -41 }
lbb_403:
    mov64 r4, r1                                    r4 = r1
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r6, 0, lbb_364                              if r6 != (0 as i32 as i64 as u64) { pc += -42 }
lbb_406:
    mov64 r8, r0                                    r8 = r0
    stxdw [r10-0x20], r8                    
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jne r6, 0, lbb_367                              if r6 != (0 as i32 as i64 as u64) { pc += -43 }
lbb_410:
    mov64 r0, r5                                    r0 = r5
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jeq r6, 0, lbb_369                              if r6 == (0 as i32 as i64 as u64) { pc += -44 }
    ja lbb_370                                      if true { pc += -44 }
lbb_414:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    stxdw [r10-0x28], r6                    
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    ldxdw r8, [r10-0x18]                    
    jlt r8, r0, lbb_382                             if r8 < r0 { pc += -37 }
lbb_419:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jgt r2, 3, lbb_384                              if r2 > (3 as i32 as i64 as u64) { pc += -38 }
lbb_422:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_386                              if r1 == (0 as i32 as i64 as u64) { pc += -39 }
    ja lbb_387                                      if true { pc += -39 }
lbb_426:
    lsh64 r4, 24                                    r4 <<= 24   ///  r4 = r4.wrapping_shl(24)
    mov64 r1, r5                                    r1 = r5
    rsh64 r1, 40                                    r1 >>= 40   ///  r1 = r1.wrapping_shr(40)
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    lsh64 r5, 24                                    r5 <<= 24   ///  r5 = r5.wrapping_shl(24)
    ldxdw r1, [r10-0x30]                    
lbb_432:
    stxdw [r1+0x0], r5                      
    stxdw [r1+0x8], r4                      
    exit                                    
lbb_435:
    lddw r1, 0x100022eb3 --> b"fixed point square root of a negative number"        r1 load str located at 4295110323
    mov64 r2, 44                                    r2 = 44 as i32 as i64 as u64
    lddw r3, 0x100023958 --> b"\x00\x00\x00\x00x-\x02\x00\x0a\x00\x00\x00\x00\x00\x00\x00\xc1\x00\x00\x0…        r3 load str located at 4295113048
    call function_15481                     
    ldxw r3, [r1+0x0]                       
    jsgt r3, 12, lbb_452                            if (r3 as i64) > (12 as i32 as i64) { pc += 9 }
    jsle r3, 5, lbb_461                             if (r3 as i64) <= (5 as i32 as i64) { pc += 17 }
    mov64 r1, r2                                    r1 = r2
    jsle r3, 8, lbb_476                             if (r3 as i64) <= (8 as i32 as i64) { pc += 30 }
    jsgt r3, 10, lbb_501                            if (r3 as i64) > (10 as i32 as i64) { pc += 54 }
    jne r3, 9, lbb_549                              if r3 != (9 as i32 as i64 as u64) { pc += 101 }
    lddw r2, 0x100022f83 --> b"UninitializedAccount"        r2 load str located at 4295110531
    mov64 r3, 20                                    r3 = 20 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += 111 }
lbb_452:
    mov64 r1, r2                                    r1 = r2
    jsle r3, 18, lbb_469                            if (r3 as i64) <= (18 as i32 as i64) { pc += 15 }
    jsle r3, 21, lbb_482                            if (r3 as i64) <= (21 as i32 as i64) { pc += 27 }
    jsgt r3, 23, lbb_506                            if (r3 as i64) > (23 as i32 as i64) { pc += 50 }
    jne r3, 22, lbb_553                             if r3 != (22 as i32 as i64 as u64) { pc += 96 }
    lddw r2, 0x100023093 --> b"InvalidAccountOwner"        r2 load str located at 4295110803
    mov64 r3, 19                                    r3 = 19 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += 102 }
lbb_461:
    jsgt r3, 2, lbb_488                             if (r3 as i64) > (2 as i32 as i64) { pc += 26 }
    jeq r3, 0, lbb_518                              if r3 == (0 as i32 as i64 as u64) { pc += 55 }
    mov64 r1, r2                                    r1 = r2
    jne r3, 1, lbb_565                              if r3 != (1 as i32 as i64 as u64) { pc += 100 }
    lddw r2, 0x100022ee5 --> b"InvalidArgument"        r2 load str located at 4295110373
    mov64 r3, 15                                    r3 = 15 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += 94 }
lbb_469:
    jsgt r3, 15, lbb_495                            if (r3 as i64) > (15 as i32 as i64) { pc += 25 }
    jeq r3, 13, lbb_530                             if r3 == (13 as i32 as i64 as u64) { pc += 59 }
    jne r3, 14, lbb_569                             if r3 != (14 as i32 as i64 as u64) { pc += 97 }
    lddw r2, 0x100022fdf --> b"BorshIoError"        r2 load str located at 4295110623
    mov64 r3, 12                                    r3 = 12 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += 87 }
lbb_476:
    jeq r3, 6, lbb_511                              if r3 == (6 as i32 as i64 as u64) { pc += 34 }
    jne r3, 7, lbb_541                              if r3 != (7 as i32 as i64 as u64) { pc += 63 }
    lddw r2, 0x100022f52 --> b"MissingRequiredSignature"        r2 load str located at 4295110482
    mov64 r3, 24                                    r3 = 24 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += 81 }
lbb_482:
    jeq r3, 19, lbb_514                             if r3 == (19 as i32 as i64 as u64) { pc += 31 }
    jne r3, 20, lbb_545                             if r3 != (20 as i32 as i64 as u64) { pc += 61 }
    lddw r2, 0x10002304c --> b"MaxInstructionTraceLengthExceeded"        r2 load str located at 4295110732
    mov64 r3, 33                                    r3 = 33 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += 75 }
lbb_488:
    mov64 r1, r2                                    r1 = r2
    jeq r3, 3, lbb_534                              if r3 == (3 as i32 as i64 as u64) { pc += 44 }
    jne r3, 4, lbb_573                              if r3 != (4 as i32 as i64 as u64) { pc += 82 }
    lddw r2, 0x100022f1c --> b"AccountDataTooSmall"        r2 load str located at 4295110428
    mov64 r3, 19                                    r3 = 19 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += 68 }
lbb_495:
    jeq r3, 16, lbb_537                             if r3 == (16 as i32 as i64 as u64) { pc += 41 }
    jne r3, 17, lbb_577                             if r3 != (17 as i32 as i64 as u64) { pc += 80 }
    lddw r2, 0x100023010 --> b"IllegalOwner"        r2 load str located at 4295110672
    mov64 r3, 12                                    r3 = 12 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += 62 }
lbb_501:
    jne r3, 11, lbb_556                             if r3 != (11 as i32 as i64 as u64) { pc += 54 }
    lddw r2, 0x100022fab --> b"AccountBorrowFailed"        r2 load str located at 4295110571
    mov64 r3, 19                                    r3 = 19 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += 57 }
lbb_506:
    jne r3, 24, lbb_560                             if r3 != (24 as i32 as i64 as u64) { pc += 53 }
    lddw r2, 0x1000230b8 --> b"Immutable"           r2 load str located at 4295110840
    mov64 r3, 9                                     r3 = 9 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += 52 }
lbb_511:
    lddw r2, 0x100022f40 --> b"IncorrectProgramIdMissingRequiredSignatureAccountA"        r2 load str located at 4295110464
    ja lbb_562                                      if true { pc += 48 }
lbb_514:
    lddw r2, 0x10002303e --> b"InvalidRealloc"        r2 load str located at 4295110718
    mov64 r3, 14                                    r3 = 14 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += 45 }
lbb_518:
    add64 r1, 4                                     r1 += 4   ///  r1 = r1.wrapping_add(4 as i32 as i64 as u64)
    stxdw [r10-0x8], r1                     
    mov64 r4, r10                                   r4 = r10
    add64 r4, -8                                    r4 += -8   ///  r4 = r4.wrapping_add(-8 as i32 as i64 as u64)
    mov64 r1, r2                                    r1 = r2
    lddw r2, 0x100022edf --> b"Custom"              r2 load str located at 4295110367
    mov64 r3, 6                                     r3 = 6 as i32 as i64 as u64
    lddw r5, 0x100023970 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r5 load str located at 4295113072
    call function_16386                     
    ja lbb_564                                      if true { pc += 34 }
lbb_530:
    lddw r2, 0x100022fd3 --> b"InvalidSeeds"        r2 load str located at 4295110611
    mov64 r3, 12                                    r3 = 12 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += 29 }
lbb_534:
    lddw r2, 0x100022f0a --> b"InvalidAccountDataAccountDataTooSmallInsufficientF"        r2 load str located at 4295110410
    ja lbb_562                                      if true { pc += 25 }
lbb_537:
    lddw r2, 0x100022fff --> b"UnsupportedSysvar"        r2 load str located at 4295110655
    mov64 r3, 17                                    r3 = 17 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += 22 }
lbb_541:
    lddw r2, 0x100022f6a --> b"AccountAlreadyInitialized"        r2 load str located at 4295110506
    mov64 r3, 25                                    r3 = 25 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += 18 }
lbb_545:
    lddw r2, 0x10002306d --> b"BuiltinProgramsMustConsumeComputeUnits"        r2 load str located at 4295110765
    mov64 r3, 38                                    r3 = 38 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += 14 }
lbb_549:
    lddw r2, 0x100022f97 --> b"NotEnoughAccountKeys"        r2 load str located at 4295110551
    mov64 r3, 20                                    r3 = 20 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += 10 }
lbb_553:
    lddw r2, 0x1000230a6 --> b"ArithmeticOverflowImmutableIncorrectAuthorityfrom_"        r2 load str located at 4295110822
    ja lbb_562                                      if true { pc += 6 }
lbb_556:
    lddw r2, 0x100022fbe --> b"MaxSeedLengthExceeded"        r2 load str located at 4295110590
    mov64 r3, 21                                    r3 = 21 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += 3 }
lbb_560:
    lddw r2, 0x1000230c1 --> b"IncorrectAuthority"        r2 load str located at 4295110849
lbb_562:
    mov64 r3, 18                                    r3 = 18 as i32 as i64 as u64
lbb_563:
    call function_16380                     
lbb_564:
    exit                                    
lbb_565:
    lddw r2, 0x100022ef4 --> b"InvalidInstructionData"        r2 load str located at 4295110388
    mov64 r3, 22                                    r3 = 22 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += -6 }
lbb_569:
    lddw r2, 0x100022feb --> b"AccountNotRentExempt"        r2 load str located at 4295110635
    mov64 r3, 20                                    r3 = 20 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += -10 }
lbb_573:
    lddw r2, 0x100022f2f --> b"InsufficientFunds"        r2 load str located at 4295110447
    mov64 r3, 17                                    r3 = 17 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += -14 }
lbb_577:
    lddw r2, 0x10002301c --> b"MaxAccountsDataAllocationsExceeded"        r2 load str located at 4295110684
    mov64 r3, 34                                    r3 = 34 as i32 as i64 as u64
    ja lbb_563                                      if true { pc += -18 }

function_581:
    stxdw [r10-0x60], r2                    
    stxdw [r10-0x68], r1                    
    stxb [r10-0x51], r3                     
    lddw r1, 0x100023990 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295113104
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    lddw r1, 0x10001e278 --> b"{\x1a\xb8\xff\x00\x00\x00\x00\x18\x01\x00\x00`1\x02\x00\x00\x00\x00\x00\x…        r1 load str located at 4295090808
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -81                                   r1 += -81   ///  r1 = r1.wrapping_add(-81 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x100000188 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x12\x08\x00\x00\x00\x00\x00y\x11\x00\x00\x…        r1 load str located at 4294967688
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    lddw r2, 0x1000239b0 --> b"\x00\x00\x00\x00\xec0\x02\x00\x0f\x00\x00\x00\x00\x00\x00\x00!\x00\x00\x0…        r2 load str located at 4295113136
    call function_15475                     

function_610:
    mov64 r5, r4                                    r5 = r4
    stxdw [r10-0xa8], r1                    
    ldxdw r4, [r3+0x8]                      
    ldxdw r1, [r3+0x0]                      
    ldxdw r3, [r2+0x8]                      
    stxdw [r10-0xb0], r3                    
    ldxdw r7, [r2+0x0]                      
    mov64 r2, r5                                    r2 = r5
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    stdw [r10-0x8], 0                       
    stdw [r10-0x10], 0                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 0                      
    jgt r2, 255, lbb_681                            if r2 > (255 as i32 as i64 as u64) { pc += 56 }
    mov64 r9, r5                                    r9 = r5
    and64 r9, 63                                    r9 &= 63   ///  r9 = r9.and(63)
    mov64 r0, r2                                    r0 = r2
    rsh64 r0, 6                                     r0 >>= 6   ///  r0 = r0.wrapping_shr(6)
    mov64 r6, r4                                    r6 = r4
    mov64 r4, r7                                    r4 = r7
    lsh64 r4, r9                                    r4 <<= r9   ///  r4 = r4.wrapping_shl(r9 as u32)
    mov64 r8, r0                                    r8 = r0
    lsh64 r8, 3                                     r8 <<= 3   ///  r8 = r8.wrapping_shl(3)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -32                                   r3 += -32   ///  r3 = r3.wrapping_add(-32 as i32 as i64 as u64)
    add64 r3, r8                                    r3 += r8   ///  r3 = r3.wrapping_add(r8)
    stxdw [r3+0x0], r4                      
    mov64 r4, r6                                    r4 = r6
    jge r2, 192, lbb_681                            if r2 >= (192 as i32 as i64 as u64) { pc += 41 }
    ldxdw r3, [r10-0xb0]                    
    lsh64 r3, r9                                    r3 <<= r9   ///  r3 = r3.wrapping_shl(r9 as u32)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -32                                   r4 += -32   ///  r4 = r4.wrapping_add(-32 as i32 as i64 as u64)
    add64 r8, r4                                    r8 += r4   ///  r8 = r8.wrapping_add(r4)
    stxdw [r8+0x8], r3                      
    mov64 r4, r5                                    r4 = r5
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    jgt r4, 127, lbb_660                            if r4 > (127 as i32 as i64 as u64) { pc += 10 }
    mov64 r3, r0                                    r3 = r0
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    stxdw [r10-0xb8], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    stdw [r1+0x10], 0                       
    ldxdw r1, [r10-0xb8]                    
    jgt r4, 63, lbb_660                             if r4 > (63 as i32 as i64 as u64) { pc += 1 }
    stdw [r10-0x8], 0                       
lbb_660:
    mov64 r4, r6                                    r4 = r6
    jeq r9, 0, lbb_681                              if r9 == (0 as i32 as i64 as u64) { pc += 19 }
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    and64 r2, 63                                    r2 &= 63   ///  r2 = r2.and(63)
    rsh64 r7, r2                                    r7 >>= r2   ///  r7 = r7.wrapping_shr(r2 as u32)
    ldxdw r3, [r8+0x0]                      
    add64 r3, r7                                    r3 += r7   ///  r3 = r3.wrapping_add(r7)
    stxdw [r8+0x0], r3                      
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    jgt r5, 127, lbb_681                            if r5 > (127 as i32 as i64 as u64) { pc += 9 }
    ldxdw r5, [r10-0xb0]                    
    rsh64 r5, r2                                    r5 >>= r2   ///  r5 = r5.wrapping_shr(r2 as u32)
    lsh64 r0, 3                                     r0 <<= 3   ///  r0 = r0.wrapping_shl(3)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -32                                   r2 += -32   ///  r2 = r2.wrapping_add(-32 as i32 as i64 as u64)
    add64 r2, r0                                    r2 += r0   ///  r2 = r2.wrapping_add(r0)
    ldxdw r3, [r2+0x10]                     
    add64 r3, r5                                    r3 += r5   ///  r3 = r3.wrapping_add(r5)
    stxdw [r2+0x10], r3                     
lbb_681:
    stxdw [r10-0x98], r4                    
    stxdw [r10-0xa0], r1                    
    stdw [r10-0x88], 0                      
    stdw [r10-0x90], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -32                                   r2 += -32   ///  r2 = r2.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -160                                  r3 += -160   ///  r3 = r3.wrapping_add(-160 as i32 as i64 as u64)
    call function_13808                     
    ldxdw r4, [r10-0x48]                    
    ldxdw r5, [r10-0x50]                    
    ldxdw r0, [r10-0x58]                    
    ldxdw r7, [r10-0x60]                    
    ldxdw r2, [r10-0x78]                    
    ldxdw r1, [r10-0x80]                    
    ldxdw r3, [r10-0x70]                    
    ldxdw r8, [r10-0x68]                    
    stxdw [r10-0x38], r8                    
    stxdw [r10-0x40], r3                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x28], 0                      
    stdw [r10-0x68], 0                      
    stdw [r10-0x70], 0                      
    stdw [r10-0x78], 0                      
    stdw [r10-0x80], 0                      
    jne r3, 0, lbb_719                              if r3 != (0 as i32 as i64 as u64) { pc += 10 }
    ldxdw r3, [r10-0x78]                    
    ldxdw r8, [r10-0x38]                    
    jne r8, r3, lbb_719                             if r8 != r3 { pc += 7 }
    ldxdw r3, [r10-0x70]                    
    ldxdw r8, [r10-0x30]                    
    jne r8, r3, lbb_719                             if r8 != r3 { pc += 4 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r8, [r10-0x68]                    
    ldxdw r9, [r10-0x28]                    
    jeq r9, r8, lbb_720                             if r9 == r8 { pc += 1 }
lbb_719:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_720:
    jne r7, 0, lbb_724                              if r7 != (0 as i32 as i64 as u64) { pc += 3 }
    jne r0, 0, lbb_724                              if r0 != (0 as i32 as i64 as u64) { pc += 2 }
    jne r5, 0, lbb_724                              if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    jeq r4, 0, lbb_735                              if r4 == (0 as i32 as i64 as u64) { pc += 11 }
lbb_724:
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_728                              if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_728:
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jeq r4, 0, lbb_735                              if r4 == (0 as i32 as i64 as u64) { pc += 5 }
    add64 r2, 1                                     r2 += 1   ///  r2 = r2.wrapping_add(1 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_734                              if r2 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_734:
    jeq r4, 1, lbb_740                              if r4 == (1 as i32 as i64 as u64) { pc += 5 }
lbb_735:
    ldxdw r4, [r10-0xa8]                    
    stxb [r4+0x10], r3                      
    stxdw [r4+0x8], r2                      
    stxdw [r4+0x0], r1                      
    exit                                    
lbb_740:
    lddw r1, 0x100023af8 --> b"\x00\x00\x00\x00\x037\x02\x00\x1d\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295113464
    stxdw [r10-0x80], r1                    
    stdw [r10-0x60], 0                      
    stdw [r10-0x78], 1                      
    stdw [r10-0x68], 0                      
    stdw [r10-0x70], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    lddw r2, 0x100023ad0 --> b"\x00\x00\x00\x00\xfb0\x02\x00\x15\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00…        r2 load str located at 4295113424
    call function_15475                     

function_752:
    mov64 r7, r4                                    r7 = r4
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r3                                    r1 = r3
    or64 r1, r7                                     r1 |= r7   ///  r1 = r1.or(r7)
    jeq r1, 0, lbb_832                              if r1 == (0 as i32 as i64 as u64) { pc += 75 }
    mov64 r0, r3                                    r0 = r3
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r0, 0, lbb_834                              if r0 == (0 as i32 as i64 as u64) { pc += 73 }
    ldxdw r4, [r2+0x0]                      
    jeq r4, 0, lbb_837                              if r4 == (0 as i32 as i64 as u64) { pc += 74 }
lbb_763:
    neg64 r3                                        r3 = -r3   ///  r3 = (r3 as i64).wrapping_neg() as u64
    jsge r7, 0, lbb_840                             if (r7 as i64) >= (0 as i32 as i64) { pc += 75 }
lbb_765:
    mov64 r0, r7                                    r0 = r7
    jsge r7, 0, lbb_769                             if (r7 as i64) >= (0 as i32 as i64) { pc += 2 }
lbb_767:
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    neg64 r0                                        r0 = -r0   ///  r0 = (r0 as i64).wrapping_neg() as u64
lbb_769:
    ldxdw r8, [r2+0x8]                      
    stxdw [r10-0xa0], r3                    
    mov64 r2, r4                                    r2 = r4
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    jslt r8, 0, lbb_775                             if (r8 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r2, r4                                    r2 = r4
lbb_775:
    mov64 r3, r2                                    r3 = r2
    lsh64 r3, 48                                    r3 <<= 48   ///  r3 = r3.wrapping_shl(48)
    stxdw [r10-0x20], r3                    
    stxdw [r10-0x98], r0                    
    mov64 r3, r8                                    r3 = r8
    jsge r8, 0, lbb_783                             if (r8 as i64) >= (0 as i32 as i64) { pc += 2 }
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    neg64 r3                                        r3 = -r3   ///  r3 = (r3 as i64).wrapping_neg() as u64
lbb_783:
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 16                                    r1 >>= 16   ///  r1 = r1.wrapping_shr(16)
    stxdw [r10-0x10], r1                    
    rsh64 r2, 16                                    r2 >>= 16   ///  r2 = r2.wrapping_shr(16)
    lsh64 r3, 48                                    r3 <<= 48   ///  r3 = r3.wrapping_shl(48)
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    stxdw [r10-0x18], r3                    
    stdw [r10-0x8], 0                       
    stdw [r10-0x90], 0                      
    stdw [r10-0x88], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -32                                   r2 += -32   ///  r2 = r2.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -160                                  r3 += -160   ///  r3 = r3.wrapping_add(-160 as i32 as i64 as u64)
    call function_13808                     
    ldxdw r2, [r10-0x58]                    
    ldxdw r1, [r10-0x60]                    
    ldxdw r3, [r10-0x50]                    
    ldxdw r4, [r10-0x48]                    
    stxdw [r10-0x78], r4                    
    stxdw [r10-0x80], r3                    
    stdw [r10-0x70], 0                      
    stdw [r10-0x68], 0                      
    stdw [r10-0x48], 0                      
    stdw [r10-0x50], 0                      
    stdw [r10-0x58], 0                      
    stdw [r10-0x60], 0                      
    ldxdw r3, [r10-0x60]                    
    ldxdw r4, [r10-0x80]                    
    jne r4, r3, lbb_825                             if r4 != r3 { pc += 10 }
    ldxdw r3, [r10-0x58]                    
    ldxdw r4, [r10-0x78]                    
    jne r4, r3, lbb_825                             if r4 != r3 { pc += 7 }
    ldxdw r3, [r10-0x50]                    
    ldxdw r4, [r10-0x70]                    
    jne r4, r3, lbb_825                             if r4 != r3 { pc += 4 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r3, [r10-0x48]                    
    ldxdw r5, [r10-0x68]                    
    jeq r5, r3, lbb_826                             if r5 == r3 { pc += 1 }
lbb_825:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
lbb_826:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jne r4, 0, lbb_876                              if r4 != (0 as i32 as i64 as u64) { pc += 48 }
    xor64 r8, r7                                    r8 ^= r7   ///  r8 = r8.xor(r7)
    jslt r8, 0, lbb_844                             if (r8 as i64) < (0 as i32 as i64) { pc += 14 }
    jsge r2, 0, lbb_873                             if (r2 as i64) >= (0 as i32 as i64) { pc += 42 }
    ja lbb_876                                      if true { pc += 44 }
lbb_832:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ja lbb_876                                      if true { pc += 42 }
lbb_834:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r4, [r2+0x0]                      
    jne r4, 0, lbb_763                              if r4 != (0 as i32 as i64 as u64) { pc += -74 }
lbb_837:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    neg64 r3                                        r3 = -r3   ///  r3 = (r3 as i64).wrapping_neg() as u64
    jslt r7, 0, lbb_765                             if (r7 as i64) < (0 as i32 as i64) { pc += -75 }
lbb_840:
    mov64 r3, r0                                    r3 = r0
    mov64 r0, r7                                    r0 = r7
    jslt r7, 0, lbb_767                             if (r7 as i64) < (0 as i32 as i64) { pc += -76 }
    ja lbb_769                                      if true { pc += -75 }
lbb_844:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_854                              if r1 == (0 as i32 as i64 as u64) { pc += 7 }
    lddw r0, 0x8000000000000000                     r0 load str located at -9223372036854775808
    jle r2, r0, lbb_858                             if r2 <= r0 { pc += 8 }
lbb_850:
    jne r2, r0, lbb_860                             if r2 != r0 { pc += 9 }
lbb_851:
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jeq r4, 0, lbb_863                              if r4 == (0 as i32 as i64 as u64) { pc += 10 }
    ja lbb_876                                      if true { pc += 22 }
lbb_854:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    lddw r0, 0x8000000000000000                     r0 load str located at -9223372036854775808
    jgt r2, r0, lbb_850                             if r2 > r0 { pc += -8 }
lbb_858:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jeq r2, r0, lbb_851                             if r2 == r0 { pc += -9 }
lbb_860:
    mov64 r4, r5                                    r4 = r5
    and64 r4, 1                                     r4 &= 1   ///  r4 = r4.and(1)
    jne r4, 0, lbb_876                              if r4 != (0 as i32 as i64 as u64) { pc += 13 }
lbb_863:
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jgt r1, 0, lbb_867                              if r1 > (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_867:
    mov64 r4, r2                                    r4 = r2
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    jne r3, 0, lbb_872                              if r3 != (0 as i32 as i64 as u64) { pc += 2 }
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    mov64 r4, r2                                    r4 = r2
lbb_872:
    mov64 r2, r4                                    r2 = r4
lbb_873:
    stxdw [r6+0x8], r1                      
    stxdw [r6+0x10], r2                     
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_876:
    stxdw [r6+0x0], r3                      
    exit                                    

function_878:
    mov64 r7, r4                                    r7 = r4
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r3                                    r1 = r3
    or64 r1, r7                                     r1 |= r7   ///  r1 = r1.or(r7)
    jeq r1, 0, lbb_923                              if r1 == (0 as i32 as i64 as u64) { pc += 40 }
    ldxdw r9, [r2+0x8]                      
    ldxdw r2, [r2+0x0]                      
    mov64 r1, r2                                    r1 = r2
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    jslt r9, 0, lbb_889                             if (r9 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r1, r2                                    r1 = r2
lbb_889:
    stxdw [r10-0x20], r1                    
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_925                              if r2 == (0 as i32 as i64 as u64) { pc += 31 }
    mov64 r2, r9                                    r2 = r9
    jslt r9, 0, lbb_928                             if (r9 as i64) < (0 as i32 as i64) { pc += 32 }
lbb_896:
    stxdw [r10-0x18], r2                    
    mov64 r2, r3                                    r2 = r3
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    jsge r7, 0, lbb_934                             if (r7 as i64) >= (0 as i32 as i64) { pc += 34 }
lbb_900:
    jeq r3, 0, lbb_936                              if r3 == (0 as i32 as i64 as u64) { pc += 35 }
lbb_901:
    mov64 r3, r7                                    r3 = r7
    jsge r7, 0, lbb_905                             if (r7 as i64) >= (0 as i32 as i64) { pc += 2 }
lbb_903:
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    neg64 r3                                        r3 = -r3   ///  r3 = (r3 as i64).wrapping_neg() as u64
lbb_905:
    stxdw [r10-0x8], r3                     
    stxdw [r10-0x10], r2                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -56                                   r1 += -56   ///  r1 = r1.wrapping_add(-56 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -32                                   r2 += -32   ///  r2 = r2.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -16                                   r3 += -16   ///  r3 = r3.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r4, 48                                    r4 = 48 as i32 as i64 as u64
    call function_610                       
    ldxb r1, [r10-0x28]                     
    jeq r1, 1, lbb_972                              if r1 == (1 as i32 as i64 as u64) { pc += 55 }
    ldxdw r2, [r10-0x30]                    
    ldxdw r1, [r10-0x38]                    
    xor64 r9, r7                                    r9 ^= r7   ///  r9 = r9.xor(r7)
    jslt r9, 0, lbb_940                             if (r9 as i64) < (0 as i32 as i64) { pc += 19 }
    jsge r2, 0, lbb_969                             if (r2 as i64) >= (0 as i32 as i64) { pc += 47 }
    ja lbb_972                                      if true { pc += 49 }
lbb_923:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ja lbb_972                                      if true { pc += 47 }
lbb_925:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r2, r9                                    r2 = r9
    jsge r9, 0, lbb_896                             if (r9 as i64) >= (0 as i32 as i64) { pc += -32 }
lbb_928:
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    stxdw [r10-0x18], r2                    
    mov64 r2, r3                                    r2 = r3
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    jslt r7, 0, lbb_900                             if (r7 as i64) < (0 as i32 as i64) { pc += -34 }
lbb_934:
    mov64 r2, r3                                    r2 = r3
    jne r3, 0, lbb_901                              if r3 != (0 as i32 as i64 as u64) { pc += -35 }
lbb_936:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r3, r7                                    r3 = r7
    jslt r7, 0, lbb_903                             if (r7 as i64) < (0 as i32 as i64) { pc += -36 }
    ja lbb_905                                      if true { pc += -35 }
lbb_940:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_950                              if r1 == (0 as i32 as i64 as u64) { pc += 7 }
    lddw r5, 0x8000000000000000                     r5 load str located at -9223372036854775808
    jle r2, r5, lbb_954                             if r2 <= r5 { pc += 8 }
lbb_946:
    jne r2, r5, lbb_956                             if r2 != r5 { pc += 9 }
lbb_947:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jeq r3, 0, lbb_959                              if r3 == (0 as i32 as i64 as u64) { pc += 10 }
    ja lbb_972                                      if true { pc += 22 }
lbb_950:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    lddw r5, 0x8000000000000000                     r5 load str located at -9223372036854775808
    jgt r2, r5, lbb_946                             if r2 > r5 { pc += -8 }
lbb_954:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jeq r2, r5, lbb_947                             if r2 == r5 { pc += -9 }
lbb_956:
    mov64 r3, r4                                    r3 = r4
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_972                              if r3 != (0 as i32 as i64 as u64) { pc += 13 }
lbb_959:
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jgt r1, 0, lbb_963                              if r1 > (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_963:
    mov64 r4, r2                                    r4 = r2
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    jne r3, 0, lbb_968                              if r3 != (0 as i32 as i64 as u64) { pc += 2 }
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    mov64 r4, r2                                    r4 = r2
lbb_968:
    mov64 r2, r4                                    r2 = r4
lbb_969:
    stxdw [r6+0x8], r1                      
    stxdw [r6+0x10], r2                     
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
lbb_972:
    stxdw [r6+0x0], r8                      
    exit                                    

function_974:
    mov64 r6, r2                                    r6 = r2
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    and64 r2, -8                                    r2 &= -8   ///  r2 = r2.and(-8)
    jeq r2, 0, lbb_994                              if r2 == (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r3, r2                                    r3 = r2
    neg64 r3                                        r3 = -r3   ///  r3 = (r3 as i64).wrapping_neg() as u64
    lddw r4, 0xc3ebbae2ff2fff3a                     r4 load str located at -4329161132679889094
    lddw r5, 0x1000100010001                        r5 load str located at 281479271743489
    mov64 r0, r1                                    r0 = r1
lbb_985:
    ldxdw r7, [r0+0x0]                      
    mov64 r8, r9                                    r8 = r9
    xor64 r8, r7                                    r8 ^= r7   ///  r8 = r8.xor(r7)
    xor64 r8, r4                                    r8 ^= r4   ///  r8 = r8.xor(r4)
    stxdw [r0+0x0], r8                      
    add64 r9, r5                                    r9 += r5   ///  r9 = r9.wrapping_add(r5)
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    jne r3, 0, lbb_985                              if r3 != (0 as i32 as i64 as u64) { pc += -9 }
lbb_994:
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    stdw [r10-0x8], 0                       
    and64 r6, 7                                     r6 &= 7   ///  r6 = r6.and(7)
    mov64 r7, r10                                   r7 = r10
    add64 r7, -8                                    r7 += -8   ///  r7 = r7.wrapping_add(-8 as i32 as i64 as u64)
    mov64 r8, r1                                    r8 = r1
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r6                                    r3 = r6
    call function_17012                     
    ldxdw r1, [r10-0x8]                     
    xor64 r9, r1                                    r9 ^= r1   ///  r9 = r9.xor(r1)
    lddw r1, 0xc3ebbae2ff2fff3a                     r1 load str located at -4329161132679889094
    xor64 r9, r1                                    r9 ^= r1   ///  r9 = r9.xor(r1)
    stxdw [r10-0x8], r9                     
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r6                                    r3 = r6
    call function_17012                     
    exit                                    

function_1015:
    ldxdw r2, [r1+0x38]                     
    lddw r3, 0xef4d578b67d2f08c                     r3 load str located at -1203209269184106356
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    ldxdw r3, [r1+0x30]                     
    lddw r4, 0xef4c578a67d3f08d                     r4 load str located at -1203490748455718771
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    ldxdw r4, [r1+0x28]                     
    lddw r5, 0xef4f578967d0f08e                     r5 load str located at -1202646327820750706
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r5, [r1+0x20]                     
    lddw r0, 0xef4e578867d1f08f                     r0 load str located at -1202927807092363121
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    ldxdw r0, [r1+0x18]                     
    lddw r6, 0xef49578f67d6f088                     r6 load str located at -1204335151910817656
    xor64 r0, r6                                    r0 ^= r6   ///  r0 = r0.xor(r6)
    ldxdw r6, [r1+0x10]                     
    lddw r7, 0xef48578e67d7f089                     r7 load str located at -1204616631182430071
    xor64 r6, r7                                    r6 ^= r7   ///  r6 = r6.xor(r7)
    ldxdw r7, [r1+0x8]                      
    lddw r8, 0xef4b578d67d4f08a                     r8 load str located at -1203772210547462006
    xor64 r7, r8                                    r7 ^= r8   ///  r7 = r7.xor(r8)
    ldxdw r8, [r1+0x0]                      
    lddw r9, 0xef4a578c67d5f08b                     r9 load str located at -1204053689819074421
    xor64 r8, r9                                    r8 ^= r9   ///  r8 = r8.xor(r9)
    stxdw [r1+0x0], r8                      
    stxdw [r1+0x8], r7                      
    stxdw [r1+0x10], r6                     
    stxdw [r1+0x18], r0                     
    stxdw [r1+0x20], r5                     
    stxdw [r1+0x28], r4                     
    stxdw [r1+0x30], r3                     
    stxdw [r1+0x38], r2                     
    ldxdw r2, [r1+0x40]                     
    lddw r3, 0xef42578467ddf083                     r3 load str located at -1206305523991973757
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r1+0x40], r2                     
    ldxdw r2, [r1+0x48]                     
    lddw r3, 0xef43578567dcf082                     r3 load str located at -1206024044720361342
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r1+0x48], r2                     
    ldxdw r2, [r1+0x50]                     
    lddw r3, 0xef40578667dff081                     r3 load str located at -1206868465355329407
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r1+0x50], r2                     
    lddw r2, 0xef41578767def080                     r2 load str located at -1206586986083716992
    ldxdw r3, [r1+0x58]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x58], r3                     
    lddw r2, 0xef46578067d9f087                     r2 load str located at -1205179641265262457
    ldxdw r3, [r1+0x60]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x60], r3                     
    lddw r2, 0xef47578167d8f086                     r2 load str located at -1204898161993650042
    ldxdw r3, [r1+0x68]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x68], r3                     
    lddw r2, 0xef44578267dbf085                     r2 load str located at -1205742582628618107
    ldxdw r3, [r1+0x70]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x70], r3                     
    lddw r2, 0xef45578367daf084                     r2 load str located at -1205461103357005692
    ldxdw r3, [r1+0x78]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x78], r3                     
    lddw r2, 0xef5a579c67c5f09b                     r2 load str located at -1199550021473275749
    ldxdw r3, [r1+0x80]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x80], r3                     
    lddw r2, 0xef5b579d67c4f09a                     r2 load str located at -1199268542201663334
    ldxdw r3, [r1+0x88]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x88], r3                     
    lddw r2, 0xef58579e67c7f099                     r2 load str located at -1200112962836631399
    ldxdw r3, [r1+0x90]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x90], r3                     
    lddw r2, 0xef59579f67c6f098                     r2 load str located at -1199831483565018984
    ldxdw r3, [r1+0x98]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x98], r3                     
    lddw r2, 0xef5e579867c1f09f                     r2 load str located at -1198424138746564449
    ldxdw r3, [r1+0xa0]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xa0], r3                     
    lddw r2, 0xef5f579967c0f09e                     r2 load str located at -1198142659474952034
    ldxdw r3, [r1+0xa8]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xa8], r3                     
    lddw r2, 0xef5c579a67c3f09d                     r2 load str located at -1198987080109920099
    ldxdw r3, [r1+0xb0]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xb0], r3                     
    lddw r2, 0xef5d579b67c2f09c                     r2 load str located at -1198705600838307684
    ldxdw r3, [r1+0xb8]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xb8], r3                     
    lddw r2, 0xef52579467cdf093                     r2 load str located at -1201801855646175085
    ldxdw r3, [r1+0xc0]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xc0], r3                     
    lddw r2, 0xef53579567ccf092                     r2 load str located at -1201520376374562670
    ldxdw r3, [r1+0xc8]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xc8], r3                     
    lddw r2, 0xef50579667cff091                     r2 load str located at -1202364797009530735
    ldxdw r3, [r1+0xd0]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xd0], r3                     
    lddw r2, 0xef51579767cef090                     r2 load str located at -1202083317737918320
    ldxdw r3, [r1+0xd8]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xd8], r3                     
    lddw r2, 0xef56579067c9f097                     r2 load str located at -1200675972919463785
    ldxdw r3, [r1+0xe0]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xe0], r3                     
    lddw r2, 0xef57579167c8f096                     r2 load str located at -1200394493647851370
    ldxdw r3, [r1+0xe8]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xe8], r3                     
    lddw r2, 0xef54579267cbf095                     r2 load str located at -1201238914282819435
    ldxdw r3, [r1+0xf0]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xf0], r3                     
    lddw r2, 0xef55579367caf094                     r2 load str located at -1200957435011207020
    ldxdw r3, [r1+0xf8]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xf8], r3                     
    exit                                    

function_1176:
    ldxdw r2, [r1+0x38]                     
    lddw r3, 0x96296f3e7c155a2a                     r3 load str located at -7626442179814794710
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    ldxdw r3, [r1+0x30]                     
    lddw r4, 0x96286f3f7c145a29                     r4 load str located at -7626723650496603607
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    ldxdw r4, [r1+0x28]                     
    lddw r5, 0x962b6f3c7c175a28                     r5 load str located at -7625879238451176920
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r5, [r1+0x20]                     
    lddw r0, 0x962a6f3d7c165a2f                     r0 load str located at -7626160709132985809
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    ldxdw r0, [r1+0x18]                     
    lddw r6, 0x962d6f3a7c115a2e                     r6 load str located at -7625316297088083410
    xor64 r0, r6                                    r0 ^= r6   ///  r0 = r0.xor(r6)
    ldxdw r6, [r1+0x10]                     
    lddw r7, 0x962c6f3b7c105a2d                     r7 load str located at -7625597767769892307
    xor64 r6, r7                                    r6 ^= r7   ///  r6 = r6.xor(r7)
    ldxdw r7, [r1+0x8]                      
    lddw r8, 0x962f6f387c135a2c                     r8 load str located at -7624753355724465620
    xor64 r7, r8                                    r7 ^= r8   ///  r7 = r7.xor(r8)
    ldxdw r8, [r1+0x0]                      
    lddw r9, 0x69d190c683eda5d3                     r9 load str located at 7625034826406274515
    xor64 r8, r9                                    r8 ^= r9   ///  r8 = r8.xor(r9)
    stxdw [r1+0x0], r8                      
    stxdw [r1+0x8], r7                      
    stxdw [r1+0x10], r6                     
    stxdw [r1+0x18], r0                     
    stxdw [r1+0x20], r5                     
    stxdw [r1+0x28], r4                     
    stxdw [r1+0x30], r3                     
    stxdw [r1+0x38], r2                     
    ldxdw r2, [r1+0x40]                     
    lddw r3, 0x96266f317c1a5a2b                     r3 load str located at -7627286660579173845
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r1+0x40], r2                     
    ldxdw r2, [r1+0x48]                     
    lddw r3, 0x96276f307c1b5a24                     r3 load str located at -7627005189897364956
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r1+0x48], r2                     
    ldxdw r2, [r1+0x50]                     
    lddw r3, 0x96246f337c185a25                     r3 load str located at -7627849601942791643
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r1+0x50], r2                     
    lddw r2, 0x96256f327c195a26                     r2 load str located at -7627568131260982746
    ldxdw r3, [r1+0x58]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x58], r3                     
    lddw r2, 0x96226f357c1e5a27                     r2 load str located at -7628412543305885145
    ldxdw r3, [r1+0x60]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x60], r3                     
    lddw r2, 0x96236f347c1f5a20                     r2 load str located at -7628131072624076256
    ldxdw r3, [r1+0x68]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x68], r3                     
    lddw r2, 0x96206f377c1c5a21                     r2 load str located at -7628975484669502943
    ldxdw r3, [r1+0x70]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x70], r3                     
    lddw r2, 0x96216f367c1d5a22                     r2 load str located at -7628694013987694046
    ldxdw r3, [r1+0x78]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x78], r3                     
    lddw r2, 0x963e6f297c025a23                     r2 load str located at -7620531295499429341
    ldxdw r3, [r1+0x80]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x80], r3                     
    lddw r2, 0x963f6f287c035a3c                     r2 load str located at -7620249824817620420
    ldxdw r3, [r1+0x88]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x88], r3                     
    exit                                    

function_1267:
    ldxdw r2, [r1+0x38]                     
    lddw r3, 0xf247e96cab542d03                     r3 load str located at -988564940244374269
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    ldxdw r3, [r1+0x30]                     
    lddw r4, 0xf246e96dab572d00                     r4 load str located at -988846410925921024
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    ldxdw r4, [r1+0x28]                     
    lddw r5, 0xf245e96eab562d01                     r5 load str located at -989127881607729919
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r5, [r1+0x20]                     
    lddw r0, 0xf244e96fab512d06                     r0 load str located at -989409352289800954
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    ldxdw r0, [r1+0x18]                     
    lddw r6, 0xf243e968ab502d07                     r6 load str located at -989690857331348217
    xor64 r0, r6                                    r0 ^= r6   ///  r0 = r0.xor(r6)
    ldxdw r6, [r1+0x10]                     
    lddw r7, 0xf242e969ab532d04                     r7 load str located at -989972328012894972
    xor64 r6, r7                                    r6 ^= r7   ///  r6 = r6.xor(r7)
    ldxdw r7, [r1+0x8]                      
    lddw r8, 0xf241e96aab522d05                     r8 load str located at -990253798694703867
    xor64 r7, r8                                    r7 ^= r8   ///  r7 = r7.xor(r8)
    ldxdw r8, [r1+0x0]                      
    lddw r9, 0xdbf169454ad22fa                      r9 load str located at 990535269376402170
    xor64 r8, r9                                    r8 ^= r9   ///  r8 = r8.xor(r9)
    stxdw [r1+0x0], r8                      
    stxdw [r1+0x8], r7                      
    stxdw [r1+0x10], r6                     
    stxdw [r1+0x18], r0                     
    stxdw [r1+0x20], r5                     
    stxdw [r1+0x28], r4                     
    stxdw [r1+0x30], r3                     
    stxdw [r1+0x38], r2                     
    ldxdw r2, [r1+0x40]                     
    lddw r3, 0xf248e963ab552d02                     r3 load str located at -988283503922303742
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r1+0x40], r2                     
    ldxdw r2, [r1+0x48]                     
    lddw r3, 0xf249e962ab5a2d0d                     r3 load str located at -988002033240232691
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r1+0x48], r2                     
    exit                                    

function_1318:
    ldxdw r2, [r1+0x38]                     
    lddw r3, 0x47d46c2877ac1406                     r3 load str located at 5175880792817800198
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    ldxdw r3, [r1+0x30]                     
    lddw r4, 0x47d56c2977ad1405                     r4 load str located at 5176162272089543685
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    ldxdw r4, [r1+0x28]                     
    lddw r5, 0x47d66c2a77ae1404                     r5 load str located at 5176443751361287172
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r5, [r1+0x20]                     
    lddw r0, 0x47d76c2b77af1403                     r0 load str located at 5176725230633030659
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    ldxdw r0, [r1+0x18]                     
    lddw r6, 0x47d06c2c77a81402                     r6 load str located at 5174754910090564610
    xor64 r0, r6                                    r0 ^= r6   ///  r0 = r0.xor(r6)
    ldxdw r6, [r1+0x10]                     
    lddw r7, 0x47d16c2d77a91401                     r7 load str located at 5175036389362308097
    xor64 r6, r7                                    r6 ^= r7   ///  r6 = r6.xor(r7)
    ldxdw r7, [r1+0x8]                      
    lddw r8, 0x47d26c2e77aa1400                     r8 load str located at 5175317868634051584
    xor64 r7, r8                                    r7 ^= r8   ///  r7 = r7.xor(r8)
    ldxdw r8, [r1+0x0]                      
    lddw r9, 0xb82c93d08854ebff                     r9 load str located at -5175599347905795073
    xor64 r8, r9                                    r8 ^= r9   ///  r8 = r8.xor(r9)
    stxdw [r1+0x0], r8                      
    stxdw [r1+0x8], r7                      
    stxdw [r1+0x10], r6                     
    stxdw [r1+0x18], r0                     
    stxdw [r1+0x20], r5                     
    stxdw [r1+0x28], r4                     
    stxdw [r1+0x30], r3                     
    stxdw [r1+0x38], r2                     
    ldxdw r2, [r1+0x40]                     
    lddw r3, 0x47db6c2777a31407                     r3 load str located at 5177851113359217671
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r1+0x40], r2                     
    ldxdw r2, [r1+0x48]                     
    lddw r3, 0x47da6c2677a21408                     r3 load str located at 5177569634087474184
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r1+0x48], r2                     
    exit                                    

function_1369:
    ldxdw r2, [r1+0x38]                     
    lddw r3, 0x40fb49d3005407bf                     r3 load str located at 4682417408174196671
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    ldxdw r3, [r1+0x30]                     
    lddw r4, 0x40fa49d2005507bc                     r4 load str located at 4682135928902584252
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    ldxdw r4, [r1+0x28]                     
    lddw r5, 0x40f949d1005607bd                     r5 load str located at 4681854449630971837
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r5, [r1+0x20]                     
    lddw r0, 0x40f849d0005707ba                     r0 load str located at 4681572970359359418
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    ldxdw r0, [r1+0x18]                     
    lddw r6, 0x40ff49d7005007bb                     r6 load str located at 4683543325260646331
    xor64 r0, r6                                    r0 ^= r6   ///  r0 = r0.xor(r6)
    ldxdw r6, [r1+0x10]                     
    lddw r7, 0x40fe49d6005107b8                     r7 load str located at 4683261845989033912
    xor64 r6, r7                                    r6 ^= r7   ///  r6 = r6.xor(r7)
    ldxdw r7, [r1+0x8]                      
    lddw r8, 0x40fd49d5005207b9                     r8 load str located at 4682980366717421497
    xor64 r7, r8                                    r7 ^= r8   ///  r7 = r7.xor(r8)
    ldxdw r8, [r1+0x0]                      
    lddw r9, 0xbf03b62bffacf846                     r9 load str located at -4682698887445809082
    xor64 r8, r9                                    r8 ^= r9   ///  r8 = r8.xor(r9)
    stxdw [r1+0x0], r8                      
    stxdw [r1+0x8], r7                      
    stxdw [r1+0x10], r6                     
    stxdw [r1+0x18], r0                     
    stxdw [r1+0x20], r5                     
    stxdw [r1+0x28], r4                     
    stxdw [r1+0x30], r3                     
    stxdw [r1+0x38], r2                     
    ldxdw r2, [r1+0x40]                     
    lddw r3, 0x40f449dc005b07be                     r3 load str located at 4680447121992386494
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r1+0x40], r2                     
    ldxdw r2, [r1+0x48]                     
    lddw r3, 0x40f549dd005a07b1                     r3 load str located at 4680728601263998897
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r1+0x48], r2                     
    ldxdw r2, [r1+0x50]                     
    lddw r3, 0x40f649de005907b0                     r3 load str located at 4681010080535611312
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r1+0x50], r2                     
    lddw r2, 0x40f749df005807b3                     r2 load str located at 4681291559807223731
    ldxdw r3, [r1+0x58]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x58], r3                     
    lddw r2, 0x40f049d8005f07b2                     r2 load str located at 4679321204905936818
    ldxdw r3, [r1+0x60]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x60], r3                     
    lddw r2, 0x40f149d9005e07b5                     r2 load str located at 4679602684177549237
    ldxdw r3, [r1+0x68]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x68], r3                     
    lddw r2, 0x40f249da005d07b4                     r2 load str located at 4679884163449161652
    ldxdw r3, [r1+0x70]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x70], r3                     
    lddw r2, 0x40f349db005c07b7                     r2 load str located at 4680165642720774071
    ldxdw r3, [r1+0x78]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x78], r3                     
    lddw r2, 0x40ec49c4004307b6                     r2 load str located at 4678195219097913270
    ldxdw r3, [r1+0x80]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x80], r3                     
    lddw r2, 0x40ed49c5004207a9                     r2 load str located at 4678476698369525673
    ldxdw r3, [r1+0x88]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x88], r3                     
    lddw r2, 0x40ee49c6004107a8                     r2 load str located at 4678758177641138088
    ldxdw r3, [r1+0x90]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x90], r3                     
    lddw r2, 0x40ef49c7004007ab                     r2 load str located at 4679039656912750507
    ldxdw r3, [r1+0x98]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x98], r3                     
    lddw r2, 0x40e849c0004707aa                     r2 load str located at 4677069302011463594
    ldxdw r3, [r1+0xa0]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xa0], r3                     
    lddw r2, 0x40e949c1004607ad                     r2 load str located at 4677350781283076013
    ldxdw r3, [r1+0xa8]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xa8], r3                     
    lddw r2, 0x40ea49c2004507ac                     r2 load str located at 4677632260554688428
    ldxdw r3, [r1+0xb0]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xb0], r3                     
    lddw r2, 0x40eb49c3004407af                     r2 load str located at 4677913739826300847
    ldxdw r3, [r1+0xb8]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xb8], r3                     
    lddw r2, 0x40e449cc004b07ae                     r2 load str located at 4675943453644490670
    ldxdw r3, [r1+0xc0]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xc0], r3                     
    lddw r2, 0x40e549cd004a07a1                     r2 load str located at 4676224932916103073
    ldxdw r3, [r1+0xc8]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xc8], r3                     
    lddw r2, 0x40e649ce004907a0                     r2 load str located at 4676506412187715488
    ldxdw r3, [r1+0xd0]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xd0], r3                     
    lddw r2, 0x40e749cf004807a3                     r2 load str located at 4676787891459327907
    ldxdw r3, [r1+0xd8]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xd8], r3                     
    lddw r2, 0x40e049c8004f07a2                     r2 load str located at 4674817536558040994
    ldxdw r3, [r1+0xe0]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xe0], r3                     
    lddw r2, 0x40e149c9004e07a5                     r2 load str located at 4675099015829653413
    ldxdw r3, [r1+0xe8]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0xe8], r3                     
    exit                                    

function_1520:
    stxdw [r10-0x28], r1                    
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r7, 195936478                             r7 = 195936478 as i32 as i64 as u64
    jne r3, 2, lbb_1681                             if r3 != (2 as i32 as i64 as u64) { pc += 157 }
    ldxdw r6, [r2+0x8]                      
    ldxdw r3, [r2+0x0]                      
    ldxb r1, [r3+0x1]                       
    ldxdw r7, [r6+0x270]                    
    stxdw [r10-0x8], r7                     
    lddw r8, 0x4a0178651b8c3c5                      r8 load str located at 333292238089536453
    xor64 r7, r8                                    r7 ^= r8   ///  r7 = r7.xor(r8)
    ldxdw r8, [r6+0x268]                    
    stxdw [r10-0x10], r8                    
    lddw r9, 0x4a1178751b9c3c6                      r9 load str located at 333573717361279942
    xor64 r8, r9                                    r8 ^= r9   ///  r8 = r8.xor(r9)
    ldxdw r9, [r6+0x260]                    
    stxdw [r10-0x18], r9                    
    lddw r2, 0x4a2178451bac3c7                      r2 load str located at 333855179453154247
    xor64 r9, r2                                    r9 ^= r2   ///  r9 = r9.xor(r2)
    ldxdw r2, [r6+0x258]                    
    stxdw [r10-0x18], r9                    
    stxdw [r10-0x10], r8                    
    stxdw [r10-0x8], r7                     
    stxdw [r10-0x20], r2                    
    lddw r7, 0xfb5ce87aae443c38                     r7 load str located at -334136658724897736
    xor64 r2, r7                                    r2 ^= r7   ///  r2 = r2.xor(r7)
    stxdw [r10-0x20], r2                    
    ldxdw r7, [r3+0x8]                      
    jne r2, r7, lbb_1563                            if r2 != r7 { pc += 10 }
    ldxdw r2, [r3+0x10]                     
    ldxdw r7, [r10-0x18]                    
    jne r7, r2, lbb_1563                            if r7 != r2 { pc += 7 }
    ldxdw r2, [r3+0x18]                     
    ldxdw r7, [r10-0x10]                    
    jne r7, r2, lbb_1563                            if r7 != r2 { pc += 4 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r2, [r3+0x20]                     
    ldxdw r3, [r10-0x8]                     
    jeq r3, r2, lbb_1564                            if r3 == r2 { pc += 1 }
lbb_1563:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
lbb_1564:
    lddw r7, 0xabad1dea                             r7 load str located at 2880249322
    jeq r1, 0, lbb_1681                             if r1 == (0 as i32 as i64 as u64) { pc += 114 }
    jne r8, 0, lbb_1681                             if r8 != (0 as i32 as i64 as u64) { pc += 113 }
    jne r5, 144, lbb_1685                           if r5 != (144 as i32 as i64 as u64) { pc += 116 }
    mov64 r1, r4                                    r1 = r4
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jne r1, 0, lbb_1690                             if r1 != (0 as i32 as i64 as u64) { pc += 118 }
    mov64 r7, 47825                                 r7 = 47825 as i32 as i64 as u64
    ldxdw r2, [r4+0x18]                     
    ldxdw r1, [r4+0x10]                     
    jsgt r1, r2, lbb_1681                           if (r1 as i64) > (r2 as i64) { pc += 105 }
    ldxdw r3, [r4+0x40]                     
    jslt r3, r2, lbb_1681                           if (r3 as i64) < (r2 as i64) { pc += 103 }
    ldxdw r5, [r4+0x48]                     
    jsgt r3, r5, lbb_1681                           if (r3 as i64) > (r5 as i64) { pc += 101 }
    ldxdw r2, [r4+0x70]                     
    jslt r2, r5, lbb_1681                           if (r2 as i64) < (r5 as i64) { pc += 99 }
    ldxdw r1, [r4+0x78]                     
    jsgt r2, r1, lbb_1681                           if (r2 as i64) > (r1 as i64) { pc += 97 }
    mov64 r2, r6                                    r2 = r6
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r2                                    r1 = r2
    mov64 r2, r4                                    r2 = r4
    mov64 r3, 144                                   r3 = 144 as i32 as i64 as u64
    call function_17012                     
    ldxdw r2, [r6+0x88]                     
    lddw r1, 0x96286f3f7c145a29                     r1 load str located at -7626723650496603607
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    ldxdw r3, [r6+0x80]                     
    lddw r1, 0x962b6f3c7c175a28                     r1 load str located at -7625879238451176920
    xor64 r3, r1                                    r3 ^= r1   ///  r3 = r3.xor(r1)
    ldxdw r4, [r6+0x78]                     
    lddw r1, 0x962a6f3d7c165a2f                     r1 load str located at -7626160709132985809
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    ldxdw r1, [r6+0x70]                     
    lddw r5, 0x962d6f3a7c115a2e                     r5 load str located at -7625316297088083410
    xor64 r1, r5                                    r1 ^= r5   ///  r1 = r1.xor(r5)
    ldxdw r5, [r6+0x68]                     
    lddw r0, 0x962c6f3b7c105a2d                     r0 load str located at -7625597767769892307
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    ldxdw r0, [r6+0x60]                     
    lddw r7, 0x962f6f387c135a2c                     r7 load str located at -7624753355724465620
    xor64 r0, r7                                    r0 ^= r7   ///  r0 = r0.xor(r7)
    ldxdw r7, [r6+0x58]                     
    lddw r8, 0x69d190c683eda5d3                     r8 load str located at 7625034826406274515
    xor64 r7, r8                                    r7 ^= r8   ///  r7 = r7.xor(r8)
    stxdw [r6+0x58], r7                     
    stxdw [r6+0x60], r0                     
    stxdw [r6+0x68], r5                     
    stxdw [r6+0x70], r1                     
    stxdw [r6+0x78], r4                     
    stxdw [r6+0x80], r3                     
    stxdw [r6+0x88], r2                     
    ldxdw r1, [r6+0x90]                     
    lddw r2, 0x96296f3e7c155a2a                     r2 load str located at -7626442179814794710
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    stxdw [r6+0x90], r1                     
    ldxdw r1, [r6+0x98]                     
    lddw r2, 0x96266f317c1a5a2b                     r2 load str located at -7627286660579173845
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    stxdw [r6+0x98], r1                     
    ldxdw r1, [r6+0xa0]                     
    lddw r2, 0x96276f307c1b5a24                     r2 load str located at -7627005189897364956
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    stxdw [r6+0xa0], r1                     
    lddw r1, 0x96246f337c185a25                     r1 load str located at -7627849601942791643
    ldxdw r2, [r6+0xa8]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0xa8], r2                     
    lddw r1, 0x96256f327c195a26                     r1 load str located at -7627568131260982746
    ldxdw r2, [r6+0xb0]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0xb0], r2                     
    lddw r1, 0x96226f357c1e5a27                     r1 load str located at -7628412543305885145
    ldxdw r2, [r6+0xb8]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0xb8], r2                     
    lddw r1, 0x96236f347c1f5a20                     r1 load str located at -7628131072624076256
    ldxdw r2, [r6+0xc0]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0xc0], r2                     
    lddw r1, 0x96206f377c1c5a21                     r1 load str located at -7628975484669502943
    ldxdw r2, [r6+0xc8]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0xc8], r2                     
    lddw r1, 0x96216f367c1d5a22                     r1 load str located at -7628694013987694046
    ldxdw r2, [r6+0xd0]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0xd0], r2                     
    lddw r1, 0x963e6f297c025a23                     r1 load str located at -7620531295499429341
    ldxdw r2, [r6+0xd8]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0xd8], r2                     
    lddw r1, 0x963f6f287c035a3c                     r1 load str located at -7620249824817620420
    ldxdw r2, [r6+0xe0]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0xe0], r2                     
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
lbb_1681:
    ldxdw r1, [r10-0x28]                    
    stxw [r1+0x4], r7                       
    stxw [r1+0x0], r0                       
    exit                                    
lbb_1685:
    lddw r1, 0x1000230d3 --> b"from_bytes"          r1 load str located at 4295110867
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_581                       
lbb_1690:
    lddw r1, 0x1000230d3 --> b"from_bytes"          r1 load str located at 4295110867
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_581                       

function_1695:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r7, 195936478                             r7 = 195936478 as i32 as i64 as u64
    jne r3, 2, lbb_1764                             if r3 != (2 as i32 as i64 as u64) { pc += 66 }
    ldxdw r6, [r2+0x8]                      
    ldxdw r3, [r2+0x0]                      
    ldxb r2, [r3+0x1]                       
    ldxdw r7, [r6+0x270]                    
    stxdw [r10-0x8], r7                     
    lddw r8, 0x4a0178651b8c3c5                      r8 load str located at 333292238089536453
    xor64 r7, r8                                    r7 ^= r8   ///  r7 = r7.xor(r8)
    ldxdw r8, [r6+0x268]                    
    stxdw [r10-0x10], r8                    
    lddw r9, 0x4a1178751b9c3c6                      r9 load str located at 333573717361279942
    xor64 r8, r9                                    r8 ^= r9   ///  r8 = r8.xor(r9)
    ldxdw r9, [r6+0x260]                    
    stxdw [r10-0x18], r9                    
    lddw r0, 0x4a2178451bac3c7                      r0 load str located at 333855179453154247
    xor64 r9, r0                                    r9 ^= r0   ///  r9 = r9.xor(r0)
    ldxdw r0, [r6+0x258]                    
    stxdw [r10-0x18], r9                    
    stxdw [r10-0x10], r8                    
    stxdw [r10-0x8], r7                     
    stxdw [r10-0x20], r0                    
    lddw r7, 0xfb5ce87aae443c38                     r7 load str located at -334136658724897736
    xor64 r0, r7                                    r0 ^= r7   ///  r0 = r0.xor(r7)
    stxdw [r10-0x20], r0                    
    ldxdw r7, [r3+0x8]                      
    jne r0, r7, lbb_1737                            if r0 != r7 { pc += 10 }
    ldxdw r0, [r3+0x10]                     
    ldxdw r7, [r10-0x18]                    
    jne r7, r0, lbb_1737                            if r7 != r0 { pc += 7 }
    ldxdw r0, [r3+0x18]                     
    ldxdw r7, [r10-0x10]                    
    jne r7, r0, lbb_1737                            if r7 != r0 { pc += 4 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r3, [r3+0x20]                     
    ldxdw r0, [r10-0x8]                     
    jeq r0, r3, lbb_1738                            if r0 == r3 { pc += 1 }
lbb_1737:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
lbb_1738:
    lddw r7, 0xabad1dea                             r7 load str located at 2880249322
    jeq r2, 0, lbb_1763                             if r2 == (0 as i32 as i64 as u64) { pc += 22 }
    jne r8, 0, lbb_1763                             if r8 != (0 as i32 as i64 as u64) { pc += 21 }
    jne r5, 240, lbb_1816                           if r5 != (240 as i32 as i64 as u64) { pc += 73 }
    mov64 r2, r4                                    r2 = r4
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    jne r2, 0, lbb_1821                             if r2 != (0 as i32 as i64 as u64) { pc += 75 }
    ldxdw r5, [r4+0x18]                     
    ldxdw r2, [r4+0x10]                     
    mov64 r0, r2                                    r0 = r2
    or64 r0, r5                                     r0 |= r5   ///  r0 = r0.or(r5)
    ldxdw r3, [r4+0x30]                     
    jeq r0, 0, lbb_1760                             if r0 == (0 as i32 as i64 as u64) { pc += 8 }
    lddw r0, 0x1000000000000                        r0 load str located at 281474976710656
    jge r5, r0, lbb_1826                            if r5 >= r0 { pc += 71 }
    rsh64 r2, 48                                    r2 >>= 48   ///  r2 = r2.wrapping_shr(48)
    lsh64 r5, 16                                    r5 <<= 16   ///  r5 = r5.wrapping_shl(16)
    or64 r2, r5                                     r2 |= r5   ///  r2 = r2.or(r5)
    mov64 r7, 47826                                 r7 = 47826 as i32 as i64 as u64
    jlt r3, r2, lbb_1763                            if r3 < r2 { pc += 3 }
lbb_1760:
    mov64 r7, 47826                                 r7 = 47826 as i32 as i64 as u64
    ldxdw r2, [r4+0x38]                     
    jle r3, r2, lbb_1767                            if r3 <= r2 { pc += 4 }
lbb_1763:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_1764:
    stxw [r1+0x4], r7                       
    stxw [r1+0x0], r0                       
    exit                                    
lbb_1767:
    ldxdw r8, [r4+0x68]                     
    ldxdw r5, [r4+0x60]                     
    mov64 r0, r5                                    r0 = r5
    or64 r0, r8                                     r0 |= r8   ///  r0 = r0.or(r8)
    ldxdw r3, [r4+0x80]                     
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jeq r0, 0, lbb_1781                             if r0 == (0 as i32 as i64 as u64) { pc += 7 }
    lddw r0, 0xffffffffffff                         r0 load str located at 281474976710655
    jgt r8, r0, lbb_1826                            if r8 > r0 { pc += 49 }
    rsh64 r5, 48                                    r5 >>= 48   ///  r5 = r5.wrapping_shr(48)
    lsh64 r8, 16                                    r8 <<= 16   ///  r8 = r8.wrapping_shl(16)
    or64 r5, r8                                     r5 |= r8   ///  r5 = r5.or(r8)
    mov64 r9, r5                                    r9 = r5
lbb_1781:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jlt r3, r2, lbb_1764                            if r3 < r2 { pc += -19 }
    jlt r3, r9, lbb_1764                            if r3 < r9 { pc += -20 }
    ldxdw r2, [r4+0x88]                     
    jgt r3, r2, lbb_1764                            if r3 > r2 { pc += -22 }
    ldxdw r8, [r4+0xb8]                     
    ldxdw r5, [r4+0xb0]                     
    mov64 r0, r5                                    r0 = r5
    or64 r0, r8                                     r0 |= r8   ///  r0 = r0.or(r8)
    ldxdw r3, [r4+0xd0]                     
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jeq r0, 0, lbb_1800                             if r0 == (0 as i32 as i64 as u64) { pc += 7 }
    lddw r0, 0xffffffffffff                         r0 load str located at 281474976710655
    jgt r8, r0, lbb_1826                            if r8 > r0 { pc += 30 }
    rsh64 r5, 48                                    r5 >>= 48   ///  r5 = r5.wrapping_shr(48)
    lsh64 r8, 16                                    r8 <<= 16   ///  r8 = r8.wrapping_shl(16)
    or64 r5, r8                                     r5 |= r8   ///  r5 = r5.or(r8)
    mov64 r9, r5                                    r9 = r5
lbb_1800:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jlt r3, r2, lbb_1764                            if r3 < r2 { pc += -38 }
    jlt r3, r9, lbb_1764                            if r3 < r9 { pc += -39 }
    ldxdw r2, [r4+0xd8]                     
    jgt r3, r2, lbb_1764                            if r3 > r2 { pc += -41 }
    add64 r6, 232                                   r6 += 232   ///  r6 = r6.wrapping_add(232 as i32 as i64 as u64)
    mov64 r7, r1                                    r7 = r1
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r4                                    r2 = r4
    mov64 r3, 240                                   r3 = 240 as i32 as i64 as u64
    call function_17012                     
    mov64 r1, r6                                    r1 = r6
    call function_1369                      
    mov64 r1, r7                                    r1 = r7
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
    ja lbb_1764                                     if true { pc += -52 }
lbb_1816:
    lddw r1, 0x1000230d3 --> b"from_bytes"          r1 load str located at 4295110867
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_581                       
lbb_1821:
    lddw r1, 0x1000230d3 --> b"from_bytes"          r1 load str located at 4295110867
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_581                       
lbb_1826:
    lddw r1, 0x100023a68 --> b"\x00\x00\x00\x00\xcd6\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00\x1d\x00\x00…        r1 load str located at 4295113320
    call function_15470                     

function_1829:
    mov64 r6, r1                                    r6 = r1
    stxdw [r10-0x38], r4                    
    stxdw [r10-0x48], r3                    
    stxdw [r10-0x58], r2                    
    lddw r1, 0x10002349c --> b"poolv1transfer_token_by_owner_with_mint calledInva"        r1 load str located at 4295111836
    stxdw [r10-0x68], r1                    
    stdw [r10-0x30], 32                     
    stdw [r10-0x40], 32                     
    stdw [r10-0x50], 32                     
    stdw [r10-0x60], 6                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -33                                   r1 += -33   ///  r1 = r1.wrapping_add(-33 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -104                                  r2 += -104   ///  r2 = r2.wrapping_add(-104 as i32 as i64 as u64)
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
    mov64 r4, r5                                    r4 = r5
    call function_15426                     
    ldxdw r1, [r10-0x9]                     
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x11]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x19]                    
    stxdw [r6+0x8], r1                      
    ldxdw r1, [r10-0x21]                    
    stxdw [r6+0x0], r1                      
    ldxb r1, [r10-0x1]                      
    stxb [r6+0x20], r1                      
    exit                                    

function_1858:
    mov64 r6, r1                                    r6 = r1
    stxdw [r10-0x58], r3                    
    lddw r1, 0x100022de8 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r1 load str located at 4295110120
    stxdw [r10-0x68], r1                    
    stxdw [r10-0x78], r2                    
    stdw [r10-0x50], 32                     
    stdw [r10-0x60], 32                     
    stdw [r10-0x70], 32                     
    lddw r1, 0xd3bb8723dd54a054                     r1 load str located at -3189807322954948524
    stxdw [r10-0x8], r1                     
    lddw r1, 0x6dd2523bce0a93a0                     r1 load str located at 7913477912056730528
    stxdw [r10-0x10], r1                    
    lddw r1, 0x7a819dd33c7070c6                     r1 load str located at 8827510275200544966
    stxdw [r10-0x18], r1                    
    lddw r1, 0xe959f7272b74fd7a                     r1 load str located at -1632001642340221574
    stxdw [r10-0x20], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -65                                   r1 += -65   ///  r1 = r1.wrapping_add(-65 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -120                                  r2 += -120   ///  r2 = r2.wrapping_add(-120 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -32                                   r4 += -32   ///  r4 = r4.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, 3                                     r3 = 3 as i32 as i64 as u64
    call function_15426                     
    ldxdw r1, [r10-0x29]                    
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x31]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x39]                    
    stxdw [r6+0x8], r1                      
    ldxdw r1, [r10-0x41]                    
    stxdw [r6+0x0], r1                      
    ldxb r1, [r10-0x21]                     
    stxb [r6+0x20], r1                      
    exit                                    

function_1898:
    mov64 r6, r1                                    r6 = r1
    stxdw [r10-0x58], r3                    
    lddw r1, 0x100022e68 --> b"\x06\xdd\xf6\xe1\xeeu\x8f\xde\x18B]\xbc\xe4l\xcd\xda\xb6\x1a\xfcM\x83\xb9…        r1 load str located at 4295110248
    stxdw [r10-0x68], r1                    
    stxdw [r10-0x78], r2                    
    stdw [r10-0x50], 32                     
    stdw [r10-0x60], 32                     
    stdw [r10-0x70], 32                     
    lddw r1, 0xd3bb8723dd54a054                     r1 load str located at -3189807322954948524
    stxdw [r10-0x8], r1                     
    lddw r1, 0x6dd2523bce0a93a0                     r1 load str located at 7913477912056730528
    stxdw [r10-0x10], r1                    
    lddw r1, 0x7a819dd33c7070c6                     r1 load str located at 8827510275200544966
    stxdw [r10-0x18], r1                    
    lddw r1, 0xe959f7272b74fd7a                     r1 load str located at -1632001642340221574
    stxdw [r10-0x20], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -65                                   r1 += -65   ///  r1 = r1.wrapping_add(-65 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -120                                  r2 += -120   ///  r2 = r2.wrapping_add(-120 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -32                                   r4 += -32   ///  r4 = r4.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, 3                                     r3 = 3 as i32 as i64 as u64
    call function_15426                     
    ldxdw r1, [r10-0x29]                    
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x31]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x39]                    
    stxdw [r6+0x8], r1                      
    ldxdw r1, [r10-0x41]                    
    stxdw [r6+0x0], r1                      
    ldxb r1, [r10-0x21]                     
    stxb [r6+0x20], r1                      
    exit                                    

function_1938:
    ldxdw r0, [r5-0xff8]                    
    stxb [r10-0x141], r0                    
    ldxdw r6, [r4+0x0]                      
    ldxdw r8, [r5-0x1000]                   
    ldxdw r4, [r8+0x0]                      
    mov64 r5, r10                                   r5 = r10
    add64 r5, -321                                  r5 += -321   ///  r5 = r5.wrapping_add(-321 as i32 as i64 as u64)
    stxdw [r10-0x110], r5                   
    lddw r5, 0x100022de8 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r5 load str located at 4295110120
    stxdw [r10-0x130], r5                   
    add64 r4, 8                                     r4 += 8   ///  r4 = r4.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x120], r4                   
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x140], r6                   
    stdw [r10-0x108], 1                     
    stdw [r10-0x118], 32                    
    stdw [r10-0x128], 32                    
    stdw [r10-0x138], 32                    
    mov64 r4, r10                                   r4 = r10
    add64 r4, -320                                  r4 += -320   ///  r4 = r4.wrapping_add(-320 as i32 as i64 as u64)
    stxdw [r10-0x100], r4                   
    stdw [r10-0xf8], 4                      
    ldxdw r0, [r2+0x0]                      
    ldxdw r4, [r3+0x0]                      
    mov64 r2, r4                                    r2 = r4
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xe0], r2                    
    mov64 r9, r0                                    r9 = r0
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xf0], r9                    
    sth [r10-0xd8], 257                     
    sth [r10-0xe8], 257                     
    lddw r7, 0x93a165d7e1f6dd06                     r7 load str located at -7808848301000303354
    stxdw [r10-0xb8], r7                    
    lddw r7, 0xac79ebce46e1cbd9                     r7 load str located at -6018520155818964007
    stxdw [r10-0xb0], r7                    
    lddw r7, 0x91375b5fed85b41c                     r7 load str located at -7982811346925931492
    stxdw [r10-0xa8], r7                    
    lddw r7, 0xa900ff7e85f58c3a                     r7 load str located at -6268729762421306310
    stxdw [r10-0xa0], r7                    
    stdw [r10-0xc0], 165                    
    stdw [r10-0xc8], 2039280                
    stw [r10-0xcc], 0                       
    ldxb r7, [r0+0x0]                       
    jne r7, 255, lbb_2020                           if r7 != (255 as i32 as i64 as u64) { pc += 32 }
    stxdw [r10-0x158], r3                   
    stxdw [r10-0x168], r8                   
    ldxb r7, [r0+0x1]                       
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r7, 0, lbb_2024                             if r7 == (0 as i32 as i64 as u64) { pc += 30 }
    ldxb r8, [r0+0x2]                       
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jeq r8, 0, lbb_2028                             if r8 == (0 as i32 as i64 as u64) { pc += 31 }
lbb_1997:
    stxdw [r10-0x160], r5                   
    ldxb r8, [r0+0x3]                       
    jne r8, 0, lbb_2001                             if r8 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_2000:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_2001:
    ldxdw r8, [r0+0x50]                     
    mov64 r5, r0                                    r5 = r0
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r5                    
    mov64 r5, r0                                    r5 = r0
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r5                    
    stxdw [r10-0x88], r8                    
    add64 r0, 72                                    r0 += 72   ///  r0 = r0.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r0                    
    stxdw [r10-0x98], r9                    
    stxb [r10-0x66], r3                     
    stxb [r10-0x67], r7                     
    ldxdw r3, [r10-0x160]                   
    stxb [r10-0x68], r3                     
    stdw [r10-0x70], 0                      
    ldxb r3, [r4+0x0]                       
    ldxdw r9, [r10-0x158]                   
    jeq r3, 255, lbb_2033                           if r3 == (255 as i32 as i64 as u64) { pc += 13 }
lbb_2020:
    mov64 r2, 11                                    r2 = 11 as i32 as i64 as u64
lbb_2021:
    stxw [r1+0x4], r3                       
    stxw [r1+0x0], r2                       
    exit                                    
lbb_2024:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxb r8, [r0+0x2]                       
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jne r8, 0, lbb_1997                             if r8 != (0 as i32 as i64 as u64) { pc += -31 }
lbb_2028:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    stxdw [r10-0x160], r5                   
    ldxb r8, [r0+0x3]                       
    jeq r8, 0, lbb_2000                             if r8 == (0 as i32 as i64 as u64) { pc += -32 }
    ja lbb_2001                                     if true { pc += -32 }
lbb_2033:
    ldxb r5, [r4+0x1]                       
    ldxb r0, [r4+0x2]                       
    ldxb r3, [r4+0x3]                       
    ldxdw r7, [r4+0x50]                     
    mov64 r8, r4                                    r8 = r4
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r8                    
    mov64 r8, r4                                    r8 = r4
    add64 r8, 88                                    r8 += 88   ///  r8 = r8.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r8                    
    stxdw [r10-0x50], r7                    
    add64 r4, 72                                    r4 += 72   ///  r4 = r4.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r4                    
    stxdw [r10-0x60], r2                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jeq r3, 0, lbb_2094                             if r3 == (0 as i32 as i64 as u64) { pc += 45 }
    stxb [r10-0x2e], r2                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jeq r0, 0, lbb_2098                             if r0 == (0 as i32 as i64 as u64) { pc += 46 }
lbb_2052:
    mov64 r8, r1                                    r8 = r1
    stxb [r10-0x2f], r2                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r5, 0, lbb_2057                             if r5 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_2056:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_2057:
    stxb [r10-0x30], r1                     
    stdw [r10-0x38], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -204                                  r1 += -204   ///  r1 = r1.wrapping_add(-204 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -240                                  r1 += -240   ///  r1 = r1.wrapping_add(-240 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100022e08 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295110152
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 52                      
    stdw [r10-0x18], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -152                                  r2 += -152   ///  r2 = r2.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -256                                  r4 += -256   ///  r4 = r4.wrapping_add(-256 as i32 as i64 as u64)
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    syscall [invalid]                       
    stxdw [r10-0x88], r6                    
    ldxdw r1, [r10-0x168]                   
    stxdw [r10-0x90], r1                    
    stxdw [r10-0x98], r9                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -336                                  r1 += -336   ///  r1 = r1.wrapping_add(-336 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -152                                  r2 += -152   ///  r2 = r2.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_15003                     
    ldxw r2, [r10-0x150]                    
    jne r2, 26, lbb_2104                            if r2 != (26 as i32 as i64 as u64) { pc += 12 }
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    ja lbb_2105                                     if true { pc += 11 }
lbb_2094:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    stxb [r10-0x2e], r2                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_2052                             if r0 != (0 as i32 as i64 as u64) { pc += -46 }
lbb_2098:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r8, r1                                    r8 = r1
    stxb [r10-0x2f], r2                     
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r5, 0, lbb_2056                             if r5 == (0 as i32 as i64 as u64) { pc += -47 }
    ja lbb_2057                                     if true { pc += -47 }
lbb_2104:
    ldxw r3, [r10-0x14c]                    
lbb_2105:
    mov64 r1, r8                                    r1 = r8
    ja lbb_2021                                     if true { pc += -86 }

function_2107:
    mov64 r6, r3                                    r6 = r3
    stxdw [r10-0x190], r1                   
    ldxdw r0, [r5-0xff8]                    
    stxb [r10-0x181], r0                    
    ldxdw r0, [r4+0x0]                      
    ldxdw r4, [r5-0x1000]                   
    ldxdw r5, [r4+0x0]                      
    mov64 r4, r10                                   r4 = r10
    add64 r4, -385                                  r4 += -385   ///  r4 = r4.wrapping_add(-385 as i32 as i64 as u64)
    stxdw [r10-0x150], r4                   
    lddw r4, 0x100022e68 --> b"\x06\xdd\xf6\xe1\xeeu\x8f\xde\x18B]\xbc\xe4l\xcd\xda\xb6\x1a\xfcM\x83\xb9…        r4 load str located at 4295110248
    stxdw [r10-0x170], r4                   
    mov64 r9, r5                                    r9 = r5
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x160], r9                   
    mov64 r7, r0                                    r7 = r0
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x180], r7                   
    stdw [r10-0x148], 1                     
    stdw [r10-0x158], 32                    
    stdw [r10-0x168], 32                    
    stdw [r10-0x178], 32                    
    mov64 r4, r10                                   r4 = r10
    add64 r4, -384                                  r4 += -384   ///  r4 = r4.wrapping_add(-384 as i32 as i64 as u64)
    stxdw [r10-0x140], r4                   
    stdw [r10-0x138], 4                     
    ldxdw r3, [r2+0x0]                      
    ldxdw r8, [r6+0x0]                      
    mov64 r6, r8                                    r6 = r8
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x120], r6                   
    mov64 r1, r3                                    r1 = r3
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x130], r1                   
    sth [r10-0x118], 257                    
    sth [r10-0x128], 257                    
    lddw r4, 0xde8f75eee1f6dd06                     r4 load str located at -2409577606766207738
    stxdw [r10-0xf4], r4                    
    lddw r4, 0xdacd6ce4bc5d4218                     r4 load str located at -2680366473547005416
    stxdw [r10-0xec], r4                    
    lddw r4, 0x270db9834dfc1ab6                     r4 load str located at 2814109315776649910
    stxdw [r10-0xe4], r4                    
    lddw r4, 0xfc8ba1d828f9bdfe                     r4 load str located at -248927404616466946
    stxdw [r10-0xdc], r4                    
    stdw [r10-0xfc], 200                    
    stdw [r10-0x104], 2282880               
    stw [r10-0x108], 0                      
    ldxb r4, [r3+0x0]                       
    jne r4, 255, lbb_2193                           if r4 != (255 as i32 as i64 as u64) { pc += 32 }
    stxdw [r10-0x1a8], r5                   
    stxdw [r10-0x1a0], r0                   
    ldxb r0, [r3+0x1]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_2197                             if r0 != (0 as i32 as i64 as u64) { pc += 30 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxb r4, [r3+0x2]                       
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r4, 0, lbb_2200                             if r4 == (0 as i32 as i64 as u64) { pc += 29 }
lbb_2171:
    stxdw [r10-0x198], r2                   
    ldxb r4, [r3+0x3]                       
    jne r4, 0, lbb_2175                             if r4 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_2174:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_2175:
    ldxdw r4, [r3+0x50]                     
    mov64 r2, r3                                    r2 = r3
    add64 r2, 40                                    r2 += 40   ///  r2 = r2.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xb0], r2                    
    mov64 r2, r3                                    r2 = r3
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xb8], r2                    
    stxdw [r10-0xc0], r4                    
    add64 r3, 72                                    r3 += 72   ///  r3 = r3.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0xc8], r3                    
    stxdw [r10-0xd0], r1                    
    stxb [r10-0x9e], r5                     
    stxb [r10-0x9f], r0                     
    ldxdw r1, [r10-0x198]                   
    stxb [r10-0xa0], r1                     
    stdw [r10-0xa8], 0                      
    ldxb r1, [r8+0x0]                       
    jeq r1, 255, lbb_2205                           if r1 == (255 as i32 as i64 as u64) { pc += 12 }
lbb_2193:
    mov64 r2, 11                                    r2 = 11 as i32 as i64 as u64
lbb_2194:
    ldxdw r3, [r10-0x190]                   
lbb_2195:
    stxw [r3+0x0], r2                       
    exit                                    
lbb_2197:
    ldxb r4, [r3+0x2]                       
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_2171                             if r4 != (0 as i32 as i64 as u64) { pc += -29 }
lbb_2200:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    stxdw [r10-0x198], r2                   
    ldxb r4, [r3+0x3]                       
    jeq r4, 0, lbb_2174                             if r4 == (0 as i32 as i64 as u64) { pc += -30 }
    ja lbb_2175                                     if true { pc += -30 }
lbb_2205:
    ldxb r1, [r8+0x1]                       
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_2381                             if r1 != (0 as i32 as i64 as u64) { pc += 173 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxb r1, [r8+0x2]                       
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_2384                             if r1 == (0 as i32 as i64 as u64) { pc += 172 }
lbb_2212:
    ldxb r4, [r8+0x3]                       
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_2216                             if r4 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_2215:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_2216:
    ldxdw r4, [r8+0x50]                     
    stxdw [r10-0x88], r4                    
    stxb [r10-0x66], r1                     
    stxb [r10-0x67], r3                     
    stxb [r10-0x68], r2                     
    mov64 r1, r8                                    r1 = r8
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x198], r1                   
    stxdw [r10-0x78], r1                    
    mov64 r1, r8                                    r1 = r8
    add64 r1, 88                                    r1 += 88   ///  r1 = r1.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x1b0], r1                   
    stxdw [r10-0x80], r1                    
    mov64 r1, r8                                    r1 = r8
    add64 r1, 72                                    r1 += 72   ///  r1 = r1.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x1b8], r1                   
    stxdw [r10-0x90], r1                    
    stxdw [r10-0x98], r6                    
    stdw [r10-0x70], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -264                                  r1 += -264   ///  r1 = r1.wrapping_add(-264 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -304                                  r1 += -304   ///  r1 = r1.wrapping_add(-304 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100022e08 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295110152
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 52                      
    stdw [r10-0x18], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -208                                  r2 += -208   ///  r2 = r2.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -320                                  r4 += -320   ///  r4 = r4.wrapping_add(-320 as i32 as i64 as u64)
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    syscall [invalid]                       
    stb [r10-0x130], 18                     
    ldxdw r1, [r7+0x0]                      
    stxdw [r10-0x12f], r1                   
    ldxdw r1, [r7+0x8]                      
    stxdw [r10-0x127], r1                   
    ldxdw r1, [r7+0x10]                     
    stxdw [r10-0x11f], r1                   
    ldxdw r1, [r7+0x18]                     
    stxdw [r10-0x117], r1                   
    stxdw [r10-0xe8], r7                    
    stxdw [r10-0xf8], r9                    
    stxdw [r10-0x108], r6                   
    sth [r10-0xe0], 0                       
    sth [r10-0xf0], 0                       
    sth [r10-0x100], 1                      
    ldxb r1, [r8+0x0]                       
    ldxdw r3, [r10-0x190]                   
    ldxdw r0, [r10-0x1a0]                   
    jne r1, 255, lbb_2389                           if r1 != (255 as i32 as i64 as u64) { pc += 115 }
    ldxb r1, [r8+0x1]                       
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_2391                             if r1 != (0 as i32 as i64 as u64) { pc += 113 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxb r1, [r8+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_2394                             if r1 == (0 as i32 as i64 as u64) { pc += 112 }
lbb_2282:
    ldxb r1, [r8+0x3]                       
    ldxdw r5, [r10-0x1a8]                   
    jne r1, 0, lbb_2286                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_2285:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_2286:
    ldxdw r1, [r8+0x50]                     
    ldxdw r8, [r10-0x198]                   
    stxdw [r10-0xb0], r8                    
    ldxdw r8, [r10-0x1b0]                   
    stxdw [r10-0xb8], r8                    
    stxdw [r10-0xc0], r1                    
    ldxdw r1, [r10-0x1b8]                   
    stxdw [r10-0xc8], r1                    
    stxdw [r10-0xd0], r6                    
    stxb [r10-0x9e], r3                     
    stxb [r10-0x9f], r4                     
    stxb [r10-0xa0], r2                     
    stdw [r10-0xa8], 0                      
    ldxb r1, [r5+0x0]                       
    and64 r1, 136                                   r1 &= 136   ///  r1 = r1.and(136)
    ldxdw r3, [r10-0x190]                   
    jne r1, 136, lbb_2389                           if r1 != (136 as i32 as i64 as u64) { pc += 86 }
    ldxdw r6, [r10-0x1a8]                   
    ldxb r1, [r6+0x1]                       
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_2399                             if r1 != (0 as i32 as i64 as u64) { pc += 91 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxb r1, [r6+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_2402                             if r1 == (0 as i32 as i64 as u64) { pc += 90 }
lbb_2312:
    ldxb r1, [r6+0x3]                       
    jne r1, 0, lbb_2315                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_2314:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_2315:
    ldxdw r1, [r6+0x50]                     
    mov64 r5, r6                                    r5 = r6
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r5                    
    mov64 r5, r6                                    r5 = r6
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r5                    
    stxdw [r10-0x88], r1                    
    add64 r6, 72                                    r6 += 72   ///  r6 = r6.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r6                    
    stxdw [r10-0x98], r9                    
    stxb [r10-0x66], r3                     
    stxb [r10-0x67], r4                     
    stxb [r10-0x68], r2                     
    stdw [r10-0x70], 0                      
    ldxb r1, [r0+0x0]                       
    and64 r1, 136                                   r1 &= 136   ///  r1 = r1.and(136)
    ldxdw r3, [r10-0x190]                   
    jne r1, 136, lbb_2389                           if r1 != (136 as i32 as i64 as u64) { pc += 55 }
    ldxb r1, [r0+0x1]                       
    ldxb r2, [r0+0x2]                       
    ldxb r5, [r0+0x3]                       
    ldxdw r3, [r0+0x50]                     
    mov64 r4, r0                                    r4 = r0
    add64 r4, 40                                    r4 += 40   ///  r4 = r4.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r4                    
    mov64 r4, r0                                    r4 = r0
    add64 r4, 88                                    r4 += 88   ///  r4 = r4.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r4                    
    stxdw [r10-0x50], r3                    
    add64 r0, 72                                    r0 += 72   ///  r0 = r0.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r0                    
    stxdw [r10-0x60], r7                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r5, 0, lbb_2406                             if r5 != (0 as i32 as i64 as u64) { pc += 55 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    stxb [r10-0x2e], r4                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_2409                             if r2 == (0 as i32 as i64 as u64) { pc += 54 }
lbb_2355:
    stxb [r10-0x2f], r4                     
    jne r1, 0, lbb_2358                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_2357:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_2358:
    stxb [r10-0x30], r3                     
    stdw [r10-0x38], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -304                                  r1 += -304   ///  r1 = r1.wrapping_add(-304 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -264                                  r1 += -264   ///  r1 = r1.wrapping_add(-264 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100022e68 --> b"\x06\xdd\xf6\xe1\xeeu\x8f\xde\x18B]\xbc\xe4l\xcd\xda\xb6\x1a\xfcM\x83\xb9…        r1 load str located at 4295110248
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 33                      
    stdw [r10-0x18], 3                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -208                                  r2 += -208   ///  r2 = r2.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r3, 3                                     r3 = 3 as i32 as i64 as u64
    mov64 r4, 8                                     r4 = 8 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    syscall [invalid]                       
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    ja lbb_2194                                     if true { pc += -187 }
lbb_2381:
    ldxb r1, [r8+0x2]                       
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_2212                             if r1 != (0 as i32 as i64 as u64) { pc += -172 }
lbb_2384:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxb r4, [r8+0x3]                       
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r4, 0, lbb_2215                             if r4 == (0 as i32 as i64 as u64) { pc += -173 }
    ja lbb_2216                                     if true { pc += -173 }
lbb_2389:
    mov64 r2, 11                                    r2 = 11 as i32 as i64 as u64
    ja lbb_2195                                     if true { pc += -196 }
lbb_2391:
    ldxb r1, [r8+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_2282                             if r1 != (0 as i32 as i64 as u64) { pc += -112 }
lbb_2394:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxb r1, [r8+0x3]                       
    ldxdw r5, [r10-0x1a8]                   
    jeq r1, 0, lbb_2285                             if r1 == (0 as i32 as i64 as u64) { pc += -113 }
    ja lbb_2286                                     if true { pc += -113 }
lbb_2399:
    ldxb r1, [r6+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_2312                             if r1 != (0 as i32 as i64 as u64) { pc += -90 }
lbb_2402:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxb r1, [r6+0x3]                       
    jeq r1, 0, lbb_2314                             if r1 == (0 as i32 as i64 as u64) { pc += -91 }
    ja lbb_2315                                     if true { pc += -91 }
lbb_2406:
    stxb [r10-0x2e], r4                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_2355                             if r2 != (0 as i32 as i64 as u64) { pc += -54 }
lbb_2409:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    stxb [r10-0x2f], r4                     
    jeq r1, 0, lbb_2357                             if r1 == (0 as i32 as i64 as u64) { pc += -55 }
    ja lbb_2358                                     if true { pc += -55 }

function_2413:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r8, 195936478                             r8 = 195936478 as i32 as i64 as u64
    jne r3, 8, lbb_2587                             if r3 != (8 as i32 as i64 as u64) { pc += 171 }
    lddw r8, 0xabad1dea                             r8 load str located at 2880249322
    ldxdw r6, [r2+0x0]                      
    ldxb r3, [r6+0x1]                       
    jeq r3, 0, lbb_2587                             if r3 == (0 as i32 as i64 as u64) { pc += 166 }
    mov64 r3, r6                                    r3 = r6
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x1d0], r3                   
    ldxdw r3, [r6+0x8]                      
    lddw r9, 0xcd4df2193b88a1c0                     r9 load str located at -3652997532540689984
    jne r3, r9, lbb_2446                            if r3 != r9 { pc += 18 }
    ldxdw r3, [r10-0x1d0]                   
    ldxdw r3, [r3+0x8]                      
    lddw r9, 0xeadff0f6e82402a1                     r9 load str located at -1522233205780643167
    jne r3, r9, lbb_2446                            if r3 != r9 { pc += 13 }
    ldxdw r3, [r10-0x1d0]                   
    ldxdw r3, [r3+0x10]                     
    lddw r9, 0xb8142190745af430                     r9 load str located at -5182480366885473232
    jne r3, r9, lbb_2446                            if r3 != r9 { pc += 8 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r7, [r10-0x1d0]                   
    ldxdw r7, [r7+0x18]                     
    stxdw [r10-0x1d8], r7                   
    lddw r9, 0x4164f987ea077494                     r9 load str located at 4712165472278639764
    ldxdw r7, [r10-0x1d8]                   
    jeq r7, r9, lbb_2447                            if r7 == r9 { pc += 1 }
lbb_2446:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_2447:
    jne r3, 0, lbb_2587                             if r3 != (0 as i32 as i64 as u64) { pc += 139 }
    jne r5, 512, lbb_2919                           if r5 != (512 as i32 as i64 as u64) { pc += 470 }
    stxdw [r10-0x1d8], r1                   
    mov64 r1, r4                                    r1 = r4
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jne r1, 0, lbb_2924                             if r1 != (0 as i32 as i64 as u64) { pc += 471 }
    stxdw [r10-0x1f0], r2                   
    mov64 r8, r2                                    r8 = r2
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    lddw r1, 0x10002349c --> b"poolv1transfer_token_by_owner_with_mint calledInva"        r1 load str located at 4295111836
    stxdw [r10-0x98], r1                    
    mov64 r1, r4                                    r1 = r4
    add64 r1, 384                                   r1 += 384   ///  r1 = r1.wrapping_add(384 as i32 as i64 as u64)
    stxdw [r10-0x1e0], r1                   
    stxdw [r10-0x68], r1                    
    mov64 r9, r4                                    r9 = r4
    add64 r9, 416                                   r9 += 416   ///  r9 = r9.wrapping_add(416 as i32 as i64 as u64)
    stxdw [r10-0x78], r9                    
    stxdw [r10-0x200], r4                   
    add64 r4, 448                                   r4 += 448   ///  r4 = r4.wrapping_add(448 as i32 as i64 as u64)
    stxdw [r10-0x1e8], r4                   
    stxdw [r10-0x88], r4                    
    stdw [r10-0x60], 32                     
    stdw [r10-0x70], 32                     
    stdw [r10-0x80], 32                     
    stdw [r10-0x90], 6                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -400                                  r1 += -400   ///  r1 = r1.wrapping_add(-400 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -152                                  r2 += -152   ///  r2 = r2.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
    lddw r4, 0x100022d88 --> b"z\xfdt+'\xf7Y\xe9\xc6pp<\xd3\x9d\x81z\xa0\x93\x0a\xce;R\xd2mT\xa0T\xdd#\x…        r4 load str located at 4295110024
    call function_15426                     
    ldxdw r1, [r10-0x178]                   
    stxdw [r10-0x1a0], r1                   
    ldxdw r1, [r10-0x180]                   
    stxdw [r10-0x1a8], r1                   
    ldxdw r1, [r10-0x188]                   
    stxdw [r10-0x1b0], r1                   
    ldxdw r1, [r10-0x190]                   
    stxdw [r10-0x1b8], r1                   
    ldxb r2, [r10-0x170]                    
    stxb [r10-0x191], r2                    
    stxdw [r10-0x1f8], r8                   
    ldxdw r8, [r8+0x0]                      
    mov64 r2, r8                                    r2 = r8
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r3, [r8+0x8]                      
    jne r1, r3, lbb_2508                            if r1 != r3 { pc += 10 }
    ldxdw r1, [r2+0x8]                      
    ldxdw r3, [r10-0x1b0]                   
    jne r3, r1, lbb_2508                            if r3 != r1 { pc += 7 }
    ldxdw r1, [r2+0x10]                     
    ldxdw r3, [r10-0x1a8]                   
    jne r3, r1, lbb_2508                            if r3 != r1 { pc += 4 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r1, [r2+0x18]                     
    ldxdw r4, [r10-0x1a0]                   
    jeq r4, r1, lbb_2509                            if r4 == r1 { pc += 1 }
lbb_2508:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_2509:
    ldxdw r1, [r10-0x1d8]                   
    jeq r3, 0, lbb_2514                             if r3 == (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r8, 195951310                             r8 = 195951310 as i32 as i64 as u64
    ja lbb_2587                                     if true { pc += 73 }
lbb_2514:
    mov64 r3, r10                                   r3 = r10
    add64 r3, -401                                  r3 += -401   ///  r3 = r3.wrapping_add(-401 as i32 as i64 as u64)
    stxdw [r10-0x150], r3                   
    ldxdw r3, [r10-0x1e0]                   
    stxdw [r10-0x160], r3                   
    stxdw [r10-0x170], r9                   
    ldxdw r3, [r10-0x1e8]                   
    stxdw [r10-0x180], r3                   
    lddw r3, 0x10002349c --> b"poolv1transfer_token_by_owner_with_mint calledInva"        r3 load str located at 4295111836
    stxdw [r10-0x190], r3                   
    stdw [r10-0x148], 1                     
    stdw [r10-0x158], 32                    
    stdw [r10-0x168], 32                    
    stdw [r10-0x178], 32                    
    stdw [r10-0x188], 6                     
    mov64 r3, r10                                   r3 = r10
    add64 r3, -400                                  r3 += -400   ///  r3 = r3.wrapping_add(-400 as i32 as i64 as u64)
    stxdw [r10-0x140], r3                   
    stdw [r10-0x138], 5                     
    stxdw [r10-0xe0], r2                    
    ldxdw r3, [r10-0x1d0]                   
    stxdw [r10-0xf0], r3                    
    sth [r10-0xd8], 257                     
    sth [r10-0xe8], 257                     
    lddw r3, 0xe959f7272b74fd7a                     r3 load str located at -1632001642340221574
    stxdw [r10-0xb8], r3                    
    lddw r3, 0x7a819dd33c7070c6                     r3 load str located at 8827510275200544966
    stxdw [r10-0xb0], r3                    
    lddw r3, 0x6dd2523bce0a93a0                     r3 load str located at 7913477912056730528
    stxdw [r10-0xa8], r3                    
    lddw r3, 0xd3bb8723dd54a054                     r3 load str located at -3189807322954948524
    stxdw [r10-0xa0], r3                    
    stdw [r10-0xc0], 1728                   
    stdw [r10-0xc8], 12917760               
    stw [r10-0xcc], 0                       
    ldxb r3, [r6+0x0]                       
    jne r3, 255, lbb_2586                           if r3 != (255 as i32 as i64 as u64) { pc += 30 }
    ldxb r5, [r6+0x1]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jeq r5, 0, lbb_2590                             if r5 == (0 as i32 as i64 as u64) { pc += 30 }
    ldxb r0, [r6+0x2]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r0, 0, lbb_2594                             if r0 == (0 as i32 as i64 as u64) { pc += 31 }
lbb_2563:
    stxdw [r10-0x208], r8                   
    ldxb r0, [r6+0x3]                       
    jne r0, 0, lbb_2567                             if r0 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_2566:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_2567:
    ldxdw r0, [r6+0x50]                     
    mov64 r8, r6                                    r8 = r6
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r8                    
    mov64 r8, r6                                    r8 = r6
    add64 r8, 88                                    r8 += 88   ///  r8 = r8.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r8                    
    stxdw [r10-0x88], r0                    
    add64 r6, 72                                    r6 += 72   ///  r6 = r6.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r6                    
    ldxdw r0, [r10-0x1d0]                   
    stxdw [r10-0x98], r0                    
    stxb [r10-0x66], r4                     
    stxb [r10-0x67], r5                     
    stxb [r10-0x68], r3                     
    stdw [r10-0x70], 0                      
    ldxdw r6, [r10-0x208]                   
    ldxb r3, [r6+0x0]                       
    jeq r3, 255, lbb_2599                           if r3 == (255 as i32 as i64 as u64) { pc += 13 }
lbb_2586:
    mov64 r0, 11                                    r0 = 11 as i32 as i64 as u64
lbb_2587:
    stxw [r1+0x4], r8                       
    stxw [r1+0x0], r0                       
    exit                                    
lbb_2590:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxb r0, [r6+0x2]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_2563                             if r0 != (0 as i32 as i64 as u64) { pc += -31 }
lbb_2594:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    stxdw [r10-0x208], r8                   
    ldxb r0, [r6+0x3]                       
    jeq r0, 0, lbb_2566                             if r0 == (0 as i32 as i64 as u64) { pc += -32 }
    ja lbb_2567                                     if true { pc += -32 }
lbb_2599:
    ldxb r1, [r6+0x1]                       
    ldxb r3, [r6+0x2]                       
    ldxb r4, [r6+0x3]                       
    ldxdw r5, [r6+0x50]                     
    mov64 r0, r6                                    r0 = r6
    add64 r0, 40                                    r0 += 40   ///  r0 = r0.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r0                    
    stxdw [r10-0x50], r5                    
    mov64 r5, r6                                    r5 = r6
    add64 r5, 72                                    r5 += 72   ///  r5 = r5.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r5                    
    stxdw [r10-0x60], r2                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_2614                             if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_2614:
    stxb [r10-0x2e], r2                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r4, [r10-0x1f0]                   
    jne r3, 0, lbb_2619                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_2619:
    add64 r4, 32                                    r4 += 32   ///  r4 = r4.wrapping_add(32 as i32 as i64 as u64)
    stxdw [r10-0x1d0], r4                   
    stxb [r10-0x2f], r2                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_2625                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_2625:
    stxb [r10-0x30], r2                     
    add64 r6, 88                                    r6 += 88   ///  r6 = r6.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x218], r6                   
    stxdw [r10-0x48], r6                    
    stdw [r10-0x38], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -204                                  r1 += -204   ///  r1 = r1.wrapping_add(-204 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -240                                  r1 += -240   ///  r1 = r1.wrapping_add(-240 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100022e08 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295110152
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 52                      
    stdw [r10-0x18], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -152                                  r2 += -152   ///  r2 = r2.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -320                                  r4 += -320   ///  r4 = r4.wrapping_add(-320 as i32 as i64 as u64)
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    syscall [invalid]                       
    stxdw [r10-0x78], r9                    
    lddw r1, 0x100022de8 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r1 load str located at 4295110120
    stxdw [r10-0x88], r1                    
    mov64 r8, r10                                   r8 = r10
    add64 r8, -440                                  r8 += -440   ///  r8 = r8.wrapping_add(-440 as i32 as i64 as u64)
    stxdw [r10-0x98], r8                    
    stdw [r10-0x70], 32                     
    stdw [r10-0x80], 32                     
    stdw [r10-0x90], 32                     
    lddw r1, 0xd3bb8723dd54a054                     r1 load str located at -3189807322954948524
    stxdw [r10-0x10], r1                    
    lddw r1, 0x6dd2523bce0a93a0                     r1 load str located at 7913477912056730528
    stxdw [r10-0x18], r1                    
    lddw r6, 0x7a819dd33c7070c6                     r6 load str located at 8827510275200544966
    stxdw [r10-0x20], r6                    
    lddw r7, 0xe959f7272b74fd7a                     r7 load str located at -1632001642340221574
    stxdw [r10-0x28], r7                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -204                                  r1 += -204   ///  r1 = r1.wrapping_add(-204 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -152                                  r2 += -152   ///  r2 = r2.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -40                                   r4 += -40   ///  r4 = r4.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r3, 3                                     r3 = 3 as i32 as i64 as u64
    call function_15426                     
    ldxdw r1, [r10-0xb4]                    
    stxdw [r10-0x118], r1                   
    ldxdw r1, [r10-0xbc]                    
    stxdw [r10-0x120], r1                   
    ldxdw r1, [r10-0xc4]                    
    stxdw [r10-0x128], r1                   
    ldxdw r1, [r10-0xcc]                    
    stxdw [r10-0x130], r1                   
    ldxb r1, [r10-0xac]                     
    stxdw [r10-0x210], r1                   
    ldxdw r1, [r10-0x1e0]                   
    stxdw [r10-0x78], r1                    
    lddw r1, 0x100022de8 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r1 load str located at 4295110120
    stxdw [r10-0x88], r1                    
    stxdw [r10-0x98], r8                    
    stdw [r10-0x70], 32                     
    stdw [r10-0x80], 32                     
    stdw [r10-0x90], 32                     
    lddw r1, 0xd3bb8723dd54a054                     r1 load str located at -3189807322954948524
    stxdw [r10-0x10], r1                    
    lddw r1, 0x6dd2523bce0a93a0                     r1 load str located at 7913477912056730528
    stxdw [r10-0x18], r1                    
    stxdw [r10-0x20], r6                    
    stxdw [r10-0x28], r7                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -204                                  r1 += -204   ///  r1 = r1.wrapping_add(-204 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -152                                  r2 += -152   ///  r2 = r2.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -40                                   r4 += -40   ///  r4 = r4.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r3, 3                                     r3 = 3 as i32 as i64 as u64
    call function_15426                     
    ldxdw r1, [r10-0xb4]                    
    stxdw [r10-0xf8], r1                    
    ldxdw r1, [r10-0xbc]                    
    stxdw [r10-0x100], r1                   
    ldxdw r1, [r10-0xc4]                    
    stxdw [r10-0x108], r1                   
    ldxdw r1, [r10-0xcc]                    
    stxdw [r10-0x110], r1                   
    ldxdw r2, [r9+0x0]                      
    ldxdw r1, [r10-0x1d0]                   
    ldxdw r1, [r1+0x0]                      
    ldxdw r3, [r1+0x8]                      
    jne r2, r3, lbb_2738                            if r2 != r3 { pc += 10 }
    ldxdw r2, [r1+0x10]                     
    ldxdw r3, [r9+0x8]                      
    jne r3, r2, lbb_2738                            if r3 != r2 { pc += 7 }
    ldxdw r2, [r1+0x18]                     
    ldxdw r3, [r9+0x10]                     
    jne r3, r2, lbb_2738                            if r3 != r2 { pc += 4 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r1, [r1+0x20]                     
    ldxdw r2, [r9+0x18]                     
    jeq r2, r1, lbb_2739                            if r2 == r1 { pc += 1 }
lbb_2738:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_2739:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    lddw r8, 0xabad1dea                             r8 load str located at 2880249322
    ldxdw r1, [r10-0x1d8]                   
    ldxdw r2, [r10-0x1f0]                   
    jne r3, 0, lbb_2587                             if r3 != (0 as i32 as i64 as u64) { pc += -158 }
    mov64 r7, r2                                    r7 = r2
    add64 r7, 40                                    r7 += 40   ///  r7 = r7.wrapping_add(40 as i32 as i64 as u64)
    ldxdw r3, [r10-0x1e0]                   
    ldxdw r4, [r3+0x0]                      
    ldxdw r3, [r7+0x0]                      
    ldxdw r5, [r3+0x8]                      
    jne r4, r5, lbb_2765                            if r4 != r5 { pc += 13 }
    ldxdw r4, [r3+0x10]                     
    ldxdw r5, [r10-0x1e0]                   
    ldxdw r5, [r5+0x8]                      
    jne r5, r4, lbb_2765                            if r5 != r4 { pc += 9 }
    ldxdw r4, [r3+0x18]                     
    ldxdw r5, [r10-0x1e0]                   
    ldxdw r5, [r5+0x10]                     
    jne r5, r4, lbb_2765                            if r5 != r4 { pc += 5 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r3, [r3+0x20]                     
    ldxdw r5, [r10-0x1e0]                   
    ldxdw r5, [r5+0x18]                     
    jeq r5, r3, lbb_2766                            if r5 == r3 { pc += 1 }
lbb_2765:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
lbb_2766:
    jne r4, 0, lbb_2587                             if r4 != (0 as i32 as i64 as u64) { pc += -180 }
    mov64 r3, r2                                    r3 = r2
    add64 r3, 16                                    r3 += 16   ///  r3 = r3.wrapping_add(16 as i32 as i64 as u64)
    ldxdw r4, [r3+0x0]                      
    ldxdw r5, [r4+0x8]                      
    ldxdw r6, [r10-0x130]                   
    jne r6, r5, lbb_2783                            if r6 != r5 { pc += 10 }
    ldxdw r5, [r4+0x10]                     
    ldxdw r6, [r10-0x128]                   
    jne r6, r5, lbb_2783                            if r6 != r5 { pc += 7 }
    ldxdw r5, [r4+0x18]                     
    ldxdw r6, [r10-0x120]                   
    jne r6, r5, lbb_2783                            if r6 != r5 { pc += 4 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r4, [r4+0x20]                     
    ldxdw r6, [r10-0x118]                   
    jeq r6, r4, lbb_2784                            if r6 == r4 { pc += 1 }
lbb_2783:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_2784:
    lddw r8, 0xbadface3                             r8 load str located at 3135220963
    jne r5, 0, lbb_2587                             if r5 != (0 as i32 as i64 as u64) { pc += -200 }
    mov64 r4, r2                                    r4 = r2
    add64 r4, 24                                    r4 += 24   ///  r4 = r4.wrapping_add(24 as i32 as i64 as u64)
    stxdw [r10-0x220], r4                   
    ldxdw r4, [r4+0x0]                      
    ldxdw r5, [r4+0x8]                      
    ldxdw r6, [r10-0x110]                   
    jne r6, r5, lbb_2804                            if r6 != r5 { pc += 10 }
    ldxdw r5, [r4+0x10]                     
    ldxdw r6, [r10-0x108]                   
    jne r6, r5, lbb_2804                            if r6 != r5 { pc += 7 }
    ldxdw r5, [r4+0x18]                     
    ldxdw r6, [r10-0x100]                   
    jne r6, r5, lbb_2804                            if r6 != r5 { pc += 4 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r4, [r4+0x20]                     
    ldxdw r6, [r10-0xf8]                    
    jeq r6, r4, lbb_2805                            if r6 == r4 { pc += 1 }
lbb_2804:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_2805:
    jne r5, 0, lbb_2587                             if r5 != (0 as i32 as i64 as u64) { pc += -219 }
    ldxb r6, [r10-0xac]                     
    ldxdw r1, [r10-0x210]                   
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0x1d0]                   
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -448                                  r1 += -448   ///  r1 = r1.wrapping_add(-448 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r8, [r10-0x1f8]                   
    mov64 r4, r8                                    r4 = r8
    call function_1938                      
    ldxw r0, [r10-0x1c0]                    
    jne r0, 26, lbb_2917                            if r0 != (26 as i32 as i64 as u64) { pc += 98 }
    stxdw [r10-0xff8], r6                   
    stxdw [r10-0x1000], r7                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -456                                  r1 += -456   ///  r1 = r1.wrapping_add(-456 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0x1f0]                   
    ldxdw r3, [r10-0x220]                   
    mov64 r4, r8                                    r4 = r8
    call function_1938                      
    ldxw r0, [r10-0x1c8]                    
    jne r0, 26, lbb_2929                            if r0 != (26 as i32 as i64 as u64) { pc += 99 }
    ldxdw r7, [r10-0x208]                   
    ldxdw r2, [r7+0x50]                     
    ldxdw r8, [r10-0x218]                   
    mov64 r1, r8                                    r1 = r8
    call function_6615                      
    ldxdw r6, [r10-0x200]                   
    mov64 r2, r6                                    r2 = r6
    add64 r2, 240                                   r2 += 240   ///  r2 = r2.wrapping_add(240 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    mov64 r3, 144                                   r3 = 144 as i32 as i64 as u64
    call function_17012                     
    mov64 r1, r7                                    r1 = r7
    add64 r1, 232                                   r1 += 232   ///  r1 = r1.wrapping_add(232 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 240                                   r3 = 240 as i32 as i64 as u64
    call function_17012                     
    ldxdw r2, [r10-0x1e8]                   
    ldxdw r1, [r2+0x18]                     
    stxdw [r7+0x270], r1                    
    ldxdw r1, [r2+0x10]                     
    stxdw [r7+0x268], r1                    
    ldxdw r1, [r2+0x8]                      
    stxdw [r7+0x260], r1                    
    ldxdw r1, [r2+0x0]                      
    stxdw [r7+0x258], r1                    
    ldxdw r1, [r9+0x18]                     
    stxdw [r7+0x210], r1                    
    ldxdw r1, [r9+0x10]                     
    stxdw [r7+0x208], r1                    
    ldxdw r1, [r9+0x8]                      
    stxdw [r7+0x200], r1                    
    ldxdw r1, [r9+0x0]                      
    stxdw [r7+0x1f8], r1                    
    ldxdw r2, [r10-0x1e0]                   
    ldxdw r1, [r2+0x0]                      
    stxdw [r7+0x1d8], r1                    
    ldxdw r1, [r2+0x8]                      
    stxdw [r7+0x1e0], r1                    
    ldxdw r1, [r2+0x10]                     
    stxdw [r7+0x1e8], r1                    
    ldxdw r1, [r2+0x18]                     
    stxdw [r7+0x1f0], r1                    
    ldxdw r1, [r10-0x130]                   
    stxdw [r7+0x238], r1                    
    ldxdw r1, [r10-0x128]                   
    stxdw [r7+0x240], r1                    
    ldxdw r1, [r10-0x120]                   
    stxdw [r7+0x248], r1                    
    ldxdw r1, [r10-0x118]                   
    stxdw [r7+0x250], r1                    
    ldxdw r1, [r10-0x110]                   
    stxdw [r7+0x218], r1                    
    ldxdw r1, [r10-0x108]                   
    stxdw [r7+0x220], r1                    
    ldxdw r1, [r10-0x100]                   
    stxdw [r7+0x228], r1                    
    ldxdw r1, [r10-0xf8]                    
    stxdw [r7+0x230], r1                    
    ldxb r1, [r10-0x191]                    
    stxb [r7+0x290], r1                     
    stw [r7+0x291], 0                       
    stw [r7+0x294], 0                       
    ldxb r1, [r6+0x1f0]                     
    stxb [r7+0x288], r1                     
    stw [r7+0x289], 0                       
    stw [r7+0x28c], 0                       
    ldxdw r1, [r6+0x1e0]                    
    ldxdw r2, [r6+0x1e8]                    
    stxdw [r7+0x280], r2                    
    stxdw [r7+0x278], r1                    
    stdw [r7+0x328], 0                      
    stdw [r7+0x330], 0                      
    stdw [r7+0x338], 0                      
    stdw [r7+0x340], 0                      
    stdw [r7+0x348], 0                      
    stdw [r7+0x350], 0                      
    stdw [r7+0x358], 0                      
    stdw [r7+0x360], 0                      
    stdw [r7+0x368], 0                      
    stdw [r7+0x370], 0                      
    ldxdw r2, [r7+0x50]                     
    mov64 r1, r8                                    r1 = r8
    call function_6615                      
    mov64 r1, r8                                    r1 = r8
    call function_14670                     
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
    ja lbb_2930                                     if true { pc += 13 }
lbb_2917:
    ldxw r8, [r10-0x1bc]                    
    ja lbb_2930                                     if true { pc += 11 }
lbb_2919:
    lddw r1, 0x1000230d3 --> b"from_bytes"          r1 load str located at 4295110867
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_581                       
lbb_2924:
    lddw r1, 0x1000230d3 --> b"from_bytes"          r1 load str located at 4295110867
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_581                       
lbb_2929:
    ldxw r8, [r10-0x1c4]                    
lbb_2930:
    ldxdw r1, [r10-0x1d8]                   
    ja lbb_2587                                     if true { pc += -345 }

function_2932:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    lddw r0, 0xbadc0de1                             r0 load str located at 3134983649
    jlt r3, 10, lbb_3746                            if r3 < (10 as i32 as i64 as u64) { pc += 810 }
    lddw r0, 0xabad1dea                             r0 load str located at 2880249322
    ldxdw r7, [r2+0x0]                      
    ldxb r3, [r7+0x1]                       
    jeq r3, 0, lbb_3746                             if r3 == (0 as i32 as i64 as u64) { pc += 805 }
    ldxdw r3, [r7+0x8]                      
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    lddw r6, 0xcd4df2193b88a1c0                     r6 load str located at -3652997532540689984
    jne r3, r6, lbb_2959                            if r3 != r6 { pc += 13 }
    ldxdw r3, [r7+0x8]                      
    lddw r6, 0xeadff0f6e82402a1                     r6 load str located at -1522233205780643167
    jne r3, r6, lbb_2959                            if r3 != r6 { pc += 9 }
    ldxdw r3, [r7+0x10]                     
    lddw r6, 0xb8142190745af430                     r6 load str located at -5182480366885473232
    jne r3, r6, lbb_2959                            if r3 != r6 { pc += 5 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r6, [r7+0x18]                     
    lddw r9, 0x4164f987ea077494                     r9 load str located at 4712165472278639764
    jeq r6, r9, lbb_2960                            if r6 == r9 { pc += 1 }
lbb_2959:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_2960:
    jne r3, 0, lbb_3746                             if r3 != (0 as i32 as i64 as u64) { pc += 785 }
    jne r5, 672, lbb_3332                           if r5 != (672 as i32 as i64 as u64) { pc += 370 }
    mov64 r6, r2                                    r6 = r2
    stxdw [r10-0x498], r1                   
    mov64 r1, r4                                    r1 = r4
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jne r1, 0, lbb_3337                             if r1 != (0 as i32 as i64 as u64) { pc += 370 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1120                                 r1 += -1120   ///  r1 = r1.wrapping_add(-1120 as i32 as i64 as u64)
    mov64 r2, r4                                    r2 = r4
    mov64 r3, 672                                   r3 = 672 as i32 as i64 as u64
    call function_17012                     
    mov64 r2, r6                                    r2 = r6
    ldxdw r4, [r2+0x38]                     
    ldxdw r1, [r4+0x8]                      
    lddw r3, 0x93a165d7e1f6dd06                     r3 load str located at -7808848301000303354
    jne r1, r3, lbb_2991                            if r1 != r3 { pc += 13 }
    ldxdw r1, [r4+0x10]                     
    lddw r3, 0xac79ebce46e1cbd9                     r3 load str located at -6018520155818964007
    jne r1, r3, lbb_2991                            if r1 != r3 { pc += 9 }
    ldxdw r1, [r4+0x18]                     
    lddw r3, 0x91375b5fed85b41c                     r3 load str located at -7982811346925931492
    jne r1, r3, lbb_2991                            if r1 != r3 { pc += 5 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r1, [r4+0x20]                     
    lddw r3, 0xa900ff7e85f58c3a                     r3 load str located at -6268729762421306310
    jeq r1, r3, lbb_2992                            if r1 == r3 { pc += 1 }
lbb_2991:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_2992:
    ldxdw r3, [r2+0x40]                     
    ldxdw r1, [r10-0x498]                   
    jeq r5, 0, lbb_3014                             if r5 == (0 as i32 as i64 as u64) { pc += 19 }
    ldxdw r5, [r4+0x8]                      
    lddw r0, 0xde8f75eee1f6dd06                     r0 load str located at -2409577606766207738
    jne r5, r0, lbb_3012                            if r5 != r0 { pc += 13 }
    ldxdw r5, [r4+0x10]                     
    lddw r0, 0xdacd6ce4bc5d4218                     r0 load str located at -2680366473547005416
    jne r5, r0, lbb_3012                            if r5 != r0 { pc += 9 }
    ldxdw r5, [r4+0x18]                     
    lddw r0, 0x270db9834dfc1ab6                     r0 load str located at 2814109315776649910
    jne r5, r0, lbb_3012                            if r5 != r0 { pc += 5 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r4, [r4+0x20]                     
    lddw r0, 0xfc8ba1d828f9bdfe                     r0 load str located at -248927404616466946
    jeq r4, r0, lbb_3013                            if r4 == r0 { pc += 1 }
lbb_3012:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_3013:
    jne r5, 0, lbb_3052                             if r5 != (0 as i32 as i64 as u64) { pc += 38 }
lbb_3014:
    ldxdw r4, [r3+0x8]                      
    lddw r5, 0x93a165d7e1f6dd06                     r5 load str located at -7808848301000303354
    jne r4, r5, lbb_3031                            if r4 != r5 { pc += 13 }
    ldxdw r4, [r3+0x10]                     
    lddw r5, 0xac79ebce46e1cbd9                     r5 load str located at -6018520155818964007
    jne r4, r5, lbb_3031                            if r4 != r5 { pc += 9 }
    ldxdw r4, [r3+0x18]                     
    lddw r5, 0x91375b5fed85b41c                     r5 load str located at -7982811346925931492
    jne r4, r5, lbb_3031                            if r4 != r5 { pc += 5 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r5, [r3+0x20]                     
    lddw r0, 0xa900ff7e85f58c3a                     r0 load str located at -6268729762421306310
    jeq r5, r0, lbb_3032                            if r5 == r0 { pc += 1 }
lbb_3031:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
lbb_3032:
    jeq r4, 0, lbb_3055                             if r4 == (0 as i32 as i64 as u64) { pc += 22 }
    ldxdw r4, [r3+0x8]                      
    lddw r5, 0xde8f75eee1f6dd06                     r5 load str located at -2409577606766207738
    jne r4, r5, lbb_3050                            if r4 != r5 { pc += 13 }
    ldxdw r4, [r3+0x10]                     
    lddw r5, 0xdacd6ce4bc5d4218                     r5 load str located at -2680366473547005416
    jne r4, r5, lbb_3050                            if r4 != r5 { pc += 9 }
    ldxdw r4, [r3+0x18]                     
    lddw r5, 0x270db9834dfc1ab6                     r5 load str located at 2814109315776649910
    jne r4, r5, lbb_3050                            if r4 != r5 { pc += 5 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r3, [r3+0x20]                     
    lddw r5, 0xfc8ba1d828f9bdfe                     r5 load str located at -248927404616466946
    jeq r3, r5, lbb_3051                            if r3 == r5 { pc += 1 }
lbb_3050:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
lbb_3051:
    jeq r4, 0, lbb_3055                             if r4 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_3052:
    lddw r0, 0xbadc0de3                             r0 load str located at 3134983651
    ja lbb_3746                                     if true { pc += 691 }
lbb_3055:
    mov64 r9, r2                                    r9 = r2
    add64 r9, 16                                    r9 += 16   ///  r9 = r9.wrapping_add(16 as i32 as i64 as u64)
    ldxdw r0, [r9+0x0]                      
    ldxdw r3, [r0+0x8]                      
    ldxdw r4, [r10-0x270]                   
    jne r3, r4, lbb_3071                            if r3 != r4 { pc += 10 }
    ldxdw r3, [r0+0x10]                     
    ldxdw r4, [r10-0x268]                   
    jne r3, r4, lbb_3071                            if r3 != r4 { pc += 7 }
    ldxdw r3, [r0+0x18]                     
    ldxdw r4, [r10-0x260]                   
    jne r3, r4, lbb_3071                            if r3 != r4 { pc += 4 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r0+0x20]                     
    ldxdw r5, [r10-0x258]                   
    jeq r4, r5, lbb_3072                            if r4 == r5 { pc += 1 }
lbb_3071:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_3072:
    stxdw [r10-0x4a0], r0                   
    lddw r0, 0xbadc0de4                             r0 load str located at 3134983652
    jne r3, 0, lbb_3746                             if r3 != (0 as i32 as i64 as u64) { pc += 670 }
    mov64 r3, r2                                    r3 = r2
    add64 r3, 24                                    r3 += 24   ///  r3 = r3.wrapping_add(24 as i32 as i64 as u64)
    stxdw [r10-0x4b0], r3                   
    ldxdw r3, [r3+0x0]                      
    stxdw [r10-0x4a8], r3                   
    ldxdw r3, [r3+0x8]                      
    ldxdw r4, [r10-0x290]                   
    jne r3, r4, lbb_3097                            if r3 != r4 { pc += 13 }
    ldxdw r3, [r10-0x4a8]                   
    ldxdw r3, [r3+0x10]                     
    ldxdw r4, [r10-0x288]                   
    jne r3, r4, lbb_3097                            if r3 != r4 { pc += 9 }
    ldxdw r3, [r10-0x4a8]                   
    ldxdw r3, [r3+0x18]                     
    ldxdw r4, [r10-0x280]                   
    jne r3, r4, lbb_3097                            if r3 != r4 { pc += 5 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0x4a8]                   
    ldxdw r4, [r4+0x20]                     
    ldxdw r5, [r10-0x278]                   
    jeq r4, r5, lbb_3098                            if r4 == r5 { pc += 1 }
lbb_3097:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_3098:
    jne r3, 0, lbb_3746                             if r3 != (0 as i32 as i64 as u64) { pc += 647 }
    mov64 r3, r2                                    r3 = r2
    add64 r3, 32                                    r3 += 32   ///  r3 = r3.wrapping_add(32 as i32 as i64 as u64)
    stxdw [r10-0x4c0], r3                   
    ldxdw r0, [r3+0x0]                      
    ldxdw r3, [r0+0x8]                      
    ldxdw r4, [r10-0x230]                   
    jne r3, r4, lbb_3116                            if r3 != r4 { pc += 10 }
    ldxdw r3, [r0+0x10]                     
    ldxdw r4, [r10-0x228]                   
    jne r3, r4, lbb_3116                            if r3 != r4 { pc += 7 }
    ldxdw r3, [r0+0x18]                     
    ldxdw r4, [r10-0x220]                   
    jne r3, r4, lbb_3116                            if r3 != r4 { pc += 4 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r0+0x20]                     
    ldxdw r5, [r10-0x218]                   
    jeq r4, r5, lbb_3117                            if r4 == r5 { pc += 1 }
lbb_3116:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_3117:
    stxdw [r10-0x4b8], r0                   
    lddw r0, 0xbadc0de5                             r0 load str located at 3134983653
    jne r3, 0, lbb_3746                             if r3 != (0 as i32 as i64 as u64) { pc += 625 }
    mov64 r3, r2                                    r3 = r2
    add64 r3, 40                                    r3 += 40   ///  r3 = r3.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x4d0], r3                   
    ldxdw r3, [r3+0x0]                      
    stxdw [r10-0x4c8], r3                   
    ldxdw r3, [r3+0x8]                      
    ldxdw r4, [r10-0x250]                   
    jne r3, r4, lbb_3142                            if r3 != r4 { pc += 13 }
    ldxdw r3, [r10-0x4c8]                   
    ldxdw r3, [r3+0x10]                     
    ldxdw r4, [r10-0x248]                   
    jne r3, r4, lbb_3142                            if r3 != r4 { pc += 9 }
    ldxdw r3, [r10-0x4c8]                   
    ldxdw r3, [r3+0x18]                     
    ldxdw r4, [r10-0x240]                   
    jne r3, r4, lbb_3142                            if r3 != r4 { pc += 5 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0x4c8]                   
    ldxdw r4, [r4+0x20]                     
    ldxdw r5, [r10-0x238]                   
    jeq r4, r5, lbb_3143                            if r4 == r5 { pc += 1 }
lbb_3142:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_3143:
    jne r3, 0, lbb_3746                             if r3 != (0 as i32 as i64 as u64) { pc += 602 }
    ldxdw r3, [r7+0x0]                      
    ldxdw r4, [r10-0x210]                   
    jne r3, r4, lbb_3157                            if r3 != r4 { pc += 10 }
    ldxdw r3, [r7+0x8]                      
    ldxdw r4, [r10-0x208]                   
    jne r3, r4, lbb_3157                            if r3 != r4 { pc += 7 }
    ldxdw r3, [r7+0x10]                     
    ldxdw r4, [r10-0x200]                   
    jne r3, r4, lbb_3157                            if r3 != r4 { pc += 4 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r7+0x18]                     
    ldxdw r5, [r10-0x1f8]                   
    jeq r4, r5, lbb_3158                            if r4 == r5 { pc += 1 }
lbb_3157:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_3158:
    lddw r0, 0xbadc0de6                             r0 load str located at 3134983654
    jne r3, 0, lbb_3746                             if r3 != (0 as i32 as i64 as u64) { pc += 585 }
    mov64 r8, r2                                    r8 = r2
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -624                                  r3 += -624   ///  r3 = r3.wrapping_add(-624 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -656                                  r4 += -656   ///  r4 = r4.wrapping_add(-656 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -528                                  r2 += -528   ///  r2 = r2.wrapping_add(-528 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    stxdw [r10-0x4f0], r2                   
    stxdw [r10-0x4d8], r3                   
    stxdw [r10-0x4e0], r4                   
    lddw r5, 0x100022d88 --> b"z\xfdt+'\xf7Y\xe9\xc6pp<\xd3\x9d\x81z\xa0\x93\x0a\xce;R\xd2mT\xa0T\xdd#\x…        r5 load str located at 4295110024
    call function_1829                      
    ldxdw r1, [r10-0xb8]                    
    stxdw [r10-0x1a8], r1                   
    ldxdw r1, [r10-0xc0]                    
    stxdw [r10-0x1b0], r1                   
    ldxdw r1, [r10-0xc8]                    
    stxdw [r10-0x1b8], r1                   
    ldxdw r2, [r10-0xd0]                    
    stxdw [r10-0x1c0], r2                   
    ldxb r1, [r10-0xb0]                     
    stxb [r10-0x199], r1                    
    stxdw [r10-0x4e8], r8                   
    ldxdw r5, [r8+0x0]                      
    mov64 r1, r5                                    r1 = r5
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r3, [r5+0x8]                      
    jne r2, r3, lbb_3203                            if r2 != r3 { pc += 10 }
    ldxdw r2, [r1+0x8]                      
    ldxdw r3, [r10-0x1b8]                   
    jne r3, r2, lbb_3203                            if r3 != r2 { pc += 7 }
    ldxdw r2, [r1+0x10]                     
    ldxdw r3, [r10-0x1b0]                   
    jne r3, r2, lbb_3203                            if r3 != r2 { pc += 4 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r3, [r1+0x18]                     
    ldxdw r4, [r10-0x1a8]                   
    jeq r4, r3, lbb_3204                            if r4 == r3 { pc += 1 }
lbb_3203:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_3204:
    jeq r2, 0, lbb_3209                             if r2 == (0 as i32 as i64 as u64) { pc += 4 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    lddw r0, 0xbadc0de7                             r0 load str located at 3134983655
    ja lbb_3745                                     if true { pc += 536 }
lbb_3209:
    stxdw [r10-0x4f8], r5                   
    mov64 r2, r10                                   r2 = r10
    add64 r2, -409                                  r2 += -409   ///  r2 = r2.wrapping_add(-409 as i32 as i64 as u64)
    stxdw [r10-0x158], r2                   
    ldxdw r2, [r10-0x4e0]                   
    stxdw [r10-0x168], r2                   
    ldxdw r2, [r10-0x4d8]                   
    stxdw [r10-0x178], r2                   
    ldxdw r2, [r10-0x4f0]                   
    stxdw [r10-0x188], r2                   
    lddw r2, 0x10002349c --> b"poolv1transfer_token_by_owner_with_mint calledInva"        r2 load str located at 4295111836
    stxdw [r10-0x198], r2                   
    stdw [r10-0x150], 1                     
    stdw [r10-0x160], 32                    
    stdw [r10-0x170], 32                    
    stdw [r10-0x180], 32                    
    stdw [r10-0x190], 6                     
    mov64 r2, r10                                   r2 = r10
    add64 r2, -408                                  r2 += -408   ///  r2 = r2.wrapping_add(-408 as i32 as i64 as u64)
    stxdw [r10-0x148], r2                   
    stdw [r10-0x140], 5                     
    stxdw [r10-0x128], r1                   
    stxdw [r10-0x138], r7                   
    sth [r10-0x120], 257                    
    sth [r10-0x130], 257                    
    lddw r1, 0xe959f7272b74fd7a                     r1 load str located at -1632001642340221574
    stxdw [r10-0x104], r1                   
    lddw r1, 0x7a819dd33c7070c6                     r1 load str located at 8827510275200544966
    stxdw [r10-0xfc], r1                    
    lddw r1, 0x6dd2523bce0a93a0                     r1 load str located at 7913477912056730528
    stxdw [r10-0xf4], r1                    
    lddw r1, 0xd3bb8723dd54a054                     r1 load str located at -3189807322954948524
    stxdw [r10-0xec], r1                    
    stdw [r10-0x10c], 1728                  
    stdw [r10-0x114], 12917760              
    stw [r10-0x118], 0                      
    ldxdw r1, [r10-0x4e8]                   
    stxdw [r10-0xd8], r1                    
    stxdw [r10-0xe0], r6                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    stxdw [r10-0x28], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -280                                  r1 += -280   ///  r1 = r1.wrapping_add(-280 as i32 as i64 as u64)
    stxdw [r10-0x48], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -312                                  r1 += -312   ///  r1 = r1.wrapping_add(-312 as i32 as i64 as u64)
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    stxdw [r10-0x58], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -224                                  r1 += -224   ///  r1 = r1.wrapping_add(-224 as i32 as i64 as u64)
    stxdw [r10-0x60], r1                    
    stdw [r10-0x8], 2                       
    stdw [r10-0x10], 2                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x30], 2                      
    stdw [r10-0x38], 2                      
    stdw [r10-0x40], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    call function_18                        
    mov64 r8, r0                                    r8 = r0
    mov64 r1, r8                                    r1 = r8
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jne r1, 26, lbb_3745                            if r1 != (26 as i32 as i64 as u64) { pc += 460 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -280                                  r1 += -280   ///  r1 = r1.wrapping_add(-280 as i32 as i64 as u64)
    stxdw [r10-0x48], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -312                                  r1 += -312   ///  r1 = r1.wrapping_add(-312 as i32 as i64 as u64)
    stxdw [r10-0x58], r1                    
    lddw r1, 0x100022e08 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295110152
    stxdw [r10-0x60], r1                    
    stdw [r10-0x40], 52                     
    stdw [r10-0x50], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -208                                  r2 += -208   ///  r2 = r2.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -328                                  r4 += -328   ///  r4 = r4.wrapping_add(-328 as i32 as i64 as u64)
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    syscall [invalid]                       
    ldxdw r3, [r10-0x4a0]                   
    ldxdw r1, [r3+0x28]                     
    lddw r2, 0xde8f75eee1f6dd06                     r2 load str located at -2409577606766207738
    jne r1, r2, lbb_3323                            if r1 != r2 { pc += 13 }
    ldxdw r1, [r3+0x30]                     
    lddw r2, 0xdacd6ce4bc5d4218                     r2 load str located at -2680366473547005416
    jne r1, r2, lbb_3323                            if r1 != r2 { pc += 9 }
    ldxdw r1, [r3+0x38]                     
    lddw r2, 0x270db9834dfc1ab6                     r2 load str located at 2814109315776649910
    jne r1, r2, lbb_3323                            if r1 != r2 { pc += 5 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r3+0x40]                     
    lddw r3, 0xfc8ba1d828f9bdfe                     r3 load str located at -248927404616466946
    jeq r2, r3, lbb_3324                            if r2 == r3 { pc += 1 }
lbb_3323:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_3324:
    jeq r1, 0, lbb_3342                             if r1 == (0 as i32 as i64 as u64) { pc += 17 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -448                                  r2 += -448   ///  r2 = r2.wrapping_add(-448 as i32 as i64 as u64)
    ldxdw r3, [r10-0x4d8]                   
    call function_1858                      
    ja lbb_3348                                     if true { pc += 16 }
lbb_3332:
    lddw r1, 0x1000230d3 --> b"from_bytes"          r1 load str located at 4295110867
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_581                       
lbb_3337:
    lddw r1, 0x1000230d3 --> b"from_bytes"          r1 load str located at 4295110867
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_581                       
lbb_3342:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -448                                  r2 += -448   ///  r2 = r2.wrapping_add(-448 as i32 as i64 as u64)
    ldxdw r3, [r10-0x4d8]                   
    call function_1898                      
lbb_3348:
    ldxdw r1, [r10-0xb8]                    
    stxdw [r10-0x100], r1                   
    ldxdw r1, [r10-0xc0]                    
    stxdw [r10-0x108], r1                   
    ldxdw r1, [r10-0xc8]                    
    stxdw [r10-0x110], r1                   
    ldxdw r1, [r10-0xd0]                    
    stxdw [r10-0x118], r1                   
    ldxdw r1, [r10-0x4a8]                   
    ldxdw r1, [r1+0x28]                     
    lddw r2, 0xde8f75eee1f6dd06                     r2 load str located at -2409577606766207738
    jne r1, r2, lbb_3377                            if r1 != r2 { pc += 16 }
    ldxdw r1, [r10-0x4a8]                   
    ldxdw r1, [r1+0x30]                     
    lddw r2, 0xdacd6ce4bc5d4218                     r2 load str located at -2680366473547005416
    jne r1, r2, lbb_3377                            if r1 != r2 { pc += 11 }
    ldxdw r1, [r10-0x4a8]                   
    ldxdw r1, [r1+0x38]                     
    lddw r2, 0x270db9834dfc1ab6                     r2 load str located at 2814109315776649910
    jne r1, r2, lbb_3377                            if r1 != r2 { pc += 6 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0x4a8]                   
    ldxdw r2, [r2+0x40]                     
    lddw r3, 0xfc8ba1d828f9bdfe                     r3 load str located at -248927404616466946
    jeq r2, r3, lbb_3378                            if r2 == r3 { pc += 1 }
lbb_3377:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_3378:
    ldxb r7, [r10-0xb0]                     
    jeq r1, 0, lbb_3387                             if r1 == (0 as i32 as i64 as u64) { pc += 7 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -448                                  r2 += -448   ///  r2 = r2.wrapping_add(-448 as i32 as i64 as u64)
    ldxdw r3, [r10-0x4e0]                   
    call function_1858                      
    ja lbb_3393                                     if true { pc += 6 }
lbb_3387:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -448                                  r2 += -448   ///  r2 = r2.wrapping_add(-448 as i32 as i64 as u64)
    ldxdw r3, [r10-0x4e0]                   
    call function_1898                      
lbb_3393:
    ldxdw r1, [r10-0xb8]                    
    stxdw [r10-0x48], r1                    
    ldxdw r1, [r10-0xc0]                    
    stxdw [r10-0x50], r1                    
    ldxdw r1, [r10-0xc8]                    
    stxdw [r10-0x58], r1                    
    ldxdw r1, [r10-0xd0]                    
    stxdw [r10-0x60], r1                    
    ldxdw r3, [r10-0x4b8]                   
    ldxdw r1, [r3+0x8]                      
    ldxdw r2, [r10-0x118]                   
    jne r2, r1, lbb_3415                            if r2 != r1 { pc += 10 }
    ldxdw r1, [r3+0x10]                     
    ldxdw r2, [r10-0x110]                   
    jne r2, r1, lbb_3415                            if r2 != r1 { pc += 7 }
    ldxdw r1, [r3+0x18]                     
    ldxdw r2, [r10-0x108]                   
    jne r2, r1, lbb_3415                            if r2 != r1 { pc += 4 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r3+0x20]                     
    ldxdw r3, [r10-0x100]                   
    jeq r3, r2, lbb_3416                            if r3 == r2 { pc += 1 }
lbb_3415:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_3416:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    lddw r0, 0xbadc0de8                             r0 load str located at 3134983656
    jne r1, 0, lbb_3745                             if r1 != (0 as i32 as i64 as u64) { pc += 325 }
    ldxdw r1, [r10-0x4c8]                   
    ldxdw r1, [r1+0x8]                      
    ldxdw r2, [r10-0x60]                    
    jne r2, r1, lbb_3437                            if r2 != r1 { pc += 13 }
    ldxdw r1, [r10-0x4c8]                   
    ldxdw r1, [r1+0x10]                     
    ldxdw r2, [r10-0x58]                    
    jne r2, r1, lbb_3437                            if r2 != r1 { pc += 9 }
    ldxdw r1, [r10-0x4c8]                   
    ldxdw r1, [r1+0x18]                     
    ldxdw r2, [r10-0x50]                    
    jne r2, r1, lbb_3437                            if r2 != r1 { pc += 5 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0x4c8]                   
    ldxdw r2, [r2+0x20]                     
    ldxdw r3, [r10-0x48]                    
    jeq r3, r2, lbb_3438                            if r3 == r2 { pc += 1 }
lbb_3437:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_3438:
    jne r1, 0, lbb_3745                             if r1 != (0 as i32 as i64 as u64) { pc += 306 }
    ldxdw r1, [r10-0x4a0]                   
    ldxdw r1, [r1+0x28]                     
    lddw r2, 0x93a165d7e1f6dd06                     r2 load str located at -7808848301000303354
    jne r1, r2, lbb_3460                            if r1 != r2 { pc += 16 }
    ldxdw r1, [r10-0x4a0]                   
    ldxdw r1, [r1+0x30]                     
    lddw r2, 0xac79ebce46e1cbd9                     r2 load str located at -6018520155818964007
    jne r1, r2, lbb_3460                            if r1 != r2 { pc += 11 }
    ldxdw r1, [r10-0x4a0]                   
    ldxdw r1, [r1+0x38]                     
    lddw r2, 0x91375b5fed85b41c                     r2 load str located at -7982811346925931492
    jne r1, r2, lbb_3460                            if r1 != r2 { pc += 6 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0x4a0]                   
    ldxdw r2, [r2+0x40]                     
    lddw r3, 0xa900ff7e85f58c3a                     r3 load str located at -6268729762421306310
    jeq r2, r3, lbb_3461                            if r2 == r3 { pc += 1 }
lbb_3460:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_3461:
    ldxb r2, [r10-0xb0]                     
    stxdw [r10-0x4b8], r2                   
    jeq r1, 0, lbb_3501                             if r1 == (0 as i32 as i64 as u64) { pc += 37 }
    ldxdw r1, [r10-0x4a0]                   
    ldxdw r1, [r1+0x28]                     
    lddw r2, 0xde8f75eee1f6dd06                     r2 load str located at -2409577606766207738
    jne r1, r2, lbb_3485                            if r1 != r2 { pc += 16 }
    ldxdw r1, [r10-0x4a0]                   
    ldxdw r1, [r1+0x30]                     
    lddw r2, 0xdacd6ce4bc5d4218                     r2 load str located at -2680366473547005416
    jne r1, r2, lbb_3485                            if r1 != r2 { pc += 11 }
    ldxdw r1, [r10-0x4a0]                   
    ldxdw r1, [r1+0x38]                     
    lddw r2, 0x270db9834dfc1ab6                     r2 load str located at 2814109315776649910
    jne r1, r2, lbb_3485                            if r1 != r2 { pc += 6 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0x4a0]                   
    ldxdw r2, [r2+0x40]                     
    lddw r3, 0xfc8ba1d828f9bdfe                     r3 load str located at -248927404616466946
    jeq r2, r3, lbb_3486                            if r2 == r3 { pc += 1 }
lbb_3485:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_3486:
    lddw r0, 0xbadc0de3                             r0 load str located at 3134983651
    jne r1, 0, lbb_3745                             if r1 != (0 as i32 as i64 as u64) { pc += 256 }
    stxdw [r10-0xff8], r7                   
    stxdw [r10-0x1000], r9                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1128                                 r1 += -1128   ///  r1 = r1.wrapping_add(-1128 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r6                                    r2 = r6
    ldxdw r3, [r10-0x4c0]                   
    ldxdw r4, [r10-0x4e8]                   
    call function_2107                      
    ldxw r8, [r10-0x468]                    
    jeq r8, 26, lbb_3512                            if r8 == (26 as i32 as i64 as u64) { pc += 12 }
    ja lbb_3745                                     if true { pc += 244 }
lbb_3501:
    stxdw [r10-0xff8], r7                   
    stxdw [r10-0x1000], r9                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1136                                 r1 += -1136   ///  r1 = r1.wrapping_add(-1136 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r6                                    r2 = r6
    ldxdw r3, [r10-0x4c0]                   
    ldxdw r4, [r10-0x4e8]                   
    call function_1938                      
    ldxw r8, [r10-0x470]                    
    jne r8, 26, lbb_3575                            if r8 != (26 as i32 as i64 as u64) { pc += 63 }
lbb_3512:
    ldxdw r1, [r10-0x4a8]                   
    ldxdw r1, [r1+0x28]                     
    lddw r2, 0x93a165d7e1f6dd06                     r2 load str located at -7808848301000303354
    jne r1, r2, lbb_3533                            if r1 != r2 { pc += 16 }
    ldxdw r1, [r10-0x4a8]                   
    ldxdw r1, [r1+0x30]                     
    lddw r2, 0xac79ebce46e1cbd9                     r2 load str located at -6018520155818964007
    jne r1, r2, lbb_3533                            if r1 != r2 { pc += 11 }
    ldxdw r1, [r10-0x4a8]                   
    ldxdw r1, [r1+0x38]                     
    lddw r2, 0x91375b5fed85b41c                     r2 load str located at -7982811346925931492
    jne r1, r2, lbb_3533                            if r1 != r2 { pc += 6 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0x4a8]                   
    ldxdw r2, [r2+0x40]                     
    lddw r3, 0xa900ff7e85f58c3a                     r3 load str located at -6268729762421306310
    jeq r2, r3, lbb_3534                            if r2 == r3 { pc += 1 }
lbb_3533:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_3534:
    jeq r1, 0, lbb_3577                             if r1 == (0 as i32 as i64 as u64) { pc += 42 }
    ldxdw r1, [r10-0x4a8]                   
    ldxdw r1, [r1+0x28]                     
    lddw r2, 0xde8f75eee1f6dd06                     r2 load str located at -2409577606766207738
    jne r1, r2, lbb_3556                            if r1 != r2 { pc += 16 }
    ldxdw r1, [r10-0x4a8]                   
    ldxdw r1, [r1+0x30]                     
    lddw r2, 0xdacd6ce4bc5d4218                     r2 load str located at -2680366473547005416
    jne r1, r2, lbb_3556                            if r1 != r2 { pc += 11 }
    ldxdw r1, [r10-0x4a8]                   
    ldxdw r1, [r1+0x38]                     
    lddw r2, 0x270db9834dfc1ab6                     r2 load str located at 2814109315776649910
    jne r1, r2, lbb_3556                            if r1 != r2 { pc += 6 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0x4a8]                   
    ldxdw r2, [r2+0x40]                     
    lddw r3, 0xfc8ba1d828f9bdfe                     r3 load str located at -248927404616466946
    jeq r2, r3, lbb_3557                            if r2 == r3 { pc += 1 }
lbb_3556:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_3557:
    lddw r0, 0xbadc0de3                             r0 load str located at 3134983651
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jne r1, 0, lbb_3745                             if r1 != (0 as i32 as i64 as u64) { pc += 184 }
    ldxdw r1, [r10-0x4b8]                   
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0x4b0]                   
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1144                                 r1 += -1144   ///  r1 = r1.wrapping_add(-1144 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r6                                    r2 = r6
    ldxdw r3, [r10-0x4d0]                   
    ldxdw r4, [r10-0x4e8]                   
    call function_2107                      
    ldxw r8, [r10-0x478]                    
    jeq r8, 26, lbb_3590                            if r8 == (26 as i32 as i64 as u64) { pc += 16 }
    ja lbb_3745                                     if true { pc += 170 }
lbb_3575:
    ldxw r0, [r10-0x46c]                    
    ja lbb_3745                                     if true { pc += 168 }
lbb_3577:
    ldxdw r1, [r10-0x4b8]                   
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0x4b0]                   
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1152                                 r1 += -1152   ///  r1 = r1.wrapping_add(-1152 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r6                                    r2 = r6
    ldxdw r3, [r10-0x4d0]                   
    ldxdw r4, [r10-0x4e8]                   
    call function_1938                      
    ldxw r8, [r10-0x480]                    
    jne r8, 26, lbb_3749                            if r8 != (26 as i32 as i64 as u64) { pc += 159 }
lbb_3590:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxb r1, [r10-0x1ce]                    
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_3596                             if r1 == (0 as i32 as i64 as u64) { pc += 2 }
    jne r1, 1, lbb_3751                             if r1 != (1 as i32 as i64 as u64) { pc += 156 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
lbb_3596:
    lddw r0, 0xbadc0dea                             r0 load str located at 3134983658
    ldxdw r1, [r10-0x358]                   
    ldxdw r2, [r10-0x360]                   
    jsgt r2, r1, lbb_3745                           if (r2 as i64) > (r1 as i64) { pc += 144 }
    ldxdw r2, [r10-0x330]                   
    jslt r2, r1, lbb_3745                           if (r2 as i64) < (r1 as i64) { pc += 142 }
    ldxdw r1, [r10-0x328]                   
    jsgt r2, r1, lbb_3745                           if (r2 as i64) > (r1 as i64) { pc += 140 }
    ldxdw r2, [r10-0x300]                   
    jslt r2, r1, lbb_3745                           if (r2 as i64) < (r1 as i64) { pc += 138 }
    ldxdw r1, [r10-0x2f8]                   
    jsgt r2, r1, lbb_3745                           if (r2 as i64) > (r1 as i64) { pc += 136 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1160                                 r1 += -1160   ///  r1 = r1.wrapping_add(-1160 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1120                                 r2 += -1120   ///  r2 = r2.wrapping_add(-1120 as i32 as i64 as u64)
    call function_6631                      
    lddw r0, 0xbadc0deb                             r0 load str located at 3134983659
    ldxw r1, [r10-0x488]                    
    jne r1, 26, lbb_3745                            if r1 != (26 as i32 as i64 as u64) { pc += 127 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -1168                                 r1 += -1168   ///  r1 = r1.wrapping_add(-1168 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -736                                  r2 += -736   ///  r2 = r2.wrapping_add(-736 as i32 as i64 as u64)
    stxdw [r10-0x4a0], r2                   
    call function_6545                      
    lddw r0, 0xbadc0dec                             r0 load str located at 3134983660
    ldxw r1, [r10-0x490]                    
    jne r1, 26, lbb_3745                            if r1 != (26 as i32 as i64 as u64) { pc += 117 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -560                                  r1 += -560   ///  r1 = r1.wrapping_add(-560 as i32 as i64 as u64)
    stxdw [r10-0x4a8], r1                   
    mov64 r6, r10                                   r6 = r10
    add64 r6, -592                                  r6 += -592   ///  r6 = r6.wrapping_add(-592 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -880                                  r1 += -880   ///  r1 = r1.wrapping_add(-880 as i32 as i64 as u64)
    stxdw [r10-0x4b0], r1                   
    ldxdw r8, [r10-0x4f8]                   
    ldxdw r2, [r8+0x50]                     
    mov64 r9, r8                                    r9 = r8
    add64 r9, 88                                    r9 += 88   ///  r9 = r9.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    call function_6615                      
    mov64 r1, r9                                    r1 = r9
    ldxdw r2, [r10-0x4b0]                   
    mov64 r3, 144                                   r3 = 144 as i32 as i64 as u64
    call function_17016                     
    mov64 r1, r8                                    r1 = r8
    add64 r1, 232                                   r1 += 232   ///  r1 = r1.wrapping_add(232 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -1120                                 r2 += -1120   ///  r2 = r2.wrapping_add(-1120 as i32 as i64 as u64)
    mov64 r3, 240                                   r3 = 240 as i32 as i64 as u64
    call function_17016                     
    ldxdw r4, [r10-0x4e0]                   
    ldxdw r1, [r4+0x8]                      
    ldxdw r2, [r4+0x10]                     
    ldxdw r3, [r4+0x18]                     
    ldxdw r4, [r4+0x0]                      
    stxdw [r8+0x1d8], r4                    
    stxdw [r8+0x1f0], r3                    
    stxdw [r8+0x1e8], r2                    
    stxdw [r8+0x1e0], r1                    
    ldxdw r4, [r10-0x4d8]                   
    ldxdw r1, [r4+0x0]                      
    ldxdw r2, [r4+0x8]                      
    ldxdw r3, [r4+0x10]                     
    ldxdw r4, [r4+0x18]                     
    stxdw [r8+0x210], r4                    
    stxdw [r8+0x208], r3                    
    stxdw [r8+0x200], r2                    
    stxdw [r8+0x1f8], r1                    
    ldxdw r1, [r6+0x0]                      
    ldxdw r2, [r6+0x8]                      
    ldxdw r3, [r6+0x10]                     
    ldxdw r4, [r6+0x18]                     
    stxdw [r8+0x230], r4                    
    stxdw [r8+0x228], r3                    
    stxdw [r8+0x220], r2                    
    stxdw [r8+0x218], r1                    
    ldxdw r4, [r10-0x4a8]                   
    ldxdw r1, [r4+0x0]                      
    ldxdw r2, [r4+0x8]                      
    ldxdw r3, [r4+0x10]                     
    ldxdw r4, [r4+0x18]                     
    stxdw [r8+0x250], r4                    
    stxdw [r8+0x248], r3                    
    stxdw [r8+0x240], r2                    
    stxdw [r8+0x238], r1                    
    ldxdw r4, [r10-0x4f0]                   
    ldxdw r1, [r4+0x0]                      
    ldxdw r2, [r4+0x8]                      
    ldxdw r3, [r4+0x10]                     
    ldxdw r4, [r4+0x18]                     
    stxdw [r8+0x270], r4                    
    stxdw [r8+0x268], r3                    
    stxdw [r8+0x260], r2                    
    stxdw [r8+0x258], r1                    
    ldxdw r1, [r10-0x1e8]                   
    ldxdw r2, [r10-0x1f0]                   
    stxdw [r8+0x278], r2                    
    stxdw [r8+0x280], r1                    
    stxb [r8+0x288], r7                     
    stw [r8+0x28c], 0                       
    stw [r8+0x289], 0                       
    ldxb r1, [r10-0x199]                    
    stxb [r8+0x290], r1                     
    stdw [r8+0x310], 0                      
    stdw [r8+0x308], 0                      
    stdw [r8+0x300], 0                      
    stdw [r8+0x2f8], 0                      
    stdw [r8+0x2f0], 0                      
    stdw [r8+0x2e8], 0                      
    stdw [r8+0x2e0], 0                      
    stdw [r8+0x2d8], 0                      
    stdw [r8+0x2d0], 0                      
    stdw [r8+0x2c8], 0                      
    stdw [r8+0x2c0], 0                      
    stdw [r8+0x2b8], 0                      
    stdw [r8+0x2b0], 0                      
    stdw [r8+0x2a8], 0                      
    stdw [r8+0x2a0], 0                      
    stdw [r8+0x298], 0                      
    stw [r8+0x294], 0                       
    stw [r8+0x291], 0                       
    ldxdw r1, [r10-0x1e0]                   
    stxdw [r8+0x320], r1                    
    ldxh r1, [r10-0x1d0]                    
    stxh [r8+0x318], r1                     
    stw [r8+0x31a], 0                       
    sth [r8+0x31e], 0                       
    mov64 r1, r8                                    r1 = r8
    add64 r1, 808                                   r1 += 808   ///  r1 = r1.wrapping_add(808 as i32 as i64 as u64)
    ldxdw r2, [r10-0x4a0]                   
    mov64 r3, 80                                    r3 = 80 as i32 as i64 as u64
    call function_17012                     
    ldxdw r1, [r10-0x1d8]                   
    stxdw [r8+0x378], r1                    
    mov64 r1, r8                                    r1 = r8
    add64 r1, 896                                   r1 += 896   ///  r1 = r1.wrapping_add(896 as i32 as i64 as u64)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r3, 912                                   r3 = 912 as i32 as i64 as u64
    call function_17020                     
    stdw [r8+0x710], 4                      
    mov64 r1, r9                                    r1 = r9
    call function_14670                     
    mov64 r8, 26                                    r8 = 26 as i32 as i64 as u64
lbb_3745:
    ldxdw r1, [r10-0x498]                   
lbb_3746:
    stxw [r1+0x4], r0                       
    stxw [r1+0x0], r8                       
    exit                                    
lbb_3749:
    ldxw r0, [r10-0x47c]                    
    ja lbb_3745                                     if true { pc += -6 }
lbb_3751:
    lddw r0, 0xbadc0de9                             r0 load str located at 3134983657
    ja lbb_3745                                     if true { pc += -9 }

function_3754:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r4+0x0]                      
    ldxdw r7, [r6+0x50]                     
    jlt r7, 44, lbb_4067                            if r7 < (44 as i32 as i64 as u64) { pc += 309 }
    jeq r7, 44, lbb_4125                            if r7 == (44 as i32 as i64 as u64) { pc += 366 }
    ldxdw r4, [r5-0xfe8]                    
    stxdw [r10-0x168], r4                   
    ldxdw r4, [r5-0xff0]                    
    stxdw [r10-0x170], r4                   
    ldxdw r4, [r5-0xff8]                    
    ldxdw r5, [r5-0x1000]                   
    ldxb r0, [r6+0x84]                      
    stxb [r10-0x149], r0                    
    stxdw [r10-0x151], r5                   
    stb [r10-0x152], 12                     
    ldxdw r0, [r1+0x0]                      
    ldxdw r2, [r2+0x0]                      
    ldxdw r1, [r3+0x0]                      
    mov64 r8, r6                                    r8 = r6
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x138], r8                   
    mov64 r9, r1                                    r9 = r1
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x118], r9                   
    stxdw [r10-0x160], r2                   
    mov64 r3, r2                                    r3 = r2
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x128], r3                   
    mov64 r2, r0                                    r2 = r0
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x148], r2                   
    sth [r10-0x110], 256                    
    sth [r10-0x120], 1                      
    sth [r10-0x130], 0                      
    sth [r10-0x140], 1                      
    jeq r4, 0, lbb_3926                             if r4 == (0 as i32 as i64 as u64) { pc += 136 }
    stxdw [r10-0x178], r9                   
    ldxb r4, [r0+0x0]                       
    jne r4, 255, lbb_4066                           if r4 != (255 as i32 as i64 as u64) { pc += 273 }
    stxdw [r10-0x180], r1                   
    ldxb r4, [r0+0x1]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r4, 0, lbb_4068                             if r4 == (0 as i32 as i64 as u64) { pc += 270 }
    ldxb r9, [r0+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jeq r9, 0, lbb_4072                             if r9 == (0 as i32 as i64 as u64) { pc += 271 }
lbb_3801:
    stxdw [r10-0x188], r1                   
    ldxb r9, [r0+0x3]                       
    jne r9, 0, lbb_3805                             if r9 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_3804:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_3805:
    ldxdw r9, [r0+0x50]                     
    mov64 r1, r0                                    r1 = r0
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xe8], r1                    
    mov64 r1, r0                                    r1 = r0
    add64 r1, 88                                    r1 += 88   ///  r1 = r1.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xf0], r1                    
    stxdw [r10-0xf8], r9                    
    add64 r0, 72                                    r0 += 72   ///  r0 = r0.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x100], r0                   
    stxdw [r10-0x108], r2                   
    stxb [r10-0xd6], r5                     
    stxb [r10-0xd7], r4                     
    ldxdw r1, [r10-0x188]                   
    stxb [r10-0xd8], r1                     
    stdw [r10-0xe0], 0                      
    ldxb r1, [r6+0x0]                       
    and64 r1, 136                                   r1 &= 136   ///  r1 = r1.and(136)
    ldxdw r9, [r10-0x180]                   
    jne r1, 136, lbb_4066                           if r1 != (136 as i32 as i64 as u64) { pc += 241 }
    ldxb r2, [r6+0x1]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_4077                             if r2 == (0 as i32 as i64 as u64) { pc += 248 }
    ldxb r2, [r6+0x2]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_4081                             if r2 == (0 as i32 as i64 as u64) { pc += 249 }
lbb_3832:
    ldxb r2, [r6+0x3]                       
    jne r2, 0, lbb_3835                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_3834:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_3835:
    mov64 r2, r6                                    r2 = r6
    add64 r2, 40                                    r2 += 40   ///  r2 = r2.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xb0], r2                    
    mov64 r2, r6                                    r2 = r6
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xb8], r2                    
    stxdw [r10-0xc0], r7                    
    add64 r6, 72                                    r6 += 72   ///  r6 = r6.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0xc8], r6                    
    stxdw [r10-0xd0], r8                    
    stxb [r10-0x9e], r4                     
    stxb [r10-0x9f], r5                     
    stxb [r10-0xa0], r1                     
    stdw [r10-0xa8], 0                      
    ldxdw r1, [r10-0x160]                   
    ldxb r1, [r1+0x0]                       
    jne r1, 255, lbb_4066                           if r1 != (255 as i32 as i64 as u64) { pc += 214 }
    ldxdw r6, [r10-0x160]                   
    ldxb r2, [r6+0x1]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_4093                             if r2 == (0 as i32 as i64 as u64) { pc += 236 }
    ldxb r2, [r6+0x2]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_4097                             if r2 == (0 as i32 as i64 as u64) { pc += 237 }
lbb_3860:
    ldxb r2, [r6+0x3]                       
    jne r2, 0, lbb_3863                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_3862:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_3863:
    ldxdw r2, [r6+0x50]                     
    mov64 r0, r6                                    r0 = r6
    add64 r0, 40                                    r0 += 40   ///  r0 = r0.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r0                    
    mov64 r0, r6                                    r0 = r6
    add64 r0, 88                                    r0 += 88   ///  r0 = r0.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r0                    
    stxdw [r10-0x88], r2                    
    add64 r6, 72                                    r6 += 72   ///  r6 = r6.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r6                    
    stxdw [r10-0x98], r3                    
    stxb [r10-0x66], r4                     
    stxb [r10-0x67], r5                     
    stxb [r10-0x68], r1                     
    stdw [r10-0x70], 0                      
    ldxb r1, [r9+0x0]                       
    and64 r1, 136                                   r1 &= 136   ///  r1 = r1.and(136)
    jne r1, 136, lbb_4066                           if r1 != (136 as i32 as i64 as u64) { pc += 185 }
    ldxb r2, [r9+0x1]                       
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_4109                             if r2 == (0 as i32 as i64 as u64) { pc += 224 }
    ldxb r2, [r9+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_4113                             if r2 == (0 as i32 as i64 as u64) { pc += 225 }
lbb_3888:
    ldxb r2, [r9+0x3]                       
    jne r2, 0, lbb_3891                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_3890:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_3891:
    ldxdw r2, [r9+0x50]                     
    mov64 r5, r9                                    r5 = r9
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r5                    
    mov64 r5, r9                                    r5 = r9
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r5                    
    stxdw [r10-0x50], r2                    
    add64 r9, 72                                    r9 += 72   ///  r9 = r9.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r9                    
    ldxdw r2, [r10-0x178]                   
    stxdw [r10-0x60], r2                    
    stxb [r10-0x2e], r3                     
    stxb [r10-0x2f], r4                     
    stxb [r10-0x30], r1                     
    stdw [r10-0x38], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -338                                  r1 += -338   ///  r1 = r1.wrapping_add(-338 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -328                                  r1 += -328   ///  r1 = r1.wrapping_add(-328 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100022e68 --> b"\x06\xdd\xf6\xe1\xeeu\x8f\xde\x18B]\xbc\xe4l\xcd\xda\xb6\x1a\xfcM\x83\xb9…        r1 load str located at 4295110248
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 10                      
    stdw [r10-0x18], 4                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -264                                  r2 += -264   ///  r2 = r2.wrapping_add(-264 as i32 as i64 as u64)
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
    ldxdw r4, [r10-0x170]                   
    ldxdw r5, [r10-0x168]                   
    ja lbb_4063                                     if true { pc += 137 }
lbb_3926:
    ldxb r4, [r0+0x0]                       
    jne r4, 255, lbb_4066                           if r4 != (255 as i32 as i64 as u64) { pc += 138 }
    ldxb r4, [r0+0x1]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_3932                             if r4 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_3932:
    stxdw [r10-0x168], r5                   
    ldxb r5, [r0+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r5, 0, lbb_3937                             if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_3937:
    stxdw [r10-0x178], r9                   
    stxdw [r10-0x180], r1                   
    ldxb r5, [r0+0x3]                       
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r5, 0, lbb_3943                             if r5 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_3943:
    ldxdw r5, [r0+0x50]                     
    mov64 r9, r0                                    r9 = r0
    add64 r9, 40                                    r9 += 40   ///  r9 = r9.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xe8], r9                    
    mov64 r9, r0                                    r9 = r0
    add64 r9, 88                                    r9 += 88   ///  r9 = r9.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xf0], r9                    
    stxdw [r10-0xf8], r5                    
    add64 r0, 72                                    r0 += 72   ///  r0 = r0.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x100], r0                   
    stxdw [r10-0x108], r2                   
    stxb [r10-0xd6], r1                     
    stxb [r10-0xd7], r4                     
    ldxdw r1, [r10-0x168]                   
    stxb [r10-0xd8], r1                     
    stdw [r10-0xe0], 0                      
    ldxb r1, [r6+0x0]                       
    and64 r1, 136                                   r1 &= 136   ///  r1 = r1.and(136)
    ldxdw r2, [r10-0x180]                   
    ldxdw r9, [r10-0x178]                   
    jne r1, 136, lbb_4066                           if r1 != (136 as i32 as i64 as u64) { pc += 102 }
    ldxb r5, [r6+0x1]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r5, 0, lbb_4085                             if r5 == (0 as i32 as i64 as u64) { pc += 117 }
    ldxb r0, [r6+0x2]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r0, 0, lbb_4089                             if r0 == (0 as i32 as i64 as u64) { pc += 118 }
lbb_3971:
    ldxb r0, [r6+0x3]                       
    jne r0, 0, lbb_3974                             if r0 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_3973:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_3974:
    mov64 r0, r6                                    r0 = r6
    add64 r0, 40                                    r0 += 40   ///  r0 = r0.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xb0], r0                    
    mov64 r0, r6                                    r0 = r6
    add64 r0, 88                                    r0 += 88   ///  r0 = r0.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xb8], r0                    
    stxdw [r10-0xc0], r7                    
    add64 r6, 72                                    r6 += 72   ///  r6 = r6.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0xc8], r6                    
    stxdw [r10-0xd0], r8                    
    stxb [r10-0x9e], r4                     
    stxb [r10-0x9f], r5                     
    stxb [r10-0xa0], r1                     
    stdw [r10-0xa8], 0                      
    ldxdw r1, [r10-0x160]                   
    ldxb r1, [r1+0x0]                       
    jne r1, 255, lbb_4066                           if r1 != (255 as i32 as i64 as u64) { pc += 75 }
    ldxdw r7, [r10-0x160]                   
    ldxb r5, [r7+0x1]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r5, 0, lbb_4101                             if r5 == (0 as i32 as i64 as u64) { pc += 105 }
    ldxb r0, [r7+0x2]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r0, 0, lbb_4105                             if r0 == (0 as i32 as i64 as u64) { pc += 106 }
lbb_3999:
    ldxb r0, [r7+0x3]                       
    jne r0, 0, lbb_4002                             if r0 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_4001:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_4002:
    ldxdw r0, [r7+0x50]                     
    mov64 r6, r7                                    r6 = r7
    add64 r6, 40                                    r6 += 40   ///  r6 = r6.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r6                    
    mov64 r6, r7                                    r6 = r7
    add64 r6, 88                                    r6 += 88   ///  r6 = r6.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r6                    
    stxdw [r10-0x88], r0                    
    add64 r7, 72                                    r7 += 72   ///  r7 = r7.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r7                    
    stxdw [r10-0x98], r3                    
    stxb [r10-0x66], r4                     
    stxb [r10-0x67], r5                     
    stxb [r10-0x68], r1                     
    stdw [r10-0x70], 0                      
    ldxb r1, [r2+0x0]                       
    and64 r1, 136                                   r1 &= 136   ///  r1 = r1.and(136)
    jne r1, 136, lbb_4066                           if r1 != (136 as i32 as i64 as u64) { pc += 46 }
    ldxb r1, [r2+0x1]                       
    ldxb r3, [r2+0x2]                       
    ldxb r5, [r2+0x3]                       
    ldxdw r4, [r2+0x50]                     
    mov64 r0, r2                                    r0 = r2
    add64 r0, 40                                    r0 += 40   ///  r0 = r0.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r0                    
    mov64 r0, r2                                    r0 = r2
    add64 r0, 88                                    r0 += 88   ///  r0 = r0.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r0                    
    stxdw [r10-0x50], r4                    
    add64 r2, 72                                    r2 += 72   ///  r2 = r2.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r2                    
    stxdw [r10-0x60], r9                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jeq r5, 0, lbb_4117                             if r5 == (0 as i32 as i64 as u64) { pc += 80 }
    stxb [r10-0x2e], r4                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jeq r3, 0, lbb_4121                             if r3 == (0 as i32 as i64 as u64) { pc += 81 }
lbb_4040:
    stxb [r10-0x2f], r4                     
    jne r1, 0, lbb_4043                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_4042:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_4043:
    stxb [r10-0x30], r2                     
    stdw [r10-0x38], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -338                                  r1 += -338   ///  r1 = r1.wrapping_add(-338 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -328                                  r1 += -328   ///  r1 = r1.wrapping_add(-328 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100022e68 --> b"\x06\xdd\xf6\xe1\xeeu\x8f\xde\x18B]\xbc\xe4l\xcd\xda\xb6\x1a\xfcM\x83\xb9…        r1 load str located at 4295110248
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 10                      
    stdw [r10-0x18], 4                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -264                                  r2 += -264   ///  r2 = r2.wrapping_add(-264 as i32 as i64 as u64)
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
    mov64 r4, 8                                     r4 = 8 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_4063:
    syscall [invalid]                       
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
    ja lbb_4067                                     if true { pc += 1 }
lbb_4066:
    mov64 r0, 11                                    r0 = 11 as i32 as i64 as u64
lbb_4067:
    exit                                    
lbb_4068:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxb r9, [r0+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r9, 0, lbb_3801                             if r9 != (0 as i32 as i64 as u64) { pc += -271 }
lbb_4072:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    stxdw [r10-0x188], r1                   
    ldxb r9, [r0+0x3]                       
    jeq r9, 0, lbb_3804                             if r9 == (0 as i32 as i64 as u64) { pc += -272 }
    ja lbb_3805                                     if true { pc += -272 }
lbb_4077:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxb r2, [r6+0x2]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_3832                             if r2 != (0 as i32 as i64 as u64) { pc += -249 }
lbb_4081:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxb r2, [r6+0x3]                       
    jeq r2, 0, lbb_3834                             if r2 == (0 as i32 as i64 as u64) { pc += -250 }
    ja lbb_3835                                     if true { pc += -250 }
lbb_4085:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxb r0, [r6+0x2]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_3971                             if r0 != (0 as i32 as i64 as u64) { pc += -118 }
lbb_4089:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxb r0, [r6+0x3]                       
    jeq r0, 0, lbb_3973                             if r0 == (0 as i32 as i64 as u64) { pc += -119 }
    ja lbb_3974                                     if true { pc += -119 }
lbb_4093:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxb r2, [r6+0x2]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_3860                             if r2 != (0 as i32 as i64 as u64) { pc += -237 }
lbb_4097:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxb r2, [r6+0x3]                       
    jeq r2, 0, lbb_3862                             if r2 == (0 as i32 as i64 as u64) { pc += -238 }
    ja lbb_3863                                     if true { pc += -238 }
lbb_4101:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxb r0, [r7+0x2]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_3999                             if r0 != (0 as i32 as i64 as u64) { pc += -106 }
lbb_4105:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxb r0, [r7+0x3]                       
    jeq r0, 0, lbb_4001                             if r0 == (0 as i32 as i64 as u64) { pc += -107 }
    ja lbb_4002                                     if true { pc += -107 }
lbb_4109:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxb r2, [r9+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_3888                             if r2 != (0 as i32 as i64 as u64) { pc += -225 }
lbb_4113:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxb r2, [r9+0x3]                       
    jeq r2, 0, lbb_3890                             if r2 == (0 as i32 as i64 as u64) { pc += -226 }
    ja lbb_3891                                     if true { pc += -226 }
lbb_4117:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    stxb [r10-0x2e], r4                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_4040                             if r3 != (0 as i32 as i64 as u64) { pc += -81 }
lbb_4121:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    stxb [r10-0x2f], r4                     
    jeq r1, 0, lbb_4042                             if r1 == (0 as i32 as i64 as u64) { pc += -82 }
    ja lbb_4043                                     if true { pc += -82 }
lbb_4125:
    mov64 r1, 44                                    r1 = 44 as i32 as i64 as u64
    mov64 r2, 44                                    r2 = 44 as i32 as i64 as u64
    lddw r3, 0x1000239e0 --> b"\x00\x00\x00\x00\x804\x02\x00\x1c\x00\x00\x00\x00\x00\x00\x00$\x04\x00\x0…        r3 load str located at 4295113184
    call function_15494                     

function_4130:
    mov64 r7, r5                                    r7 = r5
    stxdw [r10-0x30], r4                    
    stxdw [r10-0x38], r3                    
    mov64 r9, r2                                    r9 = r2
    mov64 r6, r1                                    r6 = r1
    lddw r1, 0x1000234a2 --> b"transfer_token_by_owner_with_mint called"        r1 load str located at 4295111842
    mov64 r2, 40                                    r2 = 40 as i32 as i64 as u64
    syscall [invalid]                       
    ldxdw r1, [r9+0x0]                      
    ldxdw r2, [r1+0x28]                     
    lddw r3, 0x93a165d7e1f6dd06                     r3 load str located at -7808848301000303354
    jne r2, r3, lbb_4157                            if r2 != r3 { pc += 13 }
    ldxdw r2, [r1+0x30]                     
    lddw r3, 0xac79ebce46e1cbd9                     r3 load str located at -6018520155818964007
    jne r2, r3, lbb_4157                            if r2 != r3 { pc += 9 }
    ldxdw r2, [r1+0x38]                     
    lddw r3, 0x91375b5fed85b41c                     r3 load str located at -7982811346925931492
    jne r2, r3, lbb_4157                            if r2 != r3 { pc += 5 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r3, [r1+0x40]                     
    lddw r4, 0xa900ff7e85f58c3a                     r4 load str located at -6268729762421306310
    jeq r3, r4, lbb_4158                            if r3 == r4 { pc += 1 }
lbb_4157:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_4158:
    ldxdw r8, [r7-0xff8]                    
    jeq r2, 0, lbb_4187                             if r2 == (0 as i32 as i64 as u64) { pc += 27 }
    ldxdw r2, [r1+0x28]                     
    lddw r3, 0xde8f75eee1f6dd06                     r3 load str located at -2409577606766207738
    jne r2, r3, lbb_4177                            if r2 != r3 { pc += 13 }
    ldxdw r2, [r1+0x30]                     
    lddw r3, 0xdacd6ce4bc5d4218                     r3 load str located at -2680366473547005416
    jne r2, r3, lbb_4177                            if r2 != r3 { pc += 9 }
    ldxdw r2, [r1+0x38]                     
    lddw r3, 0x270db9834dfc1ab6                     r3 load str located at 2814109315776649910
    jne r2, r3, lbb_4177                            if r2 != r3 { pc += 5 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r1, [r1+0x40]                     
    lddw r3, 0xfc8ba1d828f9bdfe                     r3 load str located at -248927404616466946
    jeq r1, r3, lbb_4178                            if r1 == r3 { pc += 1 }
lbb_4177:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_4178:
    jeq r2, 0, lbb_4207                             if r2 == (0 as i32 as i64 as u64) { pc += 28 }
    lddw r1, 0x1000234ca --> b"Invalid token program"        r1 load str located at 4295111882
    mov64 r2, 21                                    r2 = 21 as i32 as i64 as u64
    syscall [invalid]                       
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    lddw r1, 0xbadc0de3                             r1 load str located at 3134983651
    ja lbb_4224                                     if true { pc += 37 }
lbb_4187:
    lddw r1, 0x100023500 --> b"Using TOKENKEG for transfer"        r1 load str located at 4295111936
    mov64 r2, 27                                    r2 = 27 as i32 as i64 as u64
    syscall [invalid]                       
    stxdw [r10-0x8], r8                     
    ldxdw r1, [r10-0x30]                    
    stxdw [r10-0x10], r1                    
    ldxdw r1, [r10-0x38]                    
    stxdw [r10-0x18], r1                    
    stxdw [r10-0x20], r9                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -32                                   r2 += -32   ///  r2 = r2.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_15230                     
    ldxw r1, [r10-0x24]                     
    ldxw r0, [r10-0x28]                     
    ja lbb_4224                                     if true { pc += 17 }
lbb_4207:
    ldxdw r7, [r7-0x1000]                   
    lddw r1, 0x1000234df --> b"Using TOKENZ for transfer_checked"        r1 load str located at 4295111903
    mov64 r2, 33                                    r2 = 33 as i32 as i64 as u64
    syscall [invalid]                       
    stxdw [r10-0x1000], r8                  
    stdw [r10-0xfe8], 0                     
    stdw [r10-0xff0], 8                     
    stdw [r10-0xff8], 0                     
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r9                                    r1 = r9
    ldxdw r2, [r10-0x38]                    
    ldxdw r3, [r10-0x30]                    
    mov64 r4, r7                                    r4 = r7
    call function_3754                      
    lddw r1, 0xbadc0def                             r1 load str located at 3134983663
lbb_4224:
    stxw [r6+0x4], r1                       
    stxw [r6+0x0], r0                       
    exit                                    

function_4227:
    stxdw [r10-0x188], r4                   
    stxdw [r10-0x180], r1                   
    ldxdw r1, [r5-0xfe8]                    
    stxb [r10-0x16a], r1                    
    ldxdw r1, [r2+0x0]                      
    mov64 r0, r1                                    r0 = r1
    add64 r0, 40                                    r0 += 40   ///  r0 = r0.wrapping_add(40 as i32 as i64 as u64)
    ldxdw r6, [r1+0x28]                     
    lddw r7, 0x93a165d7e1f6dd06                     r7 load str located at -7808848301000303354
    jne r6, r7, lbb_4251                            if r6 != r7 { pc += 13 }
    ldxdw r6, [r0+0x8]                      
    lddw r7, 0xac79ebce46e1cbd9                     r7 load str located at -6018520155818964007
    jne r6, r7, lbb_4251                            if r6 != r7 { pc += 9 }
    ldxdw r6, [r0+0x10]                     
    lddw r7, 0x91375b5fed85b41c                     r7 load str located at -7982811346925931492
    jne r6, r7, lbb_4251                            if r6 != r7 { pc += 5 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r6, [r0+0x18]                     
    lddw r7, 0xa900ff7e85f58c3a                     r7 load str located at -6268729762421306310
    jeq r6, r7, lbb_4252                            if r6 == r7 { pc += 1 }
lbb_4251:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
lbb_4252:
    ldxdw r7, [r5-0xfe0]                    
    ldxdw r6, [r5-0xff0]                    
    ldxdw r9, [r5-0xff8]                    
    ldxdw r4, [r5-0x1000]                   
    jeq r8, 0, lbb_4282                             if r8 == (0 as i32 as i64 as u64) { pc += 25 }
    ldxdw r2, [r0+0x0]                      
    lddw r8, 0xde8f75eee1f6dd06                     r8 load str located at -2409577606766207738
    jne r2, r8, lbb_4276                            if r2 != r8 { pc += 15 }
    ldxdw r2, [r0+0x8]                      
    lddw r8, 0xdacd6ce4bc5d4218                     r8 load str located at -2680366473547005416
    jne r2, r8, lbb_4276                            if r2 != r8 { pc += 11 }
    ldxdw r2, [r0+0x10]                     
    lddw r8, 0x270db9834dfc1ab6                     r8 load str located at 2814109315776649910
    jne r2, r8, lbb_4276                            if r2 != r8 { pc += 7 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r5, [r0+0x18]                     
    stxdw [r10-0x190], r5                   
    lddw r8, 0xfc8ba1d828f9bdfe                     r8 load str located at -248927404616466946
    ldxdw r5, [r10-0x190]                   
    jeq r5, r8, lbb_4277                            if r5 == r8 { pc += 1 }
lbb_4276:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_4277:
    jeq r2, 0, lbb_4317                             if r2 == (0 as i32 as i64 as u64) { pc += 39 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    lddw r2, 0xbadc0de3                             r2 load str located at 3134983651
    ja lbb_4459                                     if true { pc += 177 }
lbb_4282:
    stxdw [r10-0x118], r7                   
    ldxdw r1, [r10-0x188]                   
    stxdw [r10-0x120], r1                   
    stxdw [r10-0x128], r3                   
    stxdw [r10-0x130], r2                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -362                                  r1 += -362   ///  r1 = r1.wrapping_add(-362 as i32 as i64 as u64)
    stxdw [r10-0x90], r1                    
    stxdw [r10-0xa0], r6                    
    stxdw [r10-0xb0], r9                    
    stxdw [r10-0xc0], r4                    
    lddw r1, 0x10002349c --> b"poolv1transfer_token_by_owner_with_mint calledInva"        r1 load str located at 4295111836
    stxdw [r10-0xd0], r1                    
    stdw [r10-0x88], 1                      
    stdw [r10-0x98], 32                     
    stdw [r10-0xa8], 32                     
    stdw [r10-0xb8], 32                     
    stdw [r10-0xc8], 6                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    stxdw [r10-0x160], r1                   
    stdw [r10-0x158], 5                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -376                                  r1 += -376   ///  r1 = r1.wrapping_add(-376 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -304                                  r2 += -304   ///  r2 = r2.wrapping_add(-304 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -352                                  r3 += -352   ///  r3 = r3.wrapping_add(-352 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_15230                     
    ldxw r1, [r10-0x178]                    
    jne r1, 26, lbb_4458                            if r1 != (26 as i32 as i64 as u64) { pc += 143 }
lbb_4315:
    mov64 r1, 26                                    r1 = 26 as i32 as i64 as u64
    ja lbb_4459                                     if true { pc += 142 }
lbb_4317:
    stxdw [r10-0x168], r7                   
    stb [r10-0x169], 3                      
    ldxdw r5, [r3+0x0]                      
    ldxdw r2, [r10-0x188]                   
    ldxdw r3, [r2+0x0]                      
    mov64 r7, r1                                    r7 = r1
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x160], r7                   
    stxdw [r10-0x188], r3                   
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x140], r3                   
    mov64 r8, r5                                    r8 = r5
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x150], r8                   
    sth [r10-0x138], 256                    
    sth [r10-0x148], 1                      
    sth [r10-0x158], 1                      
    mov64 r2, r10                                   r2 = r10
    add64 r2, -362                                  r2 += -362   ///  r2 = r2.wrapping_add(-362 as i32 as i64 as u64)
    stxdw [r10-0xf0], r2                    
    stxdw [r10-0x100], r6                   
    stxdw [r10-0x110], r9                   
    stxdw [r10-0x120], r4                   
    lddw r2, 0x10002349c --> b"poolv1transfer_token_by_owner_with_mint calledInva"        r2 load str located at 4295111836
    stxdw [r10-0x130], r2                   
    stdw [r10-0xe8], 1                      
    stdw [r10-0xf8], 32                     
    stdw [r10-0x108], 32                    
    stdw [r10-0x118], 32                    
    stdw [r10-0x128], 6                     
    mov64 r2, r10                                   r2 = r10
    add64 r2, -304                                  r2 += -304   ///  r2 = r2.wrapping_add(-304 as i32 as i64 as u64)
    stxdw [r10-0xe0], r2                    
    stdw [r10-0xd8], 5                      
    ldxb r2, [r1+0x0]                       
    jne r2, 255, lbb_4456                           if r2 != (255 as i32 as i64 as u64) { pc += 102 }
    mov64 r4, r5                                    r4 = r5
    ldxb r2, [r1+0x1]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_4463                             if r2 == (0 as i32 as i64 as u64) { pc += 104 }
    ldxb r2, [r1+0x2]                       
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_4467                             if r2 == (0 as i32 as i64 as u64) { pc += 105 }
lbb_4362:
    ldxb r2, [r1+0x3]                       
    jne r2, 0, lbb_4365                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_4364:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_4365:
    ldxdw r2, [r1+0x50]                     
    stxdw [r10-0xb0], r0                    
    mov64 r0, r1                                    r0 = r1
    add64 r0, 88                                    r0 += 88   ///  r0 = r0.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xb8], r0                    
    stxdw [r10-0xc0], r2                    
    add64 r1, 72                                    r1 += 72   ///  r1 = r1.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0xc8], r1                    
    stxdw [r10-0xd0], r7                    
    stxb [r10-0x9e], r5                     
    stxb [r10-0x9f], r9                     
    stxb [r10-0xa0], r6                     
    stdw [r10-0xa8], 0                      
    ldxb r1, [r4+0x0]                       
    jne r1, 255, lbb_4456                           if r1 != (255 as i32 as i64 as u64) { pc += 76 }
    ldxb r2, [r4+0x1]                       
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_4471                             if r2 == (0 as i32 as i64 as u64) { pc += 87 }
    ldxb r2, [r4+0x2]                       
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_4475                             if r2 == (0 as i32 as i64 as u64) { pc += 88 }
lbb_4387:
    ldxb r2, [r4+0x3]                       
    jne r2, 0, lbb_4390                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_4389:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_4390:
    ldxdw r2, [r4+0x50]                     
    mov64 r5, r4                                    r5 = r4
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r5                    
    mov64 r5, r4                                    r5 = r4
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r5                    
    stxdw [r10-0x88], r2                    
    add64 r4, 72                                    r4 += 72   ///  r4 = r4.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r4                    
    stxdw [r10-0x98], r8                    
    stxb [r10-0x66], r0                     
    stxb [r10-0x67], r6                     
    stxb [r10-0x68], r1                     
    stdw [r10-0x70], 0                      
    ldxdw r1, [r10-0x188]                   
    ldxb r1, [r1+0x0]                       
    and64 r1, 136                                   r1 &= 136   ///  r1 = r1.and(136)
    jne r1, 136, lbb_4456                           if r1 != (136 as i32 as i64 as u64) { pc += 47 }
    ldxdw r6, [r10-0x188]                   
    ldxb r1, [r6+0x1]                       
    ldxb r4, [r6+0x2]                       
    ldxb r5, [r6+0x3]                       
    ldxdw r2, [r6+0x50]                     
    mov64 r0, r6                                    r0 = r6
    add64 r0, 40                                    r0 += 40   ///  r0 = r0.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r0                    
    mov64 r0, r6                                    r0 = r6
    add64 r0, 88                                    r0 += 88   ///  r0 = r0.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r0                    
    stxdw [r10-0x50], r2                    
    add64 r6, 72                                    r6 += 72   ///  r6 = r6.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r6                    
    stxdw [r10-0x60], r3                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jeq r5, 0, lbb_4479                             if r5 == (0 as i32 as i64 as u64) { pc += 53 }
    stxb [r10-0x2e], r2                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jeq r4, 0, lbb_4483                             if r4 == (0 as i32 as i64 as u64) { pc += 54 }
lbb_4429:
    stxb [r10-0x2f], r2                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_4433                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_4432:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_4433:
    stxb [r10-0x30], r2                     
    stdw [r10-0x38], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -361                                  r1 += -361   ///  r1 = r1.wrapping_add(-361 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -352                                  r1 += -352   ///  r1 = r1.wrapping_add(-352 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100022e68 --> b"\x06\xdd\xf6\xe1\xeeu\x8f\xde\x18B]\xbc\xe4l\xcd\xda\xb6\x1a\xfcM\x83\xb9…        r1 load str located at 4295110248
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 9                       
    stdw [r10-0x18], 3                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -208                                  r2 += -208   ///  r2 = r2.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -224                                  r4 += -224   ///  r4 = r4.wrapping_add(-224 as i32 as i64 as u64)
    mov64 r3, 3                                     r3 = 3 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    syscall [invalid]                       
    ja lbb_4315                                     if true { pc += -141 }
lbb_4456:
    mov64 r1, 11                                    r1 = 11 as i32 as i64 as u64
    ja lbb_4459                                     if true { pc += 1 }
lbb_4458:
    ldxw r2, [r10-0x174]                    
lbb_4459:
    ldxdw r3, [r10-0x180]                   
    stxw [r3+0x4], r2                       
    stxw [r3+0x0], r1                       
    exit                                    
lbb_4463:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxb r2, [r1+0x2]                       
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_4362                             if r2 != (0 as i32 as i64 as u64) { pc += -105 }
lbb_4467:
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ldxb r2, [r1+0x3]                       
    jeq r2, 0, lbb_4364                             if r2 == (0 as i32 as i64 as u64) { pc += -106 }
    ja lbb_4365                                     if true { pc += -106 }
lbb_4471:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxb r2, [r4+0x2]                       
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_4387                             if r2 != (0 as i32 as i64 as u64) { pc += -88 }
lbb_4475:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxb r2, [r4+0x3]                       
    jeq r2, 0, lbb_4389                             if r2 == (0 as i32 as i64 as u64) { pc += -89 }
    ja lbb_4390                                     if true { pc += -89 }
lbb_4479:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    stxb [r10-0x2e], r2                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jne r4, 0, lbb_4429                             if r4 != (0 as i32 as i64 as u64) { pc += -54 }
lbb_4483:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    stxb [r10-0x2f], r2                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_4432                             if r1 == (0 as i32 as i64 as u64) { pc += -55 }
    ja lbb_4433                                     if true { pc += -55 }

function_4488:
    stxdw [r10-0x98], r1                    
    ldxdw r1, [r5-0xfd8]                    
    stxdw [r10-0xa0], r1                    
    ldxdw r9, [r5-0xfe0]                    
    stxb [r10-0x81], r9                     
    ldxdw r1, [r2+0x0]                      
    ldxdw r0, [r1+0x28]                     
    lddw r6, 0x93a165d7e1f6dd06                     r6 load str located at -7808848301000303354
    jne r0, r6, lbb_4511                            if r0 != r6 { pc += 13 }
    ldxdw r0, [r1+0x30]                     
    lddw r6, 0xac79ebce46e1cbd9                     r6 load str located at -6018520155818964007
    jne r0, r6, lbb_4511                            if r0 != r6 { pc += 9 }
    ldxdw r0, [r1+0x38]                     
    lddw r6, 0x91375b5fed85b41c                     r6 load str located at -7982811346925931492
    jne r0, r6, lbb_4511                            if r0 != r6 { pc += 5 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r1+0x40]                     
    lddw r7, 0xa900ff7e85f58c3a                     r7 load str located at -6268729762421306310
    jeq r6, r7, lbb_4512                            if r6 == r7 { pc += 1 }
lbb_4511:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_4512:
    ldxdw r6, [r5-0xfe8]                    
    ldxdw r8, [r5-0xff0]                    
    ldxdw r7, [r5-0xff8]                    
    jeq r0, 0, lbb_4540                             if r0 == (0 as i32 as i64 as u64) { pc += 24 }
    stxdw [r10-0xa8], r7                    
    ldxdw r0, [r1+0x28]                     
    lddw r7, 0xde8f75eee1f6dd06                     r7 load str located at -2409577606766207738
    jne r0, r7, lbb_4534                            if r0 != r7 { pc += 13 }
    ldxdw r0, [r1+0x30]                     
    lddw r7, 0xdacd6ce4bc5d4218                     r7 load str located at -2680366473547005416
    jne r0, r7, lbb_4534                            if r0 != r7 { pc += 9 }
    ldxdw r0, [r1+0x38]                     
    lddw r7, 0x270db9834dfc1ab6                     r7 load str located at 2814109315776649910
    jne r0, r7, lbb_4534                            if r0 != r7 { pc += 5 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r1, [r1+0x40]                     
    lddw r7, 0xfc8ba1d828f9bdfe                     r7 load str located at -248927404616466946
    jeq r1, r7, lbb_4535                            if r1 == r7 { pc += 1 }
lbb_4534:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_4535:
    jeq r0, 0, lbb_4575                             if r0 == (0 as i32 as i64 as u64) { pc += 39 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    lddw r1, 0xbadc0de3                             r1 load str located at 3134983651
    ja lbb_4640                                     if true { pc += 100 }
lbb_4540:
    ldxdw r1, [r10-0xa0]                    
    stxdw [r10-0x68], r1                    
    stxdw [r10-0x70], r4                    
    stxdw [r10-0x78], r3                    
    stxdw [r10-0x80], r2                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -129                                  r1 += -129   ///  r1 = r1.wrapping_add(-129 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stxdw [r10-0x20], r6                    
    stxdw [r10-0x30], r8                    
    stxdw [r10-0x40], r7                    
    lddw r1, 0x10002349c --> b"poolv1transfer_token_by_owner_with_mint calledInva"        r1 load str located at 4295111836
    stxdw [r10-0x50], r1                    
    stdw [r10-0x8], 1                       
    stdw [r10-0x18], 32                     
    stdw [r10-0x28], 32                     
    stdw [r10-0x38], 32                     
    stdw [r10-0x48], 6                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    stxdw [r10-0x60], r1                    
    stdw [r10-0x58], 5                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -128                                  r2 += -128   ///  r2 = r2.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -96                                   r3 += -96   ///  r3 = r3.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_15230                     
    ldxw r0, [r10-0x90]                     
    jne r0, 26, lbb_4639                            if r0 != (26 as i32 as i64 as u64) { pc += 66 }
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
    ja lbb_4640                                     if true { pc += 65 }
lbb_4575:
    ldxdw r7, [r5-0x1000]                   
    lddw r1, 0x10002351b --> b"About to invoke TransferChecked with accountsFrom "        r1 load str located at 4295111963
    stxdw [r10-0xb8], r2                    
    mov64 r2, 45                                    r2 = 45 as i32 as i64 as u64
    stxdw [r10-0xb0], r4                    
    stxdw [r10-0xc0], r3                    
    syscall [invalid]                       
    lddw r1, 0x100023548 --> b"From account pubkey logged"        r1 load str located at 4295112008
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    syscall [invalid]                       
    lddw r1, 0x100023562 --> b"To account pubkey logged"        r1 load str located at 4295112034
    mov64 r2, 24                                    r2 = 24 as i32 as i64 as u64
    syscall [invalid]                       
    lddw r1, 0x10002357a --> b"Mint account pubkey logged"        r1 load str located at 4295112058
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    syscall [invalid]                       
    lddw r1, 0x100023594 --> b"Authority account pubkey logged"        r1 load str located at 4295112084
    mov64 r2, 31                                    r2 = 31 as i32 as i64 as u64
    syscall [invalid]                       
    lddw r1, 0x1000235b3 --> b"Using TOKENZ program"        r1 load str located at 4295112115
    mov64 r2, 20                                    r2 = 20 as i32 as i64 as u64
    syscall [invalid]                       
    stxb [r10-0x60], r9                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stxdw [r10-0x20], r6                    
    stxdw [r10-0x30], r8                    
    ldxdw r1, [r10-0xa8]                    
    stxdw [r10-0x40], r1                    
    lddw r1, 0x10002349c --> b"poolv1transfer_token_by_owner_with_mint calledInva"        r1 load str located at 4295111836
    stxdw [r10-0x50], r1                    
    stdw [r10-0x8], 1                       
    stdw [r10-0x18], 32                     
    stdw [r10-0x28], 32                     
    stdw [r10-0x38], 32                     
    stdw [r10-0x48], 6                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    stxdw [r10-0x80], r1                    
    stdw [r10-0x78], 5                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    stxdw [r10-0xff0], r1                   
    ldxdw r1, [r10-0xa0]                    
    stxdw [r10-0x1000], r1                  
    stdw [r10-0xfe8], 1                     
    stdw [r10-0xff8], 1                     
    mov64 r5, r10                                   r5 = r10
    ldxdw r1, [r10-0xb8]                    
    ldxdw r2, [r10-0xc0]                    
    ldxdw r3, [r10-0xb0]                    
    mov64 r4, r7                                    r4 = r7
    call function_3754                      
    lddw r1, 0xbadc0def                             r1 load str located at 3134983663
    ja lbb_4640                                     if true { pc += 1 }
lbb_4639:
    ldxw r1, [r10-0x8c]                     
lbb_4640:
    ldxdw r2, [r10-0x98]                    
    stxw [r2+0x4], r1                       
    stxw [r2+0x0], r0                       
    exit                                    

function_4644:
    jne r2, 24, lbb_4650                            if r2 != (24 as i32 as i64 as u64) { pc += 5 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    jne r2, 0, lbb_4655                             if r2 != (0 as i32 as i64 as u64) { pc += 7 }
    mov64 r0, r1                                    r0 = r1
    exit                                    
lbb_4650:
    lddw r1, 0x1000230d3 --> b"from_bytes"          r1 load str located at 4295110867
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_581                       
lbb_4655:
    lddw r1, 0x1000230d3 --> b"from_bytes"          r1 load str located at 4295110867
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_581                       

function_4660:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r9, 195936478                             r9 = 195936478 as i32 as i64 as u64
    jne r3, 9, lbb_4734                             if r3 != (9 as i32 as i64 as u64) { pc += 71 }
    ldxdw r3, [r2+0x30]                     
    ldxdw r6, [r3+0x8]                      
    lddw r7, 0xc974c71817d5a706                     r7 load str located at -3930297668494579962
    jne r6, r7, lbb_4679                            if r6 != r7 { pc += 11 }
    ldxdw r6, [r3+0x10]                     
    lddw r7, 0xb65e1d6998635628                     r7 load str located at -5305770971630447064
    jne r6, r7, lbb_4679                            if r6 != r7 { pc += 7 }
    ldxdw r6, [r3+0x18]                     
    lddw r7, 0x5c6d4b9ba3b85e8b                     r7 load str located at 6660062555789614731
    jne r6, r7, lbb_4679                            if r6 != r7 { pc += 3 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxdw r7, [r3+0x20]                     
    jeq r7, 559633779, lbb_4680                     if r7 == (559633779 as i32 as i64 as u64) { pc += 1 }
lbb_4679:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
lbb_4680:
    lddw r9, 0xabad1dea                             r9 load str located at 2880249322
    jne r6, 0, lbb_4734                             if r6 != (0 as i32 as i64 as u64) { pc += 51 }
    mov64 r6, r2                                    r6 = r2
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x1a0], r6                   
    ldxdw r6, [r6+0x0]                      
    stxdw [r10-0x198], r6                   
    ldxdw r8, [r6+0x28]                     
    lddw r6, 0xe959f7272b74fd7a                     r6 load str located at -1632001642340221574
    jne r8, r6, lbb_4708                            if r8 != r6 { pc += 16 }
    ldxdw r6, [r10-0x198]                   
    ldxdw r6, [r6+0x30]                     
    lddw r8, 0x7a819dd33c7070c6                     r8 load str located at 8827510275200544966
    jne r6, r8, lbb_4708                            if r6 != r8 { pc += 11 }
    ldxdw r6, [r10-0x198]                   
    ldxdw r6, [r6+0x38]                     
    lddw r8, 0x6dd2523bce0a93a0                     r8 load str located at 7913477912056730528
    jne r6, r8, lbb_4708                            if r6 != r8 { pc += 6 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r6, [r10-0x198]                   
    ldxdw r6, [r6+0x40]                     
    lddw r7, 0xd3bb8723dd54a054                     r7 load str located at -3189807322954948524
    jeq r6, r7, lbb_4709                            if r6 == r7 { pc += 1 }
lbb_4708:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
lbb_4709:
    jne r8, 0, lbb_4734                             if r8 != (0 as i32 as i64 as u64) { pc += 24 }
    ldxdw r6, [r2+0x40]                     
    stxdw [r10-0x1a8], r6                   
    ldxdw r6, [r6+0x8]                      
    lddw r7, 0x66d17b1817d5a706                     r7 load str located at 7408838205410486022
    jne r6, r7, lbb_4732                            if r6 != r7 { pc += 16 }
    ldxdw r6, [r10-0x1a8]                   
    ldxdw r6, [r6+0x10]                     
    lddw r7, 0xc0c2fd5504d4da35                     r7 load str located at -4556801331350414795
    jne r6, r7, lbb_4732                            if r6 != r7 { pc += 11 }
    ldxdw r6, [r10-0x1a8]                   
    ldxdw r6, [r6+0x18]                     
    lddw r7, 0xa57556218fc624c1                     r7 load str located at -6524213783030258495
    jne r6, r7, lbb_4732                            if r6 != r7 { pc += 6 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r6, [r10-0x1a8]                   
    ldxdw r6, [r6+0x20]                     
    lddw r7, 0x85fcbbadb                            r7 load str located at 35966925531
    jeq r6, r7, lbb_4733                            if r6 == r7 { pc += 1 }
lbb_4732:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
lbb_4733:
    jeq r8, 0, lbb_4737                             if r8 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_4734:
    stxw [r1+0x4], r9                       
    stxw [r1+0x0], r0                       
    exit                                    
lbb_4737:
    ldxdw r9, [r10-0x198]                   
    ldxdw r6, [r9+0x50]                     
    jne r6, 1728, lbb_5248                          if r6 != (1728 as i32 as i64 as u64) { pc += 508 }
    mov64 r7, r9                                    r7 = r9
    add64 r7, 88                                    r7 += 88   ///  r7 = r7.wrapping_add(88 as i32 as i64 as u64)
    mov64 r6, r7                                    r6 = r7
    and64 r6, 7                                     r6 &= 7   ///  r6 = r6.and(7)
    jne r6, 0, lbb_5253                             if r6 != (0 as i32 as i64 as u64) { pc += 508 }
    ldxdw r8, [r9+0x288]                    
    xor64 r8, 161                                   r8 ^= 161   ///  r8 = r8.xor(161)
    and64 r8, 255                                   r8 &= 255   ///  r8 = r8.and(255)
    jeq r8, 0, lbb_4754                             if r8 == (0 as i32 as i64 as u64) { pc += 5 }
    lddw r9, 0xbadc0de2                             r9 load str located at 3134983650
    jne r8, 1, lbb_4734                             if r8 != (1 as i32 as i64 as u64) { pc += -18 }
    mov64 r9, 7405                                  r9 = 7405 as i32 as i64 as u64
    ja lbb_4734                                     if true { pc += -20 }
lbb_4754:
    stxdw [r10-0x1d0], r7                   
    mov64 r6, r2                                    r6 = r2
    add64 r6, 24                                    r6 += 24   ///  r6 = r6.wrapping_add(24 as i32 as i64 as u64)
    stxdw [r10-0x1b0], r6                   
    mov64 r6, r2                                    r6 = r2
    add64 r6, 16                                    r6 += 16   ///  r6 = r6.wrapping_add(16 as i32 as i64 as u64)
    stxdw [r10-0x1b8], r6                   
    ldxdw r3, [r3+0x58]                     
    stxdw [r10-0x1c0], r3                   
    ldxdw r3, [r9+0x2b0]                    
    stxdw [r10-0x1d8], r3                   
    ldxdw r6, [r9+0x240]                    
    lddw r3, 0x4a2178451bac3c7                      r3 load str located at 333855179453154247
    xor64 r6, r3                                    r6 ^= r3   ///  r6 = r6.xor(r3)
    ldxdw r7, [r9+0x248]                    
    lddw r3, 0x4a1178751b9c3c6                      r3 load str located at 333573717361279942
    xor64 r7, r3                                    r7 ^= r3   ///  r7 = r7.xor(r3)
    ldxdw r8, [r9+0x250]                    
    lddw r3, 0x4a0178651b8c3c5                      r3 load str located at 333292238089536453
    xor64 r8, r3                                    r8 ^= r3   ///  r8 = r8.xor(r3)
    ldxdw r3, [r9+0x238]                    
    stxdw [r10-0xe8], r8                    
    stxdw [r10-0xf0], r7                    
    stxdw [r10-0xf8], r6                    
    lddw r6, 0xfb5ce87aae443c38                     r6 load str located at -334136658724897736
    xor64 r3, r6                                    r3 ^= r6   ///  r3 = r3.xor(r6)
    stxdw [r10-0x100], r3                   
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r6, [r10-0x1b8]                   
    ldxdw r6, [r6+0x0]                      
    ldxdw r9, [r6+0x8]                      
    be64 r9                                         r9 = match 64 { 16 => (r9 as u16).swap_bytes() as u64, 32 => (r9 as u32).swap_bytes() as u64, 64 => r9.swap_bytes(), _ => r9 }
    jne r3, r9, lbb_4807                            if r3 != r9 { pc += 16 }
    ldxdw r3, [r10-0xf8]                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r9, [r6+0x10]                     
    be64 r9                                         r9 = match 64 { 16 => (r9 as u16).swap_bytes() as u64, 32 => (r9 as u32).swap_bytes() as u64, 64 => r9.swap_bytes(), _ => r9 }
    jne r3, r9, lbb_4807                            if r3 != r9 { pc += 11 }
    ldxdw r3, [r10-0xf0]                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r9, [r6+0x18]                     
    be64 r9                                         r9 = match 64 { 16 => (r9 as u16).swap_bytes() as u64, 32 => (r9 as u32).swap_bytes() as u64, 64 => r9.swap_bytes(), _ => r9 }
    jne r3, r9, lbb_4807                            if r3 != r9 { pc += 6 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r3, [r10-0xe8]                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r9, [r6+0x20]                     
    be64 r9                                         r9 = match 64 { 16 => (r9 as u16).swap_bytes() as u64, 32 => (r9 as u32).swap_bytes() as u64, 64 => r9.swap_bytes(), _ => r9 }
    jeq r3, r9, lbb_4810                            if r3 == r9 { pc += 3 }
lbb_4807:
    mov64 r7, -1                                    r7 = -1 as i32 as i64 as u64
    jlt r3, r9, lbb_4810                            if r3 < r9 { pc += 1 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
lbb_4810:
    stxdw [r10-0x1c8], r6                   
    ldxdw r8, [r10-0x198]                   
    ldxdw r6, [r8+0x228]                    
    lddw r3, 0x4a1178751b9c3c6                      r3 load str located at 333573717361279942
    xor64 r6, r3                                    r6 ^= r3   ///  r6 = r6.xor(r3)
    mov64 r9, r7                                    r9 = r7
    ldxdw r7, [r8+0x230]                    
    lddw r3, 0x4a0178651b8c3c5                      r3 load str located at 333292238089536453
    xor64 r7, r3                                    r7 ^= r3   ///  r7 = r7.xor(r3)
    ldxdw r3, [r8+0x218]                    
    ldxdw r8, [r8+0x220]                    
    stxdw [r10-0xe8], r7                    
    mov64 r7, r9                                    r7 = r9
    stxdw [r10-0xf0], r6                    
    lddw r6, 0x4a2178451bac3c7                      r6 load str located at 333855179453154247
    xor64 r8, r6                                    r8 ^= r6   ///  r8 = r8.xor(r6)
    stxdw [r10-0xf8], r8                    
    lddw r6, 0xfb5ce87aae443c38                     r6 load str located at -334136658724897736
    xor64 r3, r6                                    r3 ^= r6   ///  r3 = r3.xor(r6)
    stxdw [r10-0x100], r3                   
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r6, [r10-0x1b0]                   
    ldxdw r6, [r6+0x0]                      
    ldxdw r9, [r6+0x8]                      
    be64 r9                                         r9 = match 64 { 16 => (r9 as u16).swap_bytes() as u64, 32 => (r9 as u32).swap_bytes() as u64, 64 => r9.swap_bytes(), _ => r9 }
    jne r3, r9, lbb_4856                            if r3 != r9 { pc += 16 }
    ldxdw r3, [r10-0xf8]                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r9, [r6+0x10]                     
    be64 r9                                         r9 = match 64 { 16 => (r9 as u16).swap_bytes() as u64, 32 => (r9 as u32).swap_bytes() as u64, 64 => r9.swap_bytes(), _ => r9 }
    jne r3, r9, lbb_4856                            if r3 != r9 { pc += 11 }
    ldxdw r3, [r10-0xf0]                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r9, [r6+0x18]                     
    be64 r9                                         r9 = match 64 { 16 => (r9 as u16).swap_bytes() as u64, 32 => (r9 as u32).swap_bytes() as u64, 64 => r9.swap_bytes(), _ => r9 }
    jne r3, r9, lbb_4856                            if r3 != r9 { pc += 6 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r3, [r10-0xe8]                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r9, [r6+0x20]                     
    be64 r9                                         r9 = match 64 { 16 => (r9 as u16).swap_bytes() as u64, 32 => (r9 as u32).swap_bytes() as u64, 64 => r9.swap_bytes(), _ => r9 }
    jeq r3, r9, lbb_4859                            if r3 == r9 { pc += 3 }
lbb_4856:
    mov64 r8, -1                                    r8 = -1 as i32 as i64 as u64
    jlt r3, r9, lbb_4859                            if r3 < r9 { pc += 1 }
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
lbb_4859:
    or64 r8, r7                                     r8 |= r7   ///  r8 = r8.or(r7)
    lddw r9, 0xbadface3                             r9 load str located at 3135220963
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    rsh64 r8, 32                                    r8 >>= 32   ///  r8 = r8.wrapping_shr(32)
    jne r8, 0, lbb_4734                             if r8 != (0 as i32 as i64 as u64) { pc += -131 }
    stxdw [r10-0x1e0], r6                   
    ldxdw r7, [r10-0x198]                   
    ldxdw r8, [r7+0x2c0]                    
    ldxdw r3, [r7+0x2b8]                    
    lddw r6, 0x6e9de2b30b19f1ea                     r6 load str located at 7970776174128919018
    xor64 r3, r6                                    r3 ^= r6   ///  r3 = r3.xor(r6)
    xor64 r8, r6                                    r8 ^= r6   ///  r8 = r8.xor(r6)
    mov64 r9, 1027565                               r9 = 1027565 as i32 as i64 as u64
    ldxdw r6, [r10-0x1c0]                   
    sub64 r6, r8                                    r6 -= r8   ///  r6 = r6.wrapping_sub(r8)
    jgt r6, r3, lbb_4734                            if r6 > r3 { pc += -143 }
    stxdw [r10-0x1c0], r1                   
    mov64 r1, r4                                    r1 = r4
    mov64 r6, r2                                    r6 = r2
    mov64 r2, r5                                    r2 = r5
    mov64 r9, r4                                    r9 = r4
    call function_4644                      
    stxdw [r10-0x1f8], r6                   
    ldxdw r1, [r6+0x0]                      
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    add64 r1, 8                                     r1 += 8   ///  r1 = r1.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x1e8], r1                   
    mov64 r2, r7                                    r2 = r7
    mov64 r6, r8                                    r6 = r8
    mov64 r3, r8                                    r3 = r8
    call function_6320                      
    mov64 r2, r9                                    r2 = r9
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    ldxdw r8, [r2+0x0]                      
    jeq r8, r0, lbb_4904                            if r8 == r0 { pc += 8 }
    ldxdw r1, [r10-0x1e8]                   
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r6                                    r3 = r6
    call function_6362                      
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r8, r0, lbb_4903                            if r8 == r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_4903:
    mov64 r2, r9                                    r2 = r9
lbb_4904:
    stxdw [r10-0x1f0], r7                   
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxb r7, [r2+0x10]                      
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x1c0]                   
    ldxdw r3, [r10-0x1c8]                   
    jeq r7, 0, lbb_4913                             if r7 == (0 as i32 as i64 as u64) { pc += 2 }
    jne r7, 1, lbb_4960                             if r7 != (1 as i32 as i64 as u64) { pc += 48 }
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
lbb_4913:
    mov64 r9, 11644386                              r9 = 11644386 as i32 as i64 as u64
    ldxdw r3, [r3+0x50]                     
    jlt r3, 72, lbb_4734                            if r3 < (72 as i32 as i64 as u64) { pc += -182 }
    ldxdw r3, [r10-0x1e0]                   
    ldxdw r3, [r3+0x50]                     
    jlt r3, 72, lbb_4734                            if r3 < (72 as i32 as i64 as u64) { pc += -185 }
    ldxdw r1, [r10-0x1c8]                   
    ldxdw r4, [r1+0x98]                     
    ldxdw r1, [r10-0x1e0]                   
    ldxdw r9, [r1+0x98]                     
    ldxdw r2, [r2+0x8]                      
    ldxdw r3, [r10-0x1f0]                   
    ldxdw r1, [r3+0x18]                     
    stxdw [r10-0x148], r1                   
    ldxdw r1, [r3+0x10]                     
    stxdw [r10-0x150], r1                   
    ldxdw r1, [r3+0x8]                      
    stxdw [r10-0x158], r1                   
    ldxdw r1, [r3+0x0]                      
    stxdw [r10-0x160], r1                   
    ldxdw r3, [r10-0x1a8]                   
    ldxdw r1, [r3+0x50]                     
    stxdw [r10-0xfd8], r1                   
    add64 r3, 88                                    r3 += 88   ///  r3 = r3.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xfe0], r3                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -352                                  r1 += -352   ///  r1 = r1.wrapping_add(-352 as i32 as i64 as u64)
    stxdw [r10-0xfe8], r1                   
    mov64 r6, r5                                    r6 = r5
    mov64 r1, r5                                    r1 = r5
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    stxdw [r10-0xff0], r1                   
    ldxdw r1, [r10-0x1d0]                   
    stxdw [r10-0xff8], r1                   
    stxdw [r10-0x1000], r8                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -256                                  r1 += -256   ///  r1 = r1.wrapping_add(-256 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    stxdw [r10-0x1a8], r2                   
    mov64 r3, r9                                    r3 = r9
    stxdw [r10-0x1d0], r4                   
    call function_10790                     
    ldxw r1, [r10-0x100]                    
    jne r1, 1, lbb_4963                             if r1 != (1 as i32 as i64 as u64) { pc += 6 }
    ldxw r9, [r10-0xf8]                     
    ldxw r0, [r10-0xfc]                     
    ja lbb_5246                                     if true { pc += 286 }
lbb_4960:
    lddw r9, 0xbadb100d                             r9 load str located at 3134918669
    ja lbb_4734                                     if true { pc += -229 }
lbb_4963:
    ldxdw r1, [r10-0x1f8]                   
    mov64 r3, r1                                    r3 = r1
    add64 r3, 40                                    r3 += 40   ///  r3 = r3.wrapping_add(40 as i32 as i64 as u64)
    add64 r1, 32                                    r1 += 32   ///  r1 = r1.wrapping_add(32 as i32 as i64 as u64)
    ldxdw r2, [r10-0xc8]                    
    stxdw [r10-0x1c8], r2                   
    stxdw [r10-0x210], r6                   
    stxdw [r10-0x218], r7                   
    stxdw [r10-0x220], r9                   
    stxdw [r10-0x208], r8                   
    jeq r8, 0, lbb_4979                             if r8 == (0 as i32 as i64 as u64) { pc += 5 }
    stxdw [r10-0x200], r3                   
    ldxdw r2, [r10-0x1b8]                   
    stxdw [r10-0x1e0], r2                   
    mov64 r3, r1                                    r3 = r1
    ja lbb_4984                                     if true { pc += 5 }
lbb_4979:
    ldxdw r2, [r10-0x1b0]                   
    stxdw [r10-0x1e0], r2                   
    stxdw [r10-0x200], r1                   
    ldxdw r1, [r10-0x1b8]                   
    stxdw [r10-0x1b0], r1                   
lbb_4984:
    ldxdw r1, [r10-0xd0]                    
    stxdw [r10-0x248], r1                   
    ldxdw r1, [r10-0xd8]                    
    stxdw [r10-0x240], r1                   
    ldxdw r1, [r10-0xe0]                    
    stxdw [r10-0x238], r1                   
    ldxdw r1, [r10-0xe8]                    
    stxdw [r10-0x230], r1                   
    ldxdw r1, [r10-0xf0]                    
    stxdw [r10-0x228], r1                   
    ldxdw r1, [r10-0xf8]                    
    stxdw [r10-0x1b8], r1                   
    ldxdw r9, [r10-0x198]                   
    ldxdw r4, [r9+0x270]                    
    lddw r2, 0x4a0178651b8c3c5                      r2 load str located at 333292238089536453
    xor64 r4, r2                                    r4 ^= r2   ///  r4 = r4.xor(r2)
    ldxdw r6, [r9+0x258]                    
    ldxdw r0, [r9+0x260]                    
    ldxdw r5, [r9+0x268]                    
    stxdw [r10-0x168], r4                   
    lddw r4, 0x4a1178751b9c3c6                      r4 load str located at 333573717361279942
    xor64 r5, r4                                    r5 ^= r4   ///  r5 = r5.xor(r4)
    stxdw [r10-0x170], r5                   
    lddw r5, 0x4a2178451bac3c7                      r5 load str located at 333855179453154247
    xor64 r0, r5                                    r0 ^= r5   ///  r0 = r0.xor(r5)
    stxdw [r10-0x178], r0                   
    lddw r0, 0xfb5ce87aae443c38                     r0 load str located at -334136658724897736
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    stxdw [r10-0x180], r6                   
    ldxdw r6, [r9+0x210]                    
    xor64 r6, r2                                    r6 ^= r2   ///  r6 = r6.xor(r2)
    ldxdw r7, [r9+0x1f8]                    
    ldxdw r8, [r9+0x200]                    
    ldxdw r1, [r9+0x208]                    
    stxdw [r10-0x148], r6                   
    xor64 r1, r4                                    r1 ^= r4   ///  r1 = r1.xor(r4)
    stxdw [r10-0x150], r1                   
    xor64 r8, r5                                    r8 ^= r5   ///  r8 = r8.xor(r5)
    stxdw [r10-0x158], r8                   
    xor64 r7, r0                                    r7 ^= r0   ///  r7 = r7.xor(r0)
    stxdw [r10-0x160], r7                   
    ldxdw r1, [r9+0x1f0]                    
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    ldxdw r2, [r9+0x1d8]                    
    ldxdw r6, [r9+0x1e0]                    
    ldxdw r7, [r9+0x1e8]                    
    stxdw [r10-0xe8], r1                    
    xor64 r7, r4                                    r7 ^= r4   ///  r7 = r7.xor(r4)
    stxdw [r10-0xf0], r7                    
    xor64 r6, r5                                    r6 ^= r5   ///  r6 = r6.xor(r5)
    stxdw [r10-0xf8], r6                    
    xor64 r2, r0                                    r2 ^= r0   ///  r2 = r2.xor(r0)
    stxdw [r10-0x100], r2                   
    ldxdw r1, [r9+0x290]                    
    ldxdw r2, [r10-0x1c8]                   
    stxdw [r10-0xfe0], r2                   
    xor64 r1, 98                                    r1 ^= 98   ///  r1 = r1.xor(98)
    stxdw [r10-0xfe8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -256                                  r1 += -256   ///  r1 = r1.wrapping_add(-256 as i32 as i64 as u64)
    stxdw [r10-0xff0], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -352                                  r1 += -352   ///  r1 = r1.wrapping_add(-352 as i32 as i64 as u64)
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -384                                  r1 += -384   ///  r1 = r1.wrapping_add(-384 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -392                                  r1 += -392   ///  r1 = r1.wrapping_add(-392 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0x1e0]                   
    ldxdw r4, [r10-0x1a0]                   
    call function_4227                      
    ldxw r0, [r10-0x188]                    
    jne r0, 26, lbb_5243                            if r0 != (26 as i32 as i64 as u64) { pc += 180 }
    ldxdw r1, [r10-0x1a8]                   
    stxdw [r10-0xe8], r1                    
    ldxdw r1, [r10-0x1f8]                   
    stxdw [r10-0xf0], r1                    
    ldxdw r1, [r10-0x1b0]                   
    stxdw [r10-0xf8], r1                    
    ldxdw r1, [r10-0x200]                   
    stxdw [r10-0x100], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -400                                  r1 += -400   ///  r1 = r1.wrapping_add(-400 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -256                                  r2 += -256   ///  r2 = r2.wrapping_add(-256 as i32 as i64 as u64)
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_15230                     
    ldxw r0, [r10-0x190]                    
    jne r0, 26, lbb_5245                            if r0 != (26 as i32 as i64 as u64) { pc += 165 }
    lddw r1, 0x6e9de2b30b19f9ea                     r1 load str located at 7970776174128921066
    ldxdw r2, [r10-0x1c8]                   
    stxdw [r10-0x1a0], r2                   
    ldxdw r2, [r10-0x208]                   
    jne r2, 0, lbb_5088                             if r2 != (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r2, [r10-0x1a8]                   
    stxdw [r10-0x1a0], r2                   
lbb_5088:
    ldxdw r2, [r10-0x1d8]                   
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r10-0x1d8], r2                   
    ldxdw r1, [r10-0x208]                   
    jne r1, 0, lbb_5095                             if r1 != (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r10-0x1c8]                   
    stxdw [r10-0x1a8], r1                   
lbb_5095:
    ldxdw r4, [r10-0x1e8]                   
    ldxdw r1, [r4+0x18]                     
    stxdw [r10-0x128], r1                   
    ldxdw r1, [r4+0x10]                     
    stxdw [r10-0x130], r1                   
    ldxdw r1, [r4+0x8]                      
    stxdw [r10-0x138], r1                   
    ldxdw r1, [r4+0x0]                      
    stxdw [r10-0x140], r1                   
    ldxdw r2, [r10-0x1f0]                   
    ldxdw r1, [r2+0x0]                      
    stxdw [r10-0x120], r1                   
    ldxdw r1, [r2+0x8]                      
    stxdw [r10-0x118], r1                   
    ldxdw r1, [r2+0x10]                     
    stxdw [r10-0x110], r1                   
    ldxdw r1, [r2+0x18]                     
    stxdw [r10-0x108], r1                   
    ldxdw r5, [r4+0x0]                      
    ldxdw r2, [r4+0x8]                      
    ldxdw r3, [r4+0x10]                     
    ldxdw r4, [r4+0x18]                     
    ldxdw r1, [r10-0x198]                   
    ldxdw r6, [r1+0x1d8]                    
    stxdw [r10-0x1b0], r6                   
    ldxdw r6, [r1+0x1e0]                    
    stxdw [r10-0x1c8], r6                   
    ldxdw r6, [r1+0x1e8]                    
    stxdw [r10-0x1e0], r6                   
    ldxdw r6, [r1+0x1f8]                    
    stxdw [r10-0x1e8], r6                   
    ldxdw r6, [r1+0x200]                    
    stxdw [r10-0x1f0], r6                   
    ldxdw r6, [r1+0x208]                    
    stxdw [r10-0x1f8], r6                   
    ldxdw r0, [r1+0x1f0]                    
    stxdw [r10-0x200], r0                   
    ldxdw r9, [r1+0x210]                    
    ldxdw r8, [r1+0x378]                    
    ldxdw r7, [r1+0x320]                    
    ldxdw r6, [r1+0x298]                    
    stxdw [r10-0x208], r6                   
    ldxdw r0, [r1+0x2a0]                    
    stxdw [r10-0x250], r0                   
    stxdw [r1+0x2e0], r4                    
    stxdw [r1+0x2d8], r3                    
    stxdw [r1+0x2d0], r2                    
    stxdw [r1+0x2c8], r5                    
    stxdw [r1+0x2f0], r0                    
    stxdw [r1+0x2e8], r6                    
    ldxdw r6, [r10-0x218]                   
    stxb [r1+0x308], r6                     
    ldxdw r2, [r10-0x1a8]                   
    stxdw [r1+0x300], r2                    
    ldxdw r2, [r10-0x1a0]                   
    stxdw [r1+0x2f8], r2                    
    stdw [r1+0x309], 0                      
    stdw [r1+0x310], 0                      
    add64 r1, 712                                   r1 += 712   ///  r1 = r1.wrapping_add(712 as i32 as i64 as u64)
    call function_1267                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -256                                  r1 += -256   ///  r1 = r1.wrapping_add(-256 as i32 as i64 as u64)
    stxdw [r10-0x198], r1                   
    mov64 r2, r10                                   r2 = r10
    add64 r2, -320                                  r2 += -320   ///  r2 = r2.wrapping_add(-320 as i32 as i64 as u64)
    mov64 r3, 64                                    r3 = 64 as i32 as i64 as u64
    call function_17012                     
    lddw r1, 0xd3198133b7c1776c                     r1 load str located at -3235412798162765972
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    lddw r1, 0x504156a22548f8dd                     r1 load str located at 5782998650930657501
    xor64 r8, r1                                    r8 ^= r1   ///  r8 = r8.xor(r1)
    lddw r1, 0x4a0178651b8c3c5                      r1 load str located at 333292238089536453
    xor64 r9, r1                                    r9 ^= r1   ///  r9 = r9.xor(r1)
    ldxdw r5, [r10-0x200]                   
    xor64 r5, r1                                    r5 ^= r1   ///  r5 = r5.xor(r1)
    ldxdw r1, [r10-0x248]                   
    stxdw [r10-0x48], r1                    
    ldxdw r1, [r10-0x240]                   
    stxdw [r10-0x50], r1                    
    ldxdw r1, [r10-0x238]                   
    stxdw [r10-0x58], r1                    
    ldxdw r1, [r10-0x230]                   
    stxdw [r10-0x60], r1                    
    ldxdw r1, [r10-0x228]                   
    stxdw [r10-0x68], r1                    
    ldxdw r1, [r10-0x1b8]                   
    stxdw [r10-0x70], r1                    
    ldxdw r1, [r10-0x250]                   
    stxdw [r10-0x78], r1                    
    ldxdw r1, [r10-0x208]                   
    stxdw [r10-0x80], r1                    
    ldxdw r1, [r10-0x210]                   
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    stxb [r10-0x7], r1                      
    stxb [r10-0x8], r6                      
    stxdw [r10-0x10], r8                    
    stxdw [r10-0x18], r7                    
    ldxdw r1, [r10-0x1d8]                   
    stxdw [r10-0x20], r1                    
    ldxdw r1, [r10-0x1a8]                   
    stxdw [r10-0x28], r1                    
    ldxdw r1, [r10-0x1a0]                   
    stxdw [r10-0x30], r1                    
    ldxdw r1, [r10-0x220]                   
    stxdw [r10-0x38], r1                    
    ldxdw r1, [r10-0x1d0]                   
    stxdw [r10-0x40], r1                    
    stxdw [r10-0x88], r9                    
    lddw r1, 0x4a1178751b9c3c6                      r1 load str located at 333573717361279942
    ldxdw r2, [r10-0x1f8]                   
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r10-0x90], r2                    
    lddw r2, 0x4a2178451bac3c7                      r2 load str located at 333855179453154247
    ldxdw r3, [r10-0x1f0]                   
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r10-0x98], r3                    
    lddw r3, 0xfb5ce87aae443c38                     r3 load str located at -334136658724897736
    ldxdw r4, [r10-0x1e8]                   
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    stxdw [r10-0xa0], r4                    
    stxdw [r10-0xa8], r5                    
    ldxdw r4, [r10-0x1e0]                   
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    stxdw [r10-0xb0], r4                    
    ldxdw r1, [r10-0x1c8]                   
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    stxdw [r10-0xb8], r1                    
    ldxdw r1, [r10-0x1b0]                   
    xor64 r1, r3                                    r1 ^= r3   ///  r1 = r1.xor(r3)
    stxdw [r10-0xc0], r1                    
    sth [r10-0x6], 0                        
    ldxdw r6, [r10-0x198]                   
    mov64 r1, r6                                    r1 = r6
    call function_1015                      
    stxdw [r10-0x160], r6                   
    stdw [r10-0x158], 256                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -352                                  r1 += -352   ///  r1 = r1.wrapping_add(-352 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_15465                     
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
    ja lbb_5246                                     if true { pc += 3 }
lbb_5243:
    ldxw r9, [r10-0x184]                    
    ja lbb_5246                                     if true { pc += 1 }
lbb_5245:
    ldxw r9, [r10-0x18c]                    
lbb_5246:
    ldxdw r1, [r10-0x1c0]                   
    ja lbb_4734                                     if true { pc += -514 }
lbb_5248:
    lddw r1, 0x1000230dd --> b"from_bytes_mut"        r1 load str located at 4295110877
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_581                       
lbb_5253:
    lddw r1, 0x1000230dd --> b"from_bytes_mut"        r1 load str located at 4295110877
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_581                       

function_5258:
    mov64 r8, r5                                    r8 = r5
    stxdw [r10-0x1a0], r4                   
    mov64 r9, r3                                    r9 = r3
    stxdw [r10-0x198], r2                   
    mov64 r6, r1                                    r6 = r1
    lddw r1, 0x1000235c7 --> b"SwapV2 instruction started"        r1 load str located at 4295112135
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    syscall [invalid]                       
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    lddw r2, 0xbadc0de1                             r2 load str located at 3134983649
    jlt r9, 12, lbb_5337                            if r9 < (12 as i32 as i64 as u64) { pc += 66 }
    lddw r2, 0xbadc0ded                             r2 load str located at 3134983661
    jne r8, 24, lbb_5337                            if r8 != (24 as i32 as i64 as u64) { pc += 63 }
    ldxdw r4, [r10-0x198]                   
    ldxdw r9, [r4+0x48]                     
    ldxdw r1, [r9+0x8]                      
    lddw r2, 0x66d17b1817d5a706                     r2 load str located at 7408838205410486022
    jne r1, r2, lbb_5293                            if r1 != r2 { pc += 13 }
    ldxdw r1, [r9+0x10]                     
    lddw r2, 0xc0c2fd5504d4da35                     r2 load str located at -4556801331350414795
    jne r1, r2, lbb_5293                            if r1 != r2 { pc += 9 }
    ldxdw r1, [r9+0x18]                     
    lddw r2, 0xa57556218fc624c1                     r2 load str located at -6524213783030258495
    jne r1, r2, lbb_5293                            if r1 != r2 { pc += 5 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r9+0x20]                     
    lddw r3, 0x85fcbbadb                            r3 load str located at 35966925531
    jeq r2, r3, lbb_5294                            if r2 == r3 { pc += 1 }
lbb_5293:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_5294:
    lddw r2, 0xabad1dea                             r2 load str located at 2880249322
    jne r1, 0, lbb_5337                             if r1 != (0 as i32 as i64 as u64) { pc += 40 }
    ldxdw r8, [r4+0x30]                     
    ldxdw r1, [r8+0x8]                      
    lddw r3, 0xc974c71817d5a706                     r3 load str located at -3930297668494579962
    jne r1, r3, lbb_5313                            if r1 != r3 { pc += 11 }
    ldxdw r1, [r8+0x10]                     
    lddw r3, 0xb65e1d6998635628                     r3 load str located at -5305770971630447064
    jne r1, r3, lbb_5313                            if r1 != r3 { pc += 7 }
    ldxdw r1, [r8+0x18]                     
    lddw r3, 0x5c6d4b9ba3b85e8b                     r3 load str located at 6660062555789614731
    jne r1, r3, lbb_5313                            if r1 != r3 { pc += 3 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r3, [r8+0x20]                     
    jeq r3, 559633779, lbb_5314                     if r3 == (559633779 as i32 as i64 as u64) { pc += 1 }
lbb_5313:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_5314:
    jne r1, 0, lbb_5337                             if r1 != (0 as i32 as i64 as u64) { pc += 22 }
    mov64 r0, r4                                    r0 = r4
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r5, [r0+0x0]                      
    ldxdw r1, [r5+0x28]                     
    lddw r3, 0xe959f7272b74fd7a                     r3 load str located at -1632001642340221574
    jne r1, r3, lbb_5335                            if r1 != r3 { pc += 13 }
    ldxdw r1, [r5+0x30]                     
    lddw r3, 0x7a819dd33c7070c6                     r3 load str located at 8827510275200544966
    jne r1, r3, lbb_5335                            if r1 != r3 { pc += 9 }
    ldxdw r1, [r5+0x38]                     
    lddw r3, 0x6dd2523bce0a93a0                     r3 load str located at 7913477912056730528
    jne r1, r3, lbb_5335                            if r1 != r3 { pc += 5 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r3, [r5+0x40]                     
    lddw r4, 0xd3bb8723dd54a054                     r4 load str located at -3189807322954948524
    jeq r3, r4, lbb_5336                            if r3 == r4 { pc += 1 }
lbb_5335:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_5336:
    jeq r1, 0, lbb_5340                             if r1 == (0 as i32 as i64 as u64) { pc += 3 }
lbb_5337:
    stxw [r6+0x4], r2                       
    stxw [r6+0x0], r7                       
    exit                                    
lbb_5340:
    stxdw [r10-0x1b0], r9                   
    stxdw [r10-0x1b8], r0                   
    ldxdw r2, [r5+0x50]                     
    mov64 r9, r5                                    r9 = r5
    add64 r9, 88                                    r9 += 88   ///  r9 = r9.wrapping_add(88 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    stxdw [r10-0x1a8], r5                   
    call function_6615                      
    ldxdw r0, [r10-0x1a8]                   
    ldxdw r1, [r0+0x288]                    
    xor64 r1, 161                                   r1 ^= 161   ///  r1 = r1.xor(161)
    and64 r1, 255                                   r1 &= 255   ///  r1 = r1.and(255)
    jeq r1, 0, lbb_5358                             if r1 == (0 as i32 as i64 as u64) { pc += 5 }
    lddw r2, 0xbadc0de2                             r2 load str located at 3134983650
    jne r1, 1, lbb_5337                             if r1 != (1 as i32 as i64 as u64) { pc += -19 }
    mov64 r2, 7405                                  r2 = 7405 as i32 as i64 as u64
    ja lbb_5337                                     if true { pc += -21 }
lbb_5358:
    stxdw [r10-0x1c8], r9                   
    ldxdw r1, [r10-0x198]                   
    mov64 r2, r1                                    r2 = r1
    add64 r2, 24                                    r2 += 24   ///  r2 = r2.wrapping_add(24 as i32 as i64 as u64)
    stxdw [r10-0x1c0], r2                   
    add64 r1, 16                                    r1 += 16   ///  r1 = r1.wrapping_add(16 as i32 as i64 as u64)
    ldxdw r8, [r8+0x58]                     
    ldxdw r2, [r0+0x2b0]                    
    stxdw [r10-0x1d8], r2                   
    ldxdw r2, [r0+0x240]                    
    lddw r3, 0x4a2178451bac3c7                      r3 load str located at 333855179453154247
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    ldxdw r4, [r0+0x248]                    
    lddw r3, 0x4a1178751b9c3c6                      r3 load str located at 333573717361279942
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    ldxdw r5, [r0+0x250]                    
    lddw r3, 0x4a0178651b8c3c5                      r3 load str located at 333292238089536453
    xor64 r5, r3                                    r5 ^= r3   ///  r5 = r5.xor(r3)
    ldxdw r3, [r0+0x238]                    
    stxdw [r10-0xe8], r5                    
    stxdw [r10-0xf0], r4                    
    stxdw [r10-0xf8], r2                    
    lddw r2, 0xfb5ce87aae443c38                     r2 load str located at -334136658724897736
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r10-0x100], r3                   
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    stxdw [r10-0x1d0], r1                   
    ldxdw r1, [r1+0x0]                      
    ldxdw r4, [r1+0x8]                      
    be64 r4                                         r4 = match 64 { 16 => (r4 as u16).swap_bytes() as u64, 32 => (r4 as u32).swap_bytes() as u64, 64 => r4.swap_bytes(), _ => r4 }
    jne r3, r4, lbb_5409                            if r3 != r4 { pc += 16 }
    ldxdw r3, [r10-0xf8]                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r4, [r1+0x10]                     
    be64 r4                                         r4 = match 64 { 16 => (r4 as u16).swap_bytes() as u64, 32 => (r4 as u32).swap_bytes() as u64, 64 => r4.swap_bytes(), _ => r4 }
    jne r3, r4, lbb_5409                            if r3 != r4 { pc += 11 }
    ldxdw r3, [r10-0xf0]                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r4, [r1+0x18]                     
    be64 r4                                         r4 = match 64 { 16 => (r4 as u16).swap_bytes() as u64, 32 => (r4 as u32).swap_bytes() as u64, 64 => r4.swap_bytes(), _ => r4 }
    jne r3, r4, lbb_5409                            if r3 != r4 { pc += 6 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r3, [r10-0xe8]                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r4, [r1+0x20]                     
    be64 r4                                         r4 = match 64 { 16 => (r4 as u16).swap_bytes() as u64, 32 => (r4 as u32).swap_bytes() as u64, 64 => r4.swap_bytes(), _ => r4 }
    jeq r3, r4, lbb_5412                            if r3 == r4 { pc += 3 }
lbb_5409:
    mov64 r2, -1                                    r2 = -1 as i32 as i64 as u64
    jlt r3, r4, lbb_5412                            if r3 < r4 { pc += 1 }
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_5412:
    ldxdw r4, [r0+0x228]                    
    lddw r3, 0x4a1178751b9c3c6                      r3 load str located at 333573717361279942
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    ldxdw r5, [r0+0x230]                    
    lddw r3, 0x4a0178651b8c3c5                      r3 load str located at 333292238089536453
    xor64 r5, r3                                    r5 ^= r3   ///  r5 = r5.xor(r3)
    ldxdw r3, [r0+0x218]                    
    ldxdw r0, [r0+0x220]                    
    stxdw [r10-0xe8], r5                    
    stxdw [r10-0xf0], r4                    
    lddw r4, 0x4a2178451bac3c7                      r4 load str located at 333855179453154247
    xor64 r0, r4                                    r0 ^= r4   ///  r0 = r0.xor(r4)
    stxdw [r10-0xf8], r0                    
    lddw r4, 0xfb5ce87aae443c38                     r4 load str located at -334136658724897736
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    stxdw [r10-0x100], r3                   
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r4, [r10-0x1c0]                   
    ldxdw r0, [r4+0x0]                      
    ldxdw r4, [r0+0x8]                      
    be64 r4                                         r4 = match 64 { 16 => (r4 as u16).swap_bytes() as u64, 32 => (r4 as u32).swap_bytes() as u64, 64 => r4.swap_bytes(), _ => r4 }
    jne r3, r4, lbb_5454                            if r3 != r4 { pc += 16 }
    ldxdw r3, [r10-0xf8]                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r4, [r0+0x10]                     
    be64 r4                                         r4 = match 64 { 16 => (r4 as u16).swap_bytes() as u64, 32 => (r4 as u32).swap_bytes() as u64, 64 => r4.swap_bytes(), _ => r4 }
    jne r3, r4, lbb_5454                            if r3 != r4 { pc += 11 }
    ldxdw r3, [r10-0xf0]                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r4, [r0+0x18]                     
    be64 r4                                         r4 = match 64 { 16 => (r4 as u16).swap_bytes() as u64, 32 => (r4 as u32).swap_bytes() as u64, 64 => r4.swap_bytes(), _ => r4 }
    jne r3, r4, lbb_5454                            if r3 != r4 { pc += 6 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r3, [r10-0xe8]                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r4, [r0+0x20]                     
    be64 r4                                         r4 = match 64 { 16 => (r4 as u16).swap_bytes() as u64, 32 => (r4 as u32).swap_bytes() as u64, 64 => r4.swap_bytes(), _ => r4 }
    jeq r3, r4, lbb_5457                            if r3 == r4 { pc += 3 }
lbb_5454:
    mov64 r5, -1                                    r5 = -1 as i32 as i64 as u64
    jlt r3, r4, lbb_5457                            if r3 < r4 { pc += 1 }
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_5457:
    or64 r5, r2                                     r5 |= r2   ///  r5 = r5.or(r2)
    lddw r2, 0xbadface3                             r2 load str located at 3135220963
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    jne r5, 0, lbb_5337                             if r5 != (0 as i32 as i64 as u64) { pc += -126 }
    stxdw [r10-0x1e0], r1                   
    ldxdw r2, [r10-0x1a8]                   
    ldxdw r1, [r2+0x2c0]                    
    ldxdw r3, [r2+0x2b8]                    
    lddw r2, 0x6e9de2b30b19f1ea                     r2 load str located at 7970776174128919018
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    mov64 r2, 1027565                               r2 = 1027565 as i32 as i64 as u64
    sub64 r8, r1                                    r8 -= r1   ///  r8 = r8.wrapping_sub(r1)
    jgt r8, r3, lbb_5337                            if r8 > r3 { pc += -137 }
    mov64 r8, r1                                    r8 = r1
    stxdw [r10-0x1f0], r0                   
    lddw r1, 0x100022e48 --> b"About to deserialize SwapV2 data"        r1 load str located at 4295110216
    mov64 r2, 32                                    r2 = 32 as i32 as i64 as u64
    syscall [invalid]                       
    ldxdw r7, [r10-0x1a0]                   
    mov64 r1, r7                                    r1 = r7
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jne r1, 0, lbb_5971                             if r1 != (0 as i32 as i64 as u64) { pc += 487 }
    lddw r1, 0x1000235e1 --> b"Successfully deserialized SwapV2 data"        r1 load str located at 4295112161
    mov64 r2, 37                                    r2 = 37 as i32 as i64 as u64
    syscall [invalid]                       
    ldxdw r7, [r7+0x0]                      
    ldxdw r1, [r10-0x198]                   
    ldxdw r9, [r1+0x0]                      
    ldxdw r2, [r10-0x1a8]                   
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    mov64 r1, r9                                    r1 = r9
    stxdw [r10-0x1e8], r2                   
    mov64 r3, r8                                    r3 = r8
    call function_6320                      
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jeq r7, r0, lbb_5507                            if r7 == r0 { pc += 7 }
    mov64 r1, r9                                    r1 = r9
    ldxdw r2, [r10-0x1e8]                   
    mov64 r3, r8                                    r3 = r8
    call function_6362                      
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jeq r7, r0, lbb_5507                            if r7 == r0 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_5507:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x1a0]                   
    ldxb r0, [r1+0x10]                      
    jeq r0, 0, lbb_5516                             if r0 == (0 as i32 as i64 as u64) { pc += 5 }
    ldxdw r5, [r10-0x198]                   
    jne r0, 1, lbb_5600                             if r0 != (1 as i32 as i64 as u64) { pc += 87 }
    stxdw [r10-0x1f8], r2                   
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ja lbb_5518                                     if true { pc += 2 }
lbb_5516:
    stxdw [r10-0x1f8], r2                   
    ldxdw r5, [r10-0x198]                   
lbb_5518:
    ldxdw r2, [r5+0x38]                     
    ldxdw r1, [r2+0x8]                      
    lddw r3, 0x93a165d7e1f6dd06                     r3 load str located at -7808848301000303354
    jne r1, r3, lbb_5536                            if r1 != r3 { pc += 13 }
    ldxdw r1, [r2+0x10]                     
    lddw r3, 0xac79ebce46e1cbd9                     r3 load str located at -6018520155818964007
    jne r1, r3, lbb_5536                            if r1 != r3 { pc += 9 }
    ldxdw r1, [r2+0x18]                     
    lddw r3, 0x91375b5fed85b41c                     r3 load str located at -7982811346925931492
    jne r1, r3, lbb_5536                            if r1 != r3 { pc += 5 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r1, [r2+0x20]                     
    lddw r4, 0xa900ff7e85f58c3a                     r4 load str located at -6268729762421306310
    jeq r1, r4, lbb_5537                            if r1 == r4 { pc += 1 }
lbb_5536:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_5537:
    ldxdw r1, [r5+0x40]                     
    jeq r3, 0, lbb_5558                             if r3 == (0 as i32 as i64 as u64) { pc += 19 }
    ldxdw r3, [r2+0x8]                      
    lddw r4, 0xde8f75eee1f6dd06                     r4 load str located at -2409577606766207738
    jne r3, r4, lbb_5556                            if r3 != r4 { pc += 13 }
    ldxdw r3, [r2+0x10]                     
    lddw r4, 0xdacd6ce4bc5d4218                     r4 load str located at -2680366473547005416
    jne r3, r4, lbb_5556                            if r3 != r4 { pc += 9 }
    ldxdw r3, [r2+0x18]                     
    lddw r4, 0x270db9834dfc1ab6                     r4 load str located at 2814109315776649910
    jne r3, r4, lbb_5556                            if r3 != r4 { pc += 5 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r2, [r2+0x20]                     
    lddw r4, 0xfc8ba1d828f9bdfe                     r4 load str located at -248927404616466946
    jeq r2, r4, lbb_5557                            if r2 == r4 { pc += 1 }
lbb_5556:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_5557:
    jne r3, 0, lbb_5596                             if r3 != (0 as i32 as i64 as u64) { pc += 38 }
lbb_5558:
    ldxdw r2, [r1+0x8]                      
    lddw r3, 0x93a165d7e1f6dd06                     r3 load str located at -7808848301000303354
    jne r2, r3, lbb_5575                            if r2 != r3 { pc += 13 }
    ldxdw r2, [r1+0x10]                     
    lddw r3, 0xac79ebce46e1cbd9                     r3 load str located at -6018520155818964007
    jne r2, r3, lbb_5575                            if r2 != r3 { pc += 9 }
    ldxdw r2, [r1+0x18]                     
    lddw r3, 0x91375b5fed85b41c                     r3 load str located at -7982811346925931492
    jne r2, r3, lbb_5575                            if r2 != r3 { pc += 5 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r3, [r1+0x20]                     
    lddw r4, 0xa900ff7e85f58c3a                     r4 load str located at -6268729762421306310
    jeq r3, r4, lbb_5576                            if r3 == r4 { pc += 1 }
lbb_5575:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_5576:
    jeq r2, 0, lbb_5604                             if r2 == (0 as i32 as i64 as u64) { pc += 27 }
    ldxdw r2, [r1+0x8]                      
    lddw r3, 0xde8f75eee1f6dd06                     r3 load str located at -2409577606766207738
    jne r2, r3, lbb_5594                            if r2 != r3 { pc += 13 }
    ldxdw r2, [r1+0x10]                     
    lddw r3, 0xdacd6ce4bc5d4218                     r3 load str located at -2680366473547005416
    jne r2, r3, lbb_5594                            if r2 != r3 { pc += 9 }
    ldxdw r2, [r1+0x18]                     
    lddw r3, 0x270db9834dfc1ab6                     r3 load str located at 2814109315776649910
    jne r2, r3, lbb_5594                            if r2 != r3 { pc += 5 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r1, [r1+0x20]                     
    lddw r3, 0xfc8ba1d828f9bdfe                     r3 load str located at -248927404616466946
    jeq r1, r3, lbb_5603                            if r1 == r3 { pc += 9 }
lbb_5594:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_5604                             if r2 == (0 as i32 as i64 as u64) { pc += 8 }
lbb_5596:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    lddw r2, 0xbadc0de3                             r2 load str located at 3134983651
    ja lbb_5337                                     if true { pc += -263 }
lbb_5600:
    lddw r2, 0xbadb100d                             r2 load str located at 3134918669
    ja lbb_5337                                     if true { pc += -266 }
lbb_5603:
    jne r2, 0, lbb_5596                             if r2 != (0 as i32 as i64 as u64) { pc += -8 }
lbb_5604:
    stxdw [r10-0x200], r7                   
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r2, 11644386                              r2 = 11644386 as i32 as i64 as u64
    ldxdw r1, [r10-0x1e0]                   
    ldxdw r1, [r1+0x50]                     
    jlt r1, 72, lbb_5337                            if r1 < (72 as i32 as i64 as u64) { pc += -273 }
    stxdw [r10-0x210], r9                   
    stxdw [r10-0x208], r0                   
    ldxdw r3, [r10-0x1f0]                   
    ldxdw r1, [r3+0x50]                     
    jlt r1, 72, lbb_5337                            if r1 < (72 as i32 as i64 as u64) { pc += -278 }
    ldxdw r1, [r10-0x1e0]                   
    ldxdw r4, [r1+0x98]                     
    ldxdw r3, [r3+0x98]                     
    ldxdw r1, [r10-0x1a0]                   
    ldxdw r2, [r1+0x8]                      
    ldxdw r5, [r10-0x1e8]                   
    ldxdw r1, [r5+0x18]                     
    stxdw [r10-0x148], r1                   
    ldxdw r1, [r5+0x10]                     
    stxdw [r10-0x150], r1                   
    ldxdw r1, [r5+0x8]                      
    stxdw [r10-0x158], r1                   
    ldxdw r1, [r5+0x0]                      
    stxdw [r10-0x160], r1                   
    ldxdw r5, [r10-0x1b0]                   
    ldxdw r1, [r5+0x50]                     
    stxdw [r10-0xfd8], r1                   
    add64 r5, 88                                    r5 += 88   ///  r5 = r5.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xfe0], r5                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -352                                  r1 += -352   ///  r1 = r1.wrapping_add(-352 as i32 as i64 as u64)
    stxdw [r10-0xfe8], r1                   
    ldxdw r1, [r10-0x1f8]                   
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    stxdw [r10-0xff0], r1                   
    ldxdw r1, [r10-0x1c8]                   
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0x200]                   
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -256                                  r1 += -256   ///  r1 = r1.wrapping_add(-256 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    stxdw [r10-0x1a0], r2                   
    stxdw [r10-0x1f0], r3                   
    stxdw [r10-0x1e0], r4                   
    call function_10790                     
    ldxw r1, [r10-0x100]                    
    jne r1, 1, lbb_5656                             if r1 != (1 as i32 as i64 as u64) { pc += 3 }
    ldxw r2, [r10-0xf8]                     
    ldxw r7, [r10-0xfc]                     
    ja lbb_5337                                     if true { pc += -319 }
lbb_5656:
    ldxdw r1, [r10-0x198]                   
    mov64 r2, r1                                    r2 = r1
    add64 r2, 88                                    r2 += 88   ///  r2 = r2.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x218], r2                   
    mov64 r2, r1                                    r2 = r1
    add64 r2, 80                                    r2 += 80   ///  r2 = r2.wrapping_add(80 as i32 as i64 as u64)
    stxdw [r10-0x238], r2                   
    mov64 r2, r1                                    r2 = r1
    add64 r2, 40                                    r2 += 40   ///  r2 = r2.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x1b0], r2                   
    add64 r1, 32                                    r1 += 32   ///  r1 = r1.wrapping_add(32 as i32 as i64 as u64)
    ldxdw r2, [r10-0xc8]                    
    stxdw [r10-0x1c8], r2                   
    ldxdw r2, [r10-0x200]                   
    jeq r2, 0, lbb_5677                             if r2 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r2, [r10-0x238]                   
    stxdw [r10-0x230], r2                   
    ldxdw r2, [r10-0x1d0]                   
    stxdw [r10-0x220], r2                   
    stxdw [r10-0x228], r1                   
    ja lbb_5686                                     if true { pc += 9 }
lbb_5677:
    ldxdw r2, [r10-0x218]                   
    stxdw [r10-0x230], r2                   
    ldxdw r2, [r10-0x1c0]                   
    stxdw [r10-0x220], r2                   
    ldxdw r2, [r10-0x1b0]                   
    stxdw [r10-0x228], r2                   
    stxdw [r10-0x1b0], r1                   
    ldxdw r1, [r10-0x1d0]                   
    stxdw [r10-0x1c0], r1                   
lbb_5686:
    ldxdw r1, [r10-0xd0]                    
    stxdw [r10-0x260], r1                   
    ldxdw r1, [r10-0xd8]                    
    stxdw [r10-0x258], r1                   
    ldxdw r1, [r10-0xe0]                    
    stxdw [r10-0x250], r1                   
    ldxdw r1, [r10-0xe8]                    
    stxdw [r10-0x248], r1                   
    ldxdw r1, [r10-0xf0]                    
    stxdw [r10-0x240], r1                   
    ldxdw r1, [r10-0xf8]                    
    stxdw [r10-0x1d0], r1                   
    lddw r1, 0x100023606 --> b"About to call transfer_token_signed_invoked_with_mint"        r1 load str located at 4295112198
    mov64 r2, 53                                    r2 = 53 as i32 as i64 as u64
    syscall [invalid]                       
    lddw r1, 0x10002363b --> b"Using quote_mint for BaseToQuote direction"        r1 load str located at 4295112251
    mov64 r2, 42                                    r2 = 42 as i32 as i64 as u64
    syscall [invalid]                       
    ldxdw r9, [r10-0x1a8]                   
    ldxdw r2, [r9+0x270]                    
    lddw r1, 0x4a0178651b8c3c5                      r1 load str located at 333292238089536453
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    ldxdw r5, [r9+0x258]                    
    ldxdw r4, [r9+0x260]                    
    ldxdw r3, [r9+0x268]                    
    stxdw [r10-0x168], r2                   
    lddw r2, 0x4a1178751b9c3c6                      r2 load str located at 333573717361279942
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r10-0x170], r3                   
    lddw r3, 0x4a2178451bac3c7                      r3 load str located at 333855179453154247
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    stxdw [r10-0x178], r4                   
    lddw r4, 0xfb5ce87aae443c38                     r4 load str located at -334136658724897736
    xor64 r5, r4                                    r5 ^= r4   ///  r5 = r5.xor(r4)
    stxdw [r10-0x180], r5                   
    ldxdw r5, [r9+0x210]                    
    xor64 r5, r1                                    r5 ^= r1   ///  r5 = r5.xor(r1)
    ldxdw r0, [r9+0x1f8]                    
    ldxdw r8, [r9+0x200]                    
    ldxdw r7, [r9+0x208]                    
    stxdw [r10-0x148], r5                   
    xor64 r7, r2                                    r7 ^= r2   ///  r7 = r7.xor(r2)
    stxdw [r10-0x150], r7                   
    xor64 r8, r3                                    r8 ^= r3   ///  r8 = r8.xor(r3)
    stxdw [r10-0x158], r8                   
    xor64 r0, r4                                    r0 ^= r4   ///  r0 = r0.xor(r4)
    stxdw [r10-0x160], r0                   
    ldxdw r5, [r9+0x1f0]                    
    xor64 r5, r1                                    r5 ^= r1   ///  r5 = r5.xor(r1)
    ldxdw r1, [r9+0x1d8]                    
    ldxdw r0, [r9+0x1e0]                    
    ldxdw r7, [r9+0x1e8]                    
    stxdw [r10-0xe8], r5                    
    xor64 r7, r2                                    r7 ^= r2   ///  r7 = r7.xor(r2)
    stxdw [r10-0xf0], r7                    
    xor64 r0, r3                                    r0 ^= r3   ///  r0 = r0.xor(r3)
    stxdw [r10-0xf8], r0                    
    xor64 r1, r4                                    r1 ^= r4   ///  r1 = r1.xor(r4)
    stxdw [r10-0x100], r1                   
    ldxdw r1, [r9+0x290]                    
    ldxdw r2, [r10-0x1c8]                   
    stxdw [r10-0xfd8], r2                   
    xor64 r1, 98                                    r1 ^= 98   ///  r1 = r1.xor(98)
    stxdw [r10-0xfe0], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -256                                  r1 += -256   ///  r1 = r1.wrapping_add(-256 as i32 as i64 as u64)
    stxdw [r10-0xfe8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -352                                  r1 += -352   ///  r1 = r1.wrapping_add(-352 as i32 as i64 as u64)
    stxdw [r10-0xff0], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -384                                  r1 += -384   ///  r1 = r1.wrapping_add(-384 as i32 as i64 as u64)
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0x230]                   
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -392                                  r1 += -392   ///  r1 = r1.wrapping_add(-392 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0x220]                   
    ldxdw r3, [r10-0x228]                   
    ldxdw r4, [r10-0x1b8]                   
    call function_4488                      
    ldxw r7, [r10-0x188]                    
    jne r7, 26, lbb_5967                            if r7 != (26 as i32 as i64 as u64) { pc += 191 }
    lddw r1, 0x100023665 --> b"About to call transfer_token_by_owner_with_mint"        r1 load str located at 4295112293
    mov64 r2, 47                                    r2 = 47 as i32 as i64 as u64
    syscall [invalid]                       
    lddw r1, 0x100023694 --> b"User to pool transfer"        r1 load str located at 4295112340
    mov64 r2, 21                                    r2 = 21 as i32 as i64 as u64
    syscall [invalid]                       
    ldxdw r1, [r10-0x200]                   
    jne r1, 0, lbb_5788                             if r1 != (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r10-0x238]                   
    stxdw [r10-0x218], r1                   
lbb_5788:
    ldxdw r1, [r10-0x1a0]                   
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0x218]                   
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -400                                  r1 += -400   ///  r1 = r1.wrapping_add(-400 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0x1b0]                   
    ldxdw r3, [r10-0x1c0]                   
    ldxdw r4, [r10-0x198]                   
    call function_4130                      
    ldxw r7, [r10-0x190]                    
    jne r7, 26, lbb_5969                            if r7 != (26 as i32 as i64 as u64) { pc += 168 }
    lddw r1, 0x6e9de2b30b19f9ea                     r1 load str located at 7970776174128921066
    ldxdw r2, [r10-0x1c8]                   
    stxdw [r10-0x198], r2                   
    ldxdw r2, [r10-0x200]                   
    jne r2, 0, lbb_5809                             if r2 != (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r2, [r10-0x1a0]                   
    stxdw [r10-0x198], r2                   
lbb_5809:
    ldxdw r2, [r10-0x1d8]                   
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r10-0x1d8], r2                   
    ldxdw r1, [r10-0x200]                   
    jne r1, 0, lbb_5816                             if r1 != (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r1, [r10-0x1c8]                   
    stxdw [r10-0x1a0], r1                   
lbb_5816:
    ldxdw r4, [r10-0x210]                   
    ldxdw r1, [r4+0x18]                     
    stxdw [r10-0x128], r1                   
    ldxdw r1, [r4+0x10]                     
    stxdw [r10-0x130], r1                   
    ldxdw r1, [r4+0x8]                      
    stxdw [r10-0x138], r1                   
    ldxdw r1, [r4+0x0]                      
    stxdw [r10-0x140], r1                   
    ldxdw r2, [r10-0x1e8]                   
    ldxdw r1, [r2+0x0]                      
    stxdw [r10-0x120], r1                   
    ldxdw r1, [r2+0x8]                      
    stxdw [r10-0x118], r1                   
    ldxdw r1, [r2+0x10]                     
    stxdw [r10-0x110], r1                   
    ldxdw r1, [r2+0x18]                     
    stxdw [r10-0x108], r1                   
    ldxdw r1, [r4+0x0]                      
    stxdw [r10-0x228], r1                   
    ldxdw r5, [r4+0x8]                      
    ldxdw r3, [r4+0x10]                     
    ldxdw r4, [r4+0x18]                     
    ldxdw r1, [r10-0x1a8]                   
    ldxdw r7, [r1+0x1d8]                    
    stxdw [r10-0x1b0], r7                   
    ldxdw r7, [r1+0x1e0]                    
    stxdw [r10-0x1b8], r7                   
    ldxdw r7, [r1+0x1e8]                    
    stxdw [r10-0x1c0], r7                   
    ldxdw r7, [r1+0x1f8]                    
    stxdw [r10-0x1c8], r7                   
    ldxdw r7, [r1+0x200]                    
    stxdw [r10-0x1e8], r7                   
    ldxdw r7, [r1+0x208]                    
    stxdw [r10-0x200], r7                   
    ldxdw r2, [r1+0x1f0]                    
    stxdw [r10-0x210], r2                   
    ldxdw r8, [r1+0x210]                    
    ldxdw r7, [r1+0x378]                    
    ldxdw r9, [r1+0x320]                    
    ldxdw r0, [r1+0x298]                    
    stxdw [r10-0x218], r0                   
    ldxdw r2, [r1+0x2a0]                    
    stxdw [r10-0x220], r2                   
    stxdw [r1+0x2e0], r4                    
    stxdw [r1+0x2d8], r3                    
    stxdw [r1+0x2d0], r5                    
    ldxdw r3, [r10-0x228]                   
    stxdw [r1+0x2c8], r3                    
    stxdw [r1+0x2f0], r2                    
    stxdw [r1+0x2e8], r0                    
    ldxdw r2, [r10-0x208]                   
    stxb [r1+0x308], r2                     
    ldxdw r2, [r10-0x1a0]                   
    stxdw [r1+0x300], r2                    
    ldxdw r2, [r10-0x198]                   
    stxdw [r1+0x2f8], r2                    
    stdw [r1+0x309], 0                      
    stdw [r1+0x310], 0                      
    add64 r1, 712                                   r1 += 712   ///  r1 = r1.wrapping_add(712 as i32 as i64 as u64)
    call function_1267                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -256                                  r1 += -256   ///  r1 = r1.wrapping_add(-256 as i32 as i64 as u64)
    stxdw [r10-0x1a8], r1                   
    mov64 r2, r10                                   r2 = r10
    add64 r2, -320                                  r2 += -320   ///  r2 = r2.wrapping_add(-320 as i32 as i64 as u64)
    mov64 r3, 64                                    r3 = 64 as i32 as i64 as u64
    call function_17012                     
    lddw r1, 0xd3198133b7c1776c                     r1 load str located at -3235412798162765972
    xor64 r9, r1                                    r9 ^= r1   ///  r9 = r9.xor(r1)
    lddw r1, 0x504156a22548f8dd                     r1 load str located at 5782998650930657501
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    lddw r1, 0x4a0178651b8c3c5                      r1 load str located at 333292238089536453
    xor64 r8, r1                                    r8 ^= r1   ///  r8 = r8.xor(r1)
    ldxdw r5, [r10-0x210]                   
    xor64 r5, r1                                    r5 ^= r1   ///  r5 = r5.xor(r1)
    ldxdw r1, [r10-0x260]                   
    stxdw [r10-0x48], r1                    
    ldxdw r1, [r10-0x258]                   
    stxdw [r10-0x50], r1                    
    ldxdw r1, [r10-0x250]                   
    stxdw [r10-0x58], r1                    
    ldxdw r1, [r10-0x248]                   
    stxdw [r10-0x60], r1                    
    ldxdw r1, [r10-0x240]                   
    stxdw [r10-0x68], r1                    
    ldxdw r1, [r10-0x1d0]                   
    stxdw [r10-0x70], r1                    
    ldxdw r1, [r10-0x220]                   
    stxdw [r10-0x78], r1                    
    ldxdw r1, [r10-0x218]                   
    stxdw [r10-0x80], r1                    
    ldxdw r1, [r10-0x1f8]                   
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    stxb [r10-0x7], r1                      
    ldxdw r1, [r10-0x208]                   
    stxb [r10-0x8], r1                      
    stxdw [r10-0x10], r7                    
    stxdw [r10-0x18], r9                    
    ldxdw r1, [r10-0x1d8]                   
    stxdw [r10-0x20], r1                    
    ldxdw r1, [r10-0x1a0]                   
    stxdw [r10-0x28], r1                    
    ldxdw r1, [r10-0x198]                   
    stxdw [r10-0x30], r1                    
    ldxdw r1, [r10-0x1f0]                   
    stxdw [r10-0x38], r1                    
    ldxdw r1, [r10-0x1e0]                   
    stxdw [r10-0x40], r1                    
    stxdw [r10-0x88], r8                    
    lddw r1, 0x4a1178751b9c3c6                      r1 load str located at 333573717361279942
    ldxdw r2, [r10-0x200]                   
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r10-0x90], r2                    
    lddw r2, 0x4a2178451bac3c7                      r2 load str located at 333855179453154247
    ldxdw r3, [r10-0x1e8]                   
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r10-0x98], r3                    
    lddw r3, 0xfb5ce87aae443c38                     r3 load str located at -334136658724897736
    ldxdw r4, [r10-0x1c8]                   
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    stxdw [r10-0xa0], r4                    
    stxdw [r10-0xa8], r5                    
    ldxdw r4, [r10-0x1c0]                   
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    stxdw [r10-0xb0], r4                    
    ldxdw r1, [r10-0x1b8]                   
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    stxdw [r10-0xb8], r1                    
    ldxdw r1, [r10-0x1b0]                   
    xor64 r1, r3                                    r1 ^= r3   ///  r1 = r1.xor(r3)
    stxdw [r10-0xc0], r1                    
    sth [r10-0x6], 0                        
    ldxdw r7, [r10-0x1a8]                   
    mov64 r1, r7                                    r1 = r7
    call function_1015                      
    stxdw [r10-0x160], r7                   
    stdw [r10-0x158], 256                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -352                                  r1 += -352   ///  r1 = r1.wrapping_add(-352 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_15465                     
    mov64 r7, 26                                    r7 = 26 as i32 as i64 as u64
    ja lbb_5337                                     if true { pc += -630 }
lbb_5967:
    ldxw r2, [r10-0x184]                    
    ja lbb_5337                                     if true { pc += -632 }
lbb_5969:
    ldxw r2, [r10-0x18c]                    
    ja lbb_5337                                     if true { pc += -634 }
lbb_5971:
    lddw r1, 0x1000230d3 --> b"from_bytes"          r1 load str located at 4295110867
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_581                       

function_5976:
    mov64 r7, r4                                    r7 = r4
    mov64 r6, r1                                    r6 = r1
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r4, 195936478                             r4 = 195936478 as i32 as i64 as u64
    jne r3, 7, lbb_6228                             if r3 != (7 as i32 as i64 as u64) { pc += 247 }
    jne r5, 24, lbb_6231                            if r5 != (24 as i32 as i64 as u64) { pc += 249 }
    mov64 r1, r7                                    r1 = r7
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jne r1, 0, lbb_6236                             if r1 != (0 as i32 as i64 as u64) { pc += 251 }
    ldxdw r8, [r2+0x0]                      
    stxdw [r10-0x98], r2                    
    ldxdw r9, [r2+0x8]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -32                                   r4 += -32   ///  r4 = r4.wrapping_add(-32 as i32 as i64 as u64)
    lddw r2, 0x100022d88 --> b"z\xfdt+'\xf7Y\xe9\xc6pp<\xd3\x9d\x81z\xa0\x93\x0a\xce;R\xd2mT\xa0T\xdd#\x…        r2 load str located at 4295110024
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    syscall [invalid]                       
    ldxw r1, [r10-0x20]                     
    ldxb r2, [r8+0x1]                       
    ldxdw r4, [r9+0x270]                    
    stxdw [r10-0x8], r4                     
    lddw r3, 0x4a0178651b8c3c5                      r3 load str located at 333292238089536453
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    ldxdw r5, [r9+0x268]                    
    stxdw [r10-0x10], r5                    
    lddw r3, 0x4a1178751b9c3c6                      r3 load str located at 333573717361279942
    xor64 r5, r3                                    r5 ^= r3   ///  r5 = r5.xor(r3)
    ldxdw r0, [r9+0x260]                    
    stxdw [r10-0x18], r0                    
    lddw r3, 0x4a2178451bac3c7                      r3 load str located at 333855179453154247
    xor64 r0, r3                                    r0 ^= r3   ///  r0 = r0.xor(r3)
    ldxdw r3, [r9+0x258]                    
    stxdw [r10-0x18], r0                    
    mov64 r0, r8                                    r0 = r8
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    stxdw [r10-0x10], r5                    
    stxdw [r10-0x8], r4                     
    stxdw [r10-0x20], r3                    
    lddw r4, 0xfb5ce87aae443c38                     r4 load str located at -334136658724897736
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    stxdw [r10-0x20], r3                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r4, [r0+0x8]                      
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
    be64 r4                                         r4 = match 64 { 16 => (r4 as u16).swap_bytes() as u64, 32 => (r4 as u32).swap_bytes() as u64, 64 => r4.swap_bytes(), _ => r4 }
    jne r3, r4, lbb_6045                            if r3 != r4 { pc += 16 }
    ldxdw r3, [r10-0x18]                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r4, [r0+0x8]                      
    be64 r4                                         r4 = match 64 { 16 => (r4 as u16).swap_bytes() as u64, 32 => (r4 as u32).swap_bytes() as u64, 64 => r4.swap_bytes(), _ => r4 }
    jne r3, r4, lbb_6045                            if r3 != r4 { pc += 11 }
    ldxdw r3, [r10-0x10]                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r4, [r0+0x10]                     
    be64 r4                                         r4 = match 64 { 16 => (r4 as u16).swap_bytes() as u64, 32 => (r4 as u32).swap_bytes() as u64, 64 => r4.swap_bytes(), _ => r4 }
    jne r3, r4, lbb_6045                            if r3 != r4 { pc += 6 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r3, [r10-0x8]                     
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r4, [r0+0x18]                     
    be64 r4                                         r4 = match 64 { 16 => (r4 as u16).swap_bytes() as u64, 32 => (r4 as u32).swap_bytes() as u64, 64 => r4.swap_bytes(), _ => r4 }
    jeq r3, r4, lbb_6048                            if r3 == r4 { pc += 3 }
lbb_6045:
    mov64 r5, -1                                    r5 = -1 as i32 as i64 as u64
    jlt r3, r4, lbb_6048                            if r3 < r4 { pc += 1 }
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_6048:
    lddw r4, 0xabad1dea                             r4 load str located at 2880249322
    jeq r2, 0, lbb_6228                             if r2 == (0 as i32 as i64 as u64) { pc += 177 }
    or64 r5, r1                                     r5 |= r1   ///  r5 = r5.or(r1)
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    jne r5, 0, lbb_6228                             if r5 != (0 as i32 as i64 as u64) { pc += 173 }
    stxdw [r10-0xa8], r0                    
    stxdw [r10-0xb0], r7                    
    stxdw [r10-0xa0], r6                    
    ldxdw r2, [r10-0x98]                    
    add64 r2, 16                                    r2 += 16   ///  r2 = r2.wrapping_add(16 as i32 as i64 as u64)
    ldxdw r4, [r9+0x250]                    
    lddw r3, 0x4a0178651b8c3c5                      r3 load str located at 333292238089536453
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    ldxdw r1, [r9+0x238]                    
    ldxdw r5, [r9+0x240]                    
    ldxdw r0, [r9+0x248]                    
    stxdw [r10-0x68], r4                    
    lddw r4, 0x4a1178751b9c3c6                      r4 load str located at 333573717361279942
    xor64 r0, r4                                    r0 ^= r4   ///  r0 = r0.xor(r4)
    stxdw [r10-0x70], r0                    
    lddw r0, 0x4a2178451bac3c7                      r0 load str located at 333855179453154247
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    stxdw [r10-0x78], r5                    
    lddw r5, 0xfb5ce87aae443c38                     r5 load str located at -334136658724897736
    xor64 r1, r5                                    r1 ^= r5   ///  r1 = r1.xor(r5)
    stxdw [r10-0x80], r1                    
    ldxdw r8, [r9+0x230]                    
    xor64 r8, r3                                    r8 ^= r3   ///  r8 = r8.xor(r3)
    ldxdw r3, [r9+0x218]                    
    ldxdw r6, [r9+0x220]                    
    ldxdw r7, [r9+0x228]                    
    stxdw [r10-0x48], r8                    
    xor64 r7, r4                                    r7 ^= r4   ///  r7 = r7.xor(r4)
    stxdw [r10-0x50], r7                    
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    stxdw [r10-0x58], r6                    
    xor64 r3, r5                                    r3 ^= r5   ///  r3 = r3.xor(r5)
    stxdw [r10-0x60], r3                    
    ldxdw r3, [r2+0x0]                      
    ldxdw r4, [r3+0x8]                      
    jne r1, r4, lbb_6105                            if r1 != r4 { pc += 10 }
    ldxdw r1, [r3+0x10]                     
    ldxdw r4, [r10-0x78]                    
    jne r4, r1, lbb_6105                            if r4 != r1 { pc += 7 }
    ldxdw r1, [r3+0x18]                     
    ldxdw r4, [r10-0x70]                    
    jne r4, r1, lbb_6105                            if r4 != r1 { pc += 4 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r3, [r3+0x20]                     
    ldxdw r4, [r10-0x68]                    
    jeq r4, r3, lbb_6106                            if r4 == r3 { pc += 1 }
lbb_6105:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_6106:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    lddw r4, 0xbadface3                             r4 load str located at 3135220963
    jeq r1, 0, lbb_6112                             if r1 == (0 as i32 as i64 as u64) { pc += 2 }
    ldxdw r6, [r10-0xa0]                    
    ja lbb_6228                                     if true { pc += 116 }
lbb_6112:
    ldxdw r0, [r10-0x98]                    
    mov64 r7, r0                                    r7 = r0
    add64 r7, 32                                    r7 += 32   ///  r7 = r7.wrapping_add(32 as i32 as i64 as u64)
    ldxdw r1, [r7+0x0]                      
    ldxdw r3, [r1+0x8]                      
    ldxdw r5, [r10-0x60]                    
    ldxdw r6, [r10-0xa0]                    
    jne r5, r3, lbb_6130                            if r5 != r3 { pc += 10 }
    ldxdw r3, [r1+0x10]                     
    ldxdw r5, [r10-0x58]                    
    jne r5, r3, lbb_6130                            if r5 != r3 { pc += 7 }
    ldxdw r3, [r1+0x18]                     
    ldxdw r5, [r10-0x50]                    
    jne r5, r3, lbb_6130                            if r5 != r3 { pc += 4 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r1, [r1+0x20]                     
    ldxdw r5, [r10-0x48]                    
    jeq r5, r1, lbb_6131                            if r5 == r1 { pc += 1 }
lbb_6130:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
lbb_6131:
    jne r3, 0, lbb_6228                             if r3 != (0 as i32 as i64 as u64) { pc += 96 }
    stxdw [r10-0xb8], r7                    
    mov64 r8, r0                                    r8 = r0
    add64 r8, 8                                     r8 += 8   ///  r8 = r8.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r3, [r9+0x210]                    
    lddw r1, 0x4a0178651b8c3c5                      r1 load str located at 333292238089536453
    xor64 r3, r1                                    r3 ^= r1   ///  r3 = r3.xor(r1)
    ldxdw r4, [r9+0x1f8]                    
    ldxdw r5, [r9+0x200]                    
    ldxdw r0, [r9+0x208]                    
    stxdw [r10-0x28], r3                    
    lddw r3, 0x4a1178751b9c3c6                      r3 load str located at 333573717361279942
    xor64 r0, r3                                    r0 ^= r3   ///  r0 = r0.xor(r3)
    stxdw [r10-0x30], r0                    
    lddw r0, 0x4a2178451bac3c7                      r0 load str located at 333855179453154247
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    stxdw [r10-0x38], r5                    
    lddw r5, 0xfb5ce87aae443c38                     r5 load str located at -334136658724897736
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    stxdw [r10-0x40], r4                    
    ldxdw r4, [r9+0x1f0]                    
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    ldxdw r1, [r9+0x1d8]                    
    ldxdw r6, [r9+0x1e0]                    
    ldxdw r7, [r9+0x1e8]                    
    stxdw [r10-0x8], r4                     
    xor64 r7, r3                                    r7 ^= r3   ///  r7 = r7.xor(r3)
    stxdw [r10-0x10], r7                    
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    stxdw [r10-0x18], r6                    
    mov64 r6, r8                                    r6 = r8
    xor64 r1, r5                                    r1 ^= r5   ///  r1 = r1.xor(r5)
    stxdw [r10-0x20], r1                    
    ldxdw r5, [r9+0x290]                    
    xor64 r5, 98                                    r5 ^= 98   ///  r5 = r5.xor(98)
    ldxdw r7, [r10-0xb0]                    
    ldxdw r1, [r7+0x0]                      
    jeq r1, 0, lbb_6194                             if r1 == (0 as i32 as i64 as u64) { pc += 21 }
    ldxdw r3, [r10-0x98]                    
    add64 r3, 24                                    r3 += 24   ///  r3 = r3.wrapping_add(24 as i32 as i64 as u64)
    stxdw [r10-0xfe0], r1                   
    stxdw [r10-0xfe8], r5                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0xff0], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0xa8]                    
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -136                                  r1 += -136   ///  r1 = r1.wrapping_add(-136 as i32 as i64 as u64)
    mov64 r8, r5                                    r8 = r5
    mov64 r5, r10                                   r5 = r10
    mov64 r4, r6                                    r4 = r6
    call function_4227                      
    mov64 r5, r8                                    r5 = r8
    ldxw r8, [r10-0x88]                     
    jne r8, 26, lbb_6224                            if r8 != (26 as i32 as i64 as u64) { pc += 30 }
lbb_6194:
    mov64 r4, r6                                    r4 = r6
    ldxdw r1, [r7+0x8]                      
    ldxdw r6, [r10-0xa0]                    
    jeq r1, 0, lbb_6217                             if r1 == (0 as i32 as i64 as u64) { pc += 19 }
    ldxdw r3, [r10-0x98]                    
    add64 r3, 40                                    r3 += 40   ///  r3 = r3.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xfe0], r1                   
    stxdw [r10-0xfe8], r5                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0xff0], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    stxdw [r10-0xff8], r1                   
    ldxdw r1, [r10-0xa8]                    
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0xb8]                    
    call function_4227                      
    ldxw r8, [r10-0x90]                     
    jne r8, 26, lbb_6227                            if r8 != (26 as i32 as i64 as u64) { pc += 10 }
lbb_6217:
    ldxdw r1, [r7+0x10]                     
    lddw r2, 0x6e9de2b30b19f9ea                     r2 load str located at 7970776174128921066
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    stxdw [r9+0x2a8], r1                    
    mov64 r8, 26                                    r8 = 26 as i32 as i64 as u64
    ja lbb_6228                                     if true { pc += 4 }
lbb_6224:
    ldxw r4, [r10-0x84]                     
    ldxdw r6, [r10-0xa0]                    
    ja lbb_6228                                     if true { pc += 1 }
lbb_6227:
    ldxw r4, [r10-0x8c]                     
lbb_6228:
    stxw [r6+0x4], r4                       
    stxw [r6+0x0], r8                       
    exit                                    
lbb_6231:
    lddw r1, 0x1000230dd --> b"from_bytes_mut"        r1 load str located at 4295110877
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_581                       
lbb_6236:
    lddw r1, 0x1000230dd --> b"from_bytes_mut"        r1 load str located at 4295110877
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_581                       

function_6241:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r6, 195936478                             r6 = 195936478 as i32 as i64 as u64
    jne r3, 2, lbb_6307                             if r3 != (2 as i32 as i64 as u64) { pc += 63 }
    ldxdw r3, [r2+0x8]                      
    ldxdw r6, [r2+0x0]                      
    ldxb r2, [r6+0x1]                       
    stxdw [r10-0x28], r2                    
    ldxdw r7, [r3+0x270]                    
    stxdw [r10-0x8], r7                     
    lddw r8, 0x4a0178651b8c3c5                      r8 load str located at 333292238089536453
    xor64 r7, r8                                    r7 ^= r8   ///  r7 = r7.xor(r8)
    ldxdw r8, [r3+0x268]                    
    stxdw [r10-0x10], r8                    
    lddw r9, 0x4a1178751b9c3c6                      r9 load str located at 333573717361279942
    xor64 r8, r9                                    r8 ^= r9   ///  r8 = r8.xor(r9)
    ldxdw r9, [r3+0x260]                    
    stxdw [r10-0x18], r9                    
    lddw r2, 0x4a2178451bac3c7                      r2 load str located at 333855179453154247
    xor64 r9, r2                                    r9 ^= r2   ///  r9 = r9.xor(r2)
    ldxdw r2, [r3+0x258]                    
    stxdw [r10-0x18], r9                    
    stxdw [r10-0x10], r8                    
    stxdw [r10-0x8], r7                     
    stxdw [r10-0x20], r2                    
    lddw r7, 0xfb5ce87aae443c38                     r7 load str located at -334136658724897736
    xor64 r2, r7                                    r2 ^= r7   ///  r2 = r2.xor(r7)
    stxdw [r10-0x20], r2                    
    ldxdw r7, [r6+0x8]                      
    jne r2, r7, lbb_6284                            if r2 != r7 { pc += 10 }
    ldxdw r2, [r6+0x10]                     
    ldxdw r7, [r10-0x18]                    
    jne r7, r2, lbb_6284                            if r7 != r2 { pc += 7 }
    ldxdw r2, [r6+0x18]                     
    ldxdw r7, [r10-0x10]                    
    jne r7, r2, lbb_6284                            if r7 != r2 { pc += 4 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r2, [r6+0x20]                     
    ldxdw r6, [r10-0x8]                     
    jeq r6, r2, lbb_6285                            if r6 == r2 { pc += 1 }
lbb_6284:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
lbb_6285:
    lddw r6, 0xabad1dea                             r6 load str located at 2880249322
    ldxdw r2, [r10-0x28]                    
    jeq r2, 0, lbb_6307                             if r2 == (0 as i32 as i64 as u64) { pc += 18 }
    jne r7, 0, lbb_6307                             if r7 != (0 as i32 as i64 as u64) { pc += 17 }
    jne r5, 8, lbb_6310                             if r5 != (8 as i32 as i64 as u64) { pc += 19 }
    mov64 r2, r4                                    r2 = r4
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_6315                             if r2 != (0 as i32 as i64 as u64) { pc += 21 }
    ldxh r2, [r4+0x0]                       
    stxh [r3+0x318], r2                     
    stw [r3+0x31a], 0                       
    sth [r3+0x31e], 0                       
    ldxdw r2, [r3+0x318]                    
    lddw r4, 0xed5f563e78eee80b                     r4 load str located at -1342259337616234485
    xor64 r2, r4                                    r2 ^= r4   ///  r2 = r2.xor(r4)
    stxdw [r3+0x318], r2                    
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
    ldxdw r2, [r3+0x710]                    
    jne r2, 0, lbb_6307                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    stdw [r3+0x710], 1                      
lbb_6307:
    stxw [r1+0x4], r6                       
    stxw [r1+0x0], r0                       
    exit                                    
lbb_6310:
    lddw r1, 0x1000230d3 --> b"from_bytes"          r1 load str located at 4295110867
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_581                       
lbb_6315:
    lddw r1, 0x1000230d3 --> b"from_bytes"          r1 load str located at 4295110867
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_581                       

function_6320:
    lddw r4, 0x2a9f6e5d1c8b4a2f                     r4 load str located at 3071294817079872047
    stxdw [r10-0x80], r4                    
    lddw r4, 0x9e6d5c1b8a4f2e9d                     r4 load str located at -7030862169901158755
    stxdw [r10-0x88], r4                    
    lddw r4, 0x6c5b1a8f4e2d9c6b                     r4 load str located at 7807863581771144299
    stxdw [r10-0x90], r4                    
    lddw r4, 0x5a1f8e4d2c9b3a7f                     r4 load str located at 6494065649803541119
    stxdw [r10-0x98], r4                    
    ldxdw r4, [r1+0x0]                      
    stxdw [r10-0x78], r4                    
    ldxdw r4, [r1+0x8]                      
    stxdw [r10-0x70], r4                    
    ldxdw r4, [r1+0x10]                     
    stxdw [r10-0x68], r4                    
    ldxdw r1, [r1+0x18]                     
    stxdw [r10-0x60], r1                    
    ldxdw r1, [r2+0x0]                      
    stxdw [r10-0x58], r1                    
    ldxdw r1, [r2+0x8]                      
    stxdw [r10-0x50], r1                    
    ldxdw r1, [r2+0x10]                     
    stxdw [r10-0x48], r1                    
    ldxdw r1, [r2+0x18]                     
    stxdw [r10-0x40], r1                    
    rsh64 r3, 7                                     r3 >>= 7   ///  r3 = r3.wrapping_shr(7)
    stxdw [r10-0x38], r3                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -152                                  r1 += -152   ///  r1 = r1.wrapping_add(-152 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x8], 104                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -48                                   r3 += -48   ///  r3 = r3.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    syscall [invalid]                       
    ldxdw r0, [r10-0x30]                    
    exit                                    

function_6362:
    lddw r4, 0x2a9f6e5d1c8b4a2f                     r4 load str located at 3071294817079872047
    stxdw [r10-0x80], r4                    
    lddw r4, 0x9e6d5c1b8a4f2e9d                     r4 load str located at -7030862169901158755
    stxdw [r10-0x88], r4                    
    lddw r4, 0x6c5b1a8f4e2d9c6b                     r4 load str located at 7807863581771144299
    stxdw [r10-0x90], r4                    
    lddw r4, 0x5a1f8e4d2c9b3a7f                     r4 load str located at 6494065649803541119
    stxdw [r10-0x98], r4                    
    ldxdw r4, [r1+0x0]                      
    stxdw [r10-0x78], r4                    
    ldxdw r4, [r1+0x8]                      
    stxdw [r10-0x70], r4                    
    ldxdw r4, [r1+0x10]                     
    stxdw [r10-0x68], r4                    
    ldxdw r1, [r1+0x18]                     
    stxdw [r10-0x60], r1                    
    ldxdw r1, [r2+0x0]                      
    stxdw [r10-0x58], r1                    
    ldxdw r1, [r2+0x8]                      
    stxdw [r10-0x50], r1                    
    ldxdw r1, [r2+0x10]                     
    stxdw [r10-0x48], r1                    
    ldxdw r1, [r2+0x18]                     
    stxdw [r10-0x40], r1                    
    rsh64 r3, 6                                     r3 >>= 6   ///  r3 = r3.wrapping_shr(6)
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    stxdw [r10-0x38], r3                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -152                                  r1 += -152   ///  r1 = r1.wrapping_add(-152 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x8], 104                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -48                                   r3 += -48   ///  r3 = r3.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    syscall [invalid]                       
    ldxdw r0, [r10-0x30]                    
    exit                                    

function_6405:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r7, 195936478                             r7 = 195936478 as i32 as i64 as u64
    jne r3, 2, lbb_6532                             if r3 != (2 as i32 as i64 as u64) { pc += 124 }
    stxdw [r10-0x30], r5                    
    ldxdw r6, [r2+0x8]                      
    ldxdw r3, [r2+0x0]                      
    ldxb r5, [r3+0x1]                       
    ldxdw r7, [r6+0x270]                    
    stxdw [r10-0x8], r7                     
    lddw r8, 0x4a0178651b8c3c5                      r8 load str located at 333292238089536453
    xor64 r7, r8                                    r7 ^= r8   ///  r7 = r7.xor(r8)
    ldxdw r8, [r6+0x268]                    
    stxdw [r10-0x10], r8                    
    lddw r9, 0x4a1178751b9c3c6                      r9 load str located at 333573717361279942
    xor64 r8, r9                                    r8 ^= r9   ///  r8 = r8.xor(r9)
    ldxdw r9, [r6+0x260]                    
    stxdw [r10-0x18], r9                    
    lddw r2, 0x4a2178451bac3c7                      r2 load str located at 333855179453154247
    xor64 r9, r2                                    r9 ^= r2   ///  r9 = r9.xor(r2)
    ldxdw r2, [r6+0x258]                    
    stxdw [r10-0x18], r9                    
    stxdw [r10-0x10], r8                    
    stxdw [r10-0x8], r7                     
    stxdw [r10-0x20], r2                    
    lddw r7, 0xfb5ce87aae443c38                     r7 load str located at -334136658724897736
    xor64 r2, r7                                    r2 ^= r7   ///  r2 = r2.xor(r7)
    stxdw [r10-0x20], r2                    
    ldxdw r7, [r3+0x8]                      
    jne r2, r7, lbb_6448                            if r2 != r7 { pc += 10 }
    ldxdw r2, [r3+0x10]                     
    ldxdw r7, [r10-0x18]                    
    jne r7, r2, lbb_6448                            if r7 != r2 { pc += 7 }
    ldxdw r2, [r3+0x18]                     
    ldxdw r7, [r10-0x10]                    
    jne r7, r2, lbb_6448                            if r7 != r2 { pc += 4 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r2, [r3+0x20]                     
    ldxdw r3, [r10-0x8]                     
    jeq r3, r2, lbb_6449                            if r3 == r2 { pc += 1 }
lbb_6448:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
lbb_6449:
    lddw r7, 0xabad1dea                             r7 load str located at 2880249322
    mov64 r2, r5                                    r2 = r5
    jeq r2, 0, lbb_6532                             if r2 == (0 as i32 as i64 as u64) { pc += 79 }
    jne r8, 0, lbb_6532                             if r8 != (0 as i32 as i64 as u64) { pc += 78 }
    ldxdw r2, [r6+0x710]                    
    jeq r2, 1, lbb_6459                             if r2 == (1 as i32 as i64 as u64) { pc += 3 }
    mov64 r7, 47828                                 r7 = 47828 as i32 as i64 as u64
    jeq r2, 0, lbb_6532                             if r2 == (0 as i32 as i64 as u64) { pc += 74 }
    ja lbb_6460                                     if true { pc += 1 }
lbb_6459:
    stdw [r6+0x710], 2                      
lbb_6460:
    ldxdw r2, [r10-0x30]                    
    jne r2, 80, lbb_6535                            if r2 != (80 as i32 as i64 as u64) { pc += 73 }
    mov64 r8, r1                                    r8 = r1
    mov64 r1, r4                                    r1 = r4
    and64 r1, 3                                     r1 &= 3   ///  r1 = r1.and(3)
    jne r1, 0, lbb_6540                             if r1 != (0 as i32 as i64 as u64) { pc += 74 }
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r4                                    r2 = r4
    mov64 r7, r4                                    r7 = r4
    call function_6545                      
    mov64 r2, r7                                    r2 = r7
    mov64 r7, 47827                                 r7 = 47827 as i32 as i64 as u64
    ldxw r0, [r10-0x28]                     
    mov64 r1, r8                                    r1 = r8
    jne r0, 26, lbb_6532                            if r0 != (26 as i32 as i64 as u64) { pc += 56 }
    mov64 r1, r6                                    r1 = r6
    add64 r1, 808                                   r1 += 808   ///  r1 = r1.wrapping_add(808 as i32 as i64 as u64)
    mov64 r3, 80                                    r3 = 80 as i32 as i64 as u64
    call function_17012                     
    mov64 r1, r8                                    r1 = r8
    ldxdw r2, [r6+0x358]                    
    lddw r3, 0x47d56c2977ad1405                     r3 load str located at 5176162272089543685
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    ldxdw r3, [r6+0x350]                    
    lddw r4, 0x47d66c2a77ae1404                     r4 load str located at 5176443751361287172
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    ldxdw r4, [r6+0x348]                    
    lddw r5, 0x47d76c2b77af1403                     r5 load str located at 5176725230633030659
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r5, [r6+0x340]                    
    lddw r0, 0x47d06c2c77a81402                     r0 load str located at 5174754910090564610
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    ldxdw r0, [r6+0x338]                    
    lddw r7, 0x47d16c2d77a91401                     r7 load str located at 5175036389362308097
    xor64 r0, r7                                    r0 ^= r7   ///  r0 = r0.xor(r7)
    ldxdw r7, [r6+0x330]                    
    lddw r8, 0x47d26c2e77aa1400                     r8 load str located at 5175317868634051584
    xor64 r7, r8                                    r7 ^= r8   ///  r7 = r7.xor(r8)
    ldxdw r8, [r6+0x328]                    
    lddw r9, 0xb82c93d08854ebff                     r9 load str located at -5175599347905795073
    xor64 r8, r9                                    r8 ^= r9   ///  r8 = r8.xor(r9)
    stxdw [r6+0x328], r8                    
    stxdw [r6+0x330], r7                    
    stxdw [r6+0x338], r0                    
    stxdw [r6+0x340], r5                    
    stxdw [r6+0x348], r4                    
    stxdw [r6+0x350], r3                    
    stxdw [r6+0x358], r2                    
    ldxdw r2, [r6+0x360]                    
    lddw r3, 0x47d46c2877ac1406                     r3 load str located at 5175880792817800198
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r6+0x360], r2                    
    ldxdw r2, [r6+0x368]                    
    lddw r3, 0x47db6c2777a31407                     r3 load str located at 5177851113359217671
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r6+0x368], r2                    
    ldxdw r2, [r6+0x370]                    
    lddw r3, 0x47da6c2677a21408                     r3 load str located at 5177569634087474184
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r6+0x370], r2                    
    mov64 r0, 26                                    r0 = 26 as i32 as i64 as u64
lbb_6532:
    stxw [r1+0x4], r7                       
    stxw [r1+0x0], r0                       
    exit                                    
lbb_6535:
    lddw r1, 0x1000230d3 --> b"from_bytes"          r1 load str located at 4295110867
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_581                       
lbb_6540:
    lddw r1, 0x1000230d3 --> b"from_bytes"          r1 load str located at 4295110867
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_581                       

function_6545:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxw r5, [r2+0x0]                       
    jge r5, 50001, lbb_6568                         if r5 >= (50001 as i32 as i64 as u64) { pc += 20 }
    ldxw r4, [r2+0x8]                       
    jne r4, r5, lbb_6554                            if r4 != r5 { pc += 4 }
lbb_6550:
    mov64 r3, 26                                    r3 = 26 as i32 as i64 as u64
    jeq r5, 50000, lbb_6568                         if r5 == (50000 as i32 as i64 as u64) { pc += 16 }
lbb_6552:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ja lbb_6568                                     if true { pc += 14 }
lbb_6554:
    jgt r4, 50000, lbb_6568                         if r4 > (50000 as i32 as i64 as u64) { pc += 13 }
    jle r4, r5, lbb_6568                            if r4 <= r5 { pc += 12 }
    ldxw r5, [r2+0x4]                       
    ldxw r0, [r2+0xc]                       
    jlt r0, r5, lbb_6568                            if r0 < r5 { pc += 9 }
    ldxw r5, [r2+0x10]                      
    jne r5, r4, lbb_6564                            if r5 != r4 { pc += 3 }
lbb_6561:
    mov64 r3, 26                                    r3 = 26 as i32 as i64 as u64
    jne r4, 50000, lbb_6552                         if r4 != (50000 as i32 as i64 as u64) { pc += -11 }
    ja lbb_6568                                     if true { pc += 4 }
lbb_6564:
    jgt r5, 50000, lbb_6568                         if r5 > (50000 as i32 as i64 as u64) { pc += 3 }
    jle r5, r4, lbb_6568                            if r5 <= r4 { pc += 2 }
    ldxw r6, [r2+0x14]                      
    jge r6, r0, lbb_6571                            if r6 >= r0 { pc += 3 }
lbb_6568:
    stxw [r1+0x0], r3                       
    stw [r1+0x4], 47827                     
    exit                                    
lbb_6571:
    ldxw r4, [r2+0x18]                      
    jeq r4, r5, lbb_6550                            if r4 == r5 { pc += -23 }
    jgt r4, 50000, lbb_6568                         if r4 > (50000 as i32 as i64 as u64) { pc += -6 }
    jle r4, r5, lbb_6568                            if r4 <= r5 { pc += -7 }
    ldxw r0, [r2+0x1c]                      
    jlt r0, r6, lbb_6568                            if r0 < r6 { pc += -9 }
    ldxw r5, [r2+0x20]                      
    jeq r5, r4, lbb_6561                            if r5 == r4 { pc += -18 }
    jgt r5, 50000, lbb_6568                         if r5 > (50000 as i32 as i64 as u64) { pc += -12 }
    jle r5, r4, lbb_6568                            if r5 <= r4 { pc += -13 }
    ldxw r6, [r2+0x24]                      
    jlt r6, r0, lbb_6568                            if r6 < r0 { pc += -15 }
    ldxw r4, [r2+0x28]                      
    jeq r4, r5, lbb_6550                            if r4 == r5 { pc += -35 }
    jgt r4, 50000, lbb_6568                         if r4 > (50000 as i32 as i64 as u64) { pc += -18 }
    jle r4, r5, lbb_6568                            if r4 <= r5 { pc += -19 }
    ldxw r0, [r2+0x2c]                      
    jlt r0, r6, lbb_6568                            if r0 < r6 { pc += -21 }
    ldxw r5, [r2+0x30]                      
    jeq r5, r4, lbb_6561                            if r5 == r4 { pc += -30 }
    jgt r5, 50000, lbb_6568                         if r5 > (50000 as i32 as i64 as u64) { pc += -24 }
    jle r5, r4, lbb_6568                            if r5 <= r4 { pc += -25 }
    ldxw r6, [r2+0x34]                      
    jlt r6, r0, lbb_6568                            if r6 < r0 { pc += -27 }
    ldxw r4, [r2+0x38]                      
    jeq r4, r5, lbb_6550                            if r4 == r5 { pc += -47 }
    jgt r4, 50000, lbb_6568                         if r4 > (50000 as i32 as i64 as u64) { pc += -30 }
    jle r4, r5, lbb_6568                            if r4 <= r5 { pc += -31 }
    ldxw r0, [r2+0x3c]                      
    jlt r0, r6, lbb_6568                            if r0 < r6 { pc += -33 }
    ldxw r5, [r2+0x40]                      
    jeq r5, r4, lbb_6561                            if r5 == r4 { pc += -42 }
    jgt r5, 50000, lbb_6568                         if r5 > (50000 as i32 as i64 as u64) { pc += -36 }
    jle r5, r4, lbb_6568                            if r5 <= r4 { pc += -37 }
    ldxw r4, [r2+0x44]                      
    jlt r4, r0, lbb_6568                            if r4 < r0 { pc += -39 }
    ldxw r0, [r2+0x48]                      
    jeq r0, r5, lbb_6550                            if r0 == r5 { pc += -59 }
    jgt r0, 50000, lbb_6568                         if r0 > (50000 as i32 as i64 as u64) { pc += -42 }
    jle r0, r5, lbb_6568                            if r0 <= r5 { pc += -43 }
    ldxw r2, [r2+0x4c]                      
    jlt r2, r4, lbb_6568                            if r2 < r4 { pc += -45 }
    mov64 r3, 26                                    r3 = 26 as i32 as i64 as u64
    ja lbb_6568                                     if true { pc += -47 }

function_6615:
    jne r2, 1728, lbb_6621                          if r2 != (1728 as i32 as i64 as u64) { pc += 5 }
    mov64 r2, r1                                    r2 = r1
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    jne r2, 0, lbb_6626                             if r2 != (0 as i32 as i64 as u64) { pc += 7 }
    mov64 r0, r1                                    r0 = r1
    exit                                    
lbb_6621:
    lddw r1, 0x1000230dd --> b"from_bytes_mut"        r1 load str located at 4295110877
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_581                       
lbb_6626:
    lddw r1, 0x1000230dd --> b"from_bytes_mut"        r1 load str located at 4295110877
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_581                       

function_6631:
    ldxdw r3, [r2+0x18]                     
    ldxdw r4, [r2+0x10]                     
    mov64 r0, r4                                    r0 = r4
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    ldxdw r5, [r2+0x30]                     
    jeq r0, 0, lbb_6645                             if r0 == (0 as i32 as i64 as u64) { pc += 8 }
    lddw r0, 0xffffffffffff                         r0 load str located at 281474976710655
    jgt r3, r0, lbb_6689                            if r3 > r0 { pc += 49 }
    rsh64 r4, 48                                    r4 >>= 48   ///  r4 = r4.wrapping_shr(48)
    lsh64 r3, 16                                    r3 <<= 16   ///  r3 = r3.wrapping_shl(16)
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jlt r5, r4, lbb_6648                            if r5 < r4 { pc += 3 }
lbb_6645:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r2+0x38]                     
    jle r5, r4, lbb_6651                            if r5 <= r4 { pc += 3 }
lbb_6648:
    stxw [r1+0x0], r3                       
    stw [r1+0x4], 47826                     
    exit                                    
lbb_6651:
    ldxdw r6, [r2+0x68]                     
    ldxdw r0, [r2+0x60]                     
    mov64 r8, r0                                    r8 = r0
    or64 r8, r6                                     r8 |= r6   ///  r8 = r8.or(r6)
    ldxdw r5, [r2+0x80]                     
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_6665                             if r8 == (0 as i32 as i64 as u64) { pc += 7 }
    lddw r7, 0xffffffffffff                         r7 load str located at 281474976710655
    jgt r6, r7, lbb_6689                            if r6 > r7 { pc += 28 }
    rsh64 r0, 48                                    r0 >>= 48   ///  r0 = r0.wrapping_shr(48)
    lsh64 r6, 16                                    r6 <<= 16   ///  r6 = r6.wrapping_shl(16)
    or64 r0, r6                                     r0 |= r6   ///  r0 = r0.or(r6)
    mov64 r7, r0                                    r7 = r0
lbb_6665:
    jlt r5, r4, lbb_6648                            if r5 < r4 { pc += -18 }
    jlt r5, r7, lbb_6648                            if r5 < r7 { pc += -19 }
    ldxdw r4, [r2+0x88]                     
    jgt r5, r4, lbb_6648                            if r5 > r4 { pc += -21 }
    ldxdw r6, [r2+0xb8]                     
    ldxdw r0, [r2+0xb0]                     
    mov64 r8, r0                                    r8 = r0
    or64 r8, r6                                     r8 |= r6   ///  r8 = r8.or(r6)
    ldxdw r5, [r2+0xd0]                     
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_6683                             if r8 == (0 as i32 as i64 as u64) { pc += 7 }
    lddw r7, 0xffffffffffff                         r7 load str located at 281474976710655
    jgt r6, r7, lbb_6689                            if r6 > r7 { pc += 10 }
    rsh64 r0, 48                                    r0 >>= 48   ///  r0 = r0.wrapping_shr(48)
    lsh64 r6, 16                                    r6 <<= 16   ///  r6 = r6.wrapping_shl(16)
    or64 r0, r6                                     r0 |= r6   ///  r0 = r0.or(r6)
    mov64 r7, r0                                    r7 = r0
lbb_6683:
    jlt r5, r4, lbb_6648                            if r5 < r4 { pc += -36 }
    jlt r5, r7, lbb_6648                            if r5 < r7 { pc += -37 }
    ldxdw r2, [r2+0xd8]                     
    jgt r5, r2, lbb_6648                            if r5 > r2 { pc += -39 }
    mov64 r3, 26                                    r3 = 26 as i32 as i64 as u64
    ja lbb_6648                                     if true { pc += -41 }
lbb_6689:
    lddw r1, 0x100023a68 --> b"\x00\x00\x00\x00\xcd6\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00\x1d\x00\x00…        r1 load str located at 4295113320
    call function_15470                     

function_6692:
    ldxdw r3, [r2+0x10]                     
    jsgt r3, r1, lbb_6698                           if (r3 as i64) > (r1 as i64) { pc += 4 }
    ldxdw r3, [r2+0x18]                     
    jslt r3, r1, lbb_6698                           if (r3 as i64) < (r1 as i64) { pc += 2 }
    mov64 r0, r2                                    r0 = r2
    ja lbb_6711                                     if true { pc += 13 }
lbb_6698:
    ldxdw r3, [r2+0x40]                     
    jsgt r3, r1, lbb_6704                           if (r3 as i64) > (r1 as i64) { pc += 4 }
    ldxdw r3, [r2+0x48]                     
    mov64 r0, r2                                    r0 = r2
    add64 r0, 48                                    r0 += 48   ///  r0 = r0.wrapping_add(48 as i32 as i64 as u64)
    jsge r3, r1, lbb_6711                           if (r3 as i64) >= (r1 as i64) { pc += 7 }
lbb_6704:
    ldxdw r3, [r2+0x70]                     
    jsgt r3, r1, lbb_6710                           if (r3 as i64) > (r1 as i64) { pc += 4 }
    ldxdw r3, [r2+0x78]                     
    add64 r2, 96                                    r2 += 96   ///  r2 = r2.wrapping_add(96 as i32 as i64 as u64)
    mov64 r0, r2                                    r0 = r2
    jsge r3, r1, lbb_6711                           if (r3 as i64) >= (r1 as i64) { pc += 1 }
lbb_6710:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_6711:
    exit                                    

function_6712:
    ldxdw r3, [r2+0x30]                     
    jgt r3, r1, lbb_6718                            if r3 > r1 { pc += 4 }
    ldxdw r3, [r2+0x38]                     
    jlt r3, r1, lbb_6718                            if r3 < r1 { pc += 2 }
    mov64 r0, r2                                    r0 = r2
    ja lbb_6731                                     if true { pc += 13 }
lbb_6718:
    ldxdw r3, [r2+0x80]                     
    jgt r3, r1, lbb_6724                            if r3 > r1 { pc += 4 }
    ldxdw r3, [r2+0x88]                     
    mov64 r0, r2                                    r0 = r2
    add64 r0, 80                                    r0 += 80   ///  r0 = r0.wrapping_add(80 as i32 as i64 as u64)
    jge r3, r1, lbb_6731                            if r3 >= r1 { pc += 7 }
lbb_6724:
    ldxdw r3, [r2+0xd0]                     
    jgt r3, r1, lbb_6730                            if r3 > r1 { pc += 4 }
    ldxdw r3, [r2+0xd8]                     
    add64 r2, 160                                   r2 += 160   ///  r2 = r2.wrapping_add(160 as i32 as i64 as u64)
    mov64 r0, r2                                    r0 = r2
    jge r3, r1, lbb_6731                            if r3 >= r1 { pc += 1 }
lbb_6730:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_6731:
    exit                                    

function_6732:
    mov64 r6, r5                                    r6 = r5
    stxdw [r10-0x90], r4                    
    stxdw [r10-0xa8], r3                    
    stxdw [r10-0xa0], r2                    
    mov64 r8, r1                                    r8 = r1
    mov64 r9, r2                                    r9 = r2
    arsh64 r9, 63                                   r9 >>= 63 (signed)   ///  r9 = (r9 as i64).wrapping_shr(63)
    ldxdw r7, [r6-0x1000]                   
    mov64 r5, r7                                    r5 = r7
    arsh64 r5, 63                                   r5 >>= 63 (signed)   ///  r5 = (r5 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -104                                  r1 += -104   ///  r1 = r1.wrapping_add(-104 as i32 as i64 as u64)
    mov64 r3, r9                                    r3 = r9
    mov64 r4, r7                                    r4 = r7
    stxdw [r10-0x98], r5                    
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r7, r8                                    r7 = r8
    mov64 r4, r7                                    r4 = r7
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r4, r7                                    r4 = r7
    arsh64 r8, 63                                   r8 >>= 63 (signed)   ///  r8 = (r8 as i64).wrapping_shr(63)
    stxdw [r10-0xb0], r6                    
    ldxdw r6, [r6-0xff8]                    
    mov64 r7, r6                                    r7 = r6
    arsh64 r7, 63                                   r7 >>= 63 (signed)   ///  r7 = (r7 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -56                                   r1 += -56   ///  r1 = r1.wrapping_add(-56 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r7                                    r3 = r7
    mov64 r5, r8                                    r5 = r8
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r7                                    r3 = r7
    ldxdw r7, [r10-0xa0]                    
    mov64 r4, r7                                    r4 = r7
    mov64 r5, r9                                    r5 = r9
    call function_17083                     
    and64 r8, r6                                    r8 &= r6   ///  r8 = r8.and(r6)
    ldxdw r2, [r10-0x30]                    
    add64 r2, r8                                    r2 += r8   ///  r2 = r2.wrapping_add(r8)
    ldxdw r3, [r10-0x50]                    
    ldxdw r1, [r10-0x68]                    
    mov64 r4, r1                                    r4 = r1
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxdw r6, [r10-0x38]                    
    mov64 r3, r6                                    r3 = r6
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r3, r6, lbb_6790                            if r3 < r6 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_6790:
    add64 r2, r0                                    r2 += r0   ///  r2 = r2.wrapping_add(r0)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r4, r1, lbb_6794                            if r4 < r1 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_6794:
    ldxdw r1, [r10-0x98]                    
    and64 r1, r7                                    r1 &= r7   ///  r1 = r1.and(r7)
    ldxdw r4, [r10-0x60]                    
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    ldxdw r1, [r10-0x48]                    
    mov64 r6, r4                                    r6 = r4
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    mov64 r1, r6                                    r1 = r6
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ldxdw r7, [r10-0x90]                    
    jlt r1, r6, lbb_6809                            if r1 < r6 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_6809:
    jlt r6, r4, lbb_6811                            if r6 < r4 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_6811:
    arsh64 r4, 63                                   r4 >>= 63 (signed)   ///  r4 = (r4 as i64).wrapping_shr(63)
    ldxdw r6, [r10-0x40]                    
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    arsh64 r2, 63                                   r2 >>= 63 (signed)   ///  r2 = (r2 as i64).wrapping_shr(63)
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    mov64 r2, r4                                    r2 = r4
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    mov64 r5, r1                                    r5 = r1
    rsh64 r5, 48                                    r5 >>= 48   ///  r5 = r5.wrapping_shr(48)
    or64 r5, r2                                     r5 |= r2   ///  r5 = r5.or(r2)
    arsh64 r4, 48                                   r4 >>= 48 (signed)   ///  r4 = (r4 as i64).wrapping_shr(48)
    lsh64 r1, 16                                    r1 <<= 16   ///  r1 = r1.wrapping_shl(16)
    mov64 r2, r1                                    r2 = r1
    arsh64 r2, 63                                   r2 >>= 63 (signed)   ///  r2 = (r2 as i64).wrapping_shr(63)
    xor64 r4, r2                                    r4 ^= r2   ///  r4 = r4.xor(r2)
    xor64 r5, r2                                    r5 ^= r2   ///  r5 = r5.xor(r2)
    or64 r5, r4                                     r5 |= r4   ///  r5 = r5.or(r4)
    mov64 r0, 109                                   r0 = 109 as i32 as i64 as u64
    jne r5, 0, lbb_7036                             if r5 != (0 as i32 as i64 as u64) { pc += 204 }
    ldxdw r4, [r10-0x58]                    
    rsh64 r4, 48                                    r4 >>= 48   ///  r4 = r4.wrapping_shr(48)
    mov64 r2, r3                                    r2 = r3
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    or64 r2, r4                                     r2 |= r4   ///  r2 = r2.or(r4)
    rsh64 r3, 48                                    r3 >>= 48   ///  r3 = r3.wrapping_shr(48)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    ldxdw r3, [r10-0xb0]                    
    ldxdw r3, [r3-0xff0]                    
    jeq r3, -1, lbb_6855                            if r3 == (-1 as i32 as i64 as u64) { pc += 13 }
    jne r3, 1, lbb_6869                             if r3 != (1 as i32 as i64 as u64) { pc += 26 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    ldxdw r5, [r10-0xa8]                    
    jlt r2, r5, lbb_6851                            if r2 < r5 { pc += 3 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jslt r1, r7, lbb_6864                           if (r1 as i64) < (r7 as i64) { pc += 14 }
    ja lbb_6852                                     if true { pc += 1 }
lbb_6851:
    jslt r1, r7, lbb_6864                           if (r1 as i64) < (r7 as i64) { pc += 12 }
lbb_6852:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jne r1, r7, lbb_6865                            if r1 != r7 { pc += 11 }
    ja lbb_6866                                     if true { pc += 11 }
lbb_6855:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    ldxdw r5, [r10-0xa8]                    
    jgt r2, r5, lbb_6863                            if r2 > r5 { pc += 3 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jsgt r1, r7, lbb_6864                           if (r1 as i64) > (r7 as i64) { pc += 2 }
    ja lbb_6852                                     if true { pc += -11 }
lbb_6863:
    jsle r1, r7, lbb_6852                           if (r1 as i64) <= (r7 as i64) { pc += -12 }
lbb_6864:
    jeq r1, r7, lbb_6866                            if r1 == r7 { pc += 1 }
lbb_6865:
    mov64 r3, r4                                    r3 = r4
lbb_6866:
    mov64 r9, 26                                    r9 = 26 as i32 as i64 as u64
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_7036                             if r3 != (0 as i32 as i64 as u64) { pc += 167 }
lbb_6869:
    mov64 r5, r2                                    r5 = r2
    ldxdw r3, [r10-0xa8]                    
    add64 r5, r3                                    r5 += r3   ///  r5 = r5.wrapping_add(r3)
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r5, r2, lbb_6876                            if r5 < r2 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_6876:
    mov64 r6, r1                                    r6 = r1
    add64 r6, r7                                    r6 += r7   ///  r6 = r6.wrapping_add(r7)
    add64 r6, r3                                    r6 += r3   ///  r6 = r6.wrapping_add(r3)
    mov64 r3, r1                                    r3 = r1
    xor64 r3, r7                                    r3 ^= r7   ///  r3 = r3.xor(r7)
    mov64 r4, r1                                    r4 = r1
    xor64 r4, r6                                    r4 ^= r6   ///  r4 = r4.xor(r6)
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    and64 r3, r4                                    r3 &= r4   ///  r3 = r3.and(r4)
    mov64 r0, 130                                   r0 = 130 as i32 as i64 as u64
    jslt r3, 0, lbb_7036                            if (r3 as i64) < (0 as i32 as i64) { pc += 149 }
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 15                                    r3 >>= 15   ///  r3 = r3.wrapping_shr(15)
    mov64 r0, r6                                    r0 = r6
    lsh64 r0, 49                                    r0 <<= 49   ///  r0 = r0.wrapping_shl(49)
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    rsh64 r0, 48                                    r0 >>= 48   ///  r0 = r0.wrapping_shr(48)
    mov64 r4, r6                                    r4 = r6
    lsh64 r4, 1                                     r4 <<= 1   ///  r4 = r4.wrapping_shl(1)
    and64 r4, -65536                                r4 &= -65536   ///  r4 = r4.and(-65536)
    or64 r4, r0                                     r4 |= r0   ///  r4 = r4.or(r0)
    mov64 r0, r4                                    r0 = r4
    xor64 r0, r6                                    r0 ^= r6   ///  r0 = r0.xor(r6)
    jslt r0, 0, lbb_6914                            if (r0 as i64) < (0 as i32 as i64) { pc += 14 }
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    ldxdw r0, [r10-0xa8]                    
    jlt r2, r0, lbb_6904                            if r2 < r0 { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_6904:
    mov64 r0, r1                                    r0 = r1
    sub64 r0, r7                                    r0 -= r7   ///  r0 = r0.wrapping_sub(r7)
    sub64 r0, r6                                    r0 -= r6   ///  r0 = r0.wrapping_sub(r6)
    mov64 r6, r1                                    r6 = r1
    xor64 r6, r7                                    r6 ^= r7   ///  r6 = r6.xor(r7)
    xor64 r1, r0                                    r1 ^= r0   ///  r1 = r1.xor(r0)
    and64 r6, r1                                    r6 &= r1   ///  r6 = r6.and(r1)
    jsge r6, 0, lbb_6916                            if (r6 as i64) >= (0 as i32 as i64) { pc += 4 }
    mov64 r0, 134                                   r0 = 134 as i32 as i64 as u64
    ja lbb_7036                                     if true { pc += 122 }
lbb_6914:
    mov64 r0, 132                                   r0 = 132 as i32 as i64 as u64
    ja lbb_7036                                     if true { pc += 120 }
lbb_6916:
    lsh64 r5, 1                                     r5 <<= 1   ///  r5 = r5.wrapping_shl(1)
    ldxdw r1, [r10-0xa8]                    
    sub64 r2, r1                                    r2 -= r1   ///  r2 = r2.wrapping_sub(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_6922                             if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_6922:
    and64 r5, 65534                                 r5 &= 65534   ///  r5 = r5.and(65534)
    lsh64 r3, 16                                    r3 <<= 16   ///  r3 = r3.wrapping_shl(16)
    mov64 r6, r2                                    r6 = r2
    neg64 r6                                        r6 = -r6   ///  r6 = (r6 as i64).wrapping_neg() as u64
    jslt r0, 0, lbb_6928                            if (r0 as i64) < (0 as i32 as i64) { pc += 1 }
    mov64 r6, r2                                    r6 = r2
lbb_6928:
    or64 r3, r5                                     r3 |= r5   ///  r3 = r3.or(r5)
    stxdw [r10-0x10], r6                    
    jsge r0, 0, lbb_6933                            if (r0 as i64) >= (0 as i32 as i64) { pc += 2 }
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
    neg64 r0                                        r0 = -r0   ///  r0 = (r0 as i64).wrapping_neg() as u64
lbb_6933:
    stxdw [r10-0x8], r0                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -16                                   r2 += -16   ///  r2 = r2.wrapping_add(-16 as i32 as i64 as u64)
    call function_752                       
    mov64 r0, 137                                   r0 = 137 as i32 as i64 as u64
    ldxdw r1, [r10-0x28]                    
    jeq r1, 0, lbb_7036                             if r1 == (0 as i32 as i64 as u64) { pc += 94 }
    ldxdw r7, [r10-0x18]                    
    ldxdw r8, [r10-0x20]                    
    mov64 r6, r7                                    r6 = r7
    arsh64 r6, 63                                   r6 >>= 63 (signed)   ///  r6 = (r6 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -120                                  r1 += -120   ///  r1 = r1.wrapping_add(-120 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r6                                    r3 = r6
    lddw r4, 0x86a0000000000000                     r4 load str located at -8745990476353503232
    mov64 r5, -1                                    r5 = -1 as i32 as i64 as u64
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -136                                  r1 += -136   ///  r1 = r1.wrapping_add(-136 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    lddw r4, 0x86a0000000000000                     r4 load str located at -8745990476353503232
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r1, r8                                    r1 = r8
    rsh64 r1, 63                                    r1 >>= 63   ///  r1 = r1.wrapping_shr(63)
    mov64 r3, r8                                    r3 = r8
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    ldxdw r1, [r10-0x80]                    
    ldxdw r2, [r10-0x78]                    
    mov64 r4, r2                                    r4 = r2
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    mov64 r1, r8                                    r1 = r8
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r1, r8, lbb_6977                            if r1 < r8 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_6977:
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r4, r2, lbb_6981                            if r4 < r2 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_6981:
    ldxdw r4, [r10-0x70]                    
    add64 r4, r7                                    r4 += r7   ///  r4 = r4.wrapping_add(r7)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    mov64 r8, r4                                    r8 = r4
    add64 r8, r7                                    r8 += r7   ///  r8 = r8.wrapping_add(r7)
    mov64 r2, r8                                    r2 = r8
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r2, r8, lbb_6991                            if r2 < r8 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_6991:
    jlt r8, r4, lbb_6993                            if r8 < r4 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_6993:
    arsh64 r4, 63                                   r4 >>= 63 (signed)   ///  r4 = (r4 as i64).wrapping_shr(63)
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    mov64 r3, r4                                    r3 = r4
    lsh64 r3, 16                                    r3 <<= 16   ///  r3 = r3.wrapping_shl(16)
    mov64 r5, r2                                    r5 = r2
    rsh64 r5, 48                                    r5 >>= 48   ///  r5 = r5.wrapping_shr(48)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    arsh64 r4, 48                                   r4 >>= 48 (signed)   ///  r4 = (r4 as i64).wrapping_shr(48)
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    mov64 r3, r2                                    r3 = r2
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    xor64 r5, r3                                    r5 ^= r3   ///  r5 = r5.xor(r3)
    or64 r5, r4                                     r5 |= r4   ///  r5 = r5.or(r4)
    mov64 r0, 140                                   r0 = 140 as i32 as i64 as u64
    jne r5, 0, lbb_7036                             if r5 != (0 as i32 as i64 as u64) { pc += 23 }
    lddw r3, 0xffe0000000000000                     r3 load str located at -9007199254740992
    ldxdw r4, [r10-0x88]                    
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r4, 48                                    r4 >>= 48   ///  r4 = r4.wrapping_shr(48)
    mov64 r0, r1                                    r0 = r1
    lsh64 r0, 16                                    r0 <<= 16   ///  r0 = r0.wrapping_shl(16)
    or64 r0, r4                                     r0 |= r4   ///  r0 = r0.or(r4)
    rsh64 r1, 48                                    r1 >>= 48   ///  r1 = r1.wrapping_shr(48)
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    mov64 r1, r0                                    r1 = r0
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    mov64 r9, 26                                    r9 = 26 as i32 as i64 as u64
    jeq r1, 0, lbb_7031                             if r1 == (0 as i32 as i64 as u64) { pc += 4 }
    jle r2, 65535, lbb_7033                         if r2 <= (65535 as i32 as i64 as u64) { pc += 5 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r0, 142                                   r0 = 142 as i32 as i64 as u64
    ja lbb_7036                                     if true { pc += 5 }
lbb_7031:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_7036                                     if true { pc += 3 }
lbb_7033:
    rsh64 r0, 48                                    r0 >>= 48   ///  r0 = r0.wrapping_shr(48)
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
lbb_7036:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    or64 r0, r9                                     r0 |= r9   ///  r0 = r0.or(r9)
    exit                                    

function_7039:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r2                                    r1 = r2
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jgt r1, 50000, lbb_7052                         if r1 > (50000 as i32 as i64 as u64) { pc += 8 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxw r4, [r3+0x0]                       
    jgt r4, r1, lbb_7085                            if r4 > r1 { pc += 38 }
    ldxw r8, [r3+0x8]                       
    jne r8, r4, lbb_7054                            if r8 != r4 { pc += 5 }
    mov64 r5, r3                                    r5 = r3
    ldxw r5, [r5+0x4]                       
    ja lbb_7085                                     if true { pc += 33 }
lbb_7052:
    stw [r6+0x8], 49                        
    ja lbb_7110                                     if true { pc += 56 }
lbb_7054:
    mov64 r0, 8                                     r0 = 8 as i32 as i64 as u64
    mov64 r1, r2                                    r1 = r2
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jle r8, r1, lbb_7062                            if r8 <= r1 { pc += 3 }
    mov64 r7, r8                                    r7 = r8
    mov64 r8, r3                                    r8 = r3
    ja lbb_7076                                     if true { pc += 14 }
lbb_7062:
    mov64 r5, r3                                    r5 = r3
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    ldxw r4, [r3+0x10]                      
    jne r4, r8, lbb_7068                            if r4 != r8 { pc += 2 }
lbb_7066:
    ldxw r5, [r5+0x4]                       
    ja lbb_7085                                     if true { pc += 17 }
lbb_7068:
    mov64 r0, 16                                    r0 = 16 as i32 as i64 as u64
    mov64 r7, r2                                    r7 = r2
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    jle r4, r7, lbb_7112                            if r4 <= r7 { pc += 39 }
lbb_7073:
    mov64 r7, r4                                    r7 = r4
    mov64 r4, r8                                    r4 = r8
lbb_7075:
    mov64 r8, r5                                    r8 = r5
lbb_7076:
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    ldxw r2, [r3+0x4]                       
    ldxw r5, [r8+0x4]                       
    sub64 r2, r5                                    r2 -= r5   ///  r2 = r2.wrapping_sub(r5)
    mul64 r2, r1                                    r2 *= r1   ///  r2 = r2.wrapping_mul(r1)
    sub64 r7, r4                                    r7 -= r4   ///  r7 = r7.wrapping_sub(r4)
    div64 r2, r7                                    r2 /= r7   ///  r2 = r2 / r7
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
lbb_7085:
    mov64 r1, r5                                    r1 = r5
    lsh64 r1, 48                                    r1 <<= 48   ///  r1 = r1.wrapping_shl(48)
    stxdw [r10-0x10], r1                    
    lddw r1, 0xffff0000                             r1 load str located at 4294901760
    and64 r5, r1                                    r5 &= r1   ///  r5 = r5.and(r1)
    rsh64 r5, 16                                    r5 >>= 16   ///  r5 = r5.wrapping_shr(16)
    stxdw [r10-0x8], r5                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -16                                   r2 += -16   ///  r2 = r2.wrapping_add(-16 as i32 as i64 as u64)
    lddw r3, 0xa000000000000                        r3 load str located at 2814749767106560
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_752                       
    ldxdw r1, [r10-0x28]                    
    jeq r1, 0, lbb_7109                             if r1 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r1, [r10-0x20]                    
    ldxdw r2, [r10-0x18]                    
    stxdw [r6+0x10], r2                     
    stxdw [r6+0x8], r1                      
    stw [r6+0x0], 0                         
    ja lbb_7111                                     if true { pc += 2 }
lbb_7109:
    stw [r6+0x8], 154                       
lbb_7110:
    stdw [r6+0x0], 1                        
lbb_7111:
    exit                                    
lbb_7112:
    mov64 r5, r3                                    r5 = r3
    add64 r5, 16                                    r5 += 16   ///  r5 = r5.wrapping_add(16 as i32 as i64 as u64)
    ldxw r8, [r3+0x18]                      
    jeq r8, r4, lbb_7066                            if r8 == r4 { pc += -50 }
    mov64 r0, 24                                    r0 = 24 as i32 as i64 as u64
    mov64 r7, r2                                    r7 = r2
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    jle r8, r7, lbb_7123                            if r8 <= r7 { pc += 2 }
lbb_7121:
    mov64 r7, r8                                    r7 = r8
    ja lbb_7075                                     if true { pc += -48 }
lbb_7123:
    mov64 r5, r3                                    r5 = r3
    add64 r5, 24                                    r5 += 24   ///  r5 = r5.wrapping_add(24 as i32 as i64 as u64)
    ldxw r4, [r3+0x20]                      
    jeq r4, r8, lbb_7066                            if r4 == r8 { pc += -61 }
    mov64 r0, 32                                    r0 = 32 as i32 as i64 as u64
    mov64 r7, r2                                    r7 = r2
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    jgt r4, r7, lbb_7073                            if r4 > r7 { pc += -59 }
    mov64 r5, r3                                    r5 = r3
    add64 r5, 32                                    r5 += 32   ///  r5 = r5.wrapping_add(32 as i32 as i64 as u64)
    ldxw r8, [r3+0x28]                      
    jeq r8, r4, lbb_7066                            if r8 == r4 { pc += -70 }
    mov64 r0, 40                                    r0 = 40 as i32 as i64 as u64
    mov64 r7, r2                                    r7 = r2
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    jgt r8, r7, lbb_7121                            if r8 > r7 { pc += -20 }
    mov64 r5, r3                                    r5 = r3
    add64 r5, 40                                    r5 += 40   ///  r5 = r5.wrapping_add(40 as i32 as i64 as u64)
    ldxw r4, [r3+0x30]                      
    jeq r4, r8, lbb_7066                            if r4 == r8 { pc += -79 }
    mov64 r0, 48                                    r0 = 48 as i32 as i64 as u64
    mov64 r7, r2                                    r7 = r2
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    jgt r4, r7, lbb_7073                            if r4 > r7 { pc += -77 }
    mov64 r5, r3                                    r5 = r3
    add64 r5, 48                                    r5 += 48   ///  r5 = r5.wrapping_add(48 as i32 as i64 as u64)
    ldxw r8, [r3+0x38]                      
    jeq r8, r4, lbb_7066                            if r8 == r4 { pc += -88 }
    mov64 r0, 56                                    r0 = 56 as i32 as i64 as u64
    mov64 r7, r2                                    r7 = r2
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    jgt r8, r7, lbb_7121                            if r8 > r7 { pc += -38 }
    mov64 r5, r3                                    r5 = r3
    add64 r5, 56                                    r5 += 56   ///  r5 = r5.wrapping_add(56 as i32 as i64 as u64)
    ldxw r4, [r3+0x40]                      
    jeq r4, r8, lbb_7066                            if r4 == r8 { pc += -97 }
    mov64 r0, 64                                    r0 = 64 as i32 as i64 as u64
    mov64 r7, r2                                    r7 = r2
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    jgt r4, r7, lbb_7073                            if r4 > r7 { pc += -95 }
    mov64 r8, r3                                    r8 = r3
    add64 r8, 64                                    r8 += 64   ///  r8 = r8.wrapping_add(64 as i32 as i64 as u64)
    ldxw r7, [r3+0x48]                      
    jne r7, r4, lbb_7175                            if r7 != r4 { pc += 3 }
    mov64 r5, r8                                    r5 = r8
    ldxw r5, [r5+0x4]                       
    ja lbb_7085                                     if true { pc += -90 }
lbb_7175:
    mov64 r5, r3                                    r5 = r3
    add64 r5, 72                                    r5 += 72   ///  r5 = r5.wrapping_add(72 as i32 as i64 as u64)
    mov64 r0, 72                                    r0 = 72 as i32 as i64 as u64
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    jle r7, r2, lbb_7066                            if r7 <= r2 { pc += -115 }
    ja lbb_7076                                     if true { pc += -106 }

function_7182:
    mov64 r7, r5                                    r7 = r5
    mov64 r0, r2                                    r0 = r2
    mov64 r6, r1                                    r6 = r1
    stxdw [r10-0x1000], r4                  
    ldxdw r1, [r7-0xff0]                    
    stxdw [r10-0xff0], r1                   
    ldxdw r1, [r7-0x1000]                   
    stxdw [r10-0xff8], r1                   
    arsh64 r2, 63                                   r2 >>= 63 (signed)   ///  r2 = (r2 as i64).wrapping_shr(63)
    lsh64 r2, 48                                    r2 <<= 48   ///  r2 = r2.wrapping_shl(48)
    mov64 r1, r0                                    r1 = r0
    rsh64 r1, 16                                    r1 >>= 16   ///  r1 = r1.wrapping_shr(16)
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    mov64 r4, r3                                    r4 = r3
    arsh64 r4, 63                                   r4 >>= 63 (signed)   ///  r4 = (r4 as i64).wrapping_shr(63)
    lsh64 r4, 48                                    r4 <<= 48   ///  r4 = r4.wrapping_shl(48)
    mov64 r1, r3                                    r1 = r3
    rsh64 r1, 16                                    r1 >>= 16   ///  r1 = r1.wrapping_shr(16)
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    lsh64 r0, 48                                    r0 <<= 48   ///  r0 = r0.wrapping_shl(48)
    lsh64 r3, 48                                    r3 <<= 48   ///  r3 = r3.wrapping_shl(48)
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r0                                    r1 = r0
    call function_6732                      
    mov64 r2, r0                                    r2 = r0
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mov64 r1, r0                                    r1 = r0
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jne r1, 26, lbb_7216                            if r1 != (26 as i32 as i64 as u64) { pc += 4 }
    ldxdw r3, [r7-0xff8]                    
    mov64 r1, r6                                    r1 = r6
    call function_7039                      
    ja lbb_7219                                     if true { pc += 3 }
lbb_7216:
    stxw [r6+0x8], r2                       
    stxw [r6+0x4], r0                       
    stw [r6+0x0], 1                         
lbb_7219:
    exit                                    

function_7220:
    stxdw [r10-0x1c0], r4                   
    mov64 r6, r3                                    r6 = r3
    mov64 r8, r1                                    r8 = r1
    mov64 r1, r2                                    r1 = r2
    lsh64 r1, 48                                    r1 <<= 48   ///  r1 = r1.wrapping_shl(48)
    stxdw [r10-0x10], r1                    
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 16                                    r1 >>= 16   ///  r1 = r1.wrapping_shr(16)
    arsh64 r2, 63                                   r2 >>= 63 (signed)   ///  r2 = (r2 as i64).wrapping_shr(63)
    lsh64 r2, 48                                    r2 <<= 48   ///  r2 = r2.wrapping_shl(48)
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    stxdw [r10-0x8], r2                     
    ldxdw r9, [r5-0x1000]                   
    stxdw [r10-0x1b8], r5                   
    ldxdw r7, [r5-0xff8]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -16                                   r2 += -16   ///  r2 = r2.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r3, r9                                    r3 = r9
    mov64 r4, r7                                    r4 = r7
    call function_752                       
    ldxdw r1, [r10-0x50]                    
    jne r1, 1, lbb_7302                             if r1 != (1 as i32 as i64 as u64) { pc += 58 }
    stxdw [r10-0x1c8], r8                   
    mov64 r1, r9                                    r1 = r9
    or64 r1, r7                                     r1 |= r7   ///  r1 = r1.or(r7)
    mov64 r5, r7                                    r5 = r7
    jeq r1, 0, lbb_7384                             if r1 == (0 as i32 as i64 as u64) { pc += 135 }
    mov64 r7, r6                                    r7 = r6
    arsh64 r7, 63                                   r7 >>= 63 (signed)   ///  r7 = (r7 as i64).wrapping_shr(63)
    lsh64 r7, 48                                    r7 <<= 48   ///  r7 = r7.wrapping_shl(48)
    mov64 r1, r6                                    r1 = r6
    rsh64 r1, 16                                    r1 >>= 16   ///  r1 = r1.wrapping_shr(16)
    or64 r7, r1                                     r7 |= r1   ///  r7 = r7.or(r1)
    lsh64 r6, 48                                    r6 <<= 48   ///  r6 = r6.wrapping_shl(48)
    mov64 r2, r6                                    r2 = r6
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    jsge r7, 0, lbb_7305                            if (r7 as i64) >= (0 as i32 as i64) { pc += 46 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jeq r6, 0, lbb_7309                             if r6 == (0 as i32 as i64 as u64) { pc += 47 }
lbb_7262:
    mov64 r3, r7                                    r3 = r7
    jsge r7, 0, lbb_7266                            if (r7 as i64) >= (0 as i32 as i64) { pc += 2 }
lbb_7264:
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    neg64 r3                                        r3 = -r3   ///  r3 = (r3 as i64).wrapping_neg() as u64
lbb_7266:
    ldxdw r4, [r10-0x1b8]                   
    ldxdw r4, [r4-0xff0]                    
    stxdw [r10-0x1e8], r4                   
    ldxdw r4, [r10-0x40]                    
    stxdw [r10-0x1e0], r4                   
    ldxdw r4, [r10-0x48]                    
    stxdw [r10-0x1b8], r4                   
    stxdw [r10-0x20], r2                    
    stxdw [r10-0x18], r3                    
    jeq r9, 0, lbb_7313                             if r9 == (0 as i32 as i64 as u64) { pc += 37 }
    mov64 r6, r9                                    r6 = r9
    neg64 r6                                        r6 = -r6   ///  r6 = (r6 as i64).wrapping_neg() as u64
    jsge r5, 0, lbb_7317                            if (r5 as i64) >= (0 as i32 as i64) { pc += 38 }
lbb_7279:
    mov64 r8, r5                                    r8 = r5
    mov64 r9, r5                                    r9 = r5
    jsge r5, 0, lbb_7284                            if (r5 as i64) >= (0 as i32 as i64) { pc += 2 }
lbb_7282:
    add64 r9, r1                                    r9 += r1   ///  r9 = r9.wrapping_add(r1)
    neg64 r9                                        r9 = -r9   ///  r9 = (r9 as i64).wrapping_neg() as u64
lbb_7284:
    stxdw [r10-0x8], r9                     
    stxdw [r10-0x10], r6                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -32                                   r2 += -32   ///  r2 = r2.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -16                                   r3 += -16   ///  r3 = r3.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r4, 48                                    r4 = 48 as i32 as i64 as u64
    call function_610                       
    ldxb r1, [r10-0x40]                     
    jne r1, 0, lbb_7384                             if r1 != (0 as i32 as i64 as u64) { pc += 88 }
    ldxdw r4, [r10-0x48]                    
    ldxdw r5, [r10-0x50]                    
    xor64 r7, r8                                    r7 ^= r8   ///  r7 = r7.xor(r8)
    jslt r7, 0, lbb_7322                            if (r7 as i64) < (0 as i32 as i64) { pc += 22 }
    jsge r4, 0, lbb_7351                            if (r4 as i64) >= (0 as i32 as i64) { pc += 50 }
    ja lbb_7384                                     if true { pc += 82 }
lbb_7302:
    lddw r1, 0x5900000000                           r1 load str located at 382252089344
    ja lbb_7387                                     if true { pc += 82 }
lbb_7305:
    mov64 r2, r6                                    r2 = r6
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r6, 0, lbb_7262                             if r6 != (0 as i32 as i64 as u64) { pc += -47 }
lbb_7309:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r3, r7                                    r3 = r7
    jslt r7, 0, lbb_7264                            if (r7 as i64) < (0 as i32 as i64) { pc += -48 }
    ja lbb_7266                                     if true { pc += -47 }
lbb_7313:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r6, r9                                    r6 = r9
    neg64 r6                                        r6 = -r6   ///  r6 = (r6 as i64).wrapping_neg() as u64
    jslt r5, 0, lbb_7279                            if (r5 as i64) < (0 as i32 as i64) { pc += -38 }
lbb_7317:
    mov64 r6, r9                                    r6 = r9
    mov64 r8, r5                                    r8 = r5
    mov64 r9, r5                                    r9 = r5
    jslt r5, 0, lbb_7282                            if (r5 as i64) < (0 as i32 as i64) { pc += -39 }
    ja lbb_7284                                     if true { pc += -38 }
lbb_7322:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r5, 0, lbb_7332                             if r5 == (0 as i32 as i64 as u64) { pc += 7 }
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    jle r4, r3, lbb_7336                            if r4 <= r3 { pc += 8 }
lbb_7328:
    jne r4, r3, lbb_7338                            if r4 != r3 { pc += 9 }
lbb_7329:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jeq r1, 0, lbb_7341                             if r1 == (0 as i32 as i64 as u64) { pc += 10 }
    ja lbb_7384                                     if true { pc += 52 }
lbb_7332:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    jgt r4, r3, lbb_7328                            if r4 > r3 { pc += -8 }
lbb_7336:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jeq r4, r3, lbb_7329                            if r4 == r3 { pc += -9 }
lbb_7338:
    mov64 r1, r2                                    r1 = r2
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_7384                             if r1 != (0 as i32 as i64 as u64) { pc += 43 }
lbb_7341:
    neg64 r5                                        r5 = -r5   ///  r5 = (r5 as i64).wrapping_neg() as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jgt r5, 0, lbb_7345                             if r5 > (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_7345:
    mov64 r2, r4                                    r2 = r4
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    jne r1, 0, lbb_7350                             if r1 != (0 as i32 as i64 as u64) { pc += 2 }
    neg64 r4                                        r4 = -r4   ///  r4 = (r4 as i64).wrapping_neg() as u64
    mov64 r2, r4                                    r2 = r4
lbb_7350:
    mov64 r4, r2                                    r4 = r2
lbb_7351:
    ldxdw r2, [r10-0x1c0]                   
    mov64 r7, r2                                    r7 = r2
    arsh64 r7, 63                                   r7 >>= 63 (signed)   ///  r7 = (r7 as i64).wrapping_shr(63)
    lsh64 r7, 48                                    r7 <<= 48   ///  r7 = r7.wrapping_shl(48)
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 16                                    r1 >>= 16   ///  r1 = r1.wrapping_shr(16)
    or64 r7, r1                                     r7 |= r1   ///  r7 = r7.or(r1)
    lsh64 r2, 48                                    r2 <<= 48   ///  r2 = r2.wrapping_shl(48)
    mov64 r1, r2                                    r1 = r2
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    jsge r7, 0, lbb_7391                            if (r7 as i64) >= (0 as i32 as i64) { pc += 29 }
    stxdw [r10-0x20], r1                    
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r2, 0, lbb_7395                             if r2 == (0 as i32 as i64 as u64) { pc += 30 }
lbb_7365:
    stxdw [r10-0x1d0], r4                   
    stxdw [r10-0x1c0], r5                   
    mov64 r2, r7                                    r2 = r7
    jsge r7, 0, lbb_7371                            if (r7 as i64) >= (0 as i32 as i64) { pc += 2 }
lbb_7369:
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
lbb_7371:
    stxdw [r10-0x18], r2                    
    stxdw [r10-0x8], r9                     
    stxdw [r10-0x10], r6                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -32                                   r2 += -32   ///  r2 = r2.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -16                                   r3 += -16   ///  r3 = r3.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r4, 48                                    r4 = 48 as i32 as i64 as u64
    call function_610                       
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_7401                             if r1 == (0 as i32 as i64 as u64) { pc += 17 }
lbb_7384:
    lddw r1, 0x6300000000                           r1 load str located at 425201762304
    ldxdw r8, [r10-0x1c8]                   
lbb_7387:
    stxdw [r8+0x4], r1                      
lbb_7388:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_7389:
    stxw [r8+0x0], r1                       
    exit                                    
lbb_7391:
    mov64 r1, r2                                    r1 = r2
    stxdw [r10-0x20], r1                    
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_7365                             if r2 != (0 as i32 as i64 as u64) { pc += -30 }
lbb_7395:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x1d0], r4                   
    stxdw [r10-0x1c0], r5                   
    mov64 r2, r7                                    r2 = r7
    jslt r7, 0, lbb_7369                            if (r7 as i64) < (0 as i32 as i64) { pc += -31 }
    ja lbb_7371                                     if true { pc += -30 }
lbb_7401:
    ldxdw r5, [r10-0x48]                    
    ldxdw r0, [r10-0x50]                    
    xor64 r7, r8                                    r7 ^= r8   ///  r7 = r7.xor(r8)
    jslt r7, 0, lbb_7407                            if (r7 as i64) < (0 as i32 as i64) { pc += 2 }
    jsge r5, 0, lbb_7436                            if (r5 as i64) >= (0 as i32 as i64) { pc += 30 }
    ja lbb_7384                                     if true { pc += -23 }
lbb_7407:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r0, 0, lbb_7417                             if r0 == (0 as i32 as i64 as u64) { pc += 7 }
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    jle r5, r3, lbb_7421                            if r5 <= r3 { pc += 8 }
lbb_7413:
    jne r5, r3, lbb_7423                            if r5 != r3 { pc += 9 }
lbb_7414:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_7384                             if r1 != (0 as i32 as i64 as u64) { pc += -32 }
    ja lbb_7426                                     if true { pc += 9 }
lbb_7417:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    jgt r5, r3, lbb_7413                            if r5 > r3 { pc += -8 }
lbb_7421:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jeq r5, r3, lbb_7414                            if r5 == r3 { pc += -9 }
lbb_7423:
    mov64 r1, r2                                    r1 = r2
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_7384                             if r1 != (0 as i32 as i64 as u64) { pc += -42 }
lbb_7426:
    neg64 r0                                        r0 = -r0   ///  r0 = (r0 as i64).wrapping_neg() as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jgt r0, 0, lbb_7430                             if r0 > (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_7430:
    mov64 r2, r5                                    r2 = r5
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    jne r1, 0, lbb_7435                             if r1 != (0 as i32 as i64 as u64) { pc += 2 }
    neg64 r5                                        r5 = -r5   ///  r5 = (r5 as i64).wrapping_neg() as u64
    mov64 r2, r5                                    r2 = r5
lbb_7435:
    mov64 r5, r2                                    r5 = r2
lbb_7436:
    ldxdw r1, [r10-0x1e8]                   
    ldxb r2, [r1+0x20]                      
    jge r2, 4, lbb_8886                             if r2 >= (4 as i32 as i64 as u64) { pc += 1447 }
    stxdw [r10-0x208], r5                   
    ldxdw r7, [r10-0x1b8]                   
    stxdw [r10-0x210], r0                   
    jsgt r2, 1, lbb_7496                            if (r2 as i64) > (1 as i32 as i64) { pc += 53 }
    ldxdw r8, [r10-0x1e0]                   
    jne r2, 0, lbb_7549                             if r2 != (0 as i32 as i64 as u64) { pc += 104 }
    jslt r8, 0, lbb_8880                            if (r8 as i64) < (0 as i32 as i64) { pc += 1434 }
    mov64 r1, r7                                    r1 = r7
    or64 r1, r8                                     r1 |= r8   ///  r1 = r1.or(r8)
    jeq r1, 0, lbb_7641                             if r1 == (0 as i32 as i64 as u64) { pc += 192 }
    stxdw [r10-0x218], r2                   
    lddw r4, 0x5555555555555555                     r4 load str located at 6148914691236517205
    lddw r3, 0x3333333333333333                     r3 load str located at 3689348814741910323
    lddw r2, 0xf0f0f0f0f0f0f0f                      r2 load str located at 1085102592571150095
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
    jne r8, 0, lbb_7648                             if r8 != (0 as i32 as i64 as u64) { pc += 189 }
    mov64 r0, r7                                    r0 = r7
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    mov64 r5, r7                                    r5 = r7
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r4                                    r0 &= r4   ///  r0 = r0.and(r4)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    mov64 r4, r5                                    r4 = r5
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, r3                                    r5 &= r3   ///  r5 = r5.and(r3)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    mul64 r4, r1                                    r4 *= r1   ///  r4 = r4.wrapping_mul(r1)
    rsh64 r4, 56                                    r4 >>= 56   ///  r4 = r4.wrapping_shr(56)
    add64 r4, 64                                    r4 += 64   ///  r4 = r4.wrapping_add(64 as i32 as i64 as u64)
    ja lbb_7683                                     if true { pc += 187 }
lbb_7496:
    ldxdw r8, [r10-0x1e0]                   
    jne r2, 2, lbb_7552                             if r2 != (2 as i32 as i64 as u64) { pc += 54 }
    jslt r8, 0, lbb_8880                            if (r8 as i64) < (0 as i32 as i64) { pc += 1381 }
    mov64 r1, r7                                    r1 = r7
    or64 r1, r8                                     r1 |= r8   ///  r1 = r1.or(r8)
    stxdw [r10-0x218], r2                   
    jeq r1, 0, lbb_7645                             if r1 == (0 as i32 as i64 as u64) { pc += 142 }
    lddw r4, 0x5555555555555555                     r4 load str located at 6148914691236517205
    lddw r3, 0x3333333333333333                     r3 load str located at 3689348814741910323
    lddw r2, 0xf0f0f0f0f0f0f0f                      r2 load str located at 1085102592571150095
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
    jne r8, 0, lbb_7797                             if r8 != (0 as i32 as i64 as u64) { pc += 285 }
    mov64 r0, r7                                    r0 = r7
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    mov64 r5, r7                                    r5 = r7
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r4                                    r0 &= r4   ///  r0 = r0.and(r4)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    mov64 r4, r5                                    r4 = r5
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, r3                                    r5 &= r3   ///  r5 = r5.and(r3)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    mul64 r4, r1                                    r4 *= r1   ///  r4 = r4.wrapping_mul(r1)
    rsh64 r4, 56                                    r4 >>= 56   ///  r4 = r4.wrapping_shr(56)
    add64 r4, 64                                    r4 += 64   ///  r4 = r4.wrapping_add(64 as i32 as i64 as u64)
    ja lbb_7832                                     if true { pc += 283 }
lbb_7549:
    mov64 r4, r7                                    r4 = r7
    stxdw [r10-0x200], r8                   
    ja lbb_8114                                     if true { pc += 562 }
lbb_7552:
    mov64 r6, r7                                    r6 = r7
    arsh64 r6, 63                                   r6 >>= 63 (signed)   ///  r6 = (r6 as i64).wrapping_shr(63)
    mov64 r9, r7                                    r9 = r7
    mov64 r7, r8                                    r7 = r8
    arsh64 r7, 63                                   r7 >>= 63 (signed)   ///  r7 = (r7 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    mov64 r4, r9                                    r4 = r9
    mov64 r5, r6                                    r5 = r6
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r9                                    r4 = r9
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r7                                    r3 = r7
    mov64 r4, r8                                    r4 = r8
    mov64 r5, r7                                    r5 = r7
    call function_17083                     
    and64 r6, r8                                    r6 &= r8   ///  r6 = r6.and(r8)
    ldxdw r1, [r10-0x78]                    
    ldxdw r2, [r10-0x68]                    
    ldxdw r5, [r10-0x80]                    
    mov64 r3, r5                                    r3 = r5
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r3, r5, lbb_7588                            if r3 < r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_7588:
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    mov64 r0, r3                                    r0 = r3
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r6, r0                                    r6 = r0
    jlt r0, r3, lbb_7595                            if r0 < r3 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_7595:
    mov64 r3, r1                                    r3 = r1
    add64 r3, r5                                    r3 += r5   ///  r3 = r3.wrapping_add(r5)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    ldxdw r4, [r10-0x60]                    
    mov64 r5, r1                                    r5 = r1
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    mov64 r0, r5                                    r0 = r5
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r7, r0                                    r7 = r0
    jlt r0, r5, lbb_7607                            if r0 < r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_7607:
    jlt r5, r1, lbb_7609                            if r5 < r1 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_7609:
    arsh64 r1, 63                                   r1 >>= 63 (signed)   ///  r1 = (r1 as i64).wrapping_shr(63)
    ldxdw r5, [r10-0x58]                    
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    mov64 r2, r1                                    r2 = r1
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    mov64 r4, r7                                    r4 = r7
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 48                                    r3 >>= 48   ///  r3 = r3.wrapping_shr(48)
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    arsh64 r1, 48                                   r1 >>= 48 (signed)   ///  r1 = (r1 as i64).wrapping_shr(48)
    lsh64 r4, 16                                    r4 <<= 16   ///  r4 = r4.wrapping_shl(16)
    mov64 r2, r4                                    r2 = r4
    arsh64 r2, 63                                   r2 >>= 63 (signed)   ///  r2 = (r2 as i64).wrapping_shr(63)
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    jne r3, 0, lbb_8898                             if r3 != (0 as i32 as i64 as u64) { pc += 1268 }
    mov64 r2, r6                                    r2 = r6
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 48                                    r1 >>= 48   ///  r1 = r1.wrapping_shr(48)
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    stxdw [r10-0x200], r4                   
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    ldxdw r1, [r10-0x70]                    
    rsh64 r1, 48                                    r1 >>= 48   ///  r1 = r1.wrapping_shr(48)
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    stxdw [r10-0x1d8], r2                   
    ja lbb_8172                                     if true { pc += 531 }
lbb_7641:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x200], r1                   
    ja lbb_8062                                     if true { pc += 417 }
lbb_7645:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ja lbb_7955                                     if true { pc += 307 }
lbb_7648:
    mov64 r0, r8                                    r0 = r8
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    mov64 r5, r8                                    r5 = r8
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r4                                    r0 &= r4   ///  r0 = r0.and(r4)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    mov64 r4, r5                                    r4 = r5
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, r3                                    r5 &= r3   ///  r5 = r5.and(r3)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    mul64 r4, r1                                    r4 *= r1   ///  r4 = r4.wrapping_mul(r1)
    rsh64 r4, 56                                    r4 >>= 56   ///  r4 = r4.wrapping_shr(56)
lbb_7683:
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    and64 r4, 126                                   r4 &= 126   ///  r4 = r4.and(126)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -224                                  r1 += -224   ///  r1 = r1.wrapping_add(-224 as i32 as i64 as u64)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_17074                     
    ldxdw r1, [r10-0xd8]                    
    ldxdw r2, [r10-0xe0]                    
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ja lbb_7709                                     if true { pc += 14 }
lbb_7695:
    sub64 r8, r7                                    r8 -= r7   ///  r8 = r8.wrapping_sub(r7)
    add64 r9, r6                                    r9 += r6   ///  r9 = r9.wrapping_add(r6)
    ldxdw r7, [r10-0x1b8]                   
    sub64 r7, r4                                    r7 -= r4   ///  r7 = r7.wrapping_sub(r4)
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    mov64 r3, r1                                    r3 = r1
    lsh64 r3, 62                                    r3 <<= 62   ///  r3 = r3.wrapping_shl(62)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    ldxdw r4, [r10-0x1d8]                   
    mov64 r6, r4                                    r6 = r4
    mov64 r3, r9                                    r3 = r9
    jeq r0, 0, lbb_7788                             if r0 == (0 as i32 as i64 as u64) { pc += 79 }
lbb_7709:
    mov64 r0, r2                                    r0 = r2
    add64 r0, r6                                    r0 += r6   ///  r0 = r0.wrapping_add(r6)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r0, r2, lbb_7714                            if r0 < r2 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_7714:
    stxdw [r10-0x1b8], r7                   
    mov64 r4, r1                                    r4 = r1
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jge r8, r4, lbb_7758                            if r8 >= r4 { pc += 38 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ldxdw r9, [r10-0x1b8]                   
    jge r9, r0, lbb_7762                            if r9 >= r0 { pc += 39 }
lbb_7723:
    jne r8, r4, lbb_7764                            if r8 != r4 { pc += 40 }
lbb_7724:
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jeq r7, 0, lbb_7767                             if r7 == (0 as i32 as i64 as u64) { pc += 41 }
lbb_7726:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jeq r7, 0, lbb_7770                             if r7 == (0 as i32 as i64 as u64) { pc += 42 }
lbb_7728:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jeq r7, 0, lbb_7773                             if r7 == (0 as i32 as i64 as u64) { pc += 43 }
lbb_7730:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jne r7, 0, lbb_7733                             if r7 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_7732:
    mov64 r0, r2                                    r0 = r2
lbb_7733:
    stxdw [r10-0x1e0], r5                   
    rsh64 r6, 1                                     r6 >>= 1   ///  r6 = r6.wrapping_shr(1)
    mov64 r7, r3                                    r7 = r3
    lsh64 r7, 63                                    r7 <<= 63   ///  r7 = r7.wrapping_shl(63)
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    mov64 r7, r0                                    r7 = r0
    add64 r7, r6                                    r7 += r6   ///  r7 = r7.wrapping_add(r6)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    stxdw [r10-0x1d8], r7                   
    jge r7, r0, lbb_7777                            if r7 >= r0 { pc += 34 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ldxdw r5, [r10-0x1b8]                   
    jge r5, r4, lbb_7781                            if r5 >= r4 { pc += 35 }
lbb_7746:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jle r2, 3, lbb_7784                             if r2 <= (3 as i32 as i64 as u64) { pc += 36 }
lbb_7748:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_7751                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_7750:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_7751:
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    add64 r9, r3                                    r9 += r3   ///  r9 = r9.wrapping_add(r3)
    ldxdw r3, [r10-0x1e0]                   
    sub64 r8, r3                                    r8 -= r3   ///  r8 = r8.wrapping_sub(r3)
    jeq r1, 0, lbb_7695                             if r1 == (0 as i32 as i64 as u64) { pc += -61 }
    mov64 r0, r5                                    r0 = r5
    ja lbb_7695                                     if true { pc += -63 }
lbb_7758:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ldxdw r9, [r10-0x1b8]                   
    jlt r9, r0, lbb_7723                            if r9 < r0 { pc += -39 }
lbb_7762:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jeq r8, r4, lbb_7724                            if r8 == r4 { pc += -40 }
lbb_7764:
    mov64 r7, r5                                    r7 = r5
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jne r7, 0, lbb_7726                             if r7 != (0 as i32 as i64 as u64) { pc += -41 }
lbb_7767:
    mov64 r9, r1                                    r9 = r1
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jne r7, 0, lbb_7728                             if r7 != (0 as i32 as i64 as u64) { pc += -42 }
lbb_7770:
    mov64 r5, r4                                    r5 = r4
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jne r7, 0, lbb_7730                             if r7 != (0 as i32 as i64 as u64) { pc += -43 }
lbb_7773:
    mov64 r4, r0                                    r4 = r0
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r7, 0, lbb_7732                             if r7 == (0 as i32 as i64 as u64) { pc += -44 }
    ja lbb_7733                                     if true { pc += -44 }
lbb_7777:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ldxdw r5, [r10-0x1b8]                   
    jlt r5, r4, lbb_7746                            if r5 < r4 { pc += -35 }
lbb_7781:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jgt r2, 3, lbb_7748                             if r2 > (3 as i32 as i64 as u64) { pc += -36 }
lbb_7784:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_7750                             if r1 == (0 as i32 as i64 as u64) { pc += -37 }
    ja lbb_7751                                     if true { pc += -37 }
lbb_7788:
    lsh64 r9, 24                                    r9 <<= 24   ///  r9 = r9.wrapping_shl(24)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 40                                    r1 >>= 40   ///  r1 = r1.wrapping_shr(40)
    or64 r9, r1                                     r9 |= r1   ///  r9 = r9.or(r1)
    stxdw [r10-0x200], r9                   
    lsh64 r4, 24                                    r4 <<= 24   ///  r4 = r4.wrapping_shl(24)
    ldxdw r5, [r10-0x208]                   
    ldxdw r0, [r10-0x210]                   
    ja lbb_8059                                     if true { pc += 262 }
lbb_7797:
    mov64 r0, r8                                    r0 = r8
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    mov64 r5, r8                                    r5 = r8
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r4                                    r0 &= r4   ///  r0 = r0.and(r4)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    mov64 r4, r5                                    r4 = r5
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, r3                                    r5 &= r3   ///  r5 = r5.and(r3)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    mul64 r4, r1                                    r4 *= r1   ///  r4 = r4.wrapping_mul(r1)
    rsh64 r4, 56                                    r4 >>= 56   ///  r4 = r4.wrapping_shr(56)
lbb_7832:
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    and64 r4, 126                                   r4 &= 126   ///  r4 = r4.and(126)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_17074                     
    ldxdw r1, [r10-0x88]                    
    ldxdw r2, [r10-0x90]                    
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    stxdw [r10-0x1d8], r7                   
    mov64 r3, r8                                    r3 = r8
    ja lbb_7862                                     if true { pc += 16 }
lbb_7846:
    sub64 r3, r9                                    r3 -= r9   ///  r3 = r3.wrapping_sub(r9)
    ldxdw r4, [r10-0x200]                   
    add64 r6, r4                                    r6 += r4   ///  r6 = r6.wrapping_add(r4)
    ldxdw r4, [r10-0x1d8]                   
    sub64 r4, r0                                    r4 -= r0   ///  r4 = r4.wrapping_sub(r0)
    stxdw [r10-0x1d8], r4                   
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    mov64 r5, r1                                    r5 = r1
    lsh64 r5, 62                                    r5 <<= 62   ///  r5 = r5.wrapping_shl(62)
    or64 r2, r5                                     r2 |= r5   ///  r2 = r2.or(r5)
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    ldxdw r4, [r10-0x1f0]                   
    mov64 r9, r4                                    r9 = r4
    mov64 r5, r6                                    r5 = r6
    jeq r8, 0, lbb_7948                             if r8 == (0 as i32 as i64 as u64) { pc += 86 }
lbb_7862:
    mov64 r7, r2                                    r7 = r2
    add64 r7, r9                                    r7 += r9   ///  r7 = r7.wrapping_add(r9)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jlt r7, r2, lbb_7867                            if r7 < r2 { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_7867:
    mov64 r0, r1                                    r0 = r1
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    add64 r0, r6                                    r0 += r6   ///  r0 = r0.wrapping_add(r6)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jge r3, r0, lbb_7909                            if r3 >= r0 { pc += 37 }
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    ldxdw r4, [r10-0x1d8]                   
    jge r4, r7, lbb_7913                            if r4 >= r7 { pc += 38 }
lbb_7875:
    jne r3, r0, lbb_7915                            if r3 != r0 { pc += 39 }
lbb_7876:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_7918                             if r8 == (0 as i32 as i64 as u64) { pc += 40 }
lbb_7878:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_7921                             if r8 == (0 as i32 as i64 as u64) { pc += 41 }
lbb_7880:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_7924                             if r8 == (0 as i32 as i64 as u64) { pc += 42 }
lbb_7882:
    stxdw [r10-0x1f8], r4                   
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jne r8, 0, lbb_7886                             if r8 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_7885:
    mov64 r7, r2                                    r7 = r2
lbb_7886:
    rsh64 r9, 1                                     r9 >>= 1   ///  r9 = r9.wrapping_shr(1)
    mov64 r8, r5                                    r8 = r5
    lsh64 r8, 63                                    r8 <<= 63   ///  r8 = r8.wrapping_shl(63)
    or64 r9, r8                                     r9 |= r8   ///  r9 = r9.or(r8)
    mov64 r4, r7                                    r4 = r7
    add64 r4, r9                                    r4 += r9   ///  r4 = r4.wrapping_add(r9)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    stxdw [r10-0x1f0], r4                   
    jge r4, r7, lbb_7929                            if r4 >= r7 { pc += 34 }
    stxdw [r10-0x200], r8                   
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    ldxdw r4, [r10-0x1d8]                   
    jge r4, r0, lbb_7934                            if r4 >= r0 { pc += 35 }
lbb_7899:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jle r2, 3, lbb_7937                             if r2 <= (3 as i32 as i64 as u64) { pc += 36 }
lbb_7901:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ldxdw r4, [r10-0x1f8]                   
    jeq r1, 0, lbb_7941                             if r1 == (0 as i32 as i64 as u64) { pc += 37 }
lbb_7904:
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    add64 r6, r5                                    r6 += r5   ///  r6 = r6.wrapping_add(r5)
    sub64 r3, r4                                    r3 -= r4   ///  r3 = r3.wrapping_sub(r4)
    jeq r1, 0, lbb_7846                             if r1 == (0 as i32 as i64 as u64) { pc += -62 }
    ja lbb_7946                                     if true { pc += 37 }
lbb_7909:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    ldxdw r4, [r10-0x1d8]                   
    jlt r4, r7, lbb_7875                            if r4 < r7 { pc += -38 }
lbb_7913:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jeq r3, r0, lbb_7876                            if r3 == r0 { pc += -39 }
lbb_7915:
    mov64 r8, r6                                    r8 = r6
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r8, 0, lbb_7878                             if r8 != (0 as i32 as i64 as u64) { pc += -40 }
lbb_7918:
    mov64 r6, r1                                    r6 = r1
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jne r8, 0, lbb_7880                             if r8 != (0 as i32 as i64 as u64) { pc += -41 }
lbb_7921:
    mov64 r4, r0                                    r4 = r0
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jne r8, 0, lbb_7882                             if r8 != (0 as i32 as i64 as u64) { pc += -42 }
lbb_7924:
    mov64 r0, r7                                    r0 = r7
    stxdw [r10-0x1f8], r4                   
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_7885                             if r8 == (0 as i32 as i64 as u64) { pc += -43 }
    ja lbb_7886                                     if true { pc += -43 }
lbb_7929:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    stxdw [r10-0x200], r8                   
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    ldxdw r4, [r10-0x1d8]                   
    jlt r4, r0, lbb_7899                            if r4 < r0 { pc += -35 }
lbb_7934:
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jgt r2, 3, lbb_7901                             if r2 > (3 as i32 as i64 as u64) { pc += -36 }
lbb_7937:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ldxdw r4, [r10-0x1f8]                   
    jne r1, 0, lbb_7904                             if r1 != (0 as i32 as i64 as u64) { pc += -37 }
lbb_7941:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    add64 r6, r5                                    r6 += r5   ///  r6 = r6.wrapping_add(r5)
    sub64 r3, r4                                    r3 -= r4   ///  r3 = r3.wrapping_sub(r4)
    jeq r1, 0, lbb_7846                             if r1 == (0 as i32 as i64 as u64) { pc += -100 }
lbb_7946:
    mov64 r8, r7                                    r8 = r7
    ja lbb_7846                                     if true { pc += -102 }
lbb_7948:
    lsh64 r6, 24                                    r6 <<= 24   ///  r6 = r6.wrapping_shl(24)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 40                                    r1 >>= 40   ///  r1 = r1.wrapping_shr(40)
    or64 r6, r1                                     r6 |= r1   ///  r6 = r6.or(r1)
    lsh64 r4, 24                                    r4 <<= 24   ///  r4 = r4.wrapping_shl(24)
    ldxdw r8, [r10-0x1e0]                   
    ldxdw r7, [r10-0x1b8]                   
lbb_7955:
    mov64 r5, r4                                    r5 = r4
    arsh64 r5, 63                                   r5 >>= 63 (signed)   ///  r5 = (r5 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    stxdw [r10-0x1b8], r5                   
    mov64 r9, r4                                    r9 = r4
    call function_17083                     
    and64 r9, -16777216                             r9 &= -16777216   ///  r9 = r9.and(-16777216)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -192                                  r1 += -192   ///  r1 = r1.wrapping_add(-192 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r7                                    r4 = r7
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r4, r7                                    r4 = r7
    arsh64 r7, 63                                   r7 >>= 63 (signed)   ///  r7 = (r7 as i64).wrapping_shr(63)
    mov64 r9, r6                                    r9 = r6
    arsh64 r9, 63                                   r9 >>= 63 (signed)   ///  r9 = (r9 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -160                                  r1 += -160   ///  r1 = r1.wrapping_add(-160 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r9                                    r3 = r9
    mov64 r5, r7                                    r5 = r7
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r9                                    r3 = r9
    mov64 r4, r8                                    r4 = r8
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    and64 r7, r6                                    r7 &= r6   ///  r7 = r7.and(r6)
    ldxdw r1, [r10-0x98]                    
    add64 r1, r7                                    r1 += r7   ///  r1 = r1.wrapping_add(r7)
    ldxdw r3, [r10-0xb8]                    
    ldxdw r2, [r10-0xd0]                    
    mov64 r4, r2                                    r4 = r2
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxdw r0, [r10-0xa0]                    
    mov64 r6, r0                                    r6 = r0
    add64 r6, r4                                    r6 += r4   ///  r6 = r6.wrapping_add(r4)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r7, r6                                    r7 = r6
    jlt r6, r0, lbb_8004                            if r6 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_8004:
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r4, r2, lbb_8008                            if r4 < r2 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_8008:
    ldxdw r2, [r10-0x1b8]                   
    and64 r8, r2                                    r8 &= r2   ///  r8 = r8.and(r2)
    lddw r2, 0x7fffffffffffffff                     r2 load str located at 9223372036854775807
    and64 r8, r2                                    r8 &= r2   ///  r8 = r8.and(r2)
    ldxdw r2, [r10-0xc8]                    
    add64 r2, r8                                    r2 += r8   ///  r2 = r2.wrapping_add(r8)
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    ldxdw r4, [r10-0xb0]                    
    mov64 r5, r2                                    r5 = r2
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    mov64 r6, r5                                    r6 = r5
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r0, [r10-0x210]                   
    mov64 r8, r6                                    r8 = r6
    jlt r6, r5, lbb_8026                            if r6 < r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_8026:
    jlt r5, r2, lbb_8028                            if r5 < r2 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_8028:
    arsh64 r2, 63                                   r2 >>= 63 (signed)   ///  r2 = (r2 as i64).wrapping_shr(63)
    ldxdw r5, [r10-0xa8]                    
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    arsh64 r1, 63                                   r1 >>= 63 (signed)   ///  r1 = (r1 as i64).wrapping_shr(63)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    mov64 r1, r2                                    r1 = r2
    lsh64 r1, 16                                    r1 <<= 16   ///  r1 = r1.wrapping_shl(16)
    mov64 r5, r8                                    r5 = r8
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 48                                    r3 >>= 48   ///  r3 = r3.wrapping_shr(48)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    arsh64 r2, 48                                   r2 >>= 48 (signed)   ///  r2 = (r2 as i64).wrapping_shr(48)
    lsh64 r5, 16                                    r5 <<= 16   ///  r5 = r5.wrapping_shl(16)
    mov64 r1, r5                                    r1 = r5
    arsh64 r1, 63                                   r1 >>= 63 (signed)   ///  r1 = (r1 as i64).wrapping_shr(63)
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    xor64 r3, r1                                    r3 ^= r1   ///  r3 = r3.xor(r1)
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    jne r3, 0, lbb_8901                             if r3 != (0 as i32 as i64 as u64) { pc += 852 }
    mov64 r4, r7                                    r4 = r7
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 48                                    r1 >>= 48   ///  r1 = r1.wrapping_shr(48)
    or64 r5, r1                                     r5 |= r1   ///  r5 = r5.or(r1)
    stxdw [r10-0x200], r5                   
    lsh64 r4, 16                                    r4 <<= 16   ///  r4 = r4.wrapping_shl(16)
    ldxdw r1, [r10-0xc0]                    
    rsh64 r1, 48                                    r1 >>= 48   ///  r1 = r1.wrapping_shr(48)
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    ldxdw r5, [r10-0x208]                   
lbb_8059:
    ldxdw r1, [r10-0x218]                   
    jsgt r1, 1, lbb_8117                            if (r1 as i64) > (1 as i32 as i64) { pc += 56 }
    jne r1, 0, lbb_8114                             if r1 != (0 as i32 as i64 as u64) { pc += 52 }
lbb_8062:
    ldxdw r6, [r10-0x1d0]                   
    jslt r6, 0, lbb_8880                            if (r6 as i64) < (0 as i32 as i64) { pc += 816 }
    ldxdw r1, [r10-0x1c0]                   
    or64 r1, r6                                     r1 |= r6   ///  r1 = r1.or(r6)
    jeq r1, 0, lbb_8254                             if r1 == (0 as i32 as i64 as u64) { pc += 187 }
    stxdw [r10-0x1d8], r4                   
    lddw r4, 0x5555555555555555                     r4 load str located at 6148914691236517205
    lddw r3, 0x3333333333333333                     r3 load str located at 3689348814741910323
    lddw r2, 0xf0f0f0f0f0f0f0f                      r2 load str located at 1085102592571150095
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
    jne r6, 0, lbb_8257                             if r6 != (0 as i32 as i64 as u64) { pc += 180 }
    ldxdw r5, [r10-0x1c0]                   
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r4                                    r0 &= r4   ///  r0 = r0.and(r4)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    mov64 r4, r5                                    r4 = r5
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, r3                                    r5 &= r3   ///  r5 = r5.and(r3)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    mul64 r4, r1                                    r4 *= r1   ///  r4 = r4.wrapping_mul(r1)
    rsh64 r4, 56                                    r4 >>= 56   ///  r4 = r4.wrapping_shr(56)
    add64 r4, 64                                    r4 += 64   ///  r4 = r4.wrapping_add(64 as i32 as i64 as u64)
    ja lbb_8292                                     if true { pc += 178 }
lbb_8114:
    ldxdw r7, [r10-0x1c0]                   
    ldxdw r2, [r10-0x1d0]                   
    ja lbb_8662                                     if true { pc += 545 }
lbb_8117:
    stxdw [r10-0x1d8], r4                   
    jne r1, 2, lbb_8172                             if r1 != (2 as i32 as i64 as u64) { pc += 53 }
    ldxdw r2, [r10-0x1d0]                   
    ldxdw r9, [r10-0x1c0]                   
    jslt r2, 0, lbb_8880                            if (r2 as i64) < (0 as i32 as i64) { pc += 758 }
    mov64 r1, r9                                    r1 = r9
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    jeq r1, 0, lbb_8406                             if r1 == (0 as i32 as i64 as u64) { pc += 281 }
    lddw r4, 0x5555555555555555                     r4 load str located at 6148914691236517205
    lddw r3, 0x3333333333333333                     r3 load str located at 3689348814741910323
    lddw r7, 0xf0f0f0f0f0f0f0f                      r7 load str located at 1085102592571150095
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
    jne r2, 0, lbb_8409                             if r2 != (0 as i32 as i64 as u64) { pc += 275 }
    mov64 r0, r9                                    r0 = r9
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    mov64 r6, r2                                    r6 = r2
    mov64 r5, r9                                    r5 = r9
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r4                                    r0 &= r4   ///  r0 = r0.and(r4)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    mov64 r4, r5                                    r4 = r5
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, r3                                    r5 &= r3   ///  r5 = r5.and(r3)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    and64 r4, r7                                    r4 &= r7   ///  r4 = r4.and(r7)
    mul64 r4, r1                                    r4 *= r1   ///  r4 = r4.wrapping_mul(r1)
    rsh64 r4, 56                                    r4 >>= 56   ///  r4 = r4.wrapping_shr(56)
    add64 r4, 64                                    r4 += 64   ///  r4 = r4.wrapping_add(64 as i32 as i64 as u64)
    ja lbb_8445                                     if true { pc += 273 }
lbb_8172:
    ldxdw r9, [r10-0x1d0]                   
    ldxdw r8, [r10-0x1c0]                   
    mov64 r6, r8                                    r6 = r8
    arsh64 r6, 63                                   r6 >>= 63 (signed)   ///  r6 = (r6 as i64).wrapping_shr(63)
    mov64 r7, r9                                    r7 = r9
    arsh64 r7, 63                                   r7 >>= 63 (signed)   ///  r7 = (r7 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -272                                  r1 += -272   ///  r1 = r1.wrapping_add(-272 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, r7                                    r3 = r7
    mov64 r4, r8                                    r4 = r8
    mov64 r5, r6                                    r5 = r6
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -256                                  r1 += -256   ///  r1 = r1.wrapping_add(-256 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r8                                    r4 = r8
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -240                                  r1 += -240   ///  r1 = r1.wrapping_add(-240 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, r7                                    r3 = r7
    mov64 r4, r9                                    r4 = r9
    mov64 r5, r7                                    r5 = r7
    call function_17083                     
    and64 r6, r9                                    r6 &= r9   ///  r6 = r6.and(r9)
    ldxdw r3, [r10-0x108]                   
    ldxdw r2, [r10-0xf8]                    
    ldxdw r1, [r10-0x110]                   
    mov64 r5, r1                                    r5 = r1
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r5, r1, lbb_8209                            if r5 < r1 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_8209:
    add64 r3, r6                                    r3 += r6   ///  r3 = r3.wrapping_add(r6)
    mov64 r7, r5                                    r7 = r5
    add64 r7, r1                                    r7 += r1   ///  r7 = r7.wrapping_add(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r7, r5, lbb_8215                            if r7 < r5 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_8215:
    mov64 r5, r3                                    r5 = r3
    add64 r5, r1                                    r5 += r1   ///  r5 = r5.wrapping_add(r1)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    ldxdw r2, [r10-0xf0]                    
    mov64 r1, r3                                    r1 = r3
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, r1                                    r2 = r1
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r2, r1, lbb_8226                            if r2 < r1 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_8226:
    jlt r1, r3, lbb_8228                            if r1 < r3 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_8228:
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    ldxdw r1, [r10-0xe8]                    
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    arsh64 r5, 63                                   r5 >>= 63 (signed)   ///  r5 = (r5 as i64).wrapping_shr(63)
    add64 r3, r5                                    r3 += r5   ///  r3 = r3.wrapping_add(r5)
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
    mov64 r1, r3                                    r1 = r3
    lsh64 r1, 16                                    r1 <<= 16   ///  r1 = r1.wrapping_shl(16)
    mov64 r4, r2                                    r4 = r2
    rsh64 r4, 48                                    r4 >>= 48   ///  r4 = r4.wrapping_shr(48)
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    arsh64 r3, 48                                   r3 >>= 48 (signed)   ///  r3 = (r3 as i64).wrapping_shr(48)
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    mov64 r1, r2                                    r1 = r2
    arsh64 r1, 63                                   r1 >>= 63 (signed)   ///  r1 = (r1 as i64).wrapping_shr(63)
    xor64 r3, r1                                    r3 ^= r1   ///  r3 = r3.xor(r1)
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    jne r4, 0, lbb_8898                             if r4 != (0 as i32 as i64 as u64) { pc += 650 }
    mov64 r1, r7                                    r1 = r7
    rsh64 r1, 48                                    r1 >>= 48   ///  r1 = r1.wrapping_shr(48)
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    lsh64 r7, 16                                    r7 <<= 16   ///  r7 = r7.wrapping_shl(16)
    ldxdw r1, [r10-0x100]                   
    ja lbb_8657                                     if true { pc += 403 }
lbb_8254:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ja lbb_8662                                     if true { pc += 405 }
lbb_8257:
    mov64 r0, r6                                    r0 = r6
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    mov64 r5, r6                                    r5 = r6
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r4                                    r0 &= r4   ///  r0 = r0.and(r4)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    mov64 r4, r5                                    r4 = r5
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, r3                                    r5 &= r3   ///  r5 = r5.and(r3)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    mul64 r4, r1                                    r4 *= r1   ///  r4 = r4.wrapping_mul(r1)
    rsh64 r4, 56                                    r4 >>= 56   ///  r4 = r4.wrapping_shr(56)
lbb_8292:
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    and64 r4, 126                                   r4 &= 126   ///  r4 = r4.and(126)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -368                                  r1 += -368   ///  r1 = r1.wrapping_add(-368 as i32 as i64 as u64)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_17074                     
    ldxdw r3, [r10-0x168]                   
    ldxdw r4, [r10-0x170]                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ja lbb_8320                                     if true { pc += 16 }
lbb_8304:
    sub64 r5, r9                                    r5 -= r9   ///  r5 = r5.wrapping_sub(r9)
    stxdw [r10-0x1d0], r5                   
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    ldxdw r1, [r10-0x1c0]                   
    sub64 r1, r0                                    r1 -= r0   ///  r1 = r1.wrapping_sub(r0)
    stxdw [r10-0x1c0], r1                   
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    mov64 r1, r3                                    r1 = r3
    lsh64 r1, 62                                    r1 <<= 62   ///  r1 = r1.wrapping_shl(62)
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    ldxdw r7, [r10-0x1b8]                   
    mov64 r6, r7                                    r6 = r7
    mov64 r5, r2                                    r5 = r2
    jeq r8, 0, lbb_8400                             if r8 == (0 as i32 as i64 as u64) { pc += 80 }
lbb_8320:
    mov64 r1, r4                                    r1 = r4
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r4, lbb_8325                            if r1 < r4 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_8325:
    mov64 r0, r3                                    r0 = r3
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r7, [r10-0x1d0]                   
    jge r7, r0, lbb_8369                            if r7 >= r0 { pc += 38 }
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    ldxdw r7, [r10-0x1c0]                   
    jge r7, r1, lbb_8373                            if r7 >= r1 { pc += 39 }
lbb_8334:
    ldxdw r7, [r10-0x1d0]                   
    jne r7, r0, lbb_8376                            if r7 != r0 { pc += 40 }
lbb_8336:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_8379                             if r8 == (0 as i32 as i64 as u64) { pc += 41 }
lbb_8338:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_8382                             if r8 == (0 as i32 as i64 as u64) { pc += 42 }
lbb_8340:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_8385                             if r8 == (0 as i32 as i64 as u64) { pc += 43 }
lbb_8342:
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jne r8, 0, lbb_8345                             if r8 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_8344:
    mov64 r9, r4                                    r9 = r4
lbb_8345:
    rsh64 r6, 1                                     r6 >>= 1   ///  r6 = r6.wrapping_shr(1)
    mov64 r1, r5                                    r1 = r5
    lsh64 r1, 63                                    r1 <<= 63   ///  r1 = r1.wrapping_shl(63)
    or64 r6, r1                                     r6 |= r1   ///  r6 = r6.or(r1)
    mov64 r1, r9                                    r1 = r9
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    stxdw [r10-0x1b8], r1                   
    jge r1, r9, lbb_8389                            if r1 >= r9 { pc += 35 }
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    ldxdw r1, [r10-0x1c0]                   
    jge r1, r0, lbb_8393                            if r1 >= r0 { pc += 36 }
lbb_8357:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jle r4, 3, lbb_8396                             if r4 <= (3 as i32 as i64 as u64) { pc += 37 }
lbb_8359:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r3, 0, lbb_8362                             if r3 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_8361:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_8362:
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    ldxdw r5, [r10-0x1d0]                   
    sub64 r5, r7                                    r5 -= r7   ///  r5 = r5.wrapping_sub(r7)
    jeq r3, 0, lbb_8304                             if r3 == (0 as i32 as i64 as u64) { pc += -63 }
    mov64 r8, r1                                    r8 = r1
    ja lbb_8304                                     if true { pc += -65 }
lbb_8369:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    ldxdw r7, [r10-0x1c0]                   
    jlt r7, r1, lbb_8334                            if r7 < r1 { pc += -39 }
lbb_8373:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r7, [r10-0x1d0]                   
    jeq r7, r0, lbb_8336                            if r7 == r0 { pc += -40 }
lbb_8376:
    mov64 r8, r2                                    r8 = r2
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jne r8, 0, lbb_8338                             if r8 != (0 as i32 as i64 as u64) { pc += -41 }
lbb_8379:
    mov64 r2, r3                                    r2 = r3
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jne r8, 0, lbb_8340                             if r8 != (0 as i32 as i64 as u64) { pc += -42 }
lbb_8382:
    mov64 r7, r0                                    r7 = r0
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jne r8, 0, lbb_8342                             if r8 != (0 as i32 as i64 as u64) { pc += -43 }
lbb_8385:
    mov64 r0, r1                                    r0 = r1
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_8344                             if r8 == (0 as i32 as i64 as u64) { pc += -44 }
    ja lbb_8345                                     if true { pc += -44 }
lbb_8389:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    ldxdw r1, [r10-0x1c0]                   
    jlt r1, r0, lbb_8357                            if r1 < r0 { pc += -36 }
lbb_8393:
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jgt r4, 3, lbb_8359                             if r4 > (3 as i32 as i64 as u64) { pc += -37 }
lbb_8396:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r3, 0, lbb_8361                             if r3 == (0 as i32 as i64 as u64) { pc += -38 }
    ja lbb_8362                                     if true { pc += -38 }
lbb_8400:
    lsh64 r2, 24                                    r2 <<= 24   ///  r2 = r2.wrapping_shl(24)
    mov64 r1, r7                                    r1 = r7
    rsh64 r1, 40                                    r1 >>= 40   ///  r1 = r1.wrapping_shr(40)
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    lsh64 r7, 24                                    r7 <<= 24   ///  r7 = r7.wrapping_shl(24)
    ja lbb_8659                                     if true { pc += 253 }
lbb_8406:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ja lbb_8560                                     if true { pc += 151 }
lbb_8409:
    mov64 r0, r2                                    r0 = r2
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    mov64 r6, r2                                    r6 = r2
    mov64 r5, r6                                    r5 = r6
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r4                                    r0 &= r4   ///  r0 = r0.and(r4)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    mov64 r4, r5                                    r4 = r5
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, r3                                    r5 &= r3   ///  r5 = r5.and(r3)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    and64 r4, r7                                    r4 &= r7   ///  r4 = r4.and(r7)
    mul64 r4, r1                                    r4 *= r1   ///  r4 = r4.wrapping_mul(r1)
    rsh64 r4, 56                                    r4 >>= 56   ///  r4 = r4.wrapping_shr(56)
lbb_8445:
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    and64 r4, 126                                   r4 &= 126   ///  r4 = r4.and(126)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -288                                  r1 += -288   ///  r1 = r1.wrapping_add(-288 as i32 as i64 as u64)
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_17074                     
    ldxdw r1, [r10-0x118]                   
    ldxdw r2, [r10-0x120]                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r3, r9                                    r3 = r9
    mov64 r4, r6                                    r4 = r6
    ja lbb_8473                                     if true { pc += 14 }
lbb_8459:
    sub64 r4, r8                                    r4 -= r8   ///  r4 = r4.wrapping_sub(r8)
    ldxdw r5, [r10-0x1f8]                   
    add64 r6, r5                                    r6 += r5   ///  r6 = r6.wrapping_add(r5)
    sub64 r3, r0                                    r3 -= r0   ///  r3 = r3.wrapping_sub(r0)
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    mov64 r5, r1                                    r5 = r1
    lsh64 r5, 62                                    r5 <<= 62   ///  r5 = r5.wrapping_shl(62)
    or64 r2, r5                                     r2 |= r5   ///  r2 = r2.or(r5)
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    and64 r9, 1                                     r9 &= 1   ///  r9 = r9.and(1)
    ldxdw r7, [r10-0x1e0]                   
    mov64 r8, r7                                    r8 = r7
    mov64 r5, r6                                    r5 = r6
    jeq r9, 0, lbb_8554                             if r9 == (0 as i32 as i64 as u64) { pc += 81 }
lbb_8473:
    mov64 r7, r2                                    r7 = r2
    add64 r7, r8                                    r7 += r8   ///  r7 = r7.wrapping_add(r8)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jlt r7, r2, lbb_8478                            if r7 < r2 { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_8478:
    mov64 r0, r1                                    r0 = r1
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    add64 r0, r6                                    r0 += r6   ///  r0 = r0.wrapping_add(r6)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jge r4, r0, lbb_8522                            if r4 >= r0 { pc += 39 }
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jge r3, r7, lbb_8525                            if r3 >= r7 { pc += 40 }
lbb_8485:
    jne r4, r0, lbb_8527                            if r4 != r0 { pc += 41 }
lbb_8486:
    stxdw [r10-0x1f0], r3                   
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jeq r9, 0, lbb_8531                             if r9 == (0 as i32 as i64 as u64) { pc += 42 }
lbb_8489:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    stxdw [r10-0x1b8], r3                   
    jeq r9, 0, lbb_8535                             if r9 == (0 as i32 as i64 as u64) { pc += 43 }
lbb_8492:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r3, [r10-0x1f0]                   
    jeq r9, 0, lbb_8539                             if r9 == (0 as i32 as i64 as u64) { pc += 44 }
lbb_8495:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jne r9, 0, lbb_8498                             if r9 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_8497:
    mov64 r7, r2                                    r7 = r2
lbb_8498:
    rsh64 r8, 1                                     r8 >>= 1   ///  r8 = r8.wrapping_shr(1)
    mov64 r9, r5                                    r9 = r5
    lsh64 r9, 63                                    r9 <<= 63   ///  r9 = r9.wrapping_shl(63)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    mov64 r9, r7                                    r9 = r7
    add64 r9, r8                                    r9 += r8   ///  r9 = r9.wrapping_add(r8)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    stxdw [r10-0x1e0], r9                   
    jge r9, r7, lbb_8543                            if r9 >= r7 { pc += 36 }
    stxdw [r10-0x1f8], r8                   
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jge r3, r0, lbb_8547                            if r3 >= r0 { pc += 37 }
lbb_8510:
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jle r2, 3, lbb_8550                             if r2 <= (3 as i32 as i64 as u64) { pc += 38 }
lbb_8512:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_8515                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_8514:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_8515:
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    add64 r6, r5                                    r6 += r5   ///  r6 = r6.wrapping_add(r5)
    ldxdw r5, [r10-0x1b8]                   
    sub64 r4, r5                                    r4 -= r5   ///  r4 = r4.wrapping_sub(r5)
    jeq r1, 0, lbb_8459                             if r1 == (0 as i32 as i64 as u64) { pc += -61 }
    mov64 r9, r7                                    r9 = r7
    ja lbb_8459                                     if true { pc += -63 }
lbb_8522:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jlt r3, r7, lbb_8485                            if r3 < r7 { pc += -40 }
lbb_8525:
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jeq r4, r0, lbb_8486                            if r4 == r0 { pc += -41 }
lbb_8527:
    mov64 r9, r6                                    r9 = r6
    stxdw [r10-0x1f0], r3                   
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jne r9, 0, lbb_8489                             if r9 != (0 as i32 as i64 as u64) { pc += -42 }
lbb_8531:
    mov64 r6, r1                                    r6 = r1
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    stxdw [r10-0x1b8], r3                   
    jne r9, 0, lbb_8492                             if r9 != (0 as i32 as i64 as u64) { pc += -43 }
lbb_8535:
    stxdw [r10-0x1b8], r0                   
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r3, [r10-0x1f0]                   
    jne r9, 0, lbb_8495                             if r9 != (0 as i32 as i64 as u64) { pc += -44 }
lbb_8539:
    mov64 r0, r7                                    r0 = r7
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jeq r9, 0, lbb_8497                             if r9 == (0 as i32 as i64 as u64) { pc += -45 }
    ja lbb_8498                                     if true { pc += -45 }
lbb_8543:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    stxdw [r10-0x1f8], r8                   
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jlt r3, r0, lbb_8510                            if r3 < r0 { pc += -37 }
lbb_8547:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jgt r2, 3, lbb_8512                             if r2 > (3 as i32 as i64 as u64) { pc += -38 }
lbb_8550:
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_8514                             if r1 == (0 as i32 as i64 as u64) { pc += -39 }
    ja lbb_8515                                     if true { pc += -39 }
lbb_8554:
    lsh64 r6, 24                                    r6 <<= 24   ///  r6 = r6.wrapping_shl(24)
    mov64 r1, r7                                    r1 = r7
    rsh64 r1, 40                                    r1 >>= 40   ///  r1 = r1.wrapping_shr(40)
    or64 r6, r1                                     r6 |= r1   ///  r6 = r6.or(r1)
    lsh64 r7, 24                                    r7 <<= 24   ///  r7 = r7.wrapping_shl(24)
    ldxdw r2, [r10-0x1d0]                   
lbb_8560:
    mov64 r5, r7                                    r5 = r7
    arsh64 r5, 63                                   r5 >>= 63 (signed)   ///  r5 = (r5 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -352                                  r1 += -352   ///  r1 = r1.wrapping_add(-352 as i32 as i64 as u64)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r7                                    r4 = r7
    stxdw [r10-0x1b8], r5                   
    call function_17083                     
    and64 r7, -16777216                             r7 &= -16777216   ///  r7 = r7.and(-16777216)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -336                                  r1 += -336   ///  r1 = r1.wrapping_add(-336 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r9, [r10-0x1c0]                   
    mov64 r4, r9                                    r4 = r9
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r7, r9                                    r7 = r9
    arsh64 r7, 63                                   r7 >>= 63 (signed)   ///  r7 = (r7 as i64).wrapping_shr(63)
    mov64 r8, r6                                    r8 = r6
    arsh64 r8, 63                                   r8 >>= 63 (signed)   ///  r8 = (r8 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -304                                  r1 += -304   ///  r1 = r1.wrapping_add(-304 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r8                                    r3 = r8
    mov64 r4, r9                                    r4 = r9
    mov64 r5, r7                                    r5 = r7
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -320                                  r1 += -320   ///  r1 = r1.wrapping_add(-320 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r8                                    r3 = r8
    ldxdw r4, [r10-0x1d0]                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    and64 r7, r6                                    r7 &= r6   ///  r7 = r7.and(r6)
    ldxdw r3, [r10-0x128]                   
    add64 r3, r7                                    r3 += r7   ///  r3 = r3.wrapping_add(r7)
    ldxdw r1, [r10-0x148]                   
    ldxdw r2, [r10-0x160]                   
    mov64 r4, r2                                    r4 = r2
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    ldxdw r0, [r10-0x130]                   
    mov64 r7, r0                                    r7 = r0
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r7, r0, lbb_8609                            if r7 < r0 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_8609:
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r4, r2, lbb_8613                            if r4 < r2 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_8613:
    ldxdw r0, [r10-0x1d0]                   
    ldxdw r2, [r10-0x1b8]                   
    and64 r0, r2                                    r0 &= r2   ///  r0 = r0.and(r2)
    lddw r2, 0x7fffffffffffffff                     r2 load str located at 9223372036854775807
    and64 r0, r2                                    r0 &= r2   ///  r0 = r0.and(r2)
    ldxdw r4, [r10-0x158]                   
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    ldxdw r2, [r10-0x140]                   
    mov64 r1, r4                                    r1 = r4
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, r1                                    r2 = r1
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r2, r1, lbb_8630                            if r2 < r1 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_8630:
    jlt r1, r4, lbb_8632                            if r1 < r4 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_8632:
    arsh64 r4, 63                                   r4 >>= 63 (signed)   ///  r4 = (r4 as i64).wrapping_shr(63)
    ldxdw r1, [r10-0x138]                   
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    mov64 r1, r4                                    r1 = r4
    lsh64 r1, 16                                    r1 <<= 16   ///  r1 = r1.wrapping_shl(16)
    mov64 r3, r2                                    r3 = r2
    rsh64 r3, 48                                    r3 >>= 48   ///  r3 = r3.wrapping_shr(48)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    arsh64 r4, 48                                   r4 >>= 48 (signed)   ///  r4 = (r4 as i64).wrapping_shr(48)
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    mov64 r1, r2                                    r1 = r2
    arsh64 r1, 63                                   r1 >>= 63 (signed)   ///  r1 = (r1 as i64).wrapping_shr(63)
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    xor64 r3, r1                                    r3 ^= r1   ///  r3 = r3.xor(r1)
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    jne r3, 0, lbb_8901                             if r3 != (0 as i32 as i64 as u64) { pc += 249 }
    mov64 r1, r7                                    r1 = r7
    rsh64 r1, 48                                    r1 >>= 48   ///  r1 = r1.wrapping_shr(48)
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    lsh64 r7, 16                                    r7 <<= 16   ///  r7 = r7.wrapping_shl(16)
    ldxdw r1, [r10-0x150]                   
lbb_8657:
    rsh64 r1, 48                                    r1 >>= 48   ///  r1 = r1.wrapping_shr(48)
    or64 r7, r1                                     r7 |= r1   ///  r7 = r7.or(r1)
lbb_8659:
    ldxdw r5, [r10-0x208]                   
    ldxdw r0, [r10-0x210]                   
    ldxdw r4, [r10-0x1d8]                   
lbb_8662:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r7, r4, lbb_8665                            if r7 < r4 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_8665:
    mov64 r8, r2                                    r8 = r2
    ldxdw r3, [r10-0x200]                   
    sub64 r8, r3                                    r8 -= r3   ///  r8 = r8.wrapping_sub(r3)
    sub64 r8, r1                                    r8 -= r1   ///  r8 = r8.wrapping_sub(r1)
    mov64 r1, r2                                    r1 = r2
    xor64 r1, r3                                    r1 ^= r3   ///  r1 = r1.xor(r3)
    xor64 r2, r8                                    r2 ^= r8   ///  r2 = r2.xor(r8)
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    mov64 r6, 193                                   r6 = 193 as i32 as i64 as u64
    jslt r1, 0, lbb_8710                            if (r1 as i64) < (0 as i32 as i64) { pc += 35 }
    mov64 r1, r0                                    r1 = r0
    or64 r1, r5                                     r1 |= r5   ///  r1 = r1.or(r5)
    mov64 r6, 196                                   r6 = 196 as i32 as i64 as u64
    jeq r1, 0, lbb_8710                             if r1 == (0 as i32 as i64 as u64) { pc += 31 }
    sub64 r7, r4                                    r7 -= r4   ///  r7 = r7.wrapping_sub(r4)
    mov64 r3, r7                                    r3 = r7
    neg64 r3                                        r3 = -r3   ///  r3 = (r3 as i64).wrapping_neg() as u64
    jsge r8, 0, lbb_8714                            if (r8 as i64) >= (0 as i32 as i64) { pc += 31 }
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jeq r7, 0, lbb_8718                             if r7 == (0 as i32 as i64 as u64) { pc += 32 }
lbb_8686:
    mov64 r1, r8                                    r1 = r8
    jslt r8, 0, lbb_8721                            if (r8 as i64) < (0 as i32 as i64) { pc += 33 }
lbb_8688:
    stxdw [r10-0x20], r3                    
    stxdw [r10-0x18], r1                    
    jeq r0, 0, lbb_8726                             if r0 == (0 as i32 as i64 as u64) { pc += 35 }
lbb_8691:
    mov64 r1, r0                                    r1 = r0
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    jsge r5, 0, lbb_8730                            if (r5 as i64) >= (0 as i32 as i64) { pc += 36 }
lbb_8694:
    mov64 r3, r5                                    r3 = r5
    jsge r5, 0, lbb_8698                            if (r5 as i64) >= (0 as i32 as i64) { pc += 2 }
lbb_8696:
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    neg64 r3                                        r3 = -r3   ///  r3 = (r3 as i64).wrapping_neg() as u64
lbb_8698:
    stxdw [r10-0x8], r3                     
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -32                                   r2 += -32   ///  r2 = r2.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -16                                   r3 += -16   ///  r3 = r3.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r4, 48                                    r4 = 48 as i32 as i64 as u64
    call function_610                       
    ldxb r1, [r10-0x40]                     
    jeq r1, 0, lbb_8734                             if r1 == (0 as i32 as i64 as u64) { pc += 24 }
lbb_8710:
    ldxdw r8, [r10-0x1c8]                   
    stxw [r8+0x8], r6                       
    stw [r8+0x4], 0                         
    ja lbb_7388                                     if true { pc += -1326 }
lbb_8714:
    mov64 r3, r7                                    r3 = r7
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r7, 0, lbb_8686                             if r7 != (0 as i32 as i64 as u64) { pc += -32 }
lbb_8718:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r1, r8                                    r1 = r8
    jsge r8, 0, lbb_8688                            if (r8 as i64) >= (0 as i32 as i64) { pc += -33 }
lbb_8721:
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    stxdw [r10-0x20], r3                    
    stxdw [r10-0x18], r1                    
    jne r0, 0, lbb_8691                             if r0 != (0 as i32 as i64 as u64) { pc += -35 }
lbb_8726:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    mov64 r1, r0                                    r1 = r0
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    jslt r5, 0, lbb_8694                            if (r5 as i64) < (0 as i32 as i64) { pc += -36 }
lbb_8730:
    mov64 r1, r0                                    r1 = r0
    mov64 r3, r5                                    r3 = r5
    jslt r5, 0, lbb_8696                            if (r5 as i64) < (0 as i32 as i64) { pc += -37 }
    ja lbb_8698                                     if true { pc += -36 }
lbb_8734:
    ldxdw r4, [r10-0x48]                    
    ldxdw r7, [r10-0x50]                    
    ldxdw r1, [r10-0x208]                   
    xor64 r8, r1                                    r8 ^= r1   ///  r8 = r8.xor(r1)
    jslt r8, 0, lbb_8741                            if (r8 as i64) < (0 as i32 as i64) { pc += 2 }
    jsge r4, 0, lbb_8770                            if (r4 as i64) >= (0 as i32 as i64) { pc += 30 }
    ja lbb_8710                                     if true { pc += -31 }
lbb_8741:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r7, 0, lbb_8751                             if r7 == (0 as i32 as i64 as u64) { pc += 7 }
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    jle r4, r3, lbb_8755                            if r4 <= r3 { pc += 8 }
lbb_8747:
    jne r4, r3, lbb_8757                            if r4 != r3 { pc += 9 }
lbb_8748:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_8710                             if r1 != (0 as i32 as i64 as u64) { pc += -40 }
    ja lbb_8760                                     if true { pc += 9 }
lbb_8751:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    lddw r3, 0x8000000000000000                     r3 load str located at -9223372036854775808
    jgt r4, r3, lbb_8747                            if r4 > r3 { pc += -8 }
lbb_8755:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jeq r4, r3, lbb_8748                            if r4 == r3 { pc += -9 }
lbb_8757:
    mov64 r1, r2                                    r1 = r2
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_8710                             if r1 != (0 as i32 as i64 as u64) { pc += -50 }
lbb_8760:
    neg64 r7                                        r7 = -r7   ///  r7 = (r7 as i64).wrapping_neg() as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jgt r7, 0, lbb_8764                             if r7 > (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_8764:
    mov64 r2, r4                                    r2 = r4
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    jne r1, 0, lbb_8769                             if r1 != (0 as i32 as i64 as u64) { pc += 2 }
    neg64 r4                                        r4 = -r4   ///  r4 = (r4 as i64).wrapping_neg() as u64
    mov64 r2, r4                                    r2 = r4
lbb_8769:
    mov64 r4, r2                                    r4 = r2
lbb_8770:
    mov64 r5, r7                                    r5 = r7
    arsh64 r5, 63                                   r5 >>= 63 (signed)   ///  r5 = (r5 as i64).wrapping_shr(63)
    ldxdw r6, [r10-0x1e8]                   
    ldxdw r2, [r6+0x8]                      
    stxdw [r10-0x1b8], r2                   
    mov64 r9, r2                                    r9 = r2
    arsh64 r9, 63                                   r9 >>= 63 (signed)   ///  r9 = (r9 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -432                                  r1 += -432   ///  r1 = r1.wrapping_add(-432 as i32 as i64 as u64)
    mov64 r3, r9                                    r3 = r9
    mov64 r8, r4                                    r8 = r4
    mov64 r4, r7                                    r4 = r7
    stxdw [r10-0x1c0], r5                   
    call function_17083                     
    ldxdw r6, [r6+0x0]                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -416                                  r1 += -416   ///  r1 = r1.wrapping_add(-416 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r7                                    r4 = r7
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r2, r8                                    r2 = r8
    stxdw [r10-0x1d0], r2                   
    arsh64 r8, 63                                   r8 >>= 63 (signed)   ///  r8 = (r8 as i64).wrapping_shr(63)
    mov64 r7, r6                                    r7 = r6
    arsh64 r7, 63                                   r7 >>= 63 (signed)   ///  r7 = (r7 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -384                                  r1 += -384   ///  r1 = r1.wrapping_add(-384 as i32 as i64 as u64)
    mov64 r3, r8                                    r3 = r8
    mov64 r4, r6                                    r4 = r6
    mov64 r5, r7                                    r5 = r7
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -400                                  r1 += -400   ///  r1 = r1.wrapping_add(-400 as i32 as i64 as u64)
    ldxdw r2, [r10-0x1b8]                   
    mov64 r3, r9                                    r3 = r9
    ldxdw r6, [r10-0x1d0]                   
    mov64 r4, r6                                    r4 = r6
    mov64 r5, r8                                    r5 = r8
    call function_17083                     
    and64 r7, r6                                    r7 &= r6   ///  r7 = r7.and(r6)
    ldxdw r3, [r10-0x178]                   
    add64 r3, r7                                    r3 += r7   ///  r3 = r3.wrapping_add(r7)
    ldxdw r1, [r10-0x198]                   
    ldxdw r2, [r10-0x1b0]                   
    mov64 r4, r2                                    r4 = r2
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    ldxdw r6, [r10-0x180]                   
    mov64 r1, r6                                    r1 = r6
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r1, r6, lbb_8825                            if r1 < r6 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_8825:
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r4, r2, lbb_8829                            if r4 < r2 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_8829:
    ldxdw r2, [r10-0x1c0]                   
    ldxdw r4, [r10-0x1b8]                   
    and64 r2, r4                                    r2 &= r4   ///  r2 = r2.and(r4)
    ldxdw r4, [r10-0x1a8]                   
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    ldxdw r2, [r10-0x190]                   
    mov64 r6, r4                                    r6 = r4
    add64 r6, r2                                    r6 += r2   ///  r6 = r6.wrapping_add(r2)
    mov64 r2, r6                                    r2 = r6
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ldxdw r8, [r10-0x1c8]                   
    jlt r2, r6, lbb_8844                            if r2 < r6 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_8844:
    jlt r6, r4, lbb_8846                            if r6 < r4 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_8846:
    arsh64 r4, 63                                   r4 >>= 63 (signed)   ///  r4 = (r4 as i64).wrapping_shr(63)
    ldxdw r6, [r10-0x188]                   
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    mov64 r3, r4                                    r3 = r4
    lsh64 r3, 16                                    r3 <<= 16   ///  r3 = r3.wrapping_shl(16)
    mov64 r5, r2                                    r5 = r2
    rsh64 r5, 48                                    r5 >>= 48   ///  r5 = r5.wrapping_shr(48)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    arsh64 r4, 48                                   r4 >>= 48 (signed)   ///  r4 = (r4 as i64).wrapping_shr(48)
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    mov64 r3, r2                                    r3 = r2
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    xor64 r5, r3                                    r5 ^= r3   ///  r5 = r5.xor(r3)
    or64 r5, r4                                     r5 |= r4   ///  r5 = r5.or(r4)
    jne r5, 0, lbb_8877                             if r5 != (0 as i32 as i64 as u64) { pc += 11 }
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 48                                    r3 >>= 48   ///  r3 = r3.wrapping_shr(48)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    lsh64 r1, 16                                    r1 <<= 16   ///  r1 = r1.wrapping_shl(16)
    ldxdw r3, [r10-0x1a0]                   
    rsh64 r3, 48                                    r3 >>= 48   ///  r3 = r3.wrapping_shr(48)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    stxdw [r8+0x8], r1                      
    stxdw [r8+0x10], r2                     
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_7389                                     if true { pc += -1488 }
lbb_8877:
    lddw r1, 0xc800000000                           r1 load str located at 858993459200
    ja lbb_7387                                     if true { pc += -1493 }
lbb_8880:
    lddw r1, 0x100022eb3 --> b"fixed point square root of a negative number"        r1 load str located at 4295110323
    mov64 r2, 44                                    r2 = 44 as i32 as i64 as u64
    lddw r3, 0x100023958 --> b"\x00\x00\x00\x00x-\x02\x00\x0a\x00\x00\x00\x00\x00\x00\x00\xc1\x00\x00\x0…        r3 load str located at 4295113048
    call function_15481                     
lbb_8886:
    lddw r1, 0x100023a10 --> b"\x00\x00\x00\x00\xa96\x02\x00\x11\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295113232
    stxdw [r10-0x50], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 1                      
    stdw [r10-0x38], 0                      
    stdw [r10-0x40], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    lddw r2, 0x100023a20 --> b"\x00\x00\x00\x00\xba6\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\x1c\x00\x00…        r2 load str located at 4295113248
    call function_15475                     
lbb_8898:
    lddw r1, 0x100023a50 --> b"\x00\x00\x00\x00\xba6\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00'\x00\x00\x0…        r1 load str located at 4295113296
    call function_15470                     
lbb_8901:
    lddw r1, 0x100023a38 --> b"\x00\x00\x00\x00\xba6\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00&\x00\x00\x0…        r1 load str located at 4295113272
    call function_15470                     

function_8904:
    mov64 r9, r5                                    r9 = r5
    mov64 r8, r1                                    r8 = r1
    mov64 r1, r2                                    r1 = r2
    lsh64 r1, 48                                    r1 <<= 48   ///  r1 = r1.wrapping_shl(48)
    stxdw [r10-0x40], r1                    
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 16                                    r1 >>= 16   ///  r1 = r1.wrapping_shr(16)
    arsh64 r2, 63                                   r2 >>= 63 (signed)   ///  r2 = (r2 as i64).wrapping_shr(63)
    lsh64 r2, 48                                    r2 <<= 48   ///  r2 = r2.wrapping_shl(48)
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    stxdw [r10-0x38], r2                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    call function_752                       
    ldxdw r1, [r10-0x30]                    
    jeq r1, 0, lbb_8941                             if r1 == (0 as i32 as i64 as u64) { pc += 19 }
    ldxdw r2, [r10-0x20]                    
    ldxdw r4, [r10-0x28]                    
    ldxdw r1, [r9+0x10]                     
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    stxdw [r10-0x168], r4                   
    jlt r4, r1, lbb_8929                            if r4 < r1 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_8929:
    ldxdw r4, [r9+0x18]                     
    mov64 r6, r2                                    r6 = r2
    sub64 r6, r4                                    r6 -= r4   ///  r6 = r6.wrapping_sub(r4)
    sub64 r6, r3                                    r6 -= r3   ///  r6 = r6.wrapping_sub(r3)
    mov64 r3, r2                                    r3 = r2
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    xor64 r2, r6                                    r2 ^= r6   ///  r2 = r2.xor(r6)
    and64 r3, r2                                    r3 &= r2   ///  r3 = r3.and(r2)
    jsge r3, 0, lbb_8947                            if (r3 as i64) >= (0 as i32 as i64) { pc += 9 }
    lddw r1, 0xd800000000                           r1 load str located at 927712935936
    ja lbb_8943                                     if true { pc += 2 }
lbb_8941:
    lddw r1, 0x5900000000                           r1 load str located at 382252089344
lbb_8943:
    stxdw [r8+0x4], r1                      
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_8945:
    stxw [r8+0x0], r1                       
    exit                                    
lbb_8947:
    jslt r6, 0, lbb_9011                            if (r6 as i64) < (0 as i32 as i64) { pc += 63 }
    ldxb r2, [r9+0x40]                      
    jge r2, 4, lbb_9636                             if r2 >= (4 as i32 as i64 as u64) { pc += 686 }
    ldxdw r3, [r10-0x168]                   
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    stxdw [r10-0x168], r3                   
    ldxdw r1, [r9+0x8]                      
    stxdw [r10-0x188], r1                   
    ldxdw r7, [r9+0x0]                      
    jsgt r2, 1, lbb_9014                            if (r2 as i64) > (1 as i32 as i64) { pc += 57 }
    stxdw [r10-0x190], r8                   
    stxdw [r10-0x198], r9                   
    jne r2, 0, lbb_9133                             if r2 != (0 as i32 as i64 as u64) { pc += 173 }
    ldxdw r1, [r10-0x168]                   
    or64 r1, r6                                     r1 |= r6   ///  r1 = r1.or(r6)
    stxdw [r10-0x1a0], r7                   
    jeq r1, 0, lbb_9362                             if r1 == (0 as i32 as i64 as u64) { pc += 398 }
    lddw r4, 0x5555555555555555                     r4 load str located at 6148914691236517205
    lddw r3, 0x3333333333333333                     r3 load str located at 3689348814741910323
    lddw r2, 0xf0f0f0f0f0f0f0f                      r2 load str located at 1085102592571150095
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
    stxdw [r10-0x170], r6                   
    jne r6, 0, lbb_9365                             if r6 != (0 as i32 as i64 as u64) { pc += 391 }
    ldxdw r5, [r10-0x168]                   
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r4                                    r0 &= r4   ///  r0 = r0.and(r4)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    mov64 r4, r5                                    r4 = r5
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, r3                                    r5 &= r3   ///  r5 = r5.and(r3)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    mul64 r4, r1                                    r4 *= r1   ///  r4 = r4.wrapping_mul(r1)
    rsh64 r4, 56                                    r4 >>= 56   ///  r4 = r4.wrapping_shr(56)
    add64 r4, 64                                    r4 += 64   ///  r4 = r4.wrapping_add(64 as i32 as i64 as u64)
    ja lbb_9399                                     if true { pc += 388 }
lbb_9011:
    lddw r1, 0xdc00000000                           r1 load str located at 944892805120
    ja lbb_8943                                     if true { pc += -71 }
lbb_9014:
    stxdw [r10-0x170], r6                   
    jne r2, 2, lbb_9234                             if r2 != (2 as i32 as i64 as u64) { pc += 218 }
    ldxdw r6, [r10-0x168]                   
    mov64 r5, r6                                    r5 = r6
    arsh64 r5, 63                                   r5 >>= 63 (signed)   ///  r5 = (r5 as i64).wrapping_shr(63)
    ldxdw r2, [r10-0x188]                   
    mov64 r3, r2                                    r3 = r2
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    stxdw [r10-0x180], r3                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -192                                  r1 += -192   ///  r1 = r1.wrapping_add(-192 as i32 as i64 as u64)
    mov64 r4, r6                                    r4 = r6
    stxdw [r10-0x178], r5                   
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r6, r7                                    r6 = r7
    arsh64 r6, 63                                   r6 >>= 63 (signed)   ///  r6 = (r6 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    mov64 r4, r7                                    r4 = r7
    ldxdw r7, [r10-0x170]                   
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, r6                                    r5 = r6
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -160                                  r1 += -160   ///  r1 = r1.wrapping_add(-160 as i32 as i64 as u64)
    ldxdw r2, [r10-0x188]                   
    ldxdw r3, [r10-0x180]                   
    mov64 r4, r7                                    r4 = r7
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    and64 r7, r6                                    r7 &= r6   ///  r7 = r7.and(r6)
    lddw r1, 0x7fffffffffffffff                     r1 load str located at 9223372036854775807
    and64 r7, r1                                    r7 &= r1   ///  r7 = r7.and(r1)
    ldxdw r1, [r10-0x88]                    
    add64 r1, r7                                    r1 += r7   ///  r1 = r1.wrapping_add(r7)
    ldxdw r3, [r10-0xa8]                    
    ldxdw r2, [r10-0xc0]                    
    mov64 r4, r2                                    r4 = r2
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxdw r0, [r10-0x90]                    
    mov64 r7, r0                                    r7 = r0
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r7, r0, lbb_9069                            if r7 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_9069:
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r4, r2, lbb_9073                            if r4 < r2 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_9073:
    ldxdw r2, [r10-0x188]                   
    ldxdw r4, [r10-0x178]                   
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    ldxdw r2, [r10-0xb8]                    
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    ldxdw r4, [r10-0xa0]                    
    mov64 r5, r2                                    r5 = r2
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    mov64 r6, r5                                    r6 = r5
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r6, r5, lbb_9087                            if r6 < r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_9087:
    jlt r5, r2, lbb_9089                            if r5 < r2 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_9089:
    arsh64 r2, 63                                   r2 >>= 63 (signed)   ///  r2 = (r2 as i64).wrapping_shr(63)
    ldxdw r5, [r10-0x98]                    
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    arsh64 r1, 63                                   r1 >>= 63 (signed)   ///  r1 = (r1 as i64).wrapping_shr(63)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    mov64 r1, r2                                    r1 = r2
    lsh64 r1, 16                                    r1 <<= 16   ///  r1 = r1.wrapping_shl(16)
    mov64 r3, r6                                    r3 = r6
    rsh64 r3, 48                                    r3 >>= 48   ///  r3 = r3.wrapping_shr(48)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    arsh64 r2, 48                                   r2 >>= 48 (signed)   ///  r2 = (r2 as i64).wrapping_shr(48)
    lsh64 r6, 16                                    r6 <<= 16   ///  r6 = r6.wrapping_shl(16)
    mov64 r1, r6                                    r1 = r6
    arsh64 r1, 63                                   r1 >>= 63 (signed)   ///  r1 = (r1 as i64).wrapping_shr(63)
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    xor64 r3, r1                                    r3 ^= r1   ///  r3 = r3.xor(r1)
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    jne r3, 0, lbb_9353                             if r3 != (0 as i32 as i64 as u64) { pc += 244 }
    mov64 r1, r7                                    r1 = r7
    rsh64 r1, 48                                    r1 >>= 48   ///  r1 = r1.wrapping_shr(48)
    or64 r6, r1                                     r6 |= r1   ///  r6 = r6.or(r1)
    lsh64 r7, 16                                    r7 <<= 16   ///  r7 = r7.wrapping_shl(16)
    ldxdw r1, [r10-0xb0]                    
    rsh64 r1, 48                                    r1 >>= 48   ///  r1 = r1.wrapping_shr(48)
    or64 r7, r1                                     r7 |= r1   ///  r7 = r7.or(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    ldxdw r2, [r10-0x168]                   
    ldxdw r3, [r10-0x170]                   
    call function_228                       
    ldxdw r5, [r10-0xc8]                    
    ldxdw r4, [r10-0xd0]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r6                                    r3 = r6
    call function_120                       
    ldxdw r1, [r10-0x30]                    
    jne r1, 0, lbb_9342                             if r1 != (0 as i32 as i64 as u64) { pc += 212 }
    lddw r1, 0xf200000000                           r1 load str located at 1039382085632
    ja lbb_8943                                     if true { pc += -190 }
lbb_9133:
    ldxdw r9, [r10-0x168]                   
    mov64 r5, r9                                    r5 = r9
    arsh64 r5, 63                                   r5 >>= 63 (signed)   ///  r5 = (r5 as i64).wrapping_shr(63)
    ldxdw r2, [r10-0x188]                   
    mov64 r8, r7                                    r8 = r7
    mov64 r7, r2                                    r7 = r2
    arsh64 r7, 63                                   r7 >>= 63 (signed)   ///  r7 = (r7 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -272                                  r1 += -272   ///  r1 = r1.wrapping_add(-272 as i32 as i64 as u64)
    mov64 r3, r7                                    r3 = r7
    mov64 r4, r9                                    r4 = r9
    stxdw [r10-0x178], r5                   
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -256                                  r1 += -256   ///  r1 = r1.wrapping_add(-256 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r9                                    r4 = r9
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r9, r8                                    r9 = r8
    arsh64 r9, 63                                   r9 >>= 63 (signed)   ///  r9 = (r9 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -224                                  r1 += -224   ///  r1 = r1.wrapping_add(-224 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r8                                    r4 = r8
    mov64 r5, r9                                    r5 = r9
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -240                                  r1 += -240   ///  r1 = r1.wrapping_add(-240 as i32 as i64 as u64)
    ldxdw r2, [r10-0x188]                   
    mov64 r3, r7                                    r3 = r7
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    and64 r6, r9                                    r6 &= r9   ///  r6 = r6.and(r9)
    lddw r1, 0x7fffffffffffffff                     r1 load str located at 9223372036854775807
    and64 r6, r1                                    r6 &= r1   ///  r6 = r6.and(r1)
    ldxdw r3, [r10-0xd8]                    
    add64 r3, r6                                    r3 += r6   ///  r3 = r3.wrapping_add(r6)
    ldxdw r1, [r10-0xf8]                    
    ldxdw r2, [r10-0x110]                   
    mov64 r4, r2                                    r4 = r2
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    ldxdw r7, [r10-0xe0]                    
    mov64 r1, r7                                    r1 = r7
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r1, r7, lbb_9186                            if r1 < r7 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_9186:
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ldxdw r8, [r10-0x190]                   
    jlt r4, r2, lbb_9191                            if r4 < r2 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_9191:
    ldxdw r2, [r10-0x188]                   
    ldxdw r6, [r10-0x178]                   
    and64 r6, r2                                    r6 &= r2   ///  r6 = r6.and(r2)
    ldxdw r4, [r10-0x108]                   
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    ldxdw r2, [r10-0xf0]                    
    mov64 r6, r4                                    r6 = r4
    add64 r6, r2                                    r6 += r2   ///  r6 = r6.wrapping_add(r2)
    mov64 r2, r6                                    r2 = r6
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ldxdw r7, [r10-0x198]                   
    jlt r2, r6, lbb_9206                            if r2 < r6 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_9206:
    jlt r6, r4, lbb_9208                            if r6 < r4 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_9208:
    arsh64 r4, 63                                   r4 >>= 63 (signed)   ///  r4 = (r4 as i64).wrapping_shr(63)
    ldxdw r6, [r10-0xe8]                    
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    mov64 r3, r4                                    r3 = r4
    lsh64 r3, 16                                    r3 <<= 16   ///  r3 = r3.wrapping_shl(16)
    mov64 r5, r2                                    r5 = r2
    rsh64 r5, 48                                    r5 >>= 48   ///  r5 = r5.wrapping_shr(48)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    arsh64 r4, 48                                   r4 >>= 48 (signed)   ///  r4 = (r4 as i64).wrapping_shr(48)
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    mov64 r3, r2                                    r3 = r2
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    xor64 r5, r3                                    r5 ^= r3   ///  r5 = r5.xor(r3)
    or64 r5, r4                                     r5 |= r4   ///  r5 = r5.or(r4)
    jne r5, 0, lbb_9356                             if r5 != (0 as i32 as i64 as u64) { pc += 128 }
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 48                                    r3 >>= 48   ///  r3 = r3.wrapping_shr(48)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    lsh64 r1, 16                                    r1 <<= 16   ///  r1 = r1.wrapping_shl(16)
    ldxdw r3, [r10-0x100]                   
    ja lbb_9615                                     if true { pc += 381 }
lbb_9234:
    ldxdw r6, [r10-0x168]                   
    mov64 r5, r6                                    r5 = r6
    arsh64 r5, 63                                   r5 >>= 63 (signed)   ///  r5 = (r5 as i64).wrapping_shr(63)
    ldxdw r2, [r10-0x188]                   
    mov64 r3, r2                                    r3 = r2
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    stxdw [r10-0x180], r3                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    mov64 r4, r6                                    r4 = r6
    stxdw [r10-0x178], r5                   
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r4, r7                                    r4 = r7
    mov64 r6, r4                                    r6 = r4
    arsh64 r6, 63                                   r6 >>= 63 (signed)   ///  r6 = (r6 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    ldxdw r7, [r10-0x170]                   
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, r6                                    r5 = r6
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    ldxdw r2, [r10-0x188]                   
    ldxdw r3, [r10-0x180]                   
    mov64 r4, r7                                    r4 = r7
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r2, r7                                    r2 = r7
    and64 r2, r6                                    r2 &= r6   ///  r2 = r2.and(r6)
    lddw r1, 0x7fffffffffffffff                     r1 load str located at 9223372036854775807
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    ldxdw r1, [r10-0x48]                    
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    ldxdw r2, [r10-0x68]                    
    ldxdw r3, [r10-0x80]                    
    mov64 r4, r3                                    r4 = r3
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    ldxdw r6, [r10-0x50]                    
    mov64 r2, r6                                    r2 = r6
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r2, r6, lbb_9288                            if r2 < r6 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_9288:
    add64 r1, r0                                    r1 += r0   ///  r1 = r1.wrapping_add(r0)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r4, r3, lbb_9292                            if r4 < r3 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_9292:
    ldxdw r3, [r10-0x188]                   
    ldxdw r6, [r10-0x178]                   
    and64 r6, r3                                    r6 &= r3   ///  r6 = r6.and(r3)
    ldxdw r4, [r10-0x78]                    
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    ldxdw r3, [r10-0x60]                    
    mov64 r6, r4                                    r6 = r4
    add64 r6, r3                                    r6 += r3   ///  r6 = r6.wrapping_add(r3)
    mov64 r3, r6                                    r3 = r6
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r3, r6, lbb_9306                            if r3 < r6 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_9306:
    jlt r6, r4, lbb_9308                            if r6 < r4 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_9308:
    arsh64 r4, 63                                   r4 >>= 63 (signed)   ///  r4 = (r4 as i64).wrapping_shr(63)
    ldxdw r6, [r10-0x58]                    
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    arsh64 r1, 63                                   r1 >>= 63 (signed)   ///  r1 = (r1 as i64).wrapping_shr(63)
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    mov64 r1, r4                                    r1 = r4
    lsh64 r1, 16                                    r1 <<= 16   ///  r1 = r1.wrapping_shl(16)
    mov64 r5, r3                                    r5 = r3
    rsh64 r5, 48                                    r5 >>= 48   ///  r5 = r5.wrapping_shr(48)
    or64 r5, r1                                     r5 |= r1   ///  r5 = r5.or(r1)
    arsh64 r4, 48                                   r4 >>= 48 (signed)   ///  r4 = (r4 as i64).wrapping_shr(48)
    lsh64 r3, 16                                    r3 <<= 16   ///  r3 = r3.wrapping_shl(16)
    mov64 r1, r3                                    r1 = r3
    arsh64 r1, 63                                   r1 >>= 63 (signed)   ///  r1 = (r1 as i64).wrapping_shr(63)
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    xor64 r5, r1                                    r5 ^= r1   ///  r5 = r5.xor(r1)
    or64 r5, r4                                     r5 |= r4   ///  r5 = r5.or(r4)
    jne r5, 0, lbb_9359                             if r5 != (0 as i32 as i64 as u64) { pc += 31 }
    mov64 r1, r2                                    r1 = r2
    rsh64 r1, 48                                    r1 >>= 48   ///  r1 = r1.wrapping_shr(48)
    or64 r3, r1                                     r3 |= r1   ///  r3 = r3.or(r1)
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    ldxdw r1, [r10-0x70]                    
    rsh64 r1, 48                                    r1 >>= 48   ///  r1 = r1.wrapping_shr(48)
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    ldxdw r4, [r10-0x168]                   
    mov64 r5, r7                                    r5 = r7
    call function_120                       
    ldxdw r1, [r10-0x30]                    
    jeq r1, 0, lbb_9633                             if r1 == (0 as i32 as i64 as u64) { pc += 291 }
lbb_9342:
    ldxdw r2, [r10-0x20]                    
    ldxdw r1, [r10-0x28]                    
    ldxdw r5, [r9+0x20]                     
    mov64 r3, r5                                    r3 = r5
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r3, r5, lbb_9351                            if r3 < r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_9351:
    ldxdw r5, [r9+0x28]                     
    ja lbb_9625                                     if true { pc += 272 }
lbb_9353:
    lddw r1, 0xee00000000                           r1 load str located at 1022202216448
    ja lbb_8943                                     if true { pc += -413 }
lbb_9356:
    lddw r1, 0xe800000000                           r1 load str located at 996432412672
    ja lbb_8943                                     if true { pc += -416 }
lbb_9359:
    lddw r1, 0xf900000000                           r1 load str located at 1069446856704
    ja lbb_8943                                     if true { pc += -419 }
lbb_9362:
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ja lbb_9516                                     if true { pc += 151 }
lbb_9365:
    mov64 r0, r6                                    r0 = r6
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    or64 r6, r0                                     r6 |= r0   ///  r6 = r6.or(r0)
    mov64 r0, r6                                    r0 = r6
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r6, r0                                     r6 |= r0   ///  r6 = r6.or(r0)
    mov64 r0, r6                                    r0 = r6
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r6, r0                                     r6 |= r0   ///  r6 = r6.or(r0)
    mov64 r0, r6                                    r0 = r6
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    or64 r6, r0                                     r6 |= r0   ///  r6 = r6.or(r0)
    mov64 r0, r6                                    r0 = r6
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    or64 r6, r0                                     r6 |= r0   ///  r6 = r6.or(r0)
    mov64 r0, r6                                    r0 = r6
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    or64 r6, r0                                     r6 |= r0   ///  r6 = r6.or(r0)
    xor64 r6, -1                                    r6 ^= -1   ///  r6 = r6.xor(-1)
    mov64 r0, r6                                    r0 = r6
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r4                                    r0 &= r4   ///  r0 = r0.and(r4)
    sub64 r6, r0                                    r6 -= r0   ///  r6 = r6.wrapping_sub(r0)
    mov64 r4, r6                                    r4 = r6
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r6, 2                                     r6 >>= 2   ///  r6 = r6.wrapping_shr(2)
    and64 r6, r3                                    r6 &= r3   ///  r6 = r6.and(r3)
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    mul64 r4, r1                                    r4 *= r1   ///  r4 = r4.wrapping_mul(r1)
    rsh64 r4, 56                                    r4 >>= 56   ///  r4 = r4.wrapping_shr(56)
lbb_9399:
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    and64 r4, 126                                   r4 &= 126   ///  r4 = r4.and(126)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -288                                  r1 += -288   ///  r1 = r1.wrapping_add(-288 as i32 as i64 as u64)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_17074                     
    ldxdw r1, [r10-0x118]                   
    ldxdw r2, [r10-0x120]                   
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ja lbb_9428                                     if true { pc += 17 }
lbb_9411:
    ldxdw r3, [r10-0x170]                   
    sub64 r3, r8                                    r3 -= r8   ///  r3 = r3.wrapping_sub(r8)
    stxdw [r10-0x170], r3                   
    ldxdw r3, [r10-0x180]                   
    add64 r7, r3                                    r7 += r3   ///  r7 = r7.wrapping_add(r3)
    ldxdw r3, [r10-0x168]                   
    sub64 r3, r4                                    r3 -= r4   ///  r3 = r3.wrapping_sub(r4)
    stxdw [r10-0x168], r3                   
    rsh64 r2, 2                                     r2 >>= 2   ///  r2 = r2.wrapping_shr(2)
    mov64 r3, r1                                    r3 = r1
    lsh64 r3, 62                                    r3 <<= 62   ///  r3 = r3.wrapping_shl(62)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    mov64 r6, r9                                    r6 = r9
    mov64 r3, r7                                    r3 = r7
    jeq r0, 0, lbb_9511                             if r0 == (0 as i32 as i64 as u64) { pc += 83 }
lbb_9428:
    mov64 r0, r2                                    r0 = r2
    add64 r0, r6                                    r0 += r6   ///  r0 = r0.wrapping_add(r6)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r0, r2, lbb_9433                            if r0 < r2 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_9433:
    mov64 r4, r1                                    r4 = r1
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    ldxdw r7, [r10-0x170]                   
    jge r7, r4, lbb_9479                            if r7 >= r4 { pc += 40 }
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    ldxdw r9, [r10-0x168]                   
    jge r9, r0, lbb_9483                            if r9 >= r0 { pc += 41 }
lbb_9442:
    jne r7, r4, lbb_9485                            if r7 != r4 { pc += 42 }
lbb_9443:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_9488                             if r8 == (0 as i32 as i64 as u64) { pc += 43 }
lbb_9445:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_9491                             if r8 == (0 as i32 as i64 as u64) { pc += 44 }
lbb_9447:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_9494                             if r8 == (0 as i32 as i64 as u64) { pc += 45 }
lbb_9449:
    stxdw [r10-0x178], r5                   
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jne r8, 0, lbb_9453                             if r8 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_9452:
    mov64 r0, r2                                    r0 = r2
lbb_9453:
    rsh64 r6, 1                                     r6 >>= 1   ///  r6 = r6.wrapping_shr(1)
    mov64 r5, r3                                    r5 = r3
    lsh64 r5, 63                                    r5 <<= 63   ///  r5 = r5.wrapping_shl(63)
    or64 r6, r5                                     r6 |= r5   ///  r6 = r6.or(r5)
    mov64 r9, r0                                    r9 = r0
    add64 r9, r6                                    r9 += r6   ///  r9 = r9.wrapping_add(r6)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jge r9, r0, lbb_9499                            if r9 >= r0 { pc += 38 }
    stxdw [r10-0x180], r5                   
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    ldxdw r5, [r10-0x168]                   
    jge r5, r4, lbb_9504                            if r5 >= r4 { pc += 39 }
lbb_9465:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jle r2, 3, lbb_9507                             if r2 <= (3 as i32 as i64 as u64) { pc += 40 }
lbb_9467:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_9470                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_9469:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_9470:
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    add64 r7, r3                                    r7 += r3   ///  r7 = r7.wrapping_add(r3)
    ldxdw r3, [r10-0x170]                   
    ldxdw r6, [r10-0x178]                   
    sub64 r3, r6                                    r3 -= r6   ///  r3 = r3.wrapping_sub(r6)
    stxdw [r10-0x170], r3                   
    jeq r1, 0, lbb_9411                             if r1 == (0 as i32 as i64 as u64) { pc += -66 }
    mov64 r0, r5                                    r0 = r5
    ja lbb_9411                                     if true { pc += -68 }
lbb_9479:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    ldxdw r9, [r10-0x168]                   
    jlt r9, r0, lbb_9442                            if r9 < r0 { pc += -41 }
lbb_9483:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jeq r7, r4, lbb_9443                            if r7 == r4 { pc += -42 }
lbb_9485:
    mov64 r8, r5                                    r8 = r5
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jne r8, 0, lbb_9445                             if r8 != (0 as i32 as i64 as u64) { pc += -43 }
lbb_9488:
    mov64 r7, r1                                    r7 = r1
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jne r8, 0, lbb_9447                             if r8 != (0 as i32 as i64 as u64) { pc += -44 }
lbb_9491:
    mov64 r5, r4                                    r5 = r4
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jne r8, 0, lbb_9449                             if r8 != (0 as i32 as i64 as u64) { pc += -45 }
lbb_9494:
    mov64 r4, r0                                    r4 = r0
    stxdw [r10-0x178], r5                   
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r8, 0, lbb_9452                             if r8 == (0 as i32 as i64 as u64) { pc += -46 }
    ja lbb_9453                                     if true { pc += -46 }
lbb_9499:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    stxdw [r10-0x180], r5                   
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    ldxdw r5, [r10-0x168]                   
    jlt r5, r4, lbb_9465                            if r5 < r4 { pc += -39 }
lbb_9504:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jgt r2, 3, lbb_9467                             if r2 > (3 as i32 as i64 as u64) { pc += -40 }
lbb_9507:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_9469                             if r1 == (0 as i32 as i64 as u64) { pc += -41 }
    ja lbb_9470                                     if true { pc += -41 }
lbb_9511:
    lsh64 r7, 24                                    r7 <<= 24   ///  r7 = r7.wrapping_shl(24)
    mov64 r1, r9                                    r1 = r9
    rsh64 r1, 40                                    r1 >>= 40   ///  r1 = r1.wrapping_shr(40)
    or64 r7, r1                                     r7 |= r1   ///  r7 = r7.or(r1)
    lsh64 r9, 24                                    r9 <<= 24   ///  r9 = r9.wrapping_shl(24)
lbb_9516:
    mov64 r5, r9                                    r5 = r9
    arsh64 r5, 63                                   r5 >>= 63 (signed)   ///  r5 = (r5 as i64).wrapping_shr(63)
    ldxdw r2, [r10-0x188]                   
    mov64 r3, r2                                    r3 = r2
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    stxdw [r10-0x170], r3                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -352                                  r1 += -352   ///  r1 = r1.wrapping_add(-352 as i32 as i64 as u64)
    mov64 r4, r9                                    r4 = r9
    stxdw [r10-0x168], r5                   
    call function_17083                     
    and64 r9, -16777216                             r9 &= -16777216   ///  r9 = r9.and(-16777216)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -336                                  r1 += -336   ///  r1 = r1.wrapping_add(-336 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r9, [r10-0x1a0]                   
    mov64 r4, r9                                    r4 = r9
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r6, r9                                    r6 = r9
    arsh64 r6, 63                                   r6 >>= 63 (signed)   ///  r6 = (r6 as i64).wrapping_shr(63)
    mov64 r8, r7                                    r8 = r7
    arsh64 r8, 63                                   r8 >>= 63 (signed)   ///  r8 = (r8 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -304                                  r1 += -304   ///  r1 = r1.wrapping_add(-304 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r8                                    r3 = r8
    mov64 r4, r9                                    r4 = r9
    mov64 r5, r6                                    r5 = r6
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -320                                  r1 += -320   ///  r1 = r1.wrapping_add(-320 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r8                                    r3 = r8
    ldxdw r4, [r10-0x188]                   
    ldxdw r5, [r10-0x170]                   
    call function_17083                     
    and64 r6, r7                                    r6 &= r7   ///  r6 = r6.and(r7)
    ldxdw r3, [r10-0x128]                   
    add64 r3, r6                                    r3 += r6   ///  r3 = r3.wrapping_add(r6)
    ldxdw r1, [r10-0x148]                   
    ldxdw r2, [r10-0x160]                   
    mov64 r4, r2                                    r4 = r2
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    ldxdw r6, [r10-0x130]                   
    mov64 r1, r6                                    r1 = r6
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r1, r6, lbb_9568                            if r1 < r6 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_9568:
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ldxdw r8, [r10-0x190]                   
    ldxdw r7, [r10-0x198]                   
    jlt r4, r2, lbb_9574                            if r4 < r2 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_9574:
    ldxdw r2, [r10-0x188]                   
    ldxdw r6, [r10-0x168]                   
    and64 r6, r2                                    r6 &= r2   ///  r6 = r6.and(r2)
    ldxdw r4, [r10-0x158]                   
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    ldxdw r2, [r10-0x140]                   
    mov64 r6, r4                                    r6 = r4
    add64 r6, r2                                    r6 += r2   ///  r6 = r6.wrapping_add(r2)
    mov64 r2, r6                                    r2 = r6
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r2, r6, lbb_9588                            if r2 < r6 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_9588:
    jlt r6, r4, lbb_9590                            if r6 < r4 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_9590:
    arsh64 r4, 63                                   r4 >>= 63 (signed)   ///  r4 = (r4 as i64).wrapping_shr(63)
    ldxdw r6, [r10-0x138]                   
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    mov64 r3, r4                                    r3 = r4
    lsh64 r3, 16                                    r3 <<= 16   ///  r3 = r3.wrapping_shl(16)
    mov64 r5, r2                                    r5 = r2
    rsh64 r5, 48                                    r5 >>= 48   ///  r5 = r5.wrapping_shr(48)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    arsh64 r4, 48                                   r4 >>= 48 (signed)   ///  r4 = (r4 as i64).wrapping_shr(48)
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    mov64 r3, r2                                    r3 = r2
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    xor64 r5, r3                                    r5 ^= r3   ///  r5 = r5.xor(r3)
    or64 r5, r4                                     r5 |= r4   ///  r5 = r5.or(r4)
    jne r5, 0, lbb_9630                             if r5 != (0 as i32 as i64 as u64) { pc += 20 }
    mov64 r3, r1                                    r3 = r1
    rsh64 r3, 48                                    r3 >>= 48   ///  r3 = r3.wrapping_shr(48)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    lsh64 r1, 16                                    r1 <<= 16   ///  r1 = r1.wrapping_shl(16)
    ldxdw r3, [r10-0x150]                   
lbb_9615:
    rsh64 r3, 48                                    r3 >>= 48   ///  r3 = r3.wrapping_shr(48)
    or64 r1, r3                                     r1 |= r3   ///  r1 = r1.or(r3)
    ldxdw r5, [r7+0x20]                     
    mov64 r3, r5                                    r3 = r5
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r3, r5, lbb_9624                            if r3 < r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_9624:
    ldxdw r5, [r7+0x28]                     
lbb_9625:
    stxdw [r8+0x8], r3                      
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    stxdw [r8+0x10], r5                     
    ja lbb_8945                                     if true { pc += -685 }
lbb_9630:
    lddw r1, 0xe300000000                           r1 load str located at 974957576192
    ja lbb_8943                                     if true { pc += -690 }
lbb_9633:
    lddw r1, 0xfc00000000                           r1 load str located at 1082331758592
    ja lbb_8943                                     if true { pc += -693 }
lbb_9636:
    lddw r1, 0x100023a10 --> b"\x00\x00\x00\x00\xa96\x02\x00\x11\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295113232
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    lddw r2, 0x100023a20 --> b"\x00\x00\x00\x00\xba6\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\x1c\x00\x00…        r2 load str located at 4295113248
    call function_15475                     

function_9648:
    mov64 r8, r5                                    r8 = r5
    mov64 r7, r4                                    r7 = r4
    mov64 r6, r1                                    r6 = r1
    stxdw [r10-0x20], r3                    
    stxdw [r10-0x28], r2                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -40                                   r2 += -40   ///  r2 = r2.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r3, r7                                    r3 = r7
    call function_9702                      
    ldxw r1, [r10-0x18]                     
    jne r1, 0, lbb_9672                             if r1 != (0 as i32 as i64 as u64) { pc += 11 }
    ldxdw r4, [r8-0x1000]                   
    ldxdw r9, [r10-0x8]                     
    ldxdw r8, [r10-0x10]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -40                                   r2 += -40   ///  r2 = r2.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r3, r7                                    r3 = r7
    call function_10315                     
    ldxw r1, [r10-0x18]                     
    jeq r1, 0, lbb_9678                             if r1 == (0 as i32 as i64 as u64) { pc += 6 }
lbb_9672:
    ldxw r1, [r10-0x10]                     
    ldxw r2, [r10-0x14]                     
    stxw [r6+0x8], r1                       
    stxw [r6+0x4], r2                       
    stw [r6+0x0], 1                         
    ja lbb_9691                                     if true { pc += 13 }
lbb_9678:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x10]                    
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jle r8, r2, lbb_9692                            if r8 <= r2 { pc += 10 }
    ldxdw r3, [r10-0x8]                     
    jsle r9, r3, lbb_9695                           if (r9 as i64) <= (r3 as i64) { pc += 11 }
lbb_9684:
    jne r9, r3, lbb_9697                            if r9 != r3 { pc += 12 }
lbb_9685:
    jeq r1, 0, lbb_9699                             if r1 == (0 as i32 as i64 as u64) { pc += 13 }
lbb_9686:
    jne r1, 0, lbb_9688                             if r1 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_9687:
    mov64 r9, r3                                    r9 = r3
lbb_9688:
    stxdw [r6+0x10], r9                     
    stxdw [r6+0x8], r8                      
    stw [r6+0x0], 0                         
lbb_9691:
    exit                                    
lbb_9692:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r3, [r10-0x8]                     
    jsgt r9, r3, lbb_9684                           if (r9 as i64) > (r3 as i64) { pc += -11 }
lbb_9695:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jeq r9, r3, lbb_9685                            if r9 == r3 { pc += -12 }
lbb_9697:
    mov64 r1, r4                                    r1 = r4
    jne r1, 0, lbb_9686                             if r1 != (0 as i32 as i64 as u64) { pc += -13 }
lbb_9699:
    mov64 r8, r2                                    r8 = r2
    jeq r1, 0, lbb_9687                             if r1 == (0 as i32 as i64 as u64) { pc += -14 }
    ja lbb_9688                                     if true { pc += -14 }

function_9702:
    ldxdw r4, [r3+0x0]                      
    lddw r5, 0x6560b6dd6ee140db                     r5 load str located at 7305039657759228123
    jne r4, r5, lbb_9719                            if r4 != r5 { pc += 13 }
    ldxdw r4, [r3+0x8]                      
    lddw r5, 0xe9c5fdf17e56283c                     r5 load str located at -1601594878811822020
    jne r4, r5, lbb_9719                            if r4 != r5 { pc += 9 }
    ldxdw r4, [r3+0x10]                     
    lddw r5, 0x10742a9290fc845b                     r5 load str located at 1185619410891342939
    jne r4, r5, lbb_9719                            if r4 != r5 { pc += 5 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r5, [r3+0x18]                     
    lddw r0, 0x182e105eeb8708ad                     r0 load str located at 1742348105703426221
    jeq r5, r0, lbb_9720                            if r5 == r0 { pc += 1 }
lbb_9719:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
lbb_9720:
    jeq r4, 0, lbb_9780                             if r4 == (0 as i32 as i64 as u64) { pc += 59 }
    ldxdw r4, [r3+0x0]                      
    lddw r5, 0x8db421f2102ddb55                     r5 load str located at -8235920495016551595
    jne r4, r5, lbb_9738                            if r4 != r5 { pc += 13 }
    ldxdw r4, [r3+0x8]                      
    lddw r5, 0xb2239b75b00a26a0                     r5 load str located at -5610469781006571872
    jne r4, r5, lbb_9738                            if r4 != r5 { pc += 9 }
    ldxdw r4, [r3+0x10]                     
    lddw r5, 0x17f86227578d7956                     r5 load str located at 1727238378208721238
    jne r4, r5, lbb_9738                            if r4 != r5 { pc += 5 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r5, [r3+0x18]                     
    lddw r0, 0xd79939b6c459a1f3                     r0 load str located at -2911232226977275405
    jeq r5, r0, lbb_9739                            if r5 == r0 { pc += 1 }
lbb_9738:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
lbb_9739:
    jeq r4, 0, lbb_9780                             if r4 == (0 as i32 as i64 as u64) { pc += 40 }
    ldxdw r4, [r3+0x0]                      
    lddw r5, 0xa486d52df0a65e93                     r5 load str located at -6591346611322462573
    jne r4, r5, lbb_9757                            if r4 != r5 { pc += 13 }
    ldxdw r4, [r3+0x8]                      
    lddw r5, 0xd3ea0f66edcb857f                     r5 load str located at -3176709652405844609
    jne r4, r5, lbb_9757                            if r4 != r5 { pc += 9 }
    ldxdw r4, [r3+0x10]                     
    lddw r5, 0xccbecd97e436386                      r5 load str located at 922090967597802374
    jne r4, r5, lbb_9757                            if r4 != r5 { pc += 5 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r5, [r3+0x18]                     
    lddw r0, 0x3aa82a4cb9d28622                     r0 load str located at 4226674759310673442
    jeq r5, r0, lbb_9758                            if r5 == r0 { pc += 1 }
lbb_9757:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
lbb_9758:
    jeq r4, 0, lbb_9780                             if r4 == (0 as i32 as i64 as u64) { pc += 21 }
    ldxdw r4, [r3+0x0]                      
    lddw r5, 0x6d46af69e74bdfb4                     r5 load str located at 7874173867890827188
    jne r4, r5, lbb_9776                            if r4 != r5 { pc += 13 }
    ldxdw r4, [r3+0x8]                      
    lddw r5, 0xf0483f6a17f839a4                     r5 load str located at -1132585581382452828
    jne r4, r5, lbb_9776                            if r4 != r5 { pc += 9 }
    ldxdw r4, [r3+0x10]                     
    lddw r5, 0xb64521530cad3f67                     r5 load str located at -5312803544834949273
    jne r4, r5, lbb_9776                            if r4 != r5 { pc += 5 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r4, [r3+0x18]                     
    lddw r5, 0x494ac5dec856a9e9                     r5 load str located at 5281251073679862249
    jeq r4, r5, lbb_9777                            if r4 == r5 { pc += 1 }
lbb_9776:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_9777:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jne r0, 0, lbb_10297                            if r0 != (0 as i32 as i64 as u64) { pc += 517 }
lbb_9780:
    ldxdw r4, [r2+0x8]                      
    ldxdw r2, [r2+0x0]                      
    mov64 r5, r2                                    r5 = r2
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxh r5, [r5-0x2]                       
    ldxh r4, [r2+0x0]                       
    jle r4, r5, lbb_10304                           if r4 <= r5 { pc += 517 }
    lsh64 r5, 1                                     r5 <<= 1   ///  r5 = r5.wrapping_shl(1)
    mov64 r0, r2                                    r0 = r2
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    ldxh r0, [r0+0x2]                       
    mov64 r5, r2                                    r5 = r2
    add64 r5, r0                                    r5 += r0   ///  r5 = r5.wrapping_add(r0)
    ldxh r0, [r5+0x0]                       
    mul64 r0, 33                                    r0 *= 33   ///  r0 = r0.wrapping_mul(33 as u64)
    add64 r5, r0                                    r5 += r0   ///  r5 = r5.wrapping_add(r0)
    ldxdw r0, [r5+0x2]                      
    lddw r6, 0x4873bce2144ae3b5                     r6 load str located at 5220724072241619893
    jne r0, r6, lbb_9813                            if r0 != r6 { pc += 13 }
    ldxdw r0, [r5+0xa]                      
    lddw r6, 0xd6ee5daff5e10e69                     r6 load str located at -2959324894810010007
    jne r0, r6, lbb_9813                            if r0 != r6 { pc += 9 }
    ldxdw r0, [r5+0x12]                     
    lddw r6, 0x60b8aa6da3403855                     r6 load str located at 6969507811222894677
    jne r0, r6, lbb_9813                            if r0 != r6 { pc += 5 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r5+0x1a]                     
    lddw r7, 0x103cc0bd736050b0                     r7 load str located at 1170021923126530224
    jeq r6, r7, lbb_9814                            if r6 == r7 { pc += 1 }
lbb_9813:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_9814:
    jeq r0, 0, lbb_10295                            if r0 == (0 as i32 as i64 as u64) { pc += 480 }
    ldxdw r0, [r5+0x2]                      
    lddw r6, 0xe17c6a0d184ae3b5                     r6 load str located at -2198765913588964427
    jne r0, r6, lbb_9832                            if r0 != r6 { pc += 13 }
    ldxdw r0, [r5+0xa]                      
    lddw r6, 0xae0cda013afeb695                     r6 load str located at -5905105312569969003
    jne r0, r6, lbb_9832                            if r0 != r6 { pc += 9 }
    ldxdw r0, [r5+0x12]                     
    lddw r6, 0x98144e7e5ae3fa8                      r6 load str located at 684904381118562216
    jne r0, r6, lbb_9832                            if r0 != r6 { pc += 5 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r5+0x1a]                     
    lddw r7, 0x40ee2497930cf7ea                     r7 load str located at 4678717296310286314
    jeq r6, r7, lbb_9833                            if r6 == r7 { pc += 1 }
lbb_9832:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_9833:
    jeq r0, 0, lbb_10295                            if r0 == (0 as i32 as i64 as u64) { pc += 461 }
    ldxdw r0, [r5+0x2]                      
    lddw r6, 0x6ec031f25bd57904                     r6 load str located at 7980433456693082372
    jne r0, r6, lbb_9851                            if r0 != r6 { pc += 13 }
    ldxdw r0, [r5+0xa]                      
    lddw r6, 0x71568ce6ec574ee                      r6 load str located at 510429368607405294
    jne r0, r6, lbb_9851                            if r0 != r6 { pc += 9 }
    ldxdw r0, [r5+0x12]                     
    lddw r6, 0x518ef4a3deb2b1fd                     r6 load str located at 5876903548418175485
    jne r0, r6, lbb_9851                            if r0 != r6 { pc += 5 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r5+0x1a]                     
    lddw r7, 0x8f13bc56a2cdb102                     r7 load str located at -8136953021443755774
    jeq r6, r7, lbb_9852                            if r6 == r7 { pc += 1 }
lbb_9851:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_9852:
    jeq r0, 0, lbb_10295                            if r0 == (0 as i32 as i64 as u64) { pc += 442 }
    ldxdw r0, [r5+0x2]                      
    lddw r6, 0x715b8f7af9be1205                     r6 load str located at 8168280107505291781
    jne r0, r6, lbb_9870                            if r0 != r6 { pc += 13 }
    ldxdw r0, [r5+0xa]                      
    lddw r6, 0xc042edc6d6edf37d                     r6 load str located at -4592847231699258499
    jne r0, r6, lbb_9870                            if r0 != r6 { pc += 9 }
    ldxdw r0, [r5+0x12]                     
    lddw r6, 0xee87bee5df124fe2                     r6 load str located at -1258827676327456798
    jne r0, r6, lbb_9870                            if r0 != r6 { pc += 5 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r5+0x1a]                     
    lddw r7, 0xb96c5d3f745eec3f                     r7 load str located at -5085587352091431873
    jeq r6, r7, lbb_9871                            if r6 == r7 { pc += 1 }
lbb_9870:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_9871:
    jeq r0, 0, lbb_10295                            if r0 == (0 as i32 as i64 as u64) { pc += 423 }
    ldxdw r0, [r5+0x2]                      
    lddw r6, 0xc499d767a64dc30a                     r6 load str located at -4280153130667228406
    jne r0, r6, lbb_9889                            if r0 != r6 { pc += 13 }
    ldxdw r0, [r5+0xa]                      
    lddw r6, 0x75b1926ae1365115                     r6 load str located at 8480720561057976597
    jne r0, r6, lbb_9889                            if r0 != r6 { pc += 9 }
    ldxdw r0, [r5+0x12]                     
    lddw r6, 0x678ad2090231d088                     r6 load str located at 7461006668826005640
    jne r0, r6, lbb_9889                            if r0 != r6 { pc += 5 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r5+0x1a]                     
    lddw r7, 0xec666c5126b469e3                     r7 load str located at -1412322337336563229
    jeq r6, r7, lbb_9890                            if r6 == r7 { pc += 1 }
lbb_9889:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_9890:
    jeq r0, 0, lbb_10295                            if r0 == (0 as i32 as i64 as u64) { pc += 404 }
    ldxdw r0, [r5+0x2]                      
    lddw r6, 0x136d5ca2f1569155                     r6 load str located at 1399876914085531989
    jne r0, r6, lbb_9908                            if r0 != r6 { pc += 13 }
    ldxdw r0, [r5+0xa]                      
    lddw r6, 0x340d9a0ae6f72a4f                     r6 load str located at 3750823436284799567
    jne r0, r6, lbb_9908                            if r0 != r6 { pc += 9 }
    ldxdw r0, [r5+0x12]                     
    lddw r6, 0xd56264635691c77e                     r6 load str located at -3070781618096322690
    jne r0, r6, lbb_9908                            if r0 != r6 { pc += 5 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r5+0x1a]                     
    lddw r7, 0x698f3435f126add1                     r7 load str located at 7606355701935812049
    jeq r6, r7, lbb_9909                            if r6 == r7 { pc += 1 }
lbb_9908:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_9909:
    jeq r0, 0, lbb_10295                            if r0 == (0 as i32 as i64 as u64) { pc += 385 }
    ldxdw r0, [r5+0x2]                      
    lddw r6, 0xe959f7272b74fd7a                     r6 load str located at -1632001642340221574
    jne r0, r6, lbb_9927                            if r0 != r6 { pc += 13 }
    ldxdw r0, [r5+0xa]                      
    lddw r6, 0x7a819dd33c7070c6                     r6 load str located at 8827510275200544966
    jne r0, r6, lbb_9927                            if r0 != r6 { pc += 9 }
    ldxdw r0, [r5+0x12]                     
    lddw r6, 0x6dd2523bce0a93a0                     r6 load str located at 7913477912056730528
    jne r0, r6, lbb_9927                            if r0 != r6 { pc += 5 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r5+0x1a]                     
    lddw r7, 0xd3bb8723dd54a054                     r7 load str located at -3189807322954948524
    jeq r6, r7, lbb_9928                            if r6 == r7 { pc += 1 }
lbb_9927:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_9928:
    jeq r0, 0, lbb_10295                            if r0 == (0 as i32 as i64 as u64) { pc += 366 }
    ldxdw r0, [r5+0x2]                      
    lddw r6, 0xbb0ee7126e9ca906                     r6 load str located at -4967779272591890170
    jne r0, r6, lbb_9946                            if r0 != r6 { pc += 13 }
    ldxdw r0, [r5+0xa]                      
    lddw r6, 0x6e904b4c145c1835                     r6 load str located at 7966950530949584949
    jne r0, r6, lbb_9946                            if r0 != r6 { pc += 9 }
    ldxdw r0, [r5+0x12]                     
    lddw r6, 0x2a2f74470ab0ff18                     r6 load str located at 3039776121969245976
    jne r0, r6, lbb_9946                            if r0 != r6 { pc += 5 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r5+0x1a]                     
    lddw r7, 0xd4c988690b11045e                     r7 load str located at -3113807682611379106
    jeq r6, r7, lbb_9947                            if r6 == r7 { pc += 1 }
lbb_9946:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_9947:
    jeq r0, 0, lbb_10295                            if r0 == (0 as i32 as i64 as u64) { pc += 347 }
    ldxdw r0, [r5+0x2]                      
    lddw r6, 0xb03b11381934becb                     r6 load str located at -5747981566769578293
    jne r0, r6, lbb_9965                            if r0 != r6 { pc += 13 }
    ldxdw r0, [r5+0xa]                      
    lddw r6, 0x45acad558b7e296b                     r6 load str located at 5020578267535386987
    jne r0, r6, lbb_9965                            if r0 != r6 { pc += 9 }
    ldxdw r0, [r5+0x12]                     
    lddw r6, 0x59369b4a1734ee6f                     r6 load str located at 6428496260637191791
    jne r0, r6, lbb_9965                            if r0 != r6 { pc += 5 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r5+0x1a]                     
    lddw r7, 0x42c79970523f5e6b                     r7 load str located at 4811983434563935851
    jeq r6, r7, lbb_9966                            if r6 == r7 { pc += 1 }
lbb_9965:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_9966:
    jeq r0, 0, lbb_10295                            if r0 == (0 as i32 as i64 as u64) { pc += 328 }
    ldxdw r0, [r5+0x2]                      
    lddw r6, 0xe2cdce6a001db90d                     r6 load str located at -2103798496248350451
    jne r0, r6, lbb_9984                            if r0 != r6 { pc += 13 }
    ldxdw r0, [r5+0xa]                      
    lddw r6, 0x67889bcdcd17de84                     r6 load str located at 7460384090950721156
    jne r0, r6, lbb_9984                            if r0 != r6 { pc += 9 }
    ldxdw r0, [r5+0x12]                     
    lddw r6, 0x5666dfd02b922d2b                     r6 load str located at 6225909620063481131
    jne r0, r6, lbb_9984                            if r0 != r6 { pc += 5 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r5, [r5+0x1a]                     
    lddw r6, 0x548b03e01a423aa3                     r6 load str located at 6091967181996833443
    jeq r5, r6, lbb_9985                            if r5 == r6 { pc += 1 }
lbb_9984:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_9985:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jeq r0, 0, lbb_10067                            if r0 == (0 as i32 as i64 as u64) { pc += 80 }
    lddw r0, 0x182e105eeb8708ad                     r0 load str located at 1742348105703426221
    stxdw [r10-0x28], r0                    
    lddw r0, 0x10742a9290fc845b                     r0 load str located at 1185619410891342939
    stxdw [r10-0x30], r0                    
    lddw r0, 0xe9c5fdf17e56283c                     r0 load str located at -1601594878811822020
    stxdw [r10-0x38], r0                    
    lddw r0, 0x6560b6dd6ee140db                     r0 load str located at 7305039657759228123
    stxdw [r10-0x40], r0                    
    ldxdw r6, [r3+0x0]                      
    jne r6, r0, lbb_10011                           if r6 != r0 { pc += 10 }
    ldxdw r0, [r3+0x8]                      
    ldxdw r6, [r10-0x38]                    
    jne r6, r0, lbb_10011                           if r6 != r0 { pc += 7 }
    ldxdw r0, [r3+0x10]                     
    ldxdw r6, [r10-0x30]                    
    jne r6, r0, lbb_10011                           if r6 != r0 { pc += 4 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxdw r0, [r3+0x18]                     
    ldxdw r7, [r10-0x28]                    
    jeq r7, r0, lbb_10012                           if r7 == r0 { pc += 1 }
lbb_10011:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
lbb_10012:
    mov64 r0, r2                                    r0 = r2
    add64 r0, 2                                     r0 += 2   ///  r0 = r0.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r10-0x50], r0                    
    stxdw [r10-0x60], r4                    
    jeq r6, 0, lbb_10069                            if r6 == (0 as i32 as i64 as u64) { pc += 52 }
    lddw r0, 0x182e105eeb8708ad                     r0 load str located at 1742348105703426221
    stxdw [r10-0x8], r0                     
    lddw r0, 0x10742a9290fc845b                     r0 load str located at 1185619410891342939
    stxdw [r10-0x10], r0                    
    lddw r0, 0xe9c5fdf17e56283c                     r0 load str located at -1601594878811822020
    stxdw [r10-0x18], r0                    
    lddw r0, 0x6560b6dd6ee140db                     r0 load str located at 7305039657759228123
    stxdw [r10-0x20], r0                    
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_10035                                    if true { pc += 4 }
lbb_10031:
    ldxdw r0, [r10-0x58]                    
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    ldxdw r4, [r10-0x60]                    
    jge r0, r4, lbb_10069                           if r0 >= r4 { pc += 34 }
lbb_10035:
    stxdw [r10-0x58], r0                    
    lsh64 r0, 1                                     r0 <<= 1   ///  r0 = r0.wrapping_shl(1)
    ldxdw r7, [r10-0x50]                    
    add64 r7, r0                                    r7 += r0   ///  r7 = r7.wrapping_add(r0)
    ldxh r0, [r7+0x0]                       
    mov64 r7, r2                                    r7 = r2
    add64 r7, r0                                    r7 += r0   ///  r7 = r7.wrapping_add(r0)
    ldxh r8, [r7+0x0]                       
    jeq r8, 0, lbb_10031                            if r8 == (0 as i32 as i64 as u64) { pc += -13 }
    add64 r7, 3                                     r7 += 3   ///  r7 = r7.wrapping_add(3 as i32 as i64 as u64)
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_10046:
    ldxdw r0, [r7+0x0]                      
    ldxdw r6, [r10-0x20]                    
    jne r0, r6, lbb_10061                           if r0 != r6 { pc += 12 }
    ldxdw r0, [r7+0x8]                      
    ldxdw r6, [r10-0x18]                    
    jne r0, r6, lbb_10061                           if r0 != r6 { pc += 9 }
    ldxdw r0, [r7+0x10]                     
    ldxdw r6, [r10-0x10]                    
    jne r0, r6, lbb_10061                           if r0 != r6 { pc += 6 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r4, [r7+0x18]                     
    stxdw [r10-0x48], r4                    
    ldxdw r6, [r10-0x8]                     
    ldxdw r4, [r10-0x48]                    
    jeq r4, r6, lbb_10062                           if r4 == r6 { pc += 1 }
lbb_10061:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_10062:
    jeq r0, 0, lbb_10301                            if r0 == (0 as i32 as i64 as u64) { pc += 238 }
    add64 r7, 33                                    r7 += 33   ///  r7 = r7.wrapping_add(33 as i32 as i64 as u64)
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    jlt r9, r8, lbb_10046                           if r9 < r8 { pc += -20 }
    ja lbb_10031                                    if true { pc += -36 }
lbb_10067:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ja lbb_10297                                    if true { pc += 228 }
lbb_10069:
    lddw r0, 0xd79939b6c459a1f3                     r0 load str located at -2911232226977275405
    stxdw [r10-0x28], r0                    
    lddw r0, 0x17f86227578d7956                     r0 load str located at 1727238378208721238
    stxdw [r10-0x30], r0                    
    lddw r0, 0xb2239b75b00a26a0                     r0 load str located at -5610469781006571872
    stxdw [r10-0x38], r0                    
    lddw r0, 0x8db421f2102ddb55                     r0 load str located at -8235920495016551595
    stxdw [r10-0x40], r0                    
    ldxdw r6, [r3+0x0]                      
    jne r6, r0, lbb_10093                           if r6 != r0 { pc += 10 }
    ldxdw r0, [r3+0x8]                      
    ldxdw r6, [r10-0x38]                    
    jne r6, r0, lbb_10093                           if r6 != r0 { pc += 7 }
    ldxdw r0, [r3+0x10]                     
    ldxdw r6, [r10-0x30]                    
    jne r6, r0, lbb_10093                           if r6 != r0 { pc += 4 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r3+0x18]                     
    ldxdw r7, [r10-0x28]                    
    jeq r7, r6, lbb_10094                           if r7 == r6 { pc += 1 }
lbb_10093:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_10094:
    jeq r0, 0, lbb_10145                            if r0 == (0 as i32 as i64 as u64) { pc += 50 }
    lddw r0, 0xd79939b6c459a1f3                     r0 load str located at -2911232226977275405
    stxdw [r10-0x8], r0                     
    lddw r0, 0x17f86227578d7956                     r0 load str located at 1727238378208721238
    stxdw [r10-0x10], r0                    
    lddw r0, 0xb2239b75b00a26a0                     r0 load str located at -5610469781006571872
    stxdw [r10-0x18], r0                    
    lddw r0, 0x8db421f2102ddb55                     r0 load str located at -8235920495016551595
    stxdw [r10-0x20], r0                    
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_10113                                    if true { pc += 4 }
lbb_10109:
    ldxdw r0, [r10-0x58]                    
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    ldxdw r4, [r10-0x60]                    
    jge r0, r4, lbb_10145                           if r0 >= r4 { pc += 32 }
lbb_10113:
    stxdw [r10-0x58], r0                    
    lsh64 r0, 1                                     r0 <<= 1   ///  r0 = r0.wrapping_shl(1)
    ldxdw r7, [r10-0x50]                    
    add64 r7, r0                                    r7 += r0   ///  r7 = r7.wrapping_add(r0)
    ldxh r0, [r7+0x0]                       
    mov64 r7, r2                                    r7 = r2
    add64 r7, r0                                    r7 += r0   ///  r7 = r7.wrapping_add(r0)
    ldxh r4, [r7+0x0]                       
    jeq r4, 0, lbb_10109                            if r4 == (0 as i32 as i64 as u64) { pc += -13 }
    add64 r7, 3                                     r7 += 3   ///  r7 = r7.wrapping_add(3 as i32 as i64 as u64)
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_10124:
    ldxdw r0, [r7+0x0]                      
    ldxdw r6, [r10-0x20]                    
    jne r0, r6, lbb_10139                           if r0 != r6 { pc += 12 }
    ldxdw r0, [r7+0x8]                      
    ldxdw r6, [r10-0x18]                    
    jne r0, r6, lbb_10139                           if r0 != r6 { pc += 9 }
    ldxdw r0, [r7+0x10]                     
    ldxdw r6, [r10-0x10]                    
    jne r0, r6, lbb_10139                           if r0 != r6 { pc += 6 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r7+0x18]                     
    stxdw [r10-0x48], r6                    
    ldxdw r6, [r10-0x8]                     
    ldxdw r8, [r10-0x48]                    
    jeq r8, r6, lbb_10140                           if r8 == r6 { pc += 1 }
lbb_10139:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_10140:
    jeq r0, 0, lbb_10301                            if r0 == (0 as i32 as i64 as u64) { pc += 160 }
    add64 r7, 33                                    r7 += 33   ///  r7 = r7.wrapping_add(33 as i32 as i64 as u64)
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    jlt r9, r4, lbb_10124                           if r9 < r4 { pc += -20 }
    ja lbb_10109                                    if true { pc += -36 }
lbb_10145:
    lddw r0, 0x3aa82a4cb9d28622                     r0 load str located at 4226674759310673442
    stxdw [r10-0x28], r0                    
    lddw r0, 0xccbecd97e436386                      r0 load str located at 922090967597802374
    stxdw [r10-0x30], r0                    
    lddw r0, 0xd3ea0f66edcb857f                     r0 load str located at -3176709652405844609
    stxdw [r10-0x38], r0                    
    lddw r0, 0xa486d52df0a65e93                     r0 load str located at -6591346611322462573
    stxdw [r10-0x40], r0                    
    ldxdw r6, [r3+0x0]                      
    jne r6, r0, lbb_10169                           if r6 != r0 { pc += 10 }
    ldxdw r0, [r3+0x8]                      
    ldxdw r6, [r10-0x38]                    
    jne r6, r0, lbb_10169                           if r6 != r0 { pc += 7 }
    ldxdw r0, [r3+0x10]                     
    ldxdw r6, [r10-0x30]                    
    jne r6, r0, lbb_10169                           if r6 != r0 { pc += 4 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r3+0x18]                     
    ldxdw r7, [r10-0x28]                    
    jeq r7, r6, lbb_10170                           if r7 == r6 { pc += 1 }
lbb_10169:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_10170:
    jeq r0, 0, lbb_10222                            if r0 == (0 as i32 as i64 as u64) { pc += 51 }
    lddw r0, 0x3aa82a4cb9d28622                     r0 load str located at 4226674759310673442
    stxdw [r10-0x8], r0                     
    lddw r0, 0xccbecd97e436386                      r0 load str located at 922090967597802374
    stxdw [r10-0x10], r0                    
    lddw r0, 0xd3ea0f66edcb857f                     r0 load str located at -3176709652405844609
    stxdw [r10-0x18], r0                    
    lddw r0, 0xa486d52df0a65e93                     r0 load str located at -6591346611322462573
    stxdw [r10-0x20], r0                    
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_10189                                    if true { pc += 4 }
lbb_10185:
    ldxdw r0, [r10-0x58]                    
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    ldxdw r4, [r10-0x60]                    
    jge r0, r4, lbb_10221                           if r0 >= r4 { pc += 32 }
lbb_10189:
    stxdw [r10-0x58], r0                    
    lsh64 r0, 1                                     r0 <<= 1   ///  r0 = r0.wrapping_shl(1)
    ldxdw r7, [r10-0x50]                    
    add64 r7, r0                                    r7 += r0   ///  r7 = r7.wrapping_add(r0)
    ldxh r0, [r7+0x0]                       
    mov64 r7, r2                                    r7 = r2
    add64 r7, r0                                    r7 += r0   ///  r7 = r7.wrapping_add(r0)
    ldxh r4, [r7+0x0]                       
    jeq r4, 0, lbb_10185                            if r4 == (0 as i32 as i64 as u64) { pc += -13 }
    add64 r7, 3                                     r7 += 3   ///  r7 = r7.wrapping_add(3 as i32 as i64 as u64)
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_10200:
    ldxdw r0, [r7+0x0]                      
    ldxdw r6, [r10-0x20]                    
    jne r0, r6, lbb_10215                           if r0 != r6 { pc += 12 }
    ldxdw r0, [r7+0x8]                      
    ldxdw r6, [r10-0x18]                    
    jne r0, r6, lbb_10215                           if r0 != r6 { pc += 9 }
    ldxdw r0, [r7+0x10]                     
    ldxdw r6, [r10-0x10]                    
    jne r0, r6, lbb_10215                           if r0 != r6 { pc += 6 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r6, [r7+0x18]                     
    stxdw [r10-0x48], r6                    
    ldxdw r6, [r10-0x8]                     
    ldxdw r8, [r10-0x48]                    
    jeq r8, r6, lbb_10216                           if r8 == r6 { pc += 1 }
lbb_10215:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_10216:
    jeq r0, 0, lbb_10301                            if r0 == (0 as i32 as i64 as u64) { pc += 84 }
    add64 r7, 33                                    r7 += 33   ///  r7 = r7.wrapping_add(33 as i32 as i64 as u64)
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    jlt r9, r4, lbb_10200                           if r9 < r4 { pc += -20 }
    ja lbb_10185                                    if true { pc += -36 }
lbb_10221:
    ldxdw r4, [r10-0x60]                    
lbb_10222:
    lddw r0, 0x494ac5dec856a9e9                     r0 load str located at 5281251073679862249
    stxdw [r10-0x28], r0                    
    lddw r0, 0xb64521530cad3f67                     r0 load str located at -5312803544834949273
    stxdw [r10-0x30], r0                    
    lddw r0, 0xf0483f6a17f839a4                     r0 load str located at -1132585581382452828
    stxdw [r10-0x38], r0                    
    lddw r0, 0x6d46af69e74bdfb4                     r0 load str located at 7874173867890827188
    stxdw [r10-0x40], r0                    
    ldxdw r6, [r3+0x0]                      
    jne r6, r0, lbb_10246                           if r6 != r0 { pc += 10 }
    ldxdw r0, [r3+0x8]                      
    ldxdw r6, [r10-0x38]                    
    jne r6, r0, lbb_10246                           if r6 != r0 { pc += 7 }
    ldxdw r0, [r3+0x10]                     
    ldxdw r6, [r10-0x30]                    
    jne r6, r0, lbb_10246                           if r6 != r0 { pc += 4 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r3, [r3+0x18]                     
    ldxdw r6, [r10-0x28]                    
    jeq r6, r3, lbb_10247                           if r6 == r3 { pc += 1 }
lbb_10246:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_10247:
    jeq r0, 0, lbb_10295                            if r0 == (0 as i32 as i64 as u64) { pc += 47 }
    lddw r3, 0x494ac5dec856a9e9                     r3 load str located at 5281251073679862249
    stxdw [r10-0x8], r3                     
    lddw r3, 0xb64521530cad3f67                     r3 load str located at -5312803544834949273
    stxdw [r10-0x10], r3                    
    lddw r3, 0xf0483f6a17f839a4                     r3 load str located at -1132585581382452828
    stxdw [r10-0x18], r3                    
    lddw r3, 0x6d46af69e74bdfb4                     r3 load str located at 7874173867890827188
    stxdw [r10-0x20], r3                    
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_10265                                    if true { pc += 3 }
lbb_10262:
    ldxdw r0, [r10-0x48]                    
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    jge r0, r4, lbb_10295                           if r0 >= r4 { pc += 30 }
lbb_10265:
    stxdw [r10-0x48], r0                    
    lsh64 r0, 1                                     r0 <<= 1   ///  r0 = r0.wrapping_shl(1)
    ldxdw r6, [r10-0x50]                    
    add64 r6, r0                                    r6 += r0   ///  r6 = r6.wrapping_add(r0)
    ldxh r0, [r6+0x0]                       
    mov64 r6, r2                                    r6 = r2
    add64 r6, r0                                    r6 += r0   ///  r6 = r6.wrapping_add(r0)
    ldxh r7, [r6+0x0]                       
    jeq r7, 0, lbb_10262                            if r7 == (0 as i32 as i64 as u64) { pc += -12 }
    add64 r6, 3                                     r6 += 3   ///  r6 = r6.wrapping_add(3 as i32 as i64 as u64)
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_10276:
    ldxdw r0, [r6+0x0]                      
    ldxdw r9, [r10-0x20]                    
    jne r0, r9, lbb_10289                           if r0 != r9 { pc += 10 }
    ldxdw r0, [r6+0x8]                      
    ldxdw r9, [r10-0x18]                    
    jne r0, r9, lbb_10289                           if r0 != r9 { pc += 7 }
    ldxdw r0, [r6+0x10]                     
    ldxdw r9, [r10-0x10]                    
    jne r0, r9, lbb_10289                           if r0 != r9 { pc += 4 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r9, [r6+0x18]                     
    ldxdw r3, [r10-0x8]                     
    jeq r9, r3, lbb_10290                           if r9 == r3 { pc += 1 }
lbb_10289:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_10290:
    jeq r0, 0, lbb_10301                            if r0 == (0 as i32 as i64 as u64) { pc += 10 }
    add64 r6, 33                                    r6 += 33   ///  r6 = r6.wrapping_add(33 as i32 as i64 as u64)
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
    jlt r8, r7, lbb_10276                           if r8 < r7 { pc += -18 }
    ja lbb_10262                                    if true { pc += -33 }
lbb_10295:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_10297:
    stxdw [r1+0x8], r4                      
    stxdw [r1+0x10], r5                     
    stw [r1+0x0], 0                         
    exit                                    
lbb_10301:
    lddw r4, 0x19000000000000                       r4 load str located at 7036874417766400
    ja lbb_10297                                    if true { pc += -7 }
lbb_10304:
    stw [r10-0x20], 2                       
    mov64 r3, r10                                   r3 = r10
    add64 r3, -32                                   r3 += -32   ///  r3 = r3.wrapping_add(-32 as i32 as i64 as u64)
    lddw r1, 0x100022e88 --> b"called `Result::unwrap()` on an `Err` value"        r1 load str located at 4295110280
    mov64 r2, 43                                    r2 = 43 as i32 as i64 as u64
    lddw r4, 0x100023a80 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x04\x00\…        r4 load str located at 4295113344
    lddw r5, 0x100023aa0 --> b"\x00\x00\x00\x00\xe86\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00A\x01\x00\x0…        r5 load str located at 4295113376
    call function_15519                     

function_10315:
    ldxdw r5, [r3+0x0]                      
    lddw r0, 0x6560b6dd6ee140db                     r0 load str located at 7305039657759228123
    jne r5, r0, lbb_10332                           if r5 != r0 { pc += 13 }
    ldxdw r5, [r3+0x8]                      
    lddw r0, 0xe9c5fdf17e56283c                     r0 load str located at -1601594878811822020
    jne r5, r0, lbb_10332                           if r5 != r0 { pc += 9 }
    ldxdw r5, [r3+0x10]                     
    lddw r0, 0x10742a9290fc845b                     r0 load str located at 1185619410891342939
    jne r5, r0, lbb_10332                           if r5 != r0 { pc += 5 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r0, [r3+0x18]                     
    lddw r6, 0x182e105eeb8708ad                     r6 load str located at 1742348105703426221
    jeq r0, r6, lbb_10333                           if r0 == r6 { pc += 1 }
lbb_10332:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_10333:
    jeq r5, 0, lbb_10393                            if r5 == (0 as i32 as i64 as u64) { pc += 59 }
    ldxdw r5, [r3+0x0]                      
    lddw r0, 0x8db421f2102ddb55                     r0 load str located at -8235920495016551595
    jne r5, r0, lbb_10351                           if r5 != r0 { pc += 13 }
    ldxdw r5, [r3+0x8]                      
    lddw r0, 0xb2239b75b00a26a0                     r0 load str located at -5610469781006571872
    jne r5, r0, lbb_10351                           if r5 != r0 { pc += 9 }
    ldxdw r5, [r3+0x10]                     
    lddw r0, 0x17f86227578d7956                     r0 load str located at 1727238378208721238
    jne r5, r0, lbb_10351                           if r5 != r0 { pc += 5 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r0, [r3+0x18]                     
    lddw r6, 0xd79939b6c459a1f3                     r6 load str located at -2911232226977275405
    jeq r0, r6, lbb_10352                           if r0 == r6 { pc += 1 }
lbb_10351:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_10352:
    jeq r5, 0, lbb_10393                            if r5 == (0 as i32 as i64 as u64) { pc += 40 }
    ldxdw r5, [r3+0x0]                      
    lddw r0, 0xa486d52df0a65e93                     r0 load str located at -6591346611322462573
    jne r5, r0, lbb_10370                           if r5 != r0 { pc += 13 }
    ldxdw r5, [r3+0x8]                      
    lddw r0, 0xd3ea0f66edcb857f                     r0 load str located at -3176709652405844609
    jne r5, r0, lbb_10370                           if r5 != r0 { pc += 9 }
    ldxdw r5, [r3+0x10]                     
    lddw r0, 0xccbecd97e436386                      r0 load str located at 922090967597802374
    jne r5, r0, lbb_10370                           if r5 != r0 { pc += 5 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r0, [r3+0x18]                     
    lddw r6, 0x3aa82a4cb9d28622                     r6 load str located at 4226674759310673442
    jeq r0, r6, lbb_10371                           if r0 == r6 { pc += 1 }
lbb_10370:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_10371:
    jeq r5, 0, lbb_10393                            if r5 == (0 as i32 as i64 as u64) { pc += 21 }
    ldxdw r5, [r3+0x0]                      
    lddw r0, 0x6d46af69e74bdfb4                     r0 load str located at 7874173867890827188
    jne r5, r0, lbb_10389                           if r5 != r0 { pc += 13 }
    ldxdw r5, [r3+0x8]                      
    lddw r0, 0xf0483f6a17f839a4                     r0 load str located at -1132585581382452828
    jne r5, r0, lbb_10389                           if r5 != r0 { pc += 9 }
    ldxdw r5, [r3+0x10]                     
    lddw r0, 0xb64521530cad3f67                     r0 load str located at -5312803544834949273
    jne r5, r0, lbb_10389                           if r5 != r0 { pc += 5 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r0, [r3+0x18]                     
    lddw r6, 0x494ac5dec856a9e9                     r6 load str located at 5281251073679862249
    jeq r0, r6, lbb_10390                           if r0 == r6 { pc += 1 }
lbb_10389:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_10390:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jne r5, 0, lbb_10772                            if r5 != (0 as i32 as i64 as u64) { pc += 379 }
lbb_10393:
    ldxdw r5, [r2+0x8]                      
    ldxdw r2, [r2+0x0]                      
    mov64 r0, r2                                    r0 = r2
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    ldxh r5, [r0-0x2]                       
    ldxh r0, [r2+0x0]                       
    stxdw [r10-0x48], r0                    
    jle r0, r5, lbb_10779                           if r0 <= r5 { pc += 378 }
    lsh64 r5, 1                                     r5 <<= 1   ///  r5 = r5.wrapping_shl(1)
    mov64 r0, r2                                    r0 = r2
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    ldxh r5, [r0+0x2]                       
    mov64 r6, r2                                    r6 = r2
    add64 r6, r5                                    r6 += r5   ///  r6 = r6.wrapping_add(r5)
    ldxh r0, [r6+0x0]                       
    mov64 r5, r0                                    r5 = r0
    mul64 r5, 33                                    r5 *= 33   ///  r5 = r5.wrapping_mul(33 as u64)
    mov64 r8, r6                                    r8 = r6
    add64 r8, r5                                    r8 += r5   ///  r8 = r8.wrapping_add(r5)
    ldxdw r5, [r8+0x2]                      
    lddw r7, 0x6ec031f25bd57904                     r7 load str located at 7980433456693082372
    jne r5, r7, lbb_10429                           if r5 != r7 { pc += 13 }
    ldxdw r5, [r8+0xa]                      
    lddw r7, 0x71568ce6ec574ee                      r7 load str located at 510429368607405294
    jne r5, r7, lbb_10429                           if r5 != r7 { pc += 9 }
    ldxdw r5, [r8+0x12]                     
    lddw r7, 0x518ef4a3deb2b1fd                     r7 load str located at 5876903548418175485
    jne r5, r7, lbb_10429                           if r5 != r7 { pc += 5 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r7, [r8+0x1a]                     
    lddw r9, 0x8f13bc56a2cdb102                     r9 load str located at -8136953021443755774
    jeq r7, r9, lbb_10430                           if r7 == r9 { pc += 1 }
lbb_10429:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_10430:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jne r5, 0, lbb_10772                            if r5 != (0 as i32 as i64 as u64) { pc += 339 }
    ldxh r5, [r8+0x22]                      
    jlt r5, 8, lbb_10772                            if r5 < (8 as i32 as i64 as u64) { pc += 337 }
    ldxdw r5, [r8+0x24]                     
    lddw r7, 0x2aade37a97cb17e5                     r7 load str located at 3075364236236101605
    jeq r5, r7, lbb_10450                           if r5 == r7 { pc += 11 }
    lddw r7, 0x14afc431ccfa64bb                     r7 load str located at 1490625719854326971
    jeq r5, r7, lbb_10448                           if r5 == r7 { pc += 6 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    lddw r8, 0x819cd641339b20c1                     r8 load str located at -9107168770922962751
    jne r5, r8, lbb_10772                           if r5 != r8 { pc += 326 }
    mov64 r8, 8                                     r8 = 8 as i32 as i64 as u64
    ja lbb_10451                                    if true { pc += 3 }
lbb_10448:
    mov64 r8, 4                                     r8 = 4 as i32 as i64 as u64
    ja lbb_10451                                    if true { pc += 1 }
lbb_10450:
    mov64 r8, 5                                     r8 = 5 as i32 as i64 as u64
lbb_10451:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jge r8, r0, lbb_10772                           if r8 >= r0 { pc += 319 }
    mul64 r8, 33                                    r8 *= 33   ///  r8 = r8.wrapping_mul(33 as u64)
    add64 r6, r8                                    r6 += r8   ///  r6 = r6.wrapping_add(r8)
    ldxdw r5, [r6+0x3]                      
    ldxdw r0, [r4+0x0]                      
    jne r5, r0, lbb_10468                           if r5 != r0 { pc += 10 }
    ldxdw r5, [r4+0x8]                      
    ldxdw r0, [r6+0xb]                      
    jne r0, r5, lbb_10468                           if r0 != r5 { pc += 7 }
    ldxdw r5, [r4+0x10]                     
    ldxdw r0, [r6+0x13]                     
    jne r0, r5, lbb_10468                           if r0 != r5 { pc += 4 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r4, [r4+0x18]                     
    ldxdw r5, [r6+0x1b]                     
    jeq r5, r4, lbb_10469                           if r5 == r4 { pc += 1 }
lbb_10468:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_10469:
    stxdw [r10-0x58], r0                    
    lddw r4, 0x182e105eeb8708ad                     r4 load str located at 1742348105703426221
    stxdw [r10-0x28], r4                    
    lddw r4, 0x10742a9290fc845b                     r4 load str located at 1185619410891342939
    stxdw [r10-0x30], r4                    
    lddw r4, 0xe9c5fdf17e56283c                     r4 load str located at -1601594878811822020
    stxdw [r10-0x38], r4                    
    lddw r4, 0x6560b6dd6ee140db                     r4 load str located at 7305039657759228123
    stxdw [r10-0x40], r4                    
    ldxdw r5, [r3+0x0]                      
    jne r5, r4, lbb_10494                           if r5 != r4 { pc += 10 }
    ldxdw r4, [r3+0x8]                      
    ldxdw r5, [r10-0x38]                    
    jne r5, r4, lbb_10494                           if r5 != r4 { pc += 7 }
    ldxdw r4, [r3+0x10]                     
    ldxdw r5, [r10-0x30]                    
    jne r5, r4, lbb_10494                           if r5 != r4 { pc += 4 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r4, [r3+0x18]                     
    ldxdw r6, [r10-0x28]                    
    jeq r6, r4, lbb_10495                           if r6 == r4 { pc += 1 }
lbb_10494:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_10495:
    mov64 r4, r2                                    r4 = r2
    add64 r4, 2                                     r4 += 2   ///  r4 = r4.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r10-0x50], r4                    
    jeq r5, 0, lbb_10546                            if r5 == (0 as i32 as i64 as u64) { pc += 47 }
    lddw r5, 0x182e105eeb8708ad                     r5 load str located at 1742348105703426221
    stxdw [r10-0x8], r5                     
    lddw r5, 0x10742a9290fc845b                     r5 load str located at 1185619410891342939
    stxdw [r10-0x10], r5                    
    lddw r5, 0xe9c5fdf17e56283c                     r5 load str located at -1601594878811822020
    stxdw [r10-0x18], r5                    
    lddw r5, 0x6560b6dd6ee140db                     r5 load str located at 7305039657759228123
    stxdw [r10-0x20], r5                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ja lbb_10516                                    if true { pc += 3 }
lbb_10513:
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    ldxdw r4, [r10-0x48]                    
    jge r6, r4, lbb_10546                           if r6 >= r4 { pc += 30 }
lbb_10516:
    mov64 r5, r6                                    r5 = r6
    lsh64 r5, 1                                     r5 <<= 1   ///  r5 = r5.wrapping_shl(1)
    ldxdw r7, [r10-0x50]                    
    add64 r7, r5                                    r7 += r5   ///  r7 = r7.wrapping_add(r5)
    ldxh r5, [r7+0x0]                       
    mov64 r7, r2                                    r7 = r2
    add64 r7, r5                                    r7 += r5   ///  r7 = r7.wrapping_add(r5)
    ldxh r8, [r7+0x0]                       
    jeq r8, 0, lbb_10513                            if r8 == (0 as i32 as i64 as u64) { pc += -12 }
    add64 r7, 3                                     r7 += 3   ///  r7 = r7.wrapping_add(3 as i32 as i64 as u64)
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_10527:
    ldxdw r5, [r7+0x0]                      
    ldxdw r0, [r10-0x20]                    
    jne r5, r0, lbb_10540                           if r5 != r0 { pc += 10 }
    ldxdw r5, [r7+0x8]                      
    ldxdw r0, [r10-0x18]                    
    jne r5, r0, lbb_10540                           if r5 != r0 { pc += 7 }
    ldxdw r5, [r7+0x10]                     
    ldxdw r0, [r10-0x10]                    
    jne r5, r0, lbb_10540                           if r5 != r0 { pc += 4 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r0, [r7+0x18]                     
    ldxdw r4, [r10-0x8]                     
    jeq r0, r4, lbb_10541                           if r0 == r4 { pc += 1 }
lbb_10540:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_10541:
    jeq r5, 0, lbb_10765                            if r5 == (0 as i32 as i64 as u64) { pc += 223 }
    add64 r7, 33                                    r7 += 33   ///  r7 = r7.wrapping_add(33 as i32 as i64 as u64)
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    jlt r9, r8, lbb_10527                           if r9 < r8 { pc += -18 }
    ja lbb_10513                                    if true { pc += -33 }
lbb_10546:
    lddw r4, 0xd79939b6c459a1f3                     r4 load str located at -2911232226977275405
    stxdw [r10-0x28], r4                    
    lddw r4, 0x17f86227578d7956                     r4 load str located at 1727238378208721238
    stxdw [r10-0x30], r4                    
    lddw r4, 0xb2239b75b00a26a0                     r4 load str located at -5610469781006571872
    stxdw [r10-0x38], r4                    
    lddw r4, 0x8db421f2102ddb55                     r4 load str located at -8235920495016551595
    stxdw [r10-0x40], r4                    
    ldxdw r5, [r3+0x0]                      
    jne r5, r4, lbb_10570                           if r5 != r4 { pc += 10 }
    ldxdw r4, [r3+0x8]                      
    ldxdw r5, [r10-0x38]                    
    jne r5, r4, lbb_10570                           if r5 != r4 { pc += 7 }
    ldxdw r4, [r3+0x10]                     
    ldxdw r5, [r10-0x30]                    
    jne r5, r4, lbb_10570                           if r5 != r4 { pc += 4 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r4, [r3+0x18]                     
    ldxdw r0, [r10-0x28]                    
    jeq r0, r4, lbb_10571                           if r0 == r4 { pc += 1 }
lbb_10570:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_10571:
    jeq r5, 0, lbb_10619                            if r5 == (0 as i32 as i64 as u64) { pc += 47 }
    lddw r4, 0xd79939b6c459a1f3                     r4 load str located at -2911232226977275405
    stxdw [r10-0x8], r4                     
    lddw r4, 0x17f86227578d7956                     r4 load str located at 1727238378208721238
    stxdw [r10-0x10], r4                    
    lddw r4, 0xb2239b75b00a26a0                     r4 load str located at -5610469781006571872
    stxdw [r10-0x18], r4                    
    lddw r4, 0x8db421f2102ddb55                     r4 load str located at -8235920495016551595
    stxdw [r10-0x20], r4                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ja lbb_10589                                    if true { pc += 3 }
lbb_10586:
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    ldxdw r4, [r10-0x48]                    
    jge r6, r4, lbb_10619                           if r6 >= r4 { pc += 30 }
lbb_10589:
    mov64 r4, r6                                    r4 = r6
    lsh64 r4, 1                                     r4 <<= 1   ///  r4 = r4.wrapping_shl(1)
    ldxdw r5, [r10-0x50]                    
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxh r4, [r5+0x0]                       
    mov64 r7, r2                                    r7 = r2
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    ldxh r8, [r7+0x0]                       
    jeq r8, 0, lbb_10586                            if r8 == (0 as i32 as i64 as u64) { pc += -12 }
    add64 r7, 3                                     r7 += 3   ///  r7 = r7.wrapping_add(3 as i32 as i64 as u64)
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_10600:
    ldxdw r4, [r7+0x0]                      
    ldxdw r5, [r10-0x20]                    
    jne r4, r5, lbb_10613                           if r4 != r5 { pc += 10 }
    ldxdw r4, [r7+0x8]                      
    ldxdw r5, [r10-0x18]                    
    jne r4, r5, lbb_10613                           if r4 != r5 { pc += 7 }
    ldxdw r4, [r7+0x10]                     
    ldxdw r5, [r10-0x10]                    
    jne r4, r5, lbb_10613                           if r4 != r5 { pc += 4 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r4, [r7+0x18]                     
    ldxdw r0, [r10-0x8]                     
    jeq r4, r0, lbb_10614                           if r4 == r0 { pc += 1 }
lbb_10613:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_10614:
    jeq r5, 0, lbb_10765                            if r5 == (0 as i32 as i64 as u64) { pc += 150 }
    add64 r7, 33                                    r7 += 33   ///  r7 = r7.wrapping_add(33 as i32 as i64 as u64)
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    jlt r9, r8, lbb_10600                           if r9 < r8 { pc += -18 }
    ja lbb_10586                                    if true { pc += -33 }
lbb_10619:
    lddw r4, 0x3aa82a4cb9d28622                     r4 load str located at 4226674759310673442
    stxdw [r10-0x28], r4                    
    lddw r4, 0xccbecd97e436386                      r4 load str located at 922090967597802374
    stxdw [r10-0x30], r4                    
    lddw r4, 0xd3ea0f66edcb857f                     r4 load str located at -3176709652405844609
    stxdw [r10-0x38], r4                    
    lddw r4, 0xa486d52df0a65e93                     r4 load str located at -6591346611322462573
    stxdw [r10-0x40], r4                    
    ldxdw r5, [r3+0x0]                      
    jne r5, r4, lbb_10643                           if r5 != r4 { pc += 10 }
    ldxdw r4, [r3+0x8]                      
    ldxdw r5, [r10-0x38]                    
    jne r5, r4, lbb_10643                           if r5 != r4 { pc += 7 }
    ldxdw r4, [r3+0x10]                     
    ldxdw r5, [r10-0x30]                    
    jne r5, r4, lbb_10643                           if r5 != r4 { pc += 4 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r4, [r3+0x18]                     
    ldxdw r0, [r10-0x28]                    
    jeq r0, r4, lbb_10644                           if r0 == r4 { pc += 1 }
lbb_10643:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_10644:
    jeq r5, 0, lbb_10692                            if r5 == (0 as i32 as i64 as u64) { pc += 47 }
    lddw r4, 0x3aa82a4cb9d28622                     r4 load str located at 4226674759310673442
    stxdw [r10-0x8], r4                     
    lddw r4, 0xccbecd97e436386                      r4 load str located at 922090967597802374
    stxdw [r10-0x10], r4                    
    lddw r4, 0xd3ea0f66edcb857f                     r4 load str located at -3176709652405844609
    stxdw [r10-0x18], r4                    
    lddw r4, 0xa486d52df0a65e93                     r4 load str located at -6591346611322462573
    stxdw [r10-0x20], r4                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ja lbb_10662                                    if true { pc += 3 }
lbb_10659:
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    ldxdw r4, [r10-0x48]                    
    jge r6, r4, lbb_10692                           if r6 >= r4 { pc += 30 }
lbb_10662:
    mov64 r4, r6                                    r4 = r6
    lsh64 r4, 1                                     r4 <<= 1   ///  r4 = r4.wrapping_shl(1)
    ldxdw r5, [r10-0x50]                    
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxh r4, [r5+0x0]                       
    mov64 r7, r2                                    r7 = r2
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    ldxh r8, [r7+0x0]                       
    jeq r8, 0, lbb_10659                            if r8 == (0 as i32 as i64 as u64) { pc += -12 }
    add64 r7, 3                                     r7 += 3   ///  r7 = r7.wrapping_add(3 as i32 as i64 as u64)
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_10673:
    ldxdw r4, [r7+0x0]                      
    ldxdw r5, [r10-0x20]                    
    jne r4, r5, lbb_10686                           if r4 != r5 { pc += 10 }
    ldxdw r4, [r7+0x8]                      
    ldxdw r5, [r10-0x18]                    
    jne r4, r5, lbb_10686                           if r4 != r5 { pc += 7 }
    ldxdw r4, [r7+0x10]                     
    ldxdw r5, [r10-0x10]                    
    jne r4, r5, lbb_10686                           if r4 != r5 { pc += 4 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r4, [r7+0x18]                     
    ldxdw r0, [r10-0x8]                     
    jeq r4, r0, lbb_10687                           if r4 == r0 { pc += 1 }
lbb_10686:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_10687:
    jeq r5, 0, lbb_10765                            if r5 == (0 as i32 as i64 as u64) { pc += 77 }
    add64 r7, 33                                    r7 += 33   ///  r7 = r7.wrapping_add(33 as i32 as i64 as u64)
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    jlt r9, r8, lbb_10673                           if r9 < r8 { pc += -18 }
    ja lbb_10659                                    if true { pc += -33 }
lbb_10692:
    lddw r4, 0x494ac5dec856a9e9                     r4 load str located at 5281251073679862249
    stxdw [r10-0x28], r4                    
    lddw r4, 0xb64521530cad3f67                     r4 load str located at -5312803544834949273
    stxdw [r10-0x30], r4                    
    lddw r4, 0xf0483f6a17f839a4                     r4 load str located at -1132585581382452828
    stxdw [r10-0x38], r4                    
    lddw r4, 0x6d46af69e74bdfb4                     r4 load str located at 7874173867890827188
    stxdw [r10-0x40], r4                    
    ldxdw r5, [r3+0x0]                      
    jne r5, r4, lbb_10716                           if r5 != r4 { pc += 10 }
    ldxdw r4, [r3+0x8]                      
    ldxdw r5, [r10-0x38]                    
    jne r5, r4, lbb_10716                           if r5 != r4 { pc += 7 }
    ldxdw r4, [r3+0x10]                     
    ldxdw r5, [r10-0x30]                    
    jne r5, r4, lbb_10716                           if r5 != r4 { pc += 4 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r3, [r3+0x18]                     
    ldxdw r4, [r10-0x28]                    
    jeq r4, r3, lbb_10717                           if r4 == r3 { pc += 1 }
lbb_10716:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_10717:
    jeq r5, 0, lbb_10776                            if r5 == (0 as i32 as i64 as u64) { pc += 58 }
    lddw r3, 0x494ac5dec856a9e9                     r3 load str located at 5281251073679862249
    stxdw [r10-0x8], r3                     
    lddw r3, 0xb64521530cad3f67                     r3 load str located at -5312803544834949273
    stxdw [r10-0x10], r3                    
    lddw r3, 0xf0483f6a17f839a4                     r3 load str located at -1132585581382452828
    stxdw [r10-0x18], r3                    
    lddw r3, 0x6d46af69e74bdfb4                     r3 load str located at 7874173867890827188
    stxdw [r10-0x20], r3                    
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ja lbb_10735                                    if true { pc += 3 }
lbb_10732:
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    ldxdw r4, [r10-0x48]                    
    jge r3, r4, lbb_10776                           if r3 >= r4 { pc += 41 }
lbb_10735:
    mov64 r4, r3                                    r4 = r3
    lsh64 r4, 1                                     r4 <<= 1   ///  r4 = r4.wrapping_shl(1)
    ldxdw r5, [r10-0x50]                    
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxh r4, [r5+0x0]                       
    mov64 r6, r2                                    r6 = r2
    add64 r6, r4                                    r6 += r4   ///  r6 = r6.wrapping_add(r4)
    ldxh r7, [r6+0x0]                       
    jeq r7, 0, lbb_10732                            if r7 == (0 as i32 as i64 as u64) { pc += -12 }
    add64 r6, 3                                     r6 += 3   ///  r6 = r6.wrapping_add(3 as i32 as i64 as u64)
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_10746:
    ldxdw r4, [r6+0x0]                      
    ldxdw r5, [r10-0x20]                    
    jne r4, r5, lbb_10759                           if r4 != r5 { pc += 10 }
    ldxdw r4, [r6+0x8]                      
    ldxdw r5, [r10-0x18]                    
    jne r4, r5, lbb_10759                           if r4 != r5 { pc += 7 }
    ldxdw r4, [r6+0x10]                     
    ldxdw r5, [r10-0x10]                    
    jne r4, r5, lbb_10759                           if r4 != r5 { pc += 4 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r4, [r6+0x18]                     
    ldxdw r0, [r10-0x8]                     
    jeq r4, r0, lbb_10760                           if r4 == r0 { pc += 1 }
lbb_10759:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_10760:
    jeq r5, 0, lbb_10765                            if r5 == (0 as i32 as i64 as u64) { pc += 4 }
    add64 r6, 33                                    r6 += 33   ///  r6 = r6.wrapping_add(33 as i32 as i64 as u64)
    add64 r8, 1                                     r8 += 1   ///  r8 = r8.wrapping_add(1 as i32 as i64 as u64)
    jlt r8, r7, lbb_10746                           if r8 < r7 { pc += -18 }
    ja lbb_10732                                    if true { pc += -33 }
lbb_10765:
    ldxdw r2, [r10-0x58]                    
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jeq r2, 0, lbb_10770                            if r2 == (0 as i32 as i64 as u64) { pc += 2 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ja lbb_10772                                    if true { pc += 2 }
lbb_10770:
    lddw r7, 0x19000000000000                       r7 load str located at 7036874417766400
lbb_10772:
    stxdw [r1+0x8], r7                      
    stxdw [r1+0x10], r9                     
    stw [r1+0x0], 0                         
    exit                                    
lbb_10776:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ja lbb_10772                                    if true { pc += -7 }
lbb_10779:
    stw [r10-0x20], 2                       
    mov64 r3, r10                                   r3 = r10
    add64 r3, -32                                   r3 += -32   ///  r3 = r3.wrapping_add(-32 as i32 as i64 as u64)
    lddw r1, 0x100022e88 --> b"called `Result::unwrap()` on an `Err` value"        r1 load str located at 4295110280
    mov64 r2, 43                                    r2 = 43 as i32 as i64 as u64
    lddw r4, 0x100023a80 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x04\x00\…        r4 load str located at 4295113344
    lddw r5, 0x100023ab8 --> b"\x00\x00\x00\x00\xe86\x02\x00\x1b\x00\x00\x00\x00\x00\x00\x00a\x01\x00\x0…        r5 load str located at 4295113400
    call function_15519                     

function_10790:
    stxdw [r10-0x118], r4                   
    stxdw [r10-0x120], r3                   
    stxdw [r10-0xf0], r2                    
    stxdw [r10-0x100], r1                   
    stxdw [r10-0xe8], r5                    
    ldxdw r7, [r5-0xff8]                    
    ldxdw r2, [r7+0x6b8]                    
    lddw r1, 0x6e9de2b30b19f9ea                     r1 load str located at 7970776174128921066
    jgt r2, 3, lbb_10802                            if r2 > (3 as i32 as i64 as u64) { pc += 2 }
    lddw r1, 0x6e9de2b30b19f1ea                     r1 load str located at 7970776174128919018
lbb_10802:
    ldxdw r2, [r7+0x10]                     
    lddw r3, 0x962c6f3b7c105a2d                     r3 load str located at -7625597767769892307
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    ldxdw r3, [r7+0x8]                      
    lddw r4, 0x962f6f387c135a2c                     r4 load str located at -7624753355724465620
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    ldxdw r4, [r7+0x0]                      
    lddw r5, 0x69d190c683eda5d3                     r5 load str located at 7625034826406274515
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r5, [r7+0x228]                    
    lddw r0, 0x46a912eb237873d9                     r0 load str located at 5091621654840767449
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    ldxdw r0, [r7+0x220]                    
    lddw r6, 0xb957ed15dc877c26                     r6 load str located at -5091340175569093594
    xor64 r0, r6                                    r0 ^= r6   ///  r0 = r0.xor(r6)
    ldxdw r6, [r7+0x248]                    
    lddw r9, 0x46a912eb23798bd9                     r9 load str located at 5091621654840839129
    xor64 r6, r9                                    r6 ^= r9   ///  r6 = r6.xor(r9)
    ldxdw r9, [r7+0x240]                    
    lddw r8, 0xb957ed15dc877426                     r8 load str located at -5091340175569095642
    xor64 r9, r8                                    r9 ^= r8   ///  r9 = r9.xor(r8)
    stxdw [r7+0x240], r9                    
    stxdw [r7+0x248], r6                    
    stxdw [r7+0x220], r0                    
    stxdw [r7+0x228], r5                    
    stxdw [r7+0x0], r4                      
    stxdw [r7+0x8], r3                      
    stxdw [r7+0x10], r2                     
    ldxdw r2, [r7+0x18]                     
    lddw r3, 0x962d6f3a7c115a2e                     r3 load str located at -7625316297088083410
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r7+0x18], r2                     
    ldxdw r2, [r7+0x250]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r7+0x250], r2                    
    ldxdw r1, [r7+0x20]                     
    lddw r2, 0x962a6f3d7c165a2f                     r2 load str located at -7626160709132985809
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    stxdw [r7+0x20], r1                     
    lddw r1, 0x962b6f3c7c175a28                     r1 load str located at -7625879238451176920
    ldxdw r2, [r7+0x28]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r7+0x28], r2                     
    lddw r1, 0x96286f3f7c145a29                     r1 load str located at -7626723650496603607
    ldxdw r2, [r7+0x30]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r7+0x30], r2                     
    lddw r1, 0x96296f3e7c155a2a                     r1 load str located at -7626442179814794710
    ldxdw r2, [r7+0x38]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r7+0x38], r2                     
    lddw r1, 0x96266f317c1a5a2b                     r1 load str located at -7627286660579173845
    ldxdw r2, [r7+0x40]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r7+0x40], r2                     
    lddw r1, 0x96276f307c1b5a24                     r1 load str located at -7627005189897364956
    ldxdw r2, [r7+0x48]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r7+0x48], r2                     
    lddw r1, 0x96246f337c185a25                     r1 load str located at -7627849601942791643
    ldxdw r2, [r7+0x50]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r7+0x50], r2                     
    lddw r1, 0x96256f327c195a26                     r1 load str located at -7627568131260982746
    ldxdw r2, [r7+0x58]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r7+0x58], r2                     
    lddw r1, 0x96226f357c1e5a27                     r1 load str located at -7628412543305885145
    ldxdw r2, [r7+0x60]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r7+0x60], r2                     
    lddw r1, 0x96236f347c1f5a20                     r1 load str located at -7628131072624076256
    ldxdw r2, [r7+0x68]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r7+0x68], r2                     
    lddw r1, 0x96206f377c1c5a21                     r1 load str located at -7628975484669502943
    ldxdw r2, [r7+0x70]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r7+0x70], r2                     
    lddw r1, 0x96216f367c1d5a22                     r1 load str located at -7628694013987694046
    ldxdw r2, [r7+0x78]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r7+0x78], r2                     
    lddw r1, 0x963e6f297c025a23                     r1 load str located at -7620531295499429341
    ldxdw r2, [r7+0x80]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r7+0x80], r2                     
    lddw r1, 0x963f6f287c035a3c                     r1 load str located at -7620249824817620420
    ldxdw r2, [r7+0x88]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r7+0x88], r2                     
    mov64 r1, r7                                    r1 = r7
    add64 r1, 144                                   r1 += 144   ///  r1 = r1.wrapping_add(144 as i32 as i64 as u64)
    stxdw [r10-0x128], r1                   
    call function_1369                      
    ldxdw r1, [r10-0xe8]                    
    ldxdw r2, [r1-0x1000]                   
    mov64 r1, r7                                    r1 = r7
    add64 r1, 416                                   r1 += 416   ///  r1 = r1.wrapping_add(416 as i32 as i64 as u64)
    mov64 r3, r7                                    r3 = r7
    add64 r3, 384                                   r3 += 384   ///  r3 = r3.wrapping_add(384 as i32 as i64 as u64)
    mov64 r4, r3                                    r4 = r3
    jne r2, 0, lbb_10928                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, r1                                    r4 = r1
lbb_10928:
    stxdw [r10-0x108], r4                   
    stxdw [r10-0xf8], r2                    
    jne r2, 0, lbb_10932                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, r3                                    r1 = r3
lbb_10932:
    stxdw [r10-0x110], r1                   
    lddw r4, 0x47d46c2877ac1406                     r4 load str located at 5175880792817800198
    ldxdw r3, [r7+0x308]                    
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    lddw r5, 0x47d56c2977ad1405                     r5 load str located at 5176162272089543685
    ldxdw r4, [r7+0x300]                    
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    lddw r0, 0x47d66c2a77ae1404                     r0 load str located at 5176443751361287172
    ldxdw r5, [r7+0x2f8]                    
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    lddw r2, 0x47d76c2b77af1403                     r2 load str located at 5176725230633030659
    ldxdw r0, [r7+0x2f0]                    
    xor64 r0, r2                                    r0 ^= r2   ///  r0 = r0.xor(r2)
    lddw r2, 0x47d06c2c77a81402                     r2 load str located at 5174754910090564610
    ldxdw r8, [r7+0x2e8]                    
    xor64 r8, r2                                    r8 ^= r2   ///  r8 = r8.xor(r2)
    lddw r2, 0x47d16c2d77a91401                     r2 load str located at 5175036389362308097
    ldxdw r9, [r7+0x2e0]                    
    xor64 r9, r2                                    r9 ^= r2   ///  r9 = r9.xor(r2)
    lddw r6, 0x47d26c2e77aa1400                     r6 load str located at 5175317868634051584
    ldxdw r2, [r7+0x2d8]                    
    xor64 r2, r6                                    r2 ^= r6   ///  r2 = r2.xor(r6)
    ldxdw r6, [r7+0x2d0]                    
    lddw r1, 0xb82c93d08854ebff                     r1 load str located at -5175599347905795073
    xor64 r6, r1                                    r6 ^= r1   ///  r6 = r6.xor(r1)
    stxdw [r7+0x2d0], r6                    
    stxdw [r7+0x2d8], r2                    
    stxdw [r7+0x2e0], r9                    
    stxdw [r7+0x2e8], r8                    
    stxdw [r7+0x2f0], r0                    
    stxdw [r7+0x2f8], r5                    
    stxdw [r7+0x300], r4                    
    stxdw [r7+0x308], r3                    
    lddw r2, 0x47db6c2777a31407                     r2 load str located at 5177851113359217671
    ldxdw r3, [r7+0x310]                    
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r7+0x310], r3                    
    lddw r2, 0x47da6c2677a21408                     r2 load str located at 5177569634087474184
    ldxdw r3, [r7+0x318]                    
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r7+0x318], r3                    
    lddw r3, 0xfb5ce87aae443c38                     r3 load str located at -334136658724897736
    ldxdw r2, [r7+0x1a0]                    
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r7+0x1a0], r2                    
    lddw r4, 0x4a2178451bac3c7                      r4 load str located at 333855179453154247
    ldxdw r2, [r7+0x1a8]                    
    xor64 r2, r4                                    r2 ^= r4   ///  r2 = r2.xor(r4)
    stxdw [r7+0x1a8], r2                    
    lddw r2, 0x4a1178751b9c3c6                      r2 load str located at 333573717361279942
    ldxdw r5, [r7+0x1b0]                    
    xor64 r5, r2                                    r5 ^= r2   ///  r5 = r5.xor(r2)
    stxdw [r7+0x1b0], r5                    
    lddw r5, 0x4a0178651b8c3c5                      r5 load str located at 333292238089536453
    ldxdw r0, [r7+0x1b8]                    
    xor64 r0, r5                                    r0 ^= r5   ///  r0 = r0.xor(r5)
    stxdw [r7+0x1b8], r0                    
    ldxdw r0, [r7+0x180]                    
    xor64 r0, r3                                    r0 ^= r3   ///  r0 = r0.xor(r3)
    stxdw [r7+0x180], r0                    
    ldxdw r3, [r7+0x188]                    
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    stxdw [r7+0x188], r3                    
    ldxdw r3, [r7+0x190]                    
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r7+0x190], r3                    
    ldxdw r2, [r7+0x198]                    
    xor64 r2, r5                                    r2 ^= r5   ///  r2 = r2.xor(r5)
    stxdw [r7+0x198], r2                    
    ldxdw r5, [r10-0x108]                   
    ldxdw r2, [r5+0x0]                      
    stxdw [r10-0x40], r2                    
    ldxdw r3, [r5+0x8]                      
    stxdw [r10-0x38], r3                    
    ldxdw r4, [r5+0x10]                     
    stxdw [r10-0x30], r4                    
    ldxdw r5, [r5+0x18]                     
    stxdw [r10-0x28], r5                    
    ldxdw r1, [r10-0x110]                   
    ldxdw r0, [r1+0x0]                      
    stxdw [r10-0x20], r0                    
    ldxdw r0, [r1+0x8]                      
    stxdw [r10-0x18], r0                    
    ldxdw r0, [r1+0x10]                     
    stxdw [r10-0x10], r0                    
    ldxdw r1, [r1+0x18]                     
    stxdw [r10-0x8], r1                     
    stxdw [r10-0xa0], r2                    
    stxdw [r10-0x98], r3                    
    stxdw [r10-0x90], r4                    
    stxdw [r10-0x88], r5                    
    ldxdw r1, [r10-0x20]                    
    stxdw [r10-0x80], r1                    
    ldxdw r1, [r10-0x18]                    
    stxdw [r10-0x78], r1                    
    ldxdw r1, [r10-0x10]                    
    stxdw [r10-0x70], r1                    
    ldxdw r1, [r10-0x8]                     
    stxdw [r10-0x68], r1                    
    ldxdw r9, [r10-0xf0]                    
    mov64 r6, r9                                    r6 = r9
    rsh64 r6, 16                                    r6 >>= 16   ///  r6 = r6.wrapping_shr(16)
    stxdw [r10-0x58], r6                    
    mov64 r8, r9                                    r8 = r9
    lsh64 r8, 48                                    r8 <<= 48   ///  r8 = r8.wrapping_shl(48)
    stxdw [r10-0x60], r8                    
    ldxdw r2, [r10-0xe8]                    
    ldxdw r1, [r2-0xfd8]                    
    stxdw [r10-0x108], r1                   
    ldxdw r1, [r2-0xfe0]                    
    stxdw [r10-0x130], r1                   
    ldxdw r1, [r2-0xfe8]                    
    stxdw [r10-0x110], r1                   
    ldxdw r1, [r2-0xff0]                    
    stxdw [r10-0x138], r1                   
    ldxdw r1, [r10-0xf8]                    
    jeq r1, 0, lbb_11092                            if r1 == (0 as i32 as i64 as u64) { pc += 29 }
    ldxdw r4, [r7+0x248]                    
    ldxdw r3, [r7+0x240]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -96                                   r2 += -96   ///  r2 = r2.wrapping_add(-96 as i32 as i64 as u64)
    call function_752                       
    ldxdw r1, [r10-0x20]                    
    jeq r1, 0, lbb_11209                            if r1 == (0 as i32 as i64 as u64) { pc += 137 }
    jslt r9, 0, lbb_11215                           if (r9 as i64) < (0 as i32 as i64) { pc += 142 }
    ldxdw r3, [r10-0x10]                    
    ldxdw r2, [r10-0x18]                    
    mov64 r4, r2                                    r4 = r2
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
    stxdw [r10-0x148], r8                   
    jeq r4, 0, lbb_11223                            if r4 == (0 as i32 as i64 as u64) { pc += 143 }
    mov64 r4, 471                                   r4 = 471 as i32 as i64 as u64
    jslt r3, 0, lbb_11238                           if (r3 as i64) < (0 as i32 as i64) { pc += 156 }
    lddw r5, 0xffffffffffff                         r5 load str located at 281474976710655
    jgt r3, r5, lbb_11216                           if r3 > r5 { pc += 131 }
    rsh64 r2, 48                                    r2 >>= 48   ///  r2 = r2.wrapping_shr(48)
    lsh64 r3, 16                                    r3 <<= 16   ///  r3 = r3.wrapping_shl(16)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r8, r9                                    r8 = r9
    mov64 r9, r2                                    r9 = r2
    jsgt r2, -1, lbb_11246                          if (r2 as i64) > (-1 as i32 as i64) { pc += 155 }
    ja lbb_11216                                    if true { pc += 124 }
lbb_11092:
    stxdw [r10-0xe8], r6                    
    ldxdw r6, [r7+0x240]                    
    mov64 r5, r6                                    r5 = r6
    arsh64 r5, 63                                   r5 >>= 63 (signed)   ///  r5 = (r5 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -192                                  r1 += -192   ///  r1 = r1.wrapping_add(-192 as i32 as i64 as u64)
    ldxdw r2, [r10-0xe8]                    
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    stxdw [r10-0x140], r5                   
    call function_17083                     
    lddw r1, 0xffff000000000000                     r1 load str located at -281474976710656
    mov64 r4, r8                                    r4 = r8
    and64 r4, r1                                    r4 &= r1   ///  r4 = r4.and(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -208                                  r1 += -208   ///  r1 = r1.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    ldxdw r9, [r7+0x248]                    
    mov64 r4, r8                                    r4 = r8
    arsh64 r8, 63                                   r8 >>= 63 (signed)   ///  r8 = (r8 as i64).wrapping_shr(63)
    mov64 r6, r9                                    r6 = r9
    arsh64 r6, 63                                   r6 >>= 63 (signed)   ///  r6 = (r6 as i64).wrapping_shr(63)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -224                                  r1 += -224   ///  r1 = r1.wrapping_add(-224 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, r6                                    r3 = r6
    stxdw [r10-0x148], r4                   
    mov64 r5, r8                                    r5 = r8
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r3, r6                                    r3 = r6
    ldxdw r6, [r10-0xe8]                    
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    and64 r8, r9                                    r8 &= r9   ///  r8 = r8.and(r9)
    ldxdw r2, [r10-0xd8]                    
    add64 r2, r8                                    r2 += r8   ///  r2 = r2.wrapping_add(r8)
    ldxdw r4, [r10-0xc8]                    
    ldxdw r1, [r10-0xc0]                    
    mov64 r3, r1                                    r3 = r1
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    ldxdw r0, [r10-0xe0]                    
    mov64 r8, r0                                    r8 = r0
    add64 r8, r3                                    r8 += r3   ///  r8 = r8.wrapping_add(r3)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r8, r0, lbb_11147                           if r8 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_11147:
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r3, r1, lbb_11151                           if r3 < r1 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_11151:
    ldxdw r1, [r10-0x140]                   
    and64 r1, r6                                    r1 &= r6   ///  r1 = r1.and(r6)
    ldxdw r3, [r10-0xb8]                    
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    add64 r3, r5                                    r3 += r5   ///  r3 = r3.wrapping_add(r5)
    ldxdw r1, [r10-0xb0]                    
    mov64 r0, r3                                    r0 = r3
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
    mov64 r1, r0                                    r1 = r0
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    ldxdw r9, [r10-0xf0]                    
    jlt r1, r0, lbb_11165                           if r1 < r0 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_11165:
    jlt r0, r3, lbb_11167                           if r0 < r3 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_11167:
    arsh64 r3, 63                                   r3 >>= 63 (signed)   ///  r3 = (r3 as i64).wrapping_shr(63)
    ldxdw r0, [r10-0xa8]                    
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    arsh64 r2, 63                                   r2 >>= 63 (signed)   ///  r2 = (r2 as i64).wrapping_shr(63)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    add64 r3, r5                                    r3 += r5   ///  r3 = r3.wrapping_add(r5)
    mov64 r2, r3                                    r2 = r3
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    mov64 r4, r1                                    r4 = r1
    rsh64 r4, 48                                    r4 >>= 48   ///  r4 = r4.wrapping_shr(48)
    or64 r4, r2                                     r4 |= r2   ///  r4 = r4.or(r2)
    arsh64 r3, 48                                   r3 >>= 48 (signed)   ///  r3 = (r3 as i64).wrapping_shr(48)
    lsh64 r1, 16                                    r1 <<= 16   ///  r1 = r1.wrapping_shl(16)
    mov64 r2, r1                                    r2 = r1
    arsh64 r2, 63                                   r2 >>= 63 (signed)   ///  r2 = (r2 as i64).wrapping_shr(63)
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    xor64 r4, r2                                    r4 ^= r2   ///  r4 = r4.xor(r2)
    or64 r4, r3                                     r4 |= r3   ///  r4 = r4.or(r3)
    jne r4, 0, lbb_11212                            if r4 != (0 as i32 as i64 as u64) { pc += 25 }
    mov64 r2, r8                                    r2 = r8
    rsh64 r2, 48                                    r2 >>= 48   ///  r2 = r2.wrapping_shr(48)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
    lsh64 r8, 16                                    r8 <<= 16   ///  r8 = r8.wrapping_shl(16)
    ldxdw r2, [r10-0xd0]                    
    rsh64 r2, 48                                    r2 >>= 48   ///  r2 = r2.wrapping_shr(48)
    or64 r8, r2                                     r8 |= r2   ///  r8 = r8.or(r2)
    mov64 r2, r8                                    r2 = r8
    or64 r2, r1                                     r2 |= r1   ///  r2 = r2.or(r1)
    jeq r2, 0, lbb_11219                            if r2 == (0 as i32 as i64 as u64) { pc += 22 }
    mov64 r2, 482                                   r2 = 482 as i32 as i64 as u64
    jslt r1, 0, lbb_11226                           if (r1 as i64) < (0 as i32 as i64) { pc += 27 }
    lddw r3, 0xffffffffffff                         r3 load str located at 281474976710655
    jgt r1, r3, lbb_11235                           if r1 > r3 { pc += 33 }
    rsh64 r8, 48                                    r8 >>= 48   ///  r8 = r8.wrapping_shr(48)
    lsh64 r1, 16                                    r1 <<= 16   ///  r1 = r1.wrapping_shl(16)
    or64 r8, r1                                     r8 |= r1   ///  r8 = r8.or(r1)
    jsle r8, -1, lbb_11235                          if (r8 as i64) <= (-1 as i32 as i64) { pc += 29 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jsgt r9, -1, lbb_11246                          if (r9 as i64) > (-1 as i32 as i64) { pc += 38 }
    ja lbb_11234                                    if true { pc += 25 }
lbb_11209:
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 464                       
    ja lbb_11510                                    if true { pc += 298 }
lbb_11212:
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 476                       
    ja lbb_11510                                    if true { pc += 295 }
lbb_11215:
    mov64 r4, 468                                   r4 = 468 as i32 as i64 as u64
lbb_11216:
    ldxdw r1, [r10-0x100]                   
    stxw [r1+0x8], r4                       
    ja lbb_11510                                    if true { pc += 291 }
lbb_11219:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jsgt r9, -1, lbb_11246                          if (r9 as i64) > (-1 as i32 as i64) { pc += 24 }
    ja lbb_11234                                    if true { pc += 11 }
lbb_11223:
    mov64 r8, r9                                    r8 = r9
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ja lbb_11246                                    if true { pc += 20 }
lbb_11226:
    lddw r3, 0xffff800000000000                     r3 load str located at -140737488355328
    jlt r1, r3, lbb_11235                           if r1 < r3 { pc += 6 }
    rsh64 r8, 48                                    r8 >>= 48   ///  r8 = r8.wrapping_shr(48)
    lsh64 r1, 16                                    r1 <<= 16   ///  r1 = r1.wrapping_shl(16)
    or64 r8, r1                                     r8 |= r1   ///  r8 = r8.or(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jsgt r9, -1, lbb_11246                          if (r9 as i64) > (-1 as i32 as i64) { pc += 12 }
lbb_11234:
    mov64 r2, 483                                   r2 = 483 as i32 as i64 as u64
lbb_11235:
    ldxdw r1, [r10-0x100]                   
    stxw [r1+0x8], r2                       
    ja lbb_11510                                    if true { pc += 272 }
lbb_11238:
    lddw r5, 0xffff800000000000                     r5 load str located at -140737488355328
    jlt r3, r5, lbb_11216                           if r3 < r5 { pc += -25 }
    rsh64 r2, 48                                    r2 >>= 48   ///  r2 = r2.wrapping_shr(48)
    lsh64 r3, 16                                    r3 <<= 16   ///  r3 = r3.wrapping_shl(16)
    or64 r2, r3                                     r2 |= r3   ///  r2 = r2.or(r3)
    mov64 r8, r9                                    r8 = r9
    mov64 r9, r2                                    r9 = r2
lbb_11246:
    stxdw [r10-0xf0], r9                    
    ldxdw r4, [r7+0x250]                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jsgt r4, 0, lbb_11262                           if (r4 as i64) > (0 as i32 as i64) { pc += 11 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r5, [r10-0x118]                   
    mov64 r9, r5                                    r9 = r5
    sub64 r9, r4                                    r9 -= r4   ///  r9 = r9.wrapping_sub(r4)
    jsge r9, r5, lbb_11266                          if (r9 as i64) >= (r5 as i64) { pc += 10 }
lbb_11256:
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jeq r2, 0, lbb_11270                            if r2 == (0 as i32 as i64 as u64) { pc += 11 }
lbb_11259:
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 489                       
    ja lbb_11510                                    if true { pc += 248 }
lbb_11262:
    ldxdw r5, [r10-0x118]                   
    mov64 r9, r5                                    r9 = r5
    sub64 r9, r4                                    r9 -= r4   ///  r9 = r9.wrapping_sub(r4)
    jslt r9, r5, lbb_11256                          if (r9 as i64) < (r5 as i64) { pc += -10 }
lbb_11266:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11259                            if r2 != (0 as i32 as i64 as u64) { pc += -11 }
lbb_11270:
    ldxdw r4, [r10-0xf0]                    
    mul64 r4, r1                                    r4 *= r1   ///  r4 = r4.wrapping_mul(r1)
    mov64 r5, r9                                    r5 = r9
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jslt r5, r9, lbb_11285                          if (r5 as i64) < (r9 as i64) { pc += 8 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jsge r4, 0, lbb_11286                           if (r4 as i64) >= (0 as i32 as i64) { pc += 7 }
lbb_11279:
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jeq r2, 0, lbb_11290                            if r2 == (0 as i32 as i64 as u64) { pc += 8 }
lbb_11282:
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 492                       
    ja lbb_11510                                    if true { pc += 225 }
lbb_11285:
    jslt r4, 0, lbb_11279                           if (r4 as i64) < (0 as i32 as i64) { pc += -7 }
lbb_11286:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11282                            if r2 != (0 as i32 as i64 as u64) { pc += -8 }
lbb_11290:
    stxdw [r10-0x140], r5                   
    mov64 r5, r8                                    r5 = r8
    stxdw [r10-0x150], r1                   
    mul64 r5, r1                                    r5 *= r1   ///  r5 = r5.wrapping_mul(r1)
    ldxdw r1, [r10-0x120]                   
    mov64 r3, r1                                    r3 = r1
    sub64 r3, r5                                    r3 -= r5   ///  r3 = r3.wrapping_sub(r5)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    stxdw [r10-0xe8], r3                    
    jslt r3, r1, lbb_11309                          if (r3 as i64) < (r1 as i64) { pc += 8 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jsle r5, 0, lbb_11310                           if (r5 as i64) <= (0 as i32 as i64) { pc += 7 }
lbb_11303:
    xor64 r2, r0                                    r2 ^= r0   ///  r2 = r2.xor(r0)
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jeq r2, 0, lbb_11314                            if r2 == (0 as i32 as i64 as u64) { pc += 8 }
lbb_11306:
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 496                       
    ja lbb_11510                                    if true { pc += 201 }
lbb_11309:
    jsgt r5, 0, lbb_11303                           if (r5 as i64) > (0 as i32 as i64) { pc += -7 }
lbb_11310:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    xor64 r2, r0                                    r2 ^= r0   ///  r2 = r2.xor(r0)
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11306                            if r2 != (0 as i32 as i64 as u64) { pc += -8 }
lbb_11314:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jslt r4, 0, lbb_11328                           if (r4 as i64) < (0 as i32 as i64) { pc += 11 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r2, [r10-0x118]                   
    mov64 r0, r2                                    r0 = r2
    add64 r0, r4                                    r0 += r4   ///  r0 = r0.wrapping_add(r4)
    jsge r0, r2, lbb_11332                          if (r0 as i64) >= (r2 as i64) { pc += 10 }
lbb_11322:
    xor64 r5, r1                                    r5 ^= r1   ///  r5 = r5.xor(r1)
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jeq r5, 0, lbb_11336                            if r5 == (0 as i32 as i64 as u64) { pc += 11 }
lbb_11325:
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 499                       
    ja lbb_11510                                    if true { pc += 182 }
lbb_11328:
    ldxdw r2, [r10-0x118]                   
    mov64 r0, r2                                    r0 = r2
    add64 r0, r4                                    r0 += r4   ///  r0 = r0.wrapping_add(r4)
    jslt r0, r2, lbb_11322                          if (r0 as i64) < (r2 as i64) { pc += -10 }
lbb_11332:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    xor64 r5, r1                                    r5 ^= r1   ///  r5 = r5.xor(r1)
    and64 r5, 1                                     r5 &= 1   ///  r5 = r5.and(1)
    jne r5, 0, lbb_11325                            if r5 != (0 as i32 as i64 as u64) { pc += -11 }
lbb_11336:
    mov64 r4, r7                                    r4 = r7
    add64 r4, 720                                   r4 += 720   ///  r4 = r4.wrapping_add(720 as i32 as i64 as u64)
    stxdw [r10-0x118], r4                   
    ldxdw r4, [r7+0x6b8]                    
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jle r4, 1, lbb_11365                            if r4 <= (1 as i32 as i64 as u64) { pc += 22 }
    ldxdw r4, [r7+0x240]                    
    ldxdw r5, [r7+0x248]                    
    ldxdw r1, [r10-0x150]                   
    stxdw [r10-0xff0], r1                   
    ldxdw r1, [r10-0x118]                   
    stxdw [r10-0xff8], r1                   
    stxdw [r10-0x1000], r5                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r0                                    r2 = r0
    ldxdw r3, [r10-0xe8]                    
    call function_7182                      
    ldxw r1, [r10-0x20]                     
    jeq r1, 0, lbb_11363                            if r1 == (0 as i32 as i64 as u64) { pc += 5 }
lbb_11358:
    ldxdw r1, [r10-0x1c]                    
    ldxdw r2, [r10-0x100]                   
    stxdw [r2+0x4], r1                      
    stw [r2+0x0], 1                         
    ja lbb_11511                                    if true { pc += 148 }
lbb_11363:
    ldxdw r1, [r10-0x10]                    
    ldxdw r5, [r10-0x18]                    
lbb_11365:
    mov64 r0, r1                                    r0 = r1
    mov64 r1, r9                                    r1 = r9
    arsh64 r1, 63                                   r1 >>= 63 (signed)   ///  r1 = (r1 as i64).wrapping_shr(63)
    xor64 r9, r1                                    r9 ^= r1   ///  r9 = r9.xor(r1)
    sub64 r9, r1                                    r9 -= r1   ///  r9 = r9.wrapping_sub(r1)
    ldxdw r4, [r10-0x140]                   
    mov64 r1, r4                                    r1 = r4
    arsh64 r1, 63                                   r1 >>= 63 (signed)   ///  r1 = (r1 as i64).wrapping_shr(63)
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    sub64 r4, r1                                    r4 -= r1   ///  r4 = r4.wrapping_sub(r1)
    mov64 r1, r4                                    r1 = r4
    sub64 r1, r9                                    r1 -= r9   ///  r1 = r1.wrapping_sub(r9)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jslt r1, r4, lbb_11388                          if (r1 as i64) < (r4 as i64) { pc += 8 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jsle r9, 0, lbb_11389                           if (r9 as i64) <= (0 as i32 as i64) { pc += 7 }
lbb_11382:
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jeq r2, 0, lbb_11393                            if r2 == (0 as i32 as i64 as u64) { pc += 8 }
lbb_11385:
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 517                       
    ja lbb_11510                                    if true { pc += 122 }
lbb_11388:
    jsgt r9, 0, lbb_11382                           if (r9 as i64) > (0 as i32 as i64) { pc += -7 }
lbb_11389:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jne r2, 0, lbb_11385                            if r2 != (0 as i32 as i64 as u64) { pc += -8 }
lbb_11393:
    stxdw [r10-0x140], r4                   
    stxdw [r10-0x150], r5                   
    stxdw [r10-0x120], r0                   
    mov64 r2, r7                                    r2 = r7
    call function_6692                      
    jeq r0, 0, lbb_11508                            if r0 == (0 as i32 as i64 as u64) { pc += 109 }
    jsle r8, -1, lbb_11512                          if (r8 as i64) <= (-1 as i32 as i64) { pc += 112 }
    stxdw [r10-0xe8], r6                    
    mov64 r6, r0                                    r6 = r0
    mov64 r1, r8                                    r1 = r8
    ldxdw r2, [r10-0x128]                   
    call function_6712                      
    jeq r0, 0, lbb_11514                            if r0 == (0 as i32 as i64 as u64) { pc += 108 }
    stxdw [r10-0x158], r0                   
    ldxdw r1, [r7+0x220]                    
    ldxdw r2, [r7+0x228]                    
    stxdw [r10-0xff0], r6                   
    stxdw [r10-0xff8], r2                   
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    mov64 r2, r9                                    r2 = r9
    ldxdw r3, [r10-0x140]                   
    ldxdw r6, [r10-0xf0]                    
    mov64 r4, r6                                    r4 = r6
    call function_7220                      
    ldxw r1, [r10-0x20]                     
    jne r1, 0, lbb_11358                            if r1 != (0 as i32 as i64 as u64) { pc += -64 }
    mov64 r2, r6                                    r2 = r6
    ldxdw r9, [r10-0x10]                    
    ldxdw r6, [r10-0x18]                    
    ldxdw r4, [r7+0x228]                    
    ldxdw r3, [r7+0x220]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    ldxdw r5, [r10-0x158]                   
    call function_8904                      
    ldxw r1, [r10-0x20]                     
    jne r1, 0, lbb_11358                            if r1 != (0 as i32 as i64 as u64) { pc += -75 }
    ldxdw r1, [r10-0x10]                    
    stxdw [r10-0xf0], r1                    
    ldxdw r1, [r10-0x18]                    
    stxdw [r10-0x140], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -160                                  r1 += -160   ///  r1 = r1.wrapping_add(-160 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    ldxdw r2, [r10-0x130]                   
    ldxdw r3, [r10-0x108]                   
    ldxdw r4, [r10-0x110]                   
    call function_9648                      
    ldxw r1, [r10-0x20]                     
    jne r1, 0, lbb_11358                            if r1 != (0 as i32 as i64 as u64) { pc += -94 }
    ldxdw r2, [r10-0x150]                   
    mov64 r1, r2                                    r1 = r2
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r1, r2, lbb_11458                           if r1 < r2 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_11458:
    ldxdw r4, [r10-0x120]                   
    mov64 r2, r4                                    r2 = r4
    add64 r2, r9                                    r2 += r9   ///  r2 = r2.wrapping_add(r9)
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    mov64 r3, r4                                    r3 = r4
    xor64 r3, r9                                    r3 ^= r9   ///  r3 = r3.xor(r9)
    xor64 r4, r2                                    r4 ^= r2   ///  r4 = r4.xor(r2)
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    and64 r3, r4                                    r3 &= r4   ///  r3 = r3.and(r4)
    mov64 r5, 542                                   r5 = 542 as i32 as i64 as u64
    jslt r3, 0, lbb_11505                           if (r3 as i64) < (0 as i32 as i64) { pc += 36 }
    stxdw [r10-0x108], r6                   
    mov64 r4, r1                                    r4 = r1
    ldxdw r3, [r10-0x140]                   
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r4, r1, lbb_11476                           if r4 < r1 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_11476:
    mov64 r1, r2                                    r1 = r2
    ldxdw r5, [r10-0xf0]                    
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    mov64 r3, r2                                    r3 = r2
    xor64 r3, r5                                    r3 ^= r5   ///  r3 = r3.xor(r5)
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    xor64 r3, -1                                    r3 ^= -1   ///  r3 = r3.xor(-1)
    and64 r3, r2                                    r3 &= r2   ///  r3 = r3.and(r2)
    mov64 r5, 544                                   r5 = 544 as i32 as i64 as u64
    jslt r3, 0, lbb_11505                           if (r3 as i64) < (0 as i32 as i64) { pc += 18 }
    ldxdw r6, [r10-0x10]                    
    ldxdw r2, [r10-0x18]                    
    mov64 r3, r4                                    r3 = r4
    stxdw [r10-0x110], r2                   
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r3, r4, lbb_11495                           if r3 < r4 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_11495:
    mov64 r2, r1                                    r2 = r1
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    add64 r2, r5                                    r2 += r5   ///  r2 = r2.wrapping_add(r5)
    mov64 r4, r1                                    r4 = r1
    xor64 r4, r6                                    r4 ^= r6   ///  r4 = r4.xor(r6)
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    and64 r4, r1                                    r4 &= r1   ///  r4 = r4.and(r1)
    mov64 r5, 546                                   r5 = 546 as i32 as i64 as u64
    jsge r4, 0, lbb_11519                           if (r4 as i64) >= (0 as i32 as i64) { pc += 14 }
lbb_11505:
    ldxdw r1, [r10-0x100]                   
    stxw [r1+0x8], r5                       
    ja lbb_11510                                    if true { pc += 2 }
lbb_11508:
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 782065                    
lbb_11510:
    stdw [r1+0x0], 1                        
lbb_11511:
    exit                                    
lbb_11512:
    mov64 r1, 521                                   r1 = 521 as i32 as i64 as u64
    ja lbb_11515                                    if true { pc += 1 }
lbb_11514:
    mov64 r1, 782066                                r1 = 782066 as i32 as i64 as u64
lbb_11515:
    ldxdw r2, [r10-0x100]                   
    stxw [r2+0x8], r1                       
    stdw [r2+0x0], 1                        
    ja lbb_11511                                    if true { pc += -8 }
lbb_11519:
    stxdw [r10-0x50], r3                    
    stxdw [r10-0x48], r2                    
    ldxdw r1, [r7+0x6b8]                    
    jeq r1, 0, lbb_11545                            if r1 == (0 as i32 as i64 as u64) { pc += 22 }
    ldxdw r4, [r10-0x138]                   
    jne r4, 0, lbb_11545                            if r4 != (0 as i32 as i64 as u64) { pc += 20 }
    ldxdw r4, [r7+0x2c0]                    
    xor64 r4, 59403                                 r4 ^= 59403   ///  r4 = r4.xor(59403)
    lsh64 r4, 48                                    r4 <<= 48   ///  r4 = r4.wrapping_shl(48)
    mov64 r5, r3                                    r5 = r3
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    stxdw [r10-0x138], r5                   
    jlt r5, r3, lbb_11534                           if r5 < r3 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_11534:
    mov64 r5, r2                                    r5 = r2
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    mov64 r3, r2                                    r3 = r2
    stxdw [r10-0x130], r5                   
    xor64 r3, r5                                    r3 ^= r5   ///  r3 = r3.xor(r5)
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    and64 r2, r3                                    r2 &= r3   ///  r2 = r2.and(r3)
    jsge r2, 0, lbb_11548                           if (r2 as i64) >= (0 as i32 as i64) { pc += 6 }
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 557                       
    ja lbb_11510                                    if true { pc += -35 }
lbb_11545:
    stxdw [r10-0x138], r3                   
    stxdw [r10-0x130], r2                   
    ja lbb_11552                                    if true { pc += 4 }
lbb_11548:
    ldxdw r2, [r10-0x138]                   
    stxdw [r10-0x50], r2                    
    ldxdw r2, [r10-0x130]                   
    stxdw [r10-0x48], r2                    
lbb_11552:
    jle r1, 2, lbb_11619                            if r1 <= (2 as i32 as i64 as u64) { pc += 66 }
    ldxdw r3, [r7+0x320]                    
    lddw r1, 0x504156a22548f8dd                     r1 load str located at 5782998650930657501
    xor64 r3, r1                                    r3 ^= r1   ///  r3 = r3.xor(r1)
    mov64 r2, r3                                    r2 = r3
    lsh64 r2, 48                                    r2 <<= 48   ///  r2 = r2.wrapping_shl(48)
    rsh64 r3, 16                                    r3 >>= 16   ///  r3 = r3.wrapping_shr(16)
    ldxdw r1, [r7+0x2c8]                    
    stxdw [r10-0x158], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    lddw r4, 0x4189374bc7                           r4 load str located at 281474976711
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_120                       
    ldxdw r1, [r10-0x20]                    
    jeq r1, 0, lbb_11649                            if r1 == (0 as i32 as i64 as u64) { pc += 79 }
    ldxdw r1, [r10-0x10]                    
    stxdw [r10-0x160], r1                   
    ldxdw r1, [r10-0x18]                    
    stxdw [r10-0x168], r1                   
    lddw r1, 0xd3198133b7c1776c                     r1 load str located at -3235412798162765972
    ldxdw r5, [r10-0x158]                   
    xor64 r5, r1                                    r5 ^= r1   ///  r5 = r5.xor(r1)
    mov64 r4, r5                                    r4 = r5
    lsh64 r4, 48                                    r4 <<= 48   ///  r4 = r4.wrapping_shl(48)
    rsh64 r5, 16                                    r5 >>= 16   ///  r5 = r5.wrapping_shr(16)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    ldxdw r2, [r10-0x138]                   
    ldxdw r3, [r10-0x130]                   
    call function_120                       
    ldxdw r1, [r10-0x20]                    
    jeq r1, 0, lbb_11655                            if r1 == (0 as i32 as i64 as u64) { pc += 67 }
    ldxdw r3, [r10-0x10]                    
    ldxdw r2, [r10-0x18]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    lddw r4, 0x4189374bc7                           r4 load str located at 281474976711
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_120                       
    ldxdw r1, [r10-0x40]                    
    jeq r1, 0, lbb_11661                            if r1 == (0 as i32 as i64 as u64) { pc += 63 }
    ldxdw r1, [r10-0x30]                    
    ldxdw r3, [r10-0x38]                    
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r5, [r10-0x138]                   
    jgt r3, r5, lbb_11664                           if r3 > r5 { pc += 60 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    ldxdw r0, [r10-0x130]                   
    jsle r1, r0, lbb_11667                          if (r1 as i64) <= (r0 as i64) { pc += 59 }
lbb_11608:
    ldxdw r0, [r10-0x130]                   
    jeq r1, r0, lbb_11670                           if r1 == r0 { pc += 60 }
lbb_11610:
    mov64 r4, r5                                    r4 = r5
    jeq r4, 0, lbb_11671                            if r4 == (0 as i32 as i64 as u64) { pc += 59 }
lbb_11612:
    jne r4, 0, lbb_11673                            if r4 != (0 as i32 as i64 as u64) { pc += 60 }
lbb_11613:
    ldxdw r3, [r10-0x138]                   
    mov64 r4, r3                                    r4 = r3
    ldxdw r5, [r10-0x168]                   
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    jge r4, r3, lbb_11677                           if r4 >= r3 { pc += 59 }
    ja lbb_11678                                    if true { pc += 59 }
lbb_11619:
    jeq r1, 0, lbb_11693                            if r1 == (0 as i32 as i64 as u64) { pc += 73 }
    ldxdw r3, [r7+0x2c8]                    
    lddw r1, 0xd3198133b7c1776c                     r1 load str located at -3235412798162765972
    xor64 r3, r1                                    r3 ^= r1   ///  r3 = r3.xor(r1)
    mov64 r2, r3                                    r2 = r3
    lsh64 r2, 48                                    r2 <<= 48   ///  r2 = r2.wrapping_shl(48)
    rsh64 r3, 16                                    r3 >>= 16   ///  r3 = r3.wrapping_shr(16)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    lddw r4, 0x4189374bc7                           r4 load str located at 281474976711
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_120                       
    ldxdw r1, [r10-0x20]                    
    jne r1, 1, lbb_11652                            if r1 != (1 as i32 as i64 as u64) { pc += 17 }
    ldxdw r5, [r10-0x10]                    
    ldxdw r4, [r10-0x18]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    ldxdw r2, [r10-0x138]                   
    ldxdw r3, [r10-0x130]                   
    call function_120                       
    ldxdw r1, [r10-0x40]                    
    jeq r1, 0, lbb_11658                            if r1 == (0 as i32 as i64 as u64) { pc += 14 }
    ldxdw r1, [r10-0x30]                    
    ldxdw r2, [r10-0x38]                    
    stxdw [r10-0x50], r2                    
    stxdw [r10-0x48], r1                    
    ja lbb_11693                                    if true { pc += 44 }
lbb_11649:
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 576                       
    ja lbb_11510                                    if true { pc += -142 }
lbb_11652:
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 597                       
    ja lbb_11510                                    if true { pc += -145 }
lbb_11655:
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 579                       
    ja lbb_11510                                    if true { pc += -148 }
lbb_11658:
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 599                       
    ja lbb_11510                                    if true { pc += -151 }
lbb_11661:
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 581                       
    ja lbb_11510                                    if true { pc += -154 }
lbb_11664:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    ldxdw r0, [r10-0x130]                   
    jsgt r1, r0, lbb_11608                          if (r1 as i64) > (r0 as i64) { pc += -59 }
lbb_11667:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r0, [r10-0x130]                   
    jne r1, r0, lbb_11610                           if r1 != r0 { pc += -60 }
lbb_11670:
    jne r4, 0, lbb_11612                            if r4 != (0 as i32 as i64 as u64) { pc += -59 }
lbb_11671:
    ldxdw r1, [r10-0x130]                   
    jeq r4, 0, lbb_11613                            if r4 == (0 as i32 as i64 as u64) { pc += -60 }
lbb_11673:
    mov64 r4, r3                                    r4 = r3
    ldxdw r5, [r10-0x168]                   
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    jlt r4, r3, lbb_11678                           if r4 < r3 { pc += 1 }
lbb_11677:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_11678:
    mov64 r3, r1                                    r3 = r1
    ldxdw r5, [r10-0x160]                   
    add64 r3, r5                                    r3 += r5   ///  r3 = r3.wrapping_add(r5)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    mov64 r2, r1                                    r2 = r1
    xor64 r2, r5                                    r2 ^= r5   ///  r2 = r2.xor(r5)
    xor64 r1, r3                                    r1 ^= r3   ///  r1 = r1.xor(r3)
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    jsge r2, 0, lbb_11691                           if (r2 as i64) >= (0 as i32 as i64) { pc += 3 }
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 589                       
    ja lbb_11510                                    if true { pc += -181 }
lbb_11691:
    stxdw [r10-0x50], r4                    
    stxdw [r10-0x48], r3                    
lbb_11693:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -80                                   r2 += -80   ///  r2 = r2.wrapping_add(-80 as i32 as i64 as u64)
    lddw r3, 0x2710000000000000                     r3 load str located at 2814749767106560000
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_878                       
    ldxdw r1, [r10-0x20]                    
    jeq r1, 0, lbb_11717                            if r1 == (0 as i32 as i64 as u64) { pc += 14 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    ldxdw r2, [r10-0x110]                   
    jne r2, 0, lbb_11720                            if r2 != (0 as i32 as i64 as u64) { pc += 13 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jsle r6, 0, lbb_11721                           if (r6 as i64) <= (0 as i32 as i64) { pc += 12 }
lbb_11709:
    jeq r6, 0, lbb_11723                            if r6 == (0 as i32 as i64 as u64) { pc += 13 }
lbb_11710:
    mov64 r3, r1                                    r3 = r1
    lddw r1, 0x100023114 --> b"\xf0\x9f\xa6\x90"        r1 load str located at 4295110932
    mov64 r2, 4                                     r2 = 4 as i32 as i64 as u64
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_11728                            if r3 != (0 as i32 as i64 as u64) { pc += 12 }
    ja lbb_11856                                    if true { pc += 139 }
lbb_11717:
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 604                       
    ja lbb_11510                                    if true { pc += -210 }
lbb_11720:
    jsgt r6, 0, lbb_11709                           if (r6 as i64) > (0 as i32 as i64) { pc += -12 }
lbb_11721:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jne r6, 0, lbb_11710                            if r6 != (0 as i32 as i64 as u64) { pc += -13 }
lbb_11723:
    lddw r1, 0x100023114 --> b"\xf0\x9f\xa6\x90"        r1 load str located at 4295110932
    mov64 r2, 4                                     r2 = 4 as i32 as i64 as u64
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jeq r3, 0, lbb_11856                            if r3 == (0 as i32 as i64 as u64) { pc += 128 }
lbb_11728:
    ldxdw r3, [r10-0x10]                    
    stxdw [r10-0x130], r3                   
    ldxdw r6, [r10-0x18]                    
    syscall [invalid]                       
    ldxdw r2, [r7+0x6b8]                    
    lddw r1, 0x6e9de2b30b19f1ea                     r1 load str located at 7970776174128919018
    jlt r2, 4, lbb_11738                            if r2 < (4 as i32 as i64 as u64) { pc += 2 }
    lddw r1, 0x6e9de2b30b19f9ea                     r1 load str located at 7970776174128921066
lbb_11738:
    ldxdw r2, [r7+0x220]                    
    lddw r3, 0xb957ed15dc877c26                     r3 load str located at -5091340175569093594
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r7+0x220], r2                    
    ldxdw r2, [r7+0x228]                    
    lddw r3, 0x46a912eb237873d9                     r3 load str located at 5091621654840767449
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r7+0x228], r2                    
    ldxdw r2, [r7+0x250]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r7+0x250], r2                    
    mov64 r1, r7                                    r1 = r7
    call function_1176                      
    ldxdw r1, [r10-0x128]                   
    call function_1369                      
    ldxdw r1, [r10-0x118]                   
    call function_1318                      
    lddw r1, 0x1000000000000                        r1 load str located at 281474976710656
    mov64 r2, r6                                    r2 = r6
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    stxdw [r10-0x110], r1                   
    stxdw [r10-0x128], r2                   
    jlt r2, r6, lbb_11767                           if r2 < r6 { pc += 2 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x110], r1                   
lbb_11767:
    lddw r4, 0x4a0178651b8c3c5                      r4 load str located at 333292238089536453
    ldxdw r1, [r7+0x198]                    
    xor64 r1, r4                                    r1 ^= r4   ///  r1 = r1.xor(r4)
    stxdw [r10-0x118], r1                   
    ldxdw r3, [r7+0x1b8]                    
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    lddw r5, 0x4a1178751b9c3c6                      r5 load str located at 333573717361279942
    ldxdw r4, [r7+0x190]                    
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r0, [r7+0x1b0]                    
    xor64 r0, r5                                    r0 ^= r5   ///  r0 = r0.xor(r5)
    lddw r5, 0x4a2178451bac3c7                      r5 load str located at 333855179453154247
    ldxdw r6, [r7+0x188]                    
    xor64 r6, r5                                    r6 ^= r5   ///  r6 = r6.xor(r5)
    ldxdw r1, [r7+0x1a8]                    
    xor64 r1, r5                                    r1 ^= r5   ///  r1 = r1.xor(r5)
    lddw r5, 0xfb5ce87aae443c38                     r5 load str located at -334136658724897736
    ldxdw r8, [r7+0x180]                    
    xor64 r8, r5                                    r8 ^= r5   ///  r8 = r8.xor(r5)
    ldxdw r5, [r7+0x1a0]                    
    lddw r2, 0xfb5ce87aae443c38                     r2 load str located at -334136658724897736
    xor64 r5, r2                                    r5 ^= r2   ///  r5 = r5.xor(r2)
    stxdw [r7+0x1a0], r5                    
    stxdw [r7+0x1a8], r1                    
    stxdw [r7+0x1b0], r0                    
    stxdw [r7+0x1b8], r3                    
    stxdw [r7+0x180], r8                    
    stxdw [r7+0x188], r6                    
    stxdw [r7+0x190], r4                    
    ldxdw r1, [r10-0x118]                   
    stxdw [r7+0x198], r1                    
    ldxdw r2, [r10-0x130]                   
    mov64 r6, r2                                    r6 = r2
    ldxdw r1, [r10-0x110]                   
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    mov64 r1, r2                                    r1 = r2
    xor64 r1, r6                                    r1 ^= r6   ///  r1 = r1.xor(r6)
    xor64 r2, -1                                    r2 ^= -1   ///  r2 = r2.xor(-1)
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    jsge r2, 0, lbb_11815                           if (r2 as i64) >= (0 as i32 as i64) { pc += 3 }
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 644                       
    ja lbb_11510                                    if true { pc += -305 }
lbb_11815:
    ldxdw r1, [r10-0xf8]                    
    jeq r1, 0, lbb_11874                            if r1 == (0 as i32 as i64 as u64) { pc += 57 }
    ldxdw r3, [r7+0x248]                    
    ldxdw r2, [r7+0x240]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    ldxdw r4, [r10-0x128]                   
    mov64 r5, r6                                    r5 = r6
    call function_120                       
    ldxdw r1, [r10-0x20]                    
    jeq r1, 0, lbb_11957                            if r1 == (0 as i32 as i64 as u64) { pc += 131 }
    ldxdw r4, [r10-0x10]                    
    ldxdw r3, [r10-0x18]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -96                                   r2 += -96   ///  r2 = r2.wrapping_add(-96 as i32 as i64 as u64)
    call function_752                       
    ldxdw r1, [r10-0x20]                    
    jeq r1, 0, lbb_11964                            if r1 == (0 as i32 as i64 as u64) { pc += 129 }
    ldxdw r2, [r10-0x10]                    
    ldxdw r1, [r10-0x18]                    
    ldxdw r3, [r7+0x240]                    
    lddw r4, 0xb957ed15dc877426                     r4 load str located at -5091340175569095642
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    stxdw [r7+0x240], r3                    
    ldxdw r3, [r7+0x248]                    
    lddw r4, 0x46a912eb23798bd9                     r4 load str located at 5091621654840839129
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    stxdw [r7+0x248], r3                    
    mov64 r3, r1                                    r3 = r1
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    jeq r3, 0, lbb_11967                            if r3 == (0 as i32 as i64 as u64) { pc += 117 }
    lddw r3, 0xffffffffffff                         r3 load str located at 281474976710655
    jle r2, r3, lbb_11969                           if r2 <= r3 { pc += 116 }
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 698                       
    ja lbb_11510                                    if true { pc += -346 }
lbb_11856:
    lddw r1, 0xe8d4a50fff                           r1 load str located at 999999999999
    jle r8, r1, lbb_11920                           if r8 <= r1 { pc += 61 }
    lddw r1, 0xe8d4a51000                           r1 load str located at 1000000000000
    div64 r8, r1                                    r8 /= r1   ///  r8 = r8 / r1
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    jge r8, 19, lbb_11960                           if r8 >= (19 as i32 as i64 as u64) { pc += 96 }
    mov64 r1, r8                                    r1 = r8
    lsh64 r1, 3                                     r1 <<= 3   ///  r1 = r1.wrapping_shl(3)
    lddw r2, 0x100023b08 --> b"\x00\x00\x00\x00\x101\x02\x00\x00\x00\x00\x00<1\x02\x00\x00\x00\x00\x00D1…        r2 load str located at 4295113480
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    lsh64 r8, 2                                     r8 <<= 2   ///  r8 = r8.wrapping_shl(2)
    add64 r8, 4                                     r8 += 4   ///  r8 = r8.wrapping_add(4 as i32 as i64 as u64)
    ldxdw r1, [r2+0x0]                      
    mov64 r2, r8                                    r2 = r8
    ja lbb_11728                                    if true { pc += -146 }
lbb_11874:
    ldxdw r5, [r7+0x248]                    
    ldxdw r4, [r7+0x240]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    ldxdw r2, [r10-0x148]                   
    ldxdw r3, [r10-0xe8]                    
    call function_120                       
    mov64 r3, 653                                   r3 = 653 as i32 as i64 as u64
    ldxdw r1, [r10-0x20]                    
    jeq r1, 0, lbb_11917                            if r1 == (0 as i32 as i64 as u64) { pc += 33 }
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x18]                    
    stxdw [r10-0x40], r2                    
    stxdw [r10-0x38], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    ldxdw r3, [r10-0x128]                   
    mov64 r4, r6                                    r4 = r6
    call function_752                       
    mov64 r3, 658                                   r3 = 658 as i32 as i64 as u64
    ldxdw r1, [r10-0x20]                    
    jne r1, 1, lbb_11917                            if r1 != (1 as i32 as i64 as u64) { pc += 19 }
    ldxdw r2, [r10-0x10]                    
    ldxdw r1, [r10-0x18]                    
    ldxdw r3, [r7+0x240]                    
    lddw r4, 0xb957ed15dc877426                     r4 load str located at -5091340175569095642
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    stxdw [r7+0x240], r3                    
    ldxdw r3, [r7+0x248]                    
    lddw r4, 0x46a912eb23798bd9                     r4 load str located at 5091621654840839129
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    stxdw [r7+0x248], r3                    
    mov64 r3, r1                                    r3 = r1
    or64 r3, r2                                     r3 |= r2   ///  r3 = r3.or(r2)
    jeq r3, 0, lbb_11967                            if r3 == (0 as i32 as i64 as u64) { pc += 54 }
    mov64 r3, 668                                   r3 = 668 as i32 as i64 as u64
    lddw r4, 0xffffffffffff                         r4 load str located at 281474976710655
    jle r2, r4, lbb_11969                           if r2 <= r4 { pc += 52 }
lbb_11917:
    ldxdw r1, [r10-0x100]                   
    stxw [r1+0x8], r3                       
    ja lbb_11510                                    if true { pc += -410 }
lbb_11920:
    lddw r1, 0x100023124 --> b"\xf0\x9f\x90\xb2\xf0\x9f\xa6\x88\xf0\x9f\x90\xa0\xf0\x9f\x90\xac    \xf0\…        r1 load str located at 4295110948
    lddw r3, 0x746a5287ff                           r3 load str located at 499999999999
    jgt r8, r3, lbb_11728                           if r8 > r3 { pc += -197 }
    lddw r1, 0x100023118 --> b"\xf0\x9f\x90\x8b\xf0\x9f\xa6\x80\xf0\x9f\x90\xb3\xf0\x9f\x90\xb2\xf0\x9f\…        r1 load str located at 4295110936
    lddw r3, 0x3a352943ff                           r3 load str located at 249999999999
    jgt r8, r3, lbb_11728                           if r8 > r3 { pc += -202 }
    lddw r1, 0x100023120 --> b"\xf0\x9f\x90\xb3\xf0\x9f\x90\xb2\xf0\x9f\xa6\x88\xf0\x9f\x90\xa0\xf0\x9f\…        r1 load str located at 4295110944
    lddw r3, 0x174876e7ff                           r3 load str located at 99999999999
    jgt r8, r3, lbb_11728                           if r8 > r3 { pc += -207 }
    lddw r1, 0x100023130 --> b"\xf0\x9f\x90\xac    \xf0\x9f\x90\x9f\xf0\x9f\x90\x89\xf0\x9f\x90\x89\xf0\…        r1 load str located at 4295110960
    lddw r3, 0x5d21db9ff                            r3 load str located at 24999999999
    jgt r8, r3, lbb_11728                           if r8 > r3 { pc += -212 }
    lddw r1, 0x100023128 --> b"\xf0\x9f\xa6\x88\xf0\x9f\x90\xa0\xf0\x9f\x90\xac    \xf0\x9f\x90\x9f\xf0\…        r1 load str located at 4295110952
    lddw r3, 0x2540be3ff                            r3 load str located at 9999999999
    jgt r8, r3, lbb_11728                           if r8 > r3 { pc += -217 }
    lddw r1, 0x100023138 --> b"\xf0\x9f\x90\x9f\xf0\x9f\x90\x89\xf0\x9f\x90\x89\xf0\x9f\x90\x89\xf0\x9f\…        r1 load str located at 4295110968
    jgt r8, 999999999, lbb_11728                    if r8 > (999999999 as i32 as i64 as u64) { pc += -220 }
    lddw r1, 0x10002312c --> b"\xf0\x9f\x90\xa0\xf0\x9f\x90\xac    \xf0\x9f\x90\x9f\xf0\x9f\x90\x89\xf0\…        r1 load str located at 4295110956
    jgt r8, 99999999, lbb_11728                     if r8 > (99999999 as i32 as i64 as u64) { pc += -223 }
    lddw r1, 0x10002311c --> b"\xf0\x9f\xa6\x80\xf0\x9f\x90\xb3\xf0\x9f\x90\xb2\xf0\x9f\xa6\x88\xf0\x9f\…        r1 load str located at 4295110940
    jgt r8, 24999999, lbb_11728                     if r8 > (24999999 as i32 as i64 as u64) { pc += -226 }
    lddw r1, 0x100023114 --> b"\xf0\x9f\xa6\x90\xf0\x9f\x90\x8b\xf0\x9f\xa6\x80\xf0\x9f\x90\xb3\xf0\x9f\…        r1 load str located at 4295110932
    ja lbb_11728                                    if true { pc += -229 }
lbb_11957:
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 683                       
    ja lbb_11510                                    if true { pc += -450 }
lbb_11960:
    lddw r1, 0x100023430 --> b"\xf0\x9f\x90\x89\xf0\x9f\x90\x89\xf0\x9f\x90\x89\xf0\x9f\x90\x89\xf0\x9f\…        r1 load str located at 4295111728
    mov64 r2, 80                                    r2 = 80 as i32 as i64 as u64
    ja lbb_11728                                    if true { pc += -236 }
lbb_11964:
    ldxdw r1, [r10-0x100]                   
    stw [r1+0x8], 688                       
    ja lbb_11510                                    if true { pc += -457 }
lbb_11967:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ja lbb_11972                                    if true { pc += 3 }
lbb_11969:
    rsh64 r1, 48                                    r1 >>= 48   ///  r1 = r1.wrapping_shr(48)
    lsh64 r2, 16                                    r2 <<= 16   ///  r2 = r2.wrapping_shl(16)
    or64 r1, r2                                     r1 |= r2   ///  r1 = r1.or(r2)
lbb_11972:
    ldxdw r2, [r10-0x100]                   
    ldxdw r3, [r10-0x150]                   
    stxdw [r2+0x28], r3                     
    ldxdw r3, [r10-0x108]                   
    stxdw [r2+0x18], r3                     
    ldxdw r3, [r10-0x140]                   
    stxdw [r2+0x8], r3                      
    stxdw [r2+0x38], r1                     
    ldxdw r1, [r10-0x120]                   
    stxdw [r2+0x30], r1                     
    stxdw [r2+0x20], r9                     
    ldxdw r1, [r10-0xf0]                    
    stxdw [r2+0x10], r1                     
    stw [r2+0x0], 0                         
    ja lbb_11511                                    if true { pc += -476 }

entrypoint:
    mov64 r7, r1                                    r7 = r1
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r9, [r1+0x0]                      
    jeq r9, 0, lbb_12164                            if r9 == (0 as i32 as i64 as u64) { pc += 173 }
    stxdw [r10-0x820], r7                   
    ldxdw r2, [r1+0x58]                     
    mov64 r7, r1                                    r7 = r1
    add64 r7, r2                                    r7 += r2   ///  r7 = r7.wrapping_add(r2)
    add64 r7, 10351                                 r7 += 10351   ///  r7 = r7.wrapping_add(10351 as i32 as i64 as u64)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
    jeq r9, 1, lbb_12164                            if r9 == (1 as i32 as i64 as u64) { pc += 166 }
    jne r9, 2, lbb_12009                            if r9 != (2 as i32 as i64 as u64) { pc += 10 }
    ldxb r2, [r7+0x0]                       
    jeq r2, 255, lbb_12159                          if r2 == (255 as i32 as i64 as u64) { pc += 158 }
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -2080                                 r3 += -2080   ///  r3 = r3.wrapping_add(-2080 as i32 as i64 as u64)
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    ldxdw r2, [r3+0x0]                      
    stxdw [r10-0x818], r2                   
lbb_12007:
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    ja lbb_12164                                    if true { pc += 155 }
lbb_12009:
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2080                                 r2 += -2080   ///  r2 = r2.wrapping_add(-2080 as i32 as i64 as u64)
    mov64 r3, r9                                    r3 = r9
    jlt r9, 6, lbb_12102                            if r9 < (6 as i32 as i64 as u64) { pc += 89 }
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2080                                 r2 += -2080   ///  r2 = r2.wrapping_add(-2080 as i32 as i64 as u64)
    mov64 r3, r9                                    r3 = r9
    ja lbb_12034                                    if true { pc += 17 }
lbb_12017:
    stxdw [r2+0x20], r7                     
    ldxdw r4, [r7+0x50]                     
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    add64 r7, 10343                                 r7 += 10343   ///  r7 = r7.wrapping_add(10343 as i32 as i64 as u64)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
    add64 r2, 40                                    r2 += 40   ///  r2 = r2.wrapping_add(40 as i32 as i64 as u64)
    ldxb r4, [r7+0x0]                       
    jeq r4, 255, lbb_12095                          if r4 == (255 as i32 as i64 as u64) { pc += 70 }
lbb_12025:
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -2080                                 r5 += -2080   ///  r5 = r5.wrapping_add(-2080 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxdw r4, [r5+0x0]                      
    stxdw [r2+0x0], r4                      
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    add64 r3, -5                                    r3 += -5   ///  r3 = r3.wrapping_add(-5 as i32 as i64 as u64)
    jle r3, 5, lbb_12102                            if r3 <= (5 as i32 as i64 as u64) { pc += 68 }
lbb_12034:
    ldxb r4, [r7+0x0]                       
    jeq r4, 255, lbb_12064                          if r4 == (255 as i32 as i64 as u64) { pc += 28 }
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -2080                                 r5 += -2080   ///  r5 = r5.wrapping_add(-2080 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxdw r4, [r5+0x0]                      
    stxdw [r2+0x8], r4                      
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    ldxb r4, [r7+0x0]                       
    jeq r4, 255, lbb_12071                          if r4 == (255 as i32 as i64 as u64) { pc += 26 }
lbb_12045:
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -2080                                 r5 += -2080   ///  r5 = r5.wrapping_add(-2080 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxdw r4, [r5+0x0]                      
    stxdw [r2+0x10], r4                     
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    ldxb r4, [r7+0x0]                       
    jeq r4, 255, lbb_12078                          if r4 == (255 as i32 as i64 as u64) { pc += 24 }
lbb_12054:
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -2080                                 r5 += -2080   ///  r5 = r5.wrapping_add(-2080 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxdw r4, [r5+0x0]                      
    stxdw [r2+0x18], r4                     
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    ldxb r4, [r7+0x0]                       
    jne r4, 255, lbb_12085                          if r4 != (255 as i32 as i64 as u64) { pc += 22 }
    ja lbb_12017                                    if true { pc += -47 }
lbb_12064:
    stxdw [r2+0x8], r7                      
    ldxdw r4, [r7+0x50]                     
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    add64 r7, 10343                                 r7 += 10343   ///  r7 = r7.wrapping_add(10343 as i32 as i64 as u64)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
    ldxb r4, [r7+0x0]                       
    jne r4, 255, lbb_12045                          if r4 != (255 as i32 as i64 as u64) { pc += -26 }
lbb_12071:
    stxdw [r2+0x10], r7                     
    ldxdw r4, [r7+0x50]                     
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    add64 r7, 10343                                 r7 += 10343   ///  r7 = r7.wrapping_add(10343 as i32 as i64 as u64)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
    ldxb r4, [r7+0x0]                       
    jne r4, 255, lbb_12054                          if r4 != (255 as i32 as i64 as u64) { pc += -24 }
lbb_12078:
    stxdw [r2+0x18], r7                     
    ldxdw r4, [r7+0x50]                     
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    add64 r7, 10343                                 r7 += 10343   ///  r7 = r7.wrapping_add(10343 as i32 as i64 as u64)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
    ldxb r4, [r7+0x0]                       
    jeq r4, 255, lbb_12017                          if r4 == (255 as i32 as i64 as u64) { pc += -68 }
lbb_12085:
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -2080                                 r5 += -2080   ///  r5 = r5.wrapping_add(-2080 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxdw r4, [r5+0x0]                      
    stxdw [r2+0x20], r4                     
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, 40                                    r2 += 40   ///  r2 = r2.wrapping_add(40 as i32 as i64 as u64)
    ldxb r4, [r7+0x0]                       
    jne r4, 255, lbb_12025                          if r4 != (255 as i32 as i64 as u64) { pc += -70 }
lbb_12095:
    stxdw [r2+0x0], r7                      
    ldxdw r4, [r7+0x50]                     
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    add64 r7, 10343                                 r7 += 10343   ///  r7 = r7.wrapping_add(10343 as i32 as i64 as u64)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
    add64 r3, -5                                    r3 += -5   ///  r3 = r3.wrapping_add(-5 as i32 as i64 as u64)
    jgt r3, 5, lbb_12034                            if r3 > (5 as i32 as i64 as u64) { pc += -68 }
lbb_12102:
    jle r3, 3, lbb_12140                            if r3 <= (3 as i32 as i64 as u64) { pc += 37 }
    ldxb r4, [r7+0x0]                       
    jne r3, 5, lbb_12856                            if r3 != (5 as i32 as i64 as u64) { pc += 751 }
    jeq r4, 255, lbb_13445                          if r4 == (255 as i32 as i64 as u64) { pc += 1339 }
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -2080                                 r3 += -2080   ///  r3 = r3.wrapping_add(-2080 as i32 as i64 as u64)
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    ldxdw r3, [r3+0x0]                      
    stxdw [r2+0x8], r3                      
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    ldxb r3, [r7+0x0]                       
    jeq r3, 255, lbb_13452                          if r3 == (255 as i32 as i64 as u64) { pc += 1337 }
lbb_12115:
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -2080                                 r4 += -2080   ///  r4 = r4.wrapping_add(-2080 as i32 as i64 as u64)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxdw r3, [r4+0x0]                      
    stxdw [r2+0x10], r3                     
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    ldxb r3, [r7+0x0]                       
    jeq r3, 255, lbb_13459                          if r3 == (255 as i32 as i64 as u64) { pc += 1335 }
lbb_12124:
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -2080                                 r4 += -2080   ///  r4 = r4.wrapping_add(-2080 as i32 as i64 as u64)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxdw r3, [r4+0x0]                      
    stxdw [r2+0x18], r3                     
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    ldxb r3, [r7+0x0]                       
    jeq r3, 255, lbb_13466                          if r3 == (255 as i32 as i64 as u64) { pc += 1333 }
lbb_12133:
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -2080                                 r4 += -2080   ///  r4 = r4.wrapping_add(-2080 as i32 as i64 as u64)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxdw r3, [r4+0x0]                      
    stxdw [r2+0x20], r3                     
    ja lbb_12007                                    if true { pc += -133 }
lbb_12140:
    jne r3, 3, lbb_12882                            if r3 != (3 as i32 as i64 as u64) { pc += 741 }
    ldxb r3, [r7+0x0]                       
    jeq r3, 255, lbb_13468                          if r3 == (255 as i32 as i64 as u64) { pc += 1325 }
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -2080                                 r4 += -2080   ///  r4 = r4.wrapping_add(-2080 as i32 as i64 as u64)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxdw r3, [r4+0x0]                      
    stxdw [r2+0x8], r3                      
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    ldxb r3, [r7+0x0]                       
    jeq r3, 255, lbb_13475                          if r3 == (255 as i32 as i64 as u64) { pc += 1323 }
lbb_12152:
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -2080                                 r4 += -2080   ///  r4 = r4.wrapping_add(-2080 as i32 as i64 as u64)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxdw r3, [r4+0x0]                      
    stxdw [r2+0x10], r3                     
    ja lbb_12007                                    if true { pc += -152 }
lbb_12159:
    stxdw [r10-0x818], r7                   
lbb_12160:
    ldxdw r2, [r7+0x50]                     
    add64 r7, r2                                    r7 += r2   ///  r7 = r7.wrapping_add(r2)
    add64 r7, 10343                                 r7 += 10343   ///  r7 = r7.wrapping_add(10343 as i32 as i64 as u64)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
lbb_12164:
    mov64 r0, 195936478                             r0 = 195936478 as i32 as i64 as u64
    ldxdw r4, [r7+0x0]                      
    jeq r4, 0, lbb_13774                            if r4 == (0 as i32 as i64 as u64) { pc += 1607 }
    mov64 r6, r7                                    r6 = r7
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    mov64 r8, r4                                    r8 = r4
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    add64 r2, r8                                    r2 += r8   ///  r2 = r2.wrapping_add(r8)
    ldxb r3, [r2+0x0]                       
    mov64 r2, r8                                    r2 = r8
    rsh64 r2, 3                                     r2 >>= 3   ///  r2 = r2.wrapping_shr(3)
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    xor64 r3, 58                                    r3 ^= 58   ///  r3 = r3.xor(58)
    and64 r3, 255                                   r3 &= 255   ///  r3 = r3.and(255)
    jlt r3, 16, lbb_12181                           if r3 < (16 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 16                                    r3 = 16 as i32 as i64 as u64
lbb_12181:
    mov64 r2, r6                                    r2 = r6
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    jsle r3, 7, lbb_12242                           if (r3 as i64) <= (7 as i32 as i64) { pc += 58 }
    jsle r3, 11, lbb_12279                          if (r3 as i64) <= (11 as i32 as i64) { pc += 94 }
    jsle r3, 13, lbb_12611                          if (r3 as i64) <= (13 as i32 as i64) { pc += 425 }
    jeq r3, 14, lbb_12892                           if r3 == (14 as i32 as i64 as u64) { pc += 705 }
    jne r3, 15, lbb_13774                           if r3 != (15 as i32 as i64 as u64) { pc += 1586 }
    mov64 r1, r8                                    r1 = r8
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    stxdw [r10-0x878], r9                   
    stxdw [r10-0x870], r6                   
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_12210                            if r1 == (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r2, r1                                    r2 = r1
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lddw r3, 0xc3ebbae2ff2fff3a                     r3 load str located at -4329161132679889094
    lddw r4, 0x1000100010001                        r4 load str located at 281479271743489
    mov64 r5, r6                                    r5 = r6
lbb_12201:
    ldxdw r0, [r5+0x0]                      
    mov64 r6, r7                                    r6 = r7
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    xor64 r6, r3                                    r6 ^= r3   ///  r6 = r6.xor(r3)
    stxdw [r5+0x0], r6                      
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jne r2, 0, lbb_12201                            if r2 != (0 as i32 as i64 as u64) { pc += -9 }
lbb_12210:
    ldxdw r6, [r10-0x870]                   
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    stdw [r10-0x20], 0                      
    mov64 r9, r8                                    r9 = r8
    and64 r9, 7                                     r9 &= 7   ///  r9 = r9.and(7)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x880], r1                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    ldxdw r1, [r10-0x20]                    
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    lddw r1, 0xc3ebbae2ff2fff3a                     r1 load str located at -4329161132679889094
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    stxdw [r10-0x20], r7                    
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x880]                   
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2088                                 r1 += -2088   ///  r1 = r1.wrapping_add(-2088 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2080                                 r2 += -2080   ///  r2 = r2.wrapping_add(-2080 as i32 as i64 as u64)
    ldxdw r3, [r10-0x878]                   
    ldxdw r4, [r10-0x870]                   
    mov64 r5, r8                                    r5 = r8
    call function_5258                      
    ldxw r0, [r10-0x824]                    
    ldxw r5, [r10-0x828]                    
    ja lbb_13684                                    if true { pc += 1442 }
lbb_12242:
    jsgt r3, 3, lbb_12417                           if (r3 as i64) > (3 as i32 as i64) { pc += 174 }
    jsgt r3, 1, lbb_12686                           if (r3 as i64) > (1 as i32 as i64) { pc += 442 }
    jne r3, 0, lbb_13165                            if r3 != (0 as i32 as i64 as u64) { pc += 920 }
    ldxdw r1, [r2+0x0]                      
    lddw r3, 0xe959f7272b74fd7a                     r3 load str located at -1632001642340221574
    jne r1, r3, lbb_12262                           if r1 != r3 { pc += 13 }
    ldxdw r1, [r2+0x8]                      
    lddw r3, 0x7a819dd33c7070c6                     r3 load str located at 8827510275200544966
    jne r1, r3, lbb_12262                           if r1 != r3 { pc += 9 }
    ldxdw r1, [r2+0x10]                     
    lddw r3, 0x6dd2523bce0a93a0                     r3 load str located at 7913477912056730528
    jne r1, r3, lbb_12262                           if r1 != r3 { pc += 5 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r2+0x18]                     
    lddw r3, 0xd3bb8723dd54a054                     r3 load str located at -3189807322954948524
    jeq r2, r3, lbb_12263                           if r2 == r3 { pc += 1 }
lbb_12262:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_12263:
    mov64 r0, 233811181                             r0 = 233811181 as i32 as i64 as u64
    jne r1, 0, lbb_13774                            if r1 != (0 as i32 as i64 as u64) { pc += 1509 }
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r8                                    r2 = r8
    call function_974                       
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2152                                 r1 += -2152   ///  r1 = r1.wrapping_add(-2152 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2080                                 r2 += -2080   ///  r2 = r2.wrapping_add(-2080 as i32 as i64 as u64)
    mov64 r3, r9                                    r3 = r9
    mov64 r4, r6                                    r4 = r6
    mov64 r5, r8                                    r5 = r8
    call function_2413                      
    ldxw r0, [r10-0x864]                    
    ldxw r5, [r10-0x868]                    
    ja lbb_13684                                    if true { pc += 1405 }
lbb_12279:
    stxdw [r10-0x878], r9                   
    stxdw [r10-0x870], r6                   
    mov64 r1, r8                                    r1 = r8
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    jsgt r3, 9, lbb_12473                           if (r3 as i64) > (9 as i32 as i64) { pc += 189 }
    jne r3, 8, lbb_12926                            if r3 != (8 as i32 as i64 as u64) { pc += 641 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_12303                            if r1 == (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r2, r1                                    r2 = r1
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lddw r3, 0xc3ebbae2ff2fff3a                     r3 load str located at -4329161132679889094
    lddw r4, 0x1000100010001                        r4 load str located at 281479271743489
    ldxdw r5, [r10-0x870]                   
lbb_12294:
    ldxdw r0, [r5+0x0]                      
    mov64 r6, r9                                    r6 = r9
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    xor64 r6, r3                                    r6 ^= r3   ///  r6 = r6.xor(r3)
    stxdw [r5+0x0], r6                      
    add64 r9, r4                                    r9 += r4   ///  r9 = r9.wrapping_add(r4)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jne r2, 0, lbb_12294                            if r2 != (0 as i32 as i64 as u64) { pc += -9 }
lbb_12303:
    ldxdw r2, [r10-0x870]                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    stxdw [r10-0x870], r2                   
    stdw [r10-0x20], 0                      
    mov64 r6, r8                                    r6 = r8
    and64 r6, 7                                     r6 &= 7   ///  r6 = r6.and(7)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x880], r1                   
    mov64 r3, r6                                    r3 = r6
    call function_17012                     
    ldxdw r1, [r10-0x20]                    
    xor64 r9, r1                                    r9 ^= r1   ///  r9 = r9.xor(r1)
    lddw r1, 0xc3ebbae2ff2fff3a                     r1 load str located at -4329161132679889094
    xor64 r9, r1                                    r9 ^= r1   ///  r9 = r9.xor(r1)
    stxdw [r10-0x20], r9                    
    ldxdw r1, [r10-0x870]                   
    ldxdw r2, [r10-0x880]                   
    mov64 r3, r6                                    r3 = r6
    call function_17012                     
    mov64 r0, 195936478                             r0 = 195936478 as i32 as i64 as u64
    ldxdw r1, [r10-0x878]                   
    jne r1, 3, lbb_13683                            if r1 != (3 as i32 as i64 as u64) { pc += 1356 }
    jne r8, 64, lbb_13683                           if r8 != (64 as i32 as i64 as u64) { pc += 1355 }
    ldxdw r3, [r10-0x820]                   
    ldxb r2, [r3+0x1]                       
    ldxdw r1, [r10-0x818]                   
    ldxdw r4, [r1+0x270]                    
    stxdw [r10-0x8], r4                     
    lddw r5, 0x4a0178651b8c3c5                      r5 load str located at 333292238089536453
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r5, [r1+0x268]                    
    stxdw [r10-0x10], r5                    
    lddw r0, 0x4a1178751b9c3c6                      r0 load str located at 333573717361279942
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    ldxdw r0, [r1+0x260]                    
    stxdw [r10-0x18], r0                    
    lddw r6, 0x4a2178451bac3c7                      r6 load str located at 333855179453154247
    xor64 r0, r6                                    r0 ^= r6   ///  r0 = r0.xor(r6)
    ldxdw r6, [r1+0x258]                    
    stxdw [r10-0x18], r0                    
    stxdw [r10-0x10], r5                    
    stxdw [r10-0x8], r4                     
    stxdw [r10-0x20], r6                    
    lddw r4, 0xfb5ce87aae443c38                     r4 load str located at -334136658724897736
    xor64 r6, r4                                    r6 ^= r4   ///  r6 = r6.xor(r4)
    stxdw [r10-0x20], r6                    
    ldxdw r4, [r3+0x8]                      
    jne r6, r4, lbb_12367                           if r6 != r4 { pc += 10 }
    ldxdw r4, [r3+0x10]                     
    ldxdw r5, [r10-0x18]                    
    jne r5, r4, lbb_12367                           if r5 != r4 { pc += 7 }
    ldxdw r4, [r3+0x18]                     
    ldxdw r5, [r10-0x10]                    
    jne r5, r4, lbb_12367                           if r5 != r4 { pc += 4 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r3, [r3+0x20]                     
    ldxdw r5, [r10-0x8]                     
    jeq r5, r3, lbb_12368                           if r5 == r3 { pc += 1 }
lbb_12367:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
lbb_12368:
    lddw r0, 0xabad1dea                             r0 load str located at 2880249322
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jeq r2, 0, lbb_13684                            if r2 == (0 as i32 as i64 as u64) { pc += 1312 }
    jne r4, 0, lbb_13684                            if r4 != (0 as i32 as i64 as u64) { pc += 1311 }
    ldxdw r2, [r1+0x2b0]                    
    lddw r3, 0x6e9de2b30b19f9ea                     r3 load str located at 7970776174128921066
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    mov64 r0, 57005                                 r0 = 57005 as i32 as i64 as u64
    ldxdw r3, [r7+0x20]                     
    jge r2, r3, lbb_13684                           if r2 >= r3 { pc += 1304 }
    ldxdw r2, [r10-0x810]                   
    ldxdw r2, [r2+0x58]                     
    lddw r0, 0xdeadc0de                             r0 load str located at 3735929054
    ldxdw r4, [r7+0x30]                     
    jlt r4, r2, lbb_13684                           if r4 < r2 { pc += 1298 }
    ldxdw r4, [r7+0x10]                     
    lddw r5, 0x46a912eb23798bd9                     r5 load str located at 5091621654840839129
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r5, [r7+0x8]                      
    stxdw [r1+0x2a0], r4                    
    lddw r4, 0xb957ed15dc877426                     r4 load str located at -5091340175569095642
    xor64 r5, r4                                    r5 ^= r4   ///  r5 = r5.xor(r4)
    stxdw [r1+0x298], r5                    
    lddw r4, 0x6e9de2b30b19f9ea                     r4 load str located at 7970776174128921066
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    ldxdw r5, [r7+0x18]                     
    stxdw [r1+0x2b0], r3                    
    xor64 r5, r4                                    r5 ^= r4   ///  r5 = r5.xor(r4)
    stxdw [r1+0x2a8], r5                    
    lddw r3, 0x6e9de2b30b19f1ea                     r3 load str located at 7970776174128919018
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    ldxdw r4, [r7+0x28]                     
    stxdw [r1+0x2c0], r2                    
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    stxdw [r1+0x2b8], r4                    
    ldxdw r2, [r7+0x38]                     
    lddw r3, 0xd3198133b7c1776c                     r3 load str located at -3235412798162765972
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r1+0x320], r2                    
    mov64 r5, 26                                    r5 = 26 as i32 as i64 as u64
    ja lbb_13684                                    if true { pc += 1267 }
lbb_12417:
    stxdw [r10-0x870], r6                   
    jsgt r3, 5, lbb_12741                           if (r3 as i64) > (5 as i32 as i64) { pc += 322 }
    stxdw [r10-0x878], r9                   
    mov64 r1, r8                                    r1 = r8
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jne r3, 4, lbb_13296                            if r3 != (4 as i32 as i64 as u64) { pc += 872 }
    jeq r1, 0, lbb_12441                            if r1 == (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r2, r1                                    r2 = r1
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lddw r3, 0xc3ebbae2ff2fff3a                     r3 load str located at -4329161132679889094
    lddw r4, 0x1000100010001                        r4 load str located at 281479271743489
    mov64 r5, r6                                    r5 = r6
lbb_12432:
    ldxdw r0, [r5+0x0]                      
    mov64 r6, r7                                    r6 = r7
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    xor64 r6, r3                                    r6 ^= r3   ///  r6 = r6.xor(r3)
    stxdw [r5+0x0], r6                      
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jne r2, 0, lbb_12432                            if r2 != (0 as i32 as i64 as u64) { pc += -9 }
lbb_12441:
    ldxdw r6, [r10-0x870]                   
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    stdw [r10-0x20], 0                      
    mov64 r9, r8                                    r9 = r8
    and64 r9, 7                                     r9 &= 7   ///  r9 = r9.and(7)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x880], r1                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    ldxdw r1, [r10-0x20]                    
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    lddw r1, 0xc3ebbae2ff2fff3a                     r1 load str located at -4329161132679889094
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    stxdw [r10-0x20], r7                    
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x880]                   
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2128                                 r1 += -2128   ///  r1 = r1.wrapping_add(-2128 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2080                                 r2 += -2080   ///  r2 = r2.wrapping_add(-2080 as i32 as i64 as u64)
    ldxdw r3, [r10-0x878]                   
    ldxdw r4, [r10-0x870]                   
    mov64 r5, r8                                    r5 = r8
    call function_4660                      
    ldxw r0, [r10-0x84c]                    
    ldxw r5, [r10-0x850]                    
    ja lbb_13684                                    if true { pc += 1211 }
lbb_12473:
    jne r3, 10, lbb_12976                           if r3 != (10 as i32 as i64 as u64) { pc += 502 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_12492                            if r1 == (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r2, r1                                    r2 = r1
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lddw r3, 0xc3ebbae2ff2fff3a                     r3 load str located at -4329161132679889094
    lddw r4, 0x1000100010001                        r4 load str located at 281479271743489
    ldxdw r5, [r10-0x870]                   
lbb_12483:
    ldxdw r0, [r5+0x0]                      
    mov64 r6, r9                                    r6 = r9
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    xor64 r6, r3                                    r6 ^= r3   ///  r6 = r6.xor(r3)
    stxdw [r5+0x0], r6                      
    add64 r9, r4                                    r9 += r4   ///  r9 = r9.wrapping_add(r4)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jne r2, 0, lbb_12483                            if r2 != (0 as i32 as i64 as u64) { pc += -9 }
lbb_12492:
    ldxdw r2, [r10-0x870]                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    stxdw [r10-0x870], r2                   
    stdw [r10-0x20], 0                      
    mov64 r6, r8                                    r6 = r8
    and64 r6, 7                                     r6 &= 7   ///  r6 = r6.and(7)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x880], r1                   
    mov64 r3, r6                                    r3 = r6
    call function_17012                     
    ldxdw r1, [r10-0x20]                    
    xor64 r9, r1                                    r9 ^= r1   ///  r9 = r9.xor(r1)
    lddw r1, 0xc3ebbae2ff2fff3a                     r1 load str located at -4329161132679889094
    xor64 r9, r1                                    r9 ^= r1   ///  r9 = r9.xor(r1)
    stxdw [r10-0x20], r9                    
    ldxdw r1, [r10-0x870]                   
    ldxdw r2, [r10-0x880]                   
    mov64 r3, r6                                    r3 = r6
    call function_17012                     
    mov64 r0, 195936478                             r0 = 195936478 as i32 as i64 as u64
    ldxdw r1, [r10-0x878]                   
    jne r1, 3, lbb_13683                            if r1 != (3 as i32 as i64 as u64) { pc += 1167 }
    jne r8, 64, lbb_13683                           if r8 != (64 as i32 as i64 as u64) { pc += 1166 }
    ldxdw r1, [r10-0x810]                   
    ldxdw r2, [r1+0x58]                     
    ldxdw r4, [r10-0x820]                   
    ldxb r3, [r4+0x1]                       
    ldxdw r1, [r10-0x818]                   
    ldxdw r5, [r1+0x270]                    
    stxdw [r10-0x8], r5                     
    lddw r0, 0x4a0178651b8c3c5                      r0 load str located at 333292238089536453
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    ldxdw r0, [r1+0x268]                    
    stxdw [r10-0x10], r0                    
    lddw r6, 0x4a1178751b9c3c6                      r6 load str located at 333573717361279942
    xor64 r0, r6                                    r0 ^= r6   ///  r0 = r0.xor(r6)
    ldxdw r6, [r1+0x260]                    
    stxdw [r10-0x18], r6                    
    lddw r8, 0x4a2178451bac3c7                      r8 load str located at 333855179453154247
    xor64 r6, r8                                    r6 ^= r8   ///  r6 = r6.xor(r8)
    ldxdw r8, [r1+0x258]                    
    stxdw [r10-0x18], r6                    
    stxdw [r10-0x10], r0                    
    stxdw [r10-0x8], r5                     
    stxdw [r10-0x20], r8                    
    lddw r5, 0xfb5ce87aae443c38                     r5 load str located at -334136658724897736
    xor64 r8, r5                                    r8 ^= r5   ///  r8 = r8.xor(r5)
    stxdw [r10-0x20], r8                    
    ldxdw r5, [r4+0x8]                      
    jne r8, r5, lbb_12558                           if r8 != r5 { pc += 10 }
    ldxdw r5, [r4+0x10]                     
    ldxdw r0, [r10-0x18]                    
    jne r0, r5, lbb_12558                           if r0 != r5 { pc += 7 }
    ldxdw r5, [r4+0x18]                     
    ldxdw r0, [r10-0x10]                    
    jne r0, r5, lbb_12558                           if r0 != r5 { pc += 4 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r4, [r4+0x20]                     
    ldxdw r0, [r10-0x8]                     
    jeq r0, r4, lbb_12559                           if r0 == r4 { pc += 1 }
lbb_12558:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_12559:
    lddw r0, 0xabad1dea                             r0 load str located at 2880249322
    jeq r3, 0, lbb_13683                            if r3 == (0 as i32 as i64 as u64) { pc += 1121 }
    jne r5, 0, lbb_13683                            if r5 != (0 as i32 as i64 as u64) { pc += 1120 }
    ldxdw r4, [r1+0x2b0]                    
    lddw r3, 0x6e9de2b30b19f9ea                     r3 load str located at 7970776174128921066
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    mov64 r0, 57005                                 r0 = 57005 as i32 as i64 as u64
    ldxdw r3, [r7+0x20]                     
    jge r4, r3, lbb_13683                           if r4 >= r3 { pc += 1113 }
    lddw r0, 0xdeadc0de                             r0 load str located at 3735929054
    ldxdw r4, [r7+0x30]                     
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jlt r4, r2, lbb_13684                           if r4 < r2 { pc += 1109 }
    ldxdw r4, [r7+0x10]                     
    lddw r5, 0x46a912eb23798bd9                     r5 load str located at 5091621654840839129
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r5, [r7+0x8]                      
    stxdw [r1+0x2a0], r4                    
    lddw r4, 0xb957ed15dc877426                     r4 load str located at -5091340175569095642
    xor64 r5, r4                                    r5 ^= r4   ///  r5 = r5.xor(r4)
    stxdw [r1+0x298], r5                    
    lddw r4, 0x6e9de2b30b19f9ea                     r4 load str located at 7970776174128921066
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    ldxdw r5, [r7+0x18]                     
    stxdw [r1+0x2b0], r3                    
    xor64 r5, r4                                    r5 ^= r4   ///  r5 = r5.xor(r4)
    stxdw [r1+0x2a8], r5                    
    lddw r3, 0x6e9de2b30b19f1ea                     r3 load str located at 7970776174128919018
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    ldxdw r4, [r7+0x28]                     
    stxdw [r1+0x2c0], r2                    
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    stxdw [r1+0x2b8], r4                    
    lddw r2, 0xd3198133b7c1776c                     r2 load str located at -3235412798162765972
    ldxdw r3, [r7+0x38]                     
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r1+0x320], r3                    
    ldxdw r2, [r7+0x40]                     
    lddw r3, 0x504156a22548f8dd                     r3 load str located at 5782998650930657501
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r1+0x378], r2                    
    mov64 r5, 26                                    r5 = 26 as i32 as i64 as u64
    ja lbb_13684                                    if true { pc += 1073 }
lbb_12611:
    jne r3, 12, lbb_13079                           if r3 != (12 as i32 as i64 as u64) { pc += 467 }
    stxdw [r10-0x870], r6                   
    stxdw [r10-0x878], r9                   
    mov64 r1, r8                                    r1 = r8
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_12634                            if r1 == (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r2, r1                                    r2 = r1
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lddw r3, 0xc3ebbae2ff2fff3a                     r3 load str located at -4329161132679889094
    lddw r4, 0x1000100010001                        r4 load str located at 281479271743489
    ldxdw r5, [r10-0x870]                   
lbb_12625:
    ldxdw r0, [r5+0x0]                      
    mov64 r6, r9                                    r6 = r9
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    xor64 r6, r3                                    r6 ^= r3   ///  r6 = r6.xor(r3)
    stxdw [r5+0x0], r6                      
    add64 r9, r4                                    r9 += r4   ///  r9 = r9.wrapping_add(r4)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jne r2, 0, lbb_12625                            if r2 != (0 as i32 as i64 as u64) { pc += -9 }
lbb_12634:
    ldxdw r6, [r10-0x870]                   
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    stdw [r10-0x20], 0                      
    and64 r8, 7                                     r8 &= 7   ///  r8 = r8.and(7)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x870], r1                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r8                                    r3 = r8
    call function_17012                     
    ldxdw r1, [r10-0x20]                    
    xor64 r9, r1                                    r9 ^= r1   ///  r9 = r9.xor(r1)
    lddw r1, 0xc3ebbae2ff2fff3a                     r1 load str located at -4329161132679889094
    xor64 r9, r1                                    r9 ^= r1   ///  r9 = r9.xor(r1)
    stxdw [r10-0x20], r9                    
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x870]                   
    mov64 r3, r8                                    r3 = r8
    call function_17012                     
    mov64 r0, 195936478                             r0 = 195936478 as i32 as i64 as u64
    ldxdw r4, [r10-0x878]                   
    jlt r4, 2, lbb_13683                            if r4 < (2 as i32 as i64 as u64) { pc += 1026 }
    lddw r0, 0xabad1dea                             r0 load str located at 2880249322
    ldxdw r1, [r10-0x820]                   
    ldxb r2, [r1+0x1]                       
    jeq r2, 0, lbb_13683                            if r2 == (0 as i32 as i64 as u64) { pc += 1021 }
    ldxdw r2, [r1+0x8]                      
    lddw r3, 0xe8fcdd226d75557c                     r3 load str located at -1658207422844152452
    jne r2, r3, lbb_12679                           if r2 != r3 { pc += 13 }
    ldxdw r2, [r1+0x10]                     
    lddw r3, 0x988d0e274061ede8                     r3 load str located at -7454286246481629720
    jne r2, r3, lbb_12679                           if r2 != r3 { pc += 9 }
    ldxdw r2, [r1+0x18]                     
    lddw r3, 0xbbb2b20e2a0feca0                     r3 load str located at -4921675668857557856
    jne r2, r3, lbb_12679                           if r2 != r3 { pc += 5 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r1, [r1+0x20]                     
    lddw r3, 0x2ef828335f730f58                     r3 load str located at 3384499321078746968
    jeq r1, r3, lbb_12680                           if r1 == r3 { pc += 1 }
lbb_12679:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_12680:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jne r2, 0, lbb_13684                            if r2 != (0 as i32 as i64 as u64) { pc += 1002 }
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    jne r4, 16, lbb_13495                           if r4 != (16 as i32 as i64 as u64) { pc += 811 }
lbb_12684:
    mov64 r5, 26                                    r5 = 26 as i32 as i64 as u64
    ja lbb_13684                                    if true { pc += 998 }
lbb_12686:
    stxdw [r10-0x878], r9                   
    stxdw [r10-0x870], r6                   
    mov64 r1, r8                                    r1 = r8
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    jne r3, 2, lbb_13345                            if r3 != (2 as i32 as i64 as u64) { pc += 653 }
    jeq r1, 0, lbb_12709                            if r1 == (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r2, r1                                    r2 = r1
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lddw r3, 0xc3ebbae2ff2fff3a                     r3 load str located at -4329161132679889094
    lddw r4, 0x1000100010001                        r4 load str located at 281479271743489
    mov64 r5, r6                                    r5 = r6
lbb_12700:
    ldxdw r0, [r5+0x0]                      
    mov64 r6, r7                                    r6 = r7
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    xor64 r6, r3                                    r6 ^= r3   ///  r6 = r6.xor(r3)
    stxdw [r5+0x0], r6                      
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jne r2, 0, lbb_12700                            if r2 != (0 as i32 as i64 as u64) { pc += -9 }
lbb_12709:
    ldxdw r6, [r10-0x870]                   
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    stdw [r10-0x20], 0                      
    mov64 r9, r8                                    r9 = r8
    and64 r9, 7                                     r9 &= 7   ///  r9 = r9.and(7)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x880], r1                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    ldxdw r1, [r10-0x20]                    
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    lddw r1, 0xc3ebbae2ff2fff3a                     r1 load str located at -4329161132679889094
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    stxdw [r10-0x20], r7                    
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x880]                   
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2144                                 r1 += -2144   ///  r1 = r1.wrapping_add(-2144 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2080                                 r2 += -2080   ///  r2 = r2.wrapping_add(-2080 as i32 as i64 as u64)
    ldxdw r3, [r10-0x878]                   
    ldxdw r4, [r10-0x870]                   
    mov64 r5, r8                                    r5 = r8
    call function_1520                      
    ldxw r0, [r10-0x85c]                    
    ldxw r5, [r10-0x860]                    
    ja lbb_13684                                    if true { pc += 943 }
lbb_12741:
    mov64 r1, r8                                    r1 = r8
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    jne r3, 6, lbb_13394                            if r3 != (6 as i32 as i64 as u64) { pc += 650 }
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_12762                            if r1 == (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r2, r1                                    r2 = r1
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lddw r3, 0xc3ebbae2ff2fff3a                     r3 load str located at -4329161132679889094
    lddw r4, 0x1000100010001                        r4 load str located at 281479271743489
    ldxdw r5, [r10-0x870]                   
lbb_12753:
    ldxdw r0, [r5+0x0]                      
    mov64 r6, r7                                    r6 = r7
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    xor64 r6, r3                                    r6 ^= r3   ///  r6 = r6.xor(r3)
    stxdw [r5+0x0], r6                      
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jne r2, 0, lbb_12753                            if r2 != (0 as i32 as i64 as u64) { pc += -9 }
lbb_12762:
    ldxdw r6, [r10-0x870]                   
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    stdw [r10-0x20], 0                      
    and64 r8, 7                                     r8 &= 7   ///  r8 = r8.and(7)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x870], r1                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r8                                    r3 = r8
    call function_17012                     
    ldxdw r1, [r10-0x20]                    
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    lddw r1, 0xc3ebbae2ff2fff3a                     r1 load str located at -4329161132679889094
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    stxdw [r10-0x20], r7                    
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x870]                   
    mov64 r3, r8                                    r3 = r8
    call function_17012                     
    mov64 r0, 195936478                             r0 = 195936478 as i32 as i64 as u64
    jne r9, 2, lbb_13683                            if r9 != (2 as i32 as i64 as u64) { pc += 899 }
    ldxdw r7, [r10-0x820]                   
    ldxdw r6, [r10-0x818]                   
    mov64 r1, r6                                    r1 = r6
    add64 r1, 40                                    r1 += 40   ///  r1 = r1.wrapping_add(40 as i32 as i64 as u64)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -32                                   r4 += -32   ///  r4 = r4.wrapping_add(-32 as i32 as i64 as u64)
    lddw r2, 0x100022d88 --> b"z\xfdt+'\xf7Y\xe9\xc6pp<\xd3\x9d\x81z\xa0\x93\x0a\xce;R\xd2mT\xa0T\xdd#\x…        r2 load str located at 4295110024
    mov64 r3, 32                                    r3 = 32 as i32 as i64 as u64
    syscall [invalid]                       
    ldxw r1, [r10-0x20]                     
    ldxb r2, [r7+0x1]                       
    ldxdw r4, [r6+0x270]                    
    stxdw [r10-0x8], r4                     
    lddw r3, 0x4a0178651b8c3c5                      r3 load str located at 333292238089536453
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    ldxdw r5, [r6+0x268]                    
    stxdw [r10-0x10], r5                    
    lddw r3, 0x4a1178751b9c3c6                      r3 load str located at 333573717361279942
    xor64 r5, r3                                    r5 ^= r3   ///  r5 = r5.xor(r3)
    ldxdw r0, [r6+0x260]                    
    stxdw [r10-0x18], r0                    
    lddw r3, 0x4a2178451bac3c7                      r3 load str located at 333855179453154247
    xor64 r0, r3                                    r0 ^= r3   ///  r0 = r0.xor(r3)
    ldxdw r3, [r6+0x258]                    
    stxdw [r10-0x18], r0                    
    stxdw [r10-0x10], r5                    
    stxdw [r10-0x8], r4                     
    stxdw [r10-0x20], r3                    
    lddw r4, 0xfb5ce87aae443c38                     r4 load str located at -334136658724897736
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    stxdw [r10-0x20], r3                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r5, [r7+0x8]                      
    be64 r5                                         r5 = match 64 { 16 => (r5 as u16).swap_bytes() as u64, 32 => (r5 as u32).swap_bytes() as u64, 64 => r5.swap_bytes(), _ => r5 }
    jne r3, r5, lbb_12840                           if r3 != r5 { pc += 16 }
    ldxdw r3, [r10-0x18]                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r5, [r7+0x10]                     
    be64 r5                                         r5 = match 64 { 16 => (r5 as u16).swap_bytes() as u64, 32 => (r5 as u32).swap_bytes() as u64, 64 => r5.swap_bytes(), _ => r5 }
    jne r3, r5, lbb_12840                           if r3 != r5 { pc += 11 }
    ldxdw r3, [r10-0x10]                    
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r5, [r7+0x18]                     
    be64 r5                                         r5 = match 64 { 16 => (r5 as u16).swap_bytes() as u64, 32 => (r5 as u32).swap_bytes() as u64, 64 => r5.swap_bytes(), _ => r5 }
    jne r3, r5, lbb_12840                           if r3 != r5 { pc += 6 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r3, [r10-0x8]                     
    be64 r3                                         r3 = match 64 { 16 => (r3 as u16).swap_bytes() as u64, 32 => (r3 as u32).swap_bytes() as u64, 64 => r3.swap_bytes(), _ => r3 }
    ldxdw r5, [r7+0x20]                     
    be64 r5                                         r5 = match 64 { 16 => (r5 as u16).swap_bytes() as u64, 32 => (r5 as u32).swap_bytes() as u64, 64 => r5.swap_bytes(), _ => r5 }
    jeq r3, r5, lbb_12843                           if r3 == r5 { pc += 3 }
lbb_12840:
    mov64 r4, -1                                    r4 = -1 as i32 as i64 as u64
    jlt r3, r5, lbb_12843                           if r3 < r5 { pc += 1 }
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
lbb_12843:
    lddw r0, 0xabad1dea                             r0 load str located at 2880249322
    jeq r2, 0, lbb_13683                            if r2 == (0 as i32 as i64 as u64) { pc += 837 }
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jne r4, 0, lbb_13684                            if r4 != (0 as i32 as i64 as u64) { pc += 833 }
    lddw r1, 0x6e9de2b30b19f9ea                     r1 load str located at 7970776174128921066
    stxdw [r6+0x2b0], r1                    
    mov64 r5, 26                                    r5 = 26 as i32 as i64 as u64
    ja lbb_13684                                    if true { pc += 828 }
lbb_12856:
    jeq r4, 255, lbb_13477                          if r4 == (255 as i32 as i64 as u64) { pc += 620 }
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -2080                                 r3 += -2080   ///  r3 = r3.wrapping_add(-2080 as i32 as i64 as u64)
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    ldxdw r3, [r3+0x0]                      
    stxdw [r2+0x8], r3                      
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    ldxb r3, [r7+0x0]                       
    jeq r3, 255, lbb_13484                          if r3 == (255 as i32 as i64 as u64) { pc += 618 }
lbb_12866:
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -2080                                 r4 += -2080   ///  r4 = r4.wrapping_add(-2080 as i32 as i64 as u64)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxdw r3, [r4+0x0]                      
    stxdw [r2+0x10], r3                     
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    ldxb r3, [r7+0x0]                       
    jeq r3, 255, lbb_13491                          if r3 == (255 as i32 as i64 as u64) { pc += 616 }
lbb_12875:
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -2080                                 r4 += -2080   ///  r4 = r4.wrapping_add(-2080 as i32 as i64 as u64)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxdw r3, [r4+0x0]                      
    stxdw [r2+0x18], r3                     
    ja lbb_12007                                    if true { pc += -875 }
lbb_12882:
    jle r3, 1, lbb_12164                            if r3 <= (1 as i32 as i64 as u64) { pc += -719 }
    ldxb r3, [r7+0x0]                       
    jeq r3, 255, lbb_13493                          if r3 == (255 as i32 as i64 as u64) { pc += 608 }
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -2080                                 r4 += -2080   ///  r4 = r4.wrapping_add(-2080 as i32 as i64 as u64)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ldxdw r3, [r4+0x0]                      
    stxdw [r2+0x8], r3                      
    ja lbb_12007                                    if true { pc += -885 }
lbb_12892:
    ldxdw r1, [r2+0x0]                      
    lddw r3, 0xe959f7272b74fd7a                     r3 load str located at -1632001642340221574
    jne r1, r3, lbb_12909                           if r1 != r3 { pc += 13 }
    ldxdw r1, [r2+0x8]                      
    lddw r3, 0x7a819dd33c7070c6                     r3 load str located at 8827510275200544966
    jne r1, r3, lbb_12909                           if r1 != r3 { pc += 9 }
    ldxdw r1, [r2+0x10]                     
    lddw r3, 0x6dd2523bce0a93a0                     r3 load str located at 7913477912056730528
    jne r1, r3, lbb_12909                           if r1 != r3 { pc += 5 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r2, [r2+0x18]                     
    lddw r3, 0xd3bb8723dd54a054                     r3 load str located at -3189807322954948524
    jeq r2, r3, lbb_12910                           if r2 == r3 { pc += 1 }
lbb_12909:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
lbb_12910:
    mov64 r0, 233811181                             r0 = 233811181 as i32 as i64 as u64
    jne r1, 0, lbb_13774                            if r1 != (0 as i32 as i64 as u64) { pc += 862 }
    mov64 r1, r6                                    r1 = r6
    mov64 r2, r8                                    r2 = r8
    call function_974                       
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2096                                 r1 += -2096   ///  r1 = r1.wrapping_add(-2096 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2080                                 r2 += -2080   ///  r2 = r2.wrapping_add(-2080 as i32 as i64 as u64)
    mov64 r3, r9                                    r3 = r9
    mov64 r4, r6                                    r4 = r6
    mov64 r5, r8                                    r5 = r8
    call function_2932                      
    ldxw r0, [r10-0x82c]                    
    ldxw r5, [r10-0x830]                    
    ja lbb_13684                                    if true { pc += 758 }
lbb_12926:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_12944                            if r1 == (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r2, r1                                    r2 = r1
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lddw r3, 0xc3ebbae2ff2fff3a                     r3 load str located at -4329161132679889094
    lddw r4, 0x1000100010001                        r4 load str located at 281479271743489
    mov64 r5, r6                                    r5 = r6
lbb_12935:
    ldxdw r0, [r5+0x0]                      
    mov64 r6, r7                                    r6 = r7
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    xor64 r6, r3                                    r6 ^= r3   ///  r6 = r6.xor(r3)
    stxdw [r5+0x0], r6                      
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jne r2, 0, lbb_12935                            if r2 != (0 as i32 as i64 as u64) { pc += -9 }
lbb_12944:
    ldxdw r6, [r10-0x870]                   
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    stdw [r10-0x20], 0                      
    mov64 r9, r8                                    r9 = r8
    and64 r9, 7                                     r9 &= 7   ///  r9 = r9.and(7)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x880], r1                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    ldxdw r1, [r10-0x20]                    
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    lddw r1, 0xc3ebbae2ff2fff3a                     r1 load str located at -4329161132679889094
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    stxdw [r10-0x20], r7                    
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x880]                   
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2104                                 r1 += -2104   ///  r1 = r1.wrapping_add(-2104 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2080                                 r2 += -2080   ///  r2 = r2.wrapping_add(-2080 as i32 as i64 as u64)
    ldxdw r3, [r10-0x878]                   
    ldxdw r4, [r10-0x870]                   
    mov64 r5, r8                                    r5 = r8
    call function_6405                      
    ldxw r0, [r10-0x834]                    
    ldxw r5, [r10-0x838]                    
    ja lbb_13684                                    if true { pc += 708 }
lbb_12976:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_12994                            if r1 == (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r2, r1                                    r2 = r1
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lddw r3, 0xc3ebbae2ff2fff3a                     r3 load str located at -4329161132679889094
    lddw r4, 0x1000100010001                        r4 load str located at 281479271743489
    ldxdw r5, [r10-0x870]                   
lbb_12985:
    ldxdw r0, [r5+0x0]                      
    mov64 r6, r7                                    r6 = r7
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    xor64 r6, r3                                    r6 ^= r3   ///  r6 = r6.xor(r3)
    stxdw [r5+0x0], r6                      
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jne r2, 0, lbb_12985                            if r2 != (0 as i32 as i64 as u64) { pc += -9 }
lbb_12994:
    ldxdw r6, [r10-0x870]                   
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    stdw [r10-0x20], 0                      
    mov64 r9, r8                                    r9 = r8
    and64 r9, 7                                     r9 &= 7   ///  r9 = r9.and(7)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x880], r1                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    ldxdw r1, [r10-0x20]                    
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    lddw r1, 0xc3ebbae2ff2fff3a                     r1 load str located at -4329161132679889094
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    stxdw [r10-0x20], r7                    
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x880]                   
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    mov64 r0, 195936478                             r0 = 195936478 as i32 as i64 as u64
    ldxdw r1, [r10-0x878]                   
    jne r1, 2, lbb_13683                            if r1 != (2 as i32 as i64 as u64) { pc += 665 }
    jne r8, 8, lbb_13798                            if r8 != (8 as i32 as i64 as u64) { pc += 779 }
    ldxdw r1, [r10-0x870]                   
    and64 r1, 7                                     r1 &= 7   ///  r1 = r1.and(7)
    jne r1, 0, lbb_13803                            if r1 != (0 as i32 as i64 as u64) { pc += 781 }
    ldxdw r3, [r10-0x820]                   
    ldxb r2, [r3+0x1]                       
    ldxdw r1, [r10-0x818]                   
    ldxdw r4, [r1+0x270]                    
    stxdw [r10-0x8], r4                     
    lddw r5, 0x4a0178651b8c3c5                      r5 load str located at 333292238089536453
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r5, [r1+0x268]                    
    stxdw [r10-0x10], r5                    
    lddw r0, 0x4a1178751b9c3c6                      r0 load str located at 333573717361279942
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    ldxdw r0, [r1+0x260]                    
    stxdw [r10-0x18], r0                    
    lddw r6, 0x4a2178451bac3c7                      r6 load str located at 333855179453154247
    xor64 r0, r6                                    r0 ^= r6   ///  r0 = r0.xor(r6)
    ldxdw r6, [r1+0x258]                    
    stxdw [r10-0x18], r0                    
    stxdw [r10-0x10], r5                    
    stxdw [r10-0x8], r4                     
    stxdw [r10-0x20], r6                    
    lddw r4, 0xfb5ce87aae443c38                     r4 load str located at -334136658724897736
    xor64 r6, r4                                    r6 ^= r4   ///  r6 = r6.xor(r4)
    stxdw [r10-0x20], r6                    
    ldxdw r4, [r3+0x8]                      
    jne r6, r4, lbb_13061                           if r6 != r4 { pc += 10 }
    ldxdw r4, [r3+0x10]                     
    ldxdw r5, [r10-0x18]                    
    jne r5, r4, lbb_13061                           if r5 != r4 { pc += 7 }
    ldxdw r4, [r3+0x18]                     
    ldxdw r5, [r10-0x10]                    
    jne r5, r4, lbb_13061                           if r5 != r4 { pc += 4 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r3, [r3+0x20]                     
    ldxdw r5, [r10-0x8]                     
    jeq r5, r3, lbb_13062                           if r5 == r3 { pc += 1 }
lbb_13061:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
lbb_13062:
    lddw r0, 0xabad1dea                             r0 load str located at 2880249322
    jeq r2, 0, lbb_13683                            if r2 == (0 as i32 as i64 as u64) { pc += 618 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jne r4, 0, lbb_13684                            if r4 != (0 as i32 as i64 as u64) { pc += 617 }
    ldxdw r2, [r10-0x870]                   
    ldxdw r2, [r2+0x0]                      
    lddw r3, 0x504156a22548f8dd                     r3 load str located at 5782998650930657501
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    stxdw [r1+0x378], r2                    
    mov64 r0, 47828                                 r0 = 47828 as i32 as i64 as u64
    ldxdw r2, [r1+0x710]                    
    jne r2, 2, lbb_13684                            if r2 != (2 as i32 as i64 as u64) { pc += 608 }
    stdw [r1+0x710], 4                      
    mov64 r5, 26                                    r5 = 26 as i32 as i64 as u64
    ja lbb_13684                                    if true { pc += 605 }
lbb_13079:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jne r9, 3, lbb_13684                            if r9 != (3 as i32 as i64 as u64) { pc += 603 }
    jne r8, 64, lbb_13684                           if r8 != (64 as i32 as i64 as u64) { pc += 602 }
    ldxdw r2, [r10-0x810]                   
    ldxdw r2, [r2+0x58]                     
    ldxb r4, [r1+0x9]                       
    ldxdw r3, [r10-0x818]                   
    ldxdw r5, [r3+0x270]                    
    stxdw [r10-0x8], r5                     
    lddw r0, 0x4a0178651b8c3c5                      r0 load str located at 333292238089536453
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    ldxdw r0, [r3+0x268]                    
    stxdw [r10-0x10], r0                    
    lddw r6, 0x4a1178751b9c3c6                      r6 load str located at 333573717361279942
    xor64 r0, r6                                    r0 ^= r6   ///  r0 = r0.xor(r6)
    ldxdw r6, [r3+0x260]                    
    stxdw [r10-0x18], r6                    
    lddw r8, 0x4a2178451bac3c7                      r8 load str located at 333855179453154247
    xor64 r6, r8                                    r6 ^= r8   ///  r6 = r6.xor(r8)
    ldxdw r8, [r3+0x258]                    
    stxdw [r10-0x18], r6                    
    stxdw [r10-0x10], r0                    
    stxdw [r10-0x8], r5                     
    stxdw [r10-0x20], r8                    
    lddw r5, 0xfb5ce87aae443c38                     r5 load str located at -334136658724897736
    xor64 r8, r5                                    r8 ^= r5   ///  r8 = r8.xor(r5)
    stxdw [r10-0x20], r8                    
    ldxdw r5, [r1+0x10]                     
    jne r8, r5, lbb_13122                           if r8 != r5 { pc += 10 }
    ldxdw r5, [r1+0x18]                     
    ldxdw r0, [r10-0x18]                    
    jne r0, r5, lbb_13122                           if r0 != r5 { pc += 7 }
    ldxdw r5, [r1+0x20]                     
    ldxdw r0, [r10-0x10]                    
    jne r0, r5, lbb_13122                           if r0 != r5 { pc += 4 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r1, [r1+0x28]                     
    ldxdw r0, [r10-0x8]                     
    jeq r0, r1, lbb_13123                           if r0 == r1 { pc += 1 }
lbb_13122:
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_13123:
    lddw r0, 0xabad1dea                             r0 load str located at 2880249322
    mov64 r1, r4                                    r1 = r4
    jeq r1, 0, lbb_13683                            if r1 == (0 as i32 as i64 as u64) { pc += 556 }
    jne r5, 0, lbb_13683                            if r5 != (0 as i32 as i64 as u64) { pc += 555 }
    mov64 r0, 57005                                 r0 = 57005 as i32 as i64 as u64
    ldxdw r4, [r3+0x2b0]                    
    lddw r5, 0x6e9de2b30b19f9ea                     r5 load str located at 7970776174128921066
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r1, [r7+0x20]                     
    mov64 r6, r1                                    r6 = r1
    xor64 r6, r5                                    r6 ^= r5   ///  r6 = r6.xor(r5)
    jge r4, r6, lbb_13683                           if r4 >= r6 { pc += 546 }
    ldxdw r4, [r7+0x30]                     
    lddw r5, 0x6edde0930b59ebea                     r5 load str located at 7988788236180384746
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    stxdw [r7+0x30], r4                     
    lddw r0, 0xdeadc0de                             r0 load str located at 3735929054
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jlt r4, r2, lbb_13684                           if r4 < r2 { pc += 538 }
    ldxdw r4, [r7+0x8]                      
    ldxdw r5, [r7+0x10]                     
    stxdw [r3+0x2a0], r5                    
    stxdw [r3+0x298], r4                    
    ldxdw r4, [r7+0x18]                     
    stxdw [r3+0x2b0], r1                    
    stxdw [r3+0x2a8], r4                    
    ldxdw r1, [r7+0x28]                     
    stxdw [r3+0x2b8], r1                    
    lddw r1, 0x6e9de2b30b19f1ea                     r1 load str located at 7970776174128919018
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    ldxdw r1, [r7+0x38]                     
    stxdw [r3+0x320], r1                    
    ldxdw r1, [r7+0x40]                     
    stxdw [r3+0x2c0], r2                    
    stxdw [r3+0x378], r1                    
    mov64 r5, 26                                    r5 = 26 as i32 as i64 as u64
    ja lbb_13684                                    if true { pc += 519 }
lbb_13165:
    stxdw [r10-0x870], r6                   
    stxdw [r10-0x878], r9                   
    mov64 r1, r8                                    r1 = r8
    and64 r1, -8                                    r1 &= -8   ///  r1 = r1.and(-8)
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_13187                            if r1 == (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r2, r1                                    r2 = r1
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lddw r3, 0xc3ebbae2ff2fff3a                     r3 load str located at -4329161132679889094
    lddw r4, 0x1000100010001                        r4 load str located at 281479271743489
    ldxdw r5, [r10-0x870]                   
lbb_13178:
    ldxdw r0, [r5+0x0]                      
    mov64 r9, r6                                    r9 = r6
    xor64 r9, r0                                    r9 ^= r0   ///  r9 = r9.xor(r0)
    xor64 r9, r3                                    r9 ^= r3   ///  r9 = r9.xor(r3)
    stxdw [r5+0x0], r9                      
    add64 r6, r4                                    r6 += r4   ///  r6 = r6.wrapping_add(r4)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jne r2, 0, lbb_13178                            if r2 != (0 as i32 as i64 as u64) { pc += -9 }
lbb_13187:
    ldxdw r2, [r10-0x870]                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    stxdw [r10-0x870], r2                   
    stdw [r10-0x20], 0                      
    mov64 r9, r8                                    r9 = r8
    and64 r9, 7                                     r9 &= 7   ///  r9 = r9.and(7)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x880], r1                   
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    ldxdw r1, [r10-0x20]                    
    xor64 r6, r1                                    r6 ^= r1   ///  r6 = r6.xor(r1)
    lddw r1, 0xc3ebbae2ff2fff3a                     r1 load str located at -4329161132679889094
    xor64 r6, r1                                    r6 ^= r1   ///  r6 = r6.xor(r1)
    stxdw [r10-0x20], r6                    
    ldxdw r1, [r10-0x870]                   
    ldxdw r2, [r10-0x880]                   
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    mov64 r0, 195936478                             r0 = 195936478 as i32 as i64 as u64
    ldxdw r1, [r10-0x878]                   
    jne r1, 3, lbb_13683                            if r1 != (3 as i32 as i64 as u64) { pc += 472 }
    jne r8, 48, lbb_13683                           if r8 != (48 as i32 as i64 as u64) { pc += 471 }
    ldxdw r3, [r10-0x820]                   
    ldxb r2, [r3+0x1]                       
    ldxdw r1, [r10-0x818]                   
    ldxdw r4, [r1+0x270]                    
    stxdw [r10-0x8], r4                     
    lddw r5, 0x4a0178651b8c3c5                      r5 load str located at 333292238089536453
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r5, [r1+0x268]                    
    stxdw [r10-0x10], r5                    
    lddw r0, 0x4a1178751b9c3c6                      r0 load str located at 333573717361279942
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    ldxdw r0, [r1+0x260]                    
    stxdw [r10-0x18], r0                    
    lddw r6, 0x4a2178451bac3c7                      r6 load str located at 333855179453154247
    xor64 r0, r6                                    r0 ^= r6   ///  r0 = r0.xor(r6)
    ldxdw r6, [r1+0x258]                    
    stxdw [r10-0x18], r0                    
    stxdw [r10-0x10], r5                    
    stxdw [r10-0x8], r4                     
    stxdw [r10-0x20], r6                    
    lddw r4, 0xfb5ce87aae443c38                     r4 load str located at -334136658724897736
    xor64 r6, r4                                    r6 ^= r4   ///  r6 = r6.xor(r4)
    stxdw [r10-0x20], r6                    
    ldxdw r4, [r3+0x8]                      
    jne r6, r4, lbb_13251                           if r6 != r4 { pc += 10 }
    ldxdw r4, [r3+0x10]                     
    ldxdw r5, [r10-0x18]                    
    jne r5, r4, lbb_13251                           if r5 != r4 { pc += 7 }
    ldxdw r4, [r3+0x18]                     
    ldxdw r5, [r10-0x10]                    
    jne r5, r4, lbb_13251                           if r5 != r4 { pc += 4 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r3, [r3+0x20]                     
    ldxdw r5, [r10-0x8]                     
    jeq r5, r3, lbb_13252                           if r5 == r3 { pc += 1 }
lbb_13251:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
lbb_13252:
    lddw r0, 0xabad1dea                             r0 load str located at 2880249322
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jeq r2, 0, lbb_13684                            if r2 == (0 as i32 as i64 as u64) { pc += 428 }
    jne r4, 0, lbb_13684                            if r4 != (0 as i32 as i64 as u64) { pc += 427 }
    ldxdw r2, [r1+0x2b0]                    
    lddw r3, 0x6e9de2b30b19f9ea                     r3 load str located at 7970776174128921066
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    mov64 r0, 57005                                 r0 = 57005 as i32 as i64 as u64
    ldxdw r3, [r7+0x20]                     
    jge r2, r3, lbb_13684                           if r2 >= r3 { pc += 420 }
    ldxdw r2, [r10-0x810]                   
    ldxdw r2, [r2+0x58]                     
    lddw r0, 0xdeadc0de                             r0 load str located at 3735929054
    ldxdw r4, [r7+0x30]                     
    jlt r4, r2, lbb_13684                           if r4 < r2 { pc += 414 }
    ldxdw r4, [r7+0x10]                     
    lddw r5, 0x46a912eb23798bd9                     r5 load str located at 5091621654840839129
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r5, [r7+0x8]                      
    stxdw [r1+0x2a0], r4                    
    lddw r4, 0xb957ed15dc877426                     r4 load str located at -5091340175569095642
    xor64 r5, r4                                    r5 ^= r4   ///  r5 = r5.xor(r4)
    stxdw [r1+0x298], r5                    
    lddw r4, 0x6e9de2b30b19f9ea                     r4 load str located at 7970776174128921066
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    ldxdw r5, [r7+0x18]                     
    stxdw [r1+0x2b0], r3                    
    xor64 r5, r4                                    r5 ^= r4   ///  r5 = r5.xor(r4)
    stxdw [r1+0x2a8], r5                    
    lddw r3, 0x6e9de2b30b19f1ea                     r3 load str located at 7970776174128919018
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    ldxdw r4, [r7+0x28]                     
    stxdw [r1+0x2c0], r2                    
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    stxdw [r1+0x2b8], r4                    
    mov64 r5, 26                                    r5 = 26 as i32 as i64 as u64
    ja lbb_13684                                    if true { pc += 388 }
lbb_13296:
    jeq r1, 0, lbb_13313                            if r1 == (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r2, r1                                    r2 = r1
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lddw r3, 0xc3ebbae2ff2fff3a                     r3 load str located at -4329161132679889094
    lddw r4, 0x1000100010001                        r4 load str located at 281479271743489
    mov64 r5, r6                                    r5 = r6
lbb_13304:
    ldxdw r0, [r5+0x0]                      
    mov64 r6, r7                                    r6 = r7
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    xor64 r6, r3                                    r6 ^= r3   ///  r6 = r6.xor(r3)
    stxdw [r5+0x0], r6                      
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jne r2, 0, lbb_13304                            if r2 != (0 as i32 as i64 as u64) { pc += -9 }
lbb_13313:
    ldxdw r6, [r10-0x870]                   
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    stdw [r10-0x20], 0                      
    mov64 r9, r8                                    r9 = r8
    and64 r9, 7                                     r9 &= 7   ///  r9 = r9.and(7)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x880], r1                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    ldxdw r1, [r10-0x20]                    
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    lddw r1, 0xc3ebbae2ff2fff3a                     r1 load str located at -4329161132679889094
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    stxdw [r10-0x20], r7                    
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x880]                   
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2120                                 r1 += -2120   ///  r1 = r1.wrapping_add(-2120 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2080                                 r2 += -2080   ///  r2 = r2.wrapping_add(-2080 as i32 as i64 as u64)
    ldxdw r3, [r10-0x878]                   
    ldxdw r4, [r10-0x870]                   
    mov64 r5, r8                                    r5 = r8
    call function_5976                      
    ldxw r0, [r10-0x844]                    
    ldxw r5, [r10-0x848]                    
    ja lbb_13684                                    if true { pc += 339 }
lbb_13345:
    jeq r1, 0, lbb_13362                            if r1 == (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r2, r1                                    r2 = r1
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lddw r3, 0xc3ebbae2ff2fff3a                     r3 load str located at -4329161132679889094
    lddw r4, 0x1000100010001                        r4 load str located at 281479271743489
    mov64 r5, r6                                    r5 = r6
lbb_13353:
    ldxdw r0, [r5+0x0]                      
    mov64 r6, r7                                    r6 = r7
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    xor64 r6, r3                                    r6 ^= r3   ///  r6 = r6.xor(r3)
    stxdw [r5+0x0], r6                      
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jne r2, 0, lbb_13353                            if r2 != (0 as i32 as i64 as u64) { pc += -9 }
lbb_13362:
    ldxdw r6, [r10-0x870]                   
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    stdw [r10-0x20], 0                      
    mov64 r9, r8                                    r9 = r8
    and64 r9, 7                                     r9 &= 7   ///  r9 = r9.and(7)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x880], r1                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    ldxdw r1, [r10-0x20]                    
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    lddw r1, 0xc3ebbae2ff2fff3a                     r1 load str located at -4329161132679889094
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    stxdw [r10-0x20], r7                    
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x880]                   
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2136                                 r1 += -2136   ///  r1 = r1.wrapping_add(-2136 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2080                                 r2 += -2080   ///  r2 = r2.wrapping_add(-2080 as i32 as i64 as u64)
    ldxdw r3, [r10-0x878]                   
    ldxdw r4, [r10-0x870]                   
    mov64 r5, r8                                    r5 = r8
    call function_1695                      
    ldxw r0, [r10-0x854]                    
    ldxw r5, [r10-0x858]                    
    ja lbb_13684                                    if true { pc += 290 }
lbb_13394:
    stxdw [r10-0x878], r9                   
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    jeq r1, 0, lbb_13413                            if r1 == (0 as i32 as i64 as u64) { pc += 16 }
    mov64 r2, r1                                    r2 = r1
    neg64 r2                                        r2 = -r2   ///  r2 = (r2 as i64).wrapping_neg() as u64
    lddw r3, 0xc3ebbae2ff2fff3a                     r3 load str located at -4329161132679889094
    lddw r4, 0x1000100010001                        r4 load str located at 281479271743489
    mov64 r5, r6                                    r5 = r6
lbb_13404:
    ldxdw r0, [r5+0x0]                      
    mov64 r6, r7                                    r6 = r7
    xor64 r6, r0                                    r6 ^= r0   ///  r6 = r6.xor(r0)
    xor64 r6, r3                                    r6 ^= r3   ///  r6 = r6.xor(r3)
    stxdw [r5+0x0], r6                      
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    jne r2, 0, lbb_13404                            if r2 != (0 as i32 as i64 as u64) { pc += -9 }
lbb_13413:
    ldxdw r6, [r10-0x870]                   
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    stdw [r10-0x20], 0                      
    mov64 r9, r8                                    r9 = r8
    and64 r9, 7                                     r9 &= 7   ///  r9 = r9.and(7)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x880], r1                   
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    ldxdw r1, [r10-0x20]                    
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    lddw r1, 0xc3ebbae2ff2fff3a                     r1 load str located at -4329161132679889094
    xor64 r7, r1                                    r7 ^= r1   ///  r7 = r7.xor(r1)
    stxdw [r10-0x20], r7                    
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x880]                   
    mov64 r3, r9                                    r3 = r9
    call function_17012                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -2112                                 r1 += -2112   ///  r1 = r1.wrapping_add(-2112 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -2080                                 r2 += -2080   ///  r2 = r2.wrapping_add(-2080 as i32 as i64 as u64)
    ldxdw r3, [r10-0x878]                   
    ldxdw r4, [r10-0x870]                   
    mov64 r5, r8                                    r5 = r8
    call function_6241                      
    ldxw r0, [r10-0x83c]                    
    ldxw r5, [r10-0x840]                    
    ja lbb_13684                                    if true { pc += 239 }
lbb_13445:
    stxdw [r2+0x8], r7                      
    ldxdw r3, [r7+0x50]                     
    add64 r7, r3                                    r7 += r3   ///  r7 = r7.wrapping_add(r3)
    add64 r7, 10343                                 r7 += 10343   ///  r7 = r7.wrapping_add(10343 as i32 as i64 as u64)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
    ldxb r3, [r7+0x0]                       
    jne r3, 255, lbb_12115                          if r3 != (255 as i32 as i64 as u64) { pc += -1337 }
lbb_13452:
    stxdw [r2+0x10], r7                     
    ldxdw r3, [r7+0x50]                     
    add64 r7, r3                                    r7 += r3   ///  r7 = r7.wrapping_add(r3)
    add64 r7, 10343                                 r7 += 10343   ///  r7 = r7.wrapping_add(10343 as i32 as i64 as u64)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
    ldxb r3, [r7+0x0]                       
    jne r3, 255, lbb_12124                          if r3 != (255 as i32 as i64 as u64) { pc += -1335 }
lbb_13459:
    stxdw [r2+0x18], r7                     
    ldxdw r3, [r7+0x50]                     
    add64 r7, r3                                    r7 += r3   ///  r7 = r7.wrapping_add(r3)
    add64 r7, 10343                                 r7 += 10343   ///  r7 = r7.wrapping_add(10343 as i32 as i64 as u64)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
    ldxb r3, [r7+0x0]                       
    jne r3, 255, lbb_12133                          if r3 != (255 as i32 as i64 as u64) { pc += -1333 }
lbb_13466:
    stxdw [r2+0x20], r7                     
    ja lbb_12160                                    if true { pc += -1308 }
lbb_13468:
    stxdw [r2+0x8], r7                      
    ldxdw r3, [r7+0x50]                     
    add64 r7, r3                                    r7 += r3   ///  r7 = r7.wrapping_add(r3)
    add64 r7, 10343                                 r7 += 10343   ///  r7 = r7.wrapping_add(10343 as i32 as i64 as u64)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
    ldxb r3, [r7+0x0]                       
    jne r3, 255, lbb_12152                          if r3 != (255 as i32 as i64 as u64) { pc += -1323 }
lbb_13475:
    stxdw [r2+0x10], r7                     
    ja lbb_12160                                    if true { pc += -1317 }
lbb_13477:
    stxdw [r2+0x8], r7                      
    ldxdw r3, [r7+0x50]                     
    add64 r7, r3                                    r7 += r3   ///  r7 = r7.wrapping_add(r3)
    add64 r7, 10343                                 r7 += 10343   ///  r7 = r7.wrapping_add(10343 as i32 as i64 as u64)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
    ldxb r3, [r7+0x0]                       
    jne r3, 255, lbb_12866                          if r3 != (255 as i32 as i64 as u64) { pc += -618 }
lbb_13484:
    stxdw [r2+0x10], r7                     
    ldxdw r3, [r7+0x50]                     
    add64 r7, r3                                    r7 += r3   ///  r7 = r7.wrapping_add(r3)
    add64 r7, 10343                                 r7 += 10343   ///  r7 = r7.wrapping_add(10343 as i32 as i64 as u64)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
    ldxb r3, [r7+0x0]                       
    jne r3, 255, lbb_12875                          if r3 != (255 as i32 as i64 as u64) { pc += -616 }
lbb_13491:
    stxdw [r2+0x18], r7                     
    ja lbb_12160                                    if true { pc += -1333 }
lbb_13493:
    stxdw [r2+0x8], r7                      
    ja lbb_12160                                    if true { pc += -1335 }
lbb_13495:
    stxdw [r10-0x878], r4                   
    mov64 r0, 57005                                 r0 = 57005 as i32 as i64 as u64
    ldxdw r8, [r10-0x810]                   
    ldxdw r1, [r8+0x2b0]                    
    lddw r9, 0x6e9de2b30b19f9ea                     r9 load str located at 7970776174128921066
    xor64 r1, r9                                    r1 ^= r9   ///  r1 = r1.xor(r9)
    ldxdw r4, [r7+0x18]                     
    jge r1, r4, lbb_13684                           if r1 >= r4 { pc += 180 }
    ldxdw r1, [r10-0x818]                   
    ldxdw r3, [r1+0x58]                     
    ldxdw r2, [r7+0x28]                     
    jle r3, r2, lbb_13511                           if r3 <= r2 { pc += 3 }
    lddw r0, 0xdeadc0de                             r0 load str located at 3735929054
    ja lbb_13684                                    if true { pc += 173 }
lbb_13511:
    stxdw [r10-0x870], r4                   
    lddw r2, 0x6e9de2b30b19f1ea                     r2 load str located at 7970776174128919018
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r10-0x880], r3                   
    ldxdw r4, [r7+0x20]                     
    xor64 r4, r2                                    r4 ^= r2   ///  r4 = r4.xor(r2)
    ldxdw r3, [r7+0x10]                     
    lddw r5, 0x46a912eb23798bd9                     r5 load str located at 5091621654840839129
    xor64 r3, r5                                    r3 ^= r5   ///  r3 = r3.xor(r5)
    ldxdw r2, [r7+0x8]                      
    lddw r6, 0xb957ed15dc877426                     r6 load str located at -5091340175569095642
    xor64 r2, r6                                    r2 ^= r6   ///  r2 = r2.xor(r6)
    ldxdw r6, [r7+0x80]                     
    ldxdw r5, [r7+0x58]                     
    ldxdw r1, [r7+0x30]                     
    stxdw [r10-0x888], r1                   
    ldxdw r1, [r10-0x870]                   
    xor64 r1, r9                                    r1 ^= r9   ///  r1 = r1.xor(r9)
    lddw r9, 0xd3198133b7c1776c                     r9 load str located at -3235412798162765972
    xor64 r5, r9                                    r5 ^= r9   ///  r5 = r5.xor(r9)
    lddw r9, 0x504156a22548f8dd                     r9 load str located at 5782998650930657501
    xor64 r6, r9                                    r6 ^= r9   ///  r6 = r6.xor(r9)
    ldxdw r9, [r10-0x880]                   
    stxdw [r8+0x2c0], r9                    
    stxdw [r10-0x890], r4                   
    stxdw [r8+0x2b8], r4                    
    stxdw [r10-0x8a8], r1                   
    stxdw [r8+0x2b0], r1                    
    stxdw [r10-0x898], r3                   
    stxdw [r8+0x2a0], r3                    
    stxdw [r10-0x8a0], r2                   
    stxdw [r8+0x298], r2                    
    stxdw [r8+0x378], r6                    
    stxdw [r8+0x320], r5                    
    lddw r9, 0x6e9de2b30b19f9ea                     r9 load str located at 7970776174128921066
    ldxdw r1, [r10-0x888]                   
    xor64 r1, r9                                    r1 ^= r9   ///  r1 = r1.xor(r9)
    stxdw [r8+0x2a8], r1                    
    ldxdw r2, [r10-0x878]                   
    jeq r2, 24, lbb_12684                           if r2 == (24 as i32 as i64 as u64) { pc += -873 }
    ldxdw r8, [r10-0x808]                   
    ldxdw r2, [r8+0x2b0]                    
    xor64 r2, r9                                    r2 ^= r9   ///  r2 = r2.xor(r9)
    ldxdw r3, [r10-0x870]                   
    jge r2, r3, lbb_13683                           if r2 >= r3 { pc += 121 }
    ldxdw r2, [r7+0x60]                     
    lddw r4, 0xd3198133b7c1776c                     r4 load str located at -3235412798162765972
    xor64 r2, r4                                    r2 ^= r4   ///  r2 = r2.xor(r4)
    ldxdw r4, [r7+0x88]                     
    lddw r5, 0x504156a22548f8dd                     r5 load str located at 5782998650930657501
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r5, [r7+0x38]                     
    ldxdw r1, [r10-0x880]                   
    stxdw [r8+0x2c0], r1                    
    ldxdw r1, [r10-0x890]                   
    stxdw [r8+0x2b8], r1                    
    ldxdw r1, [r10-0x8a8]                   
    stxdw [r8+0x2b0], r1                    
    ldxdw r1, [r10-0x898]                   
    stxdw [r8+0x2a0], r1                    
    ldxdw r1, [r10-0x8a0]                   
    stxdw [r8+0x298], r1                    
    stxdw [r8+0x378], r4                    
    stxdw [r8+0x320], r2                    
    xor64 r5, r9                                    r5 ^= r9   ///  r5 = r5.xor(r9)
    stxdw [r8+0x2a8], r5                    
    ldxdw r2, [r10-0x878]                   
    jeq r2, 32, lbb_12684                           if r2 == (32 as i32 as i64 as u64) { pc += -903 }
    ldxdw r8, [r10-0x800]                   
    ldxdw r2, [r8+0x2b0]                    
    xor64 r2, r9                                    r2 ^= r9   ///  r2 = r2.xor(r9)
    jge r2, r3, lbb_13683                           if r2 >= r3 { pc += 92 }
    ldxdw r2, [r7+0x68]                     
    lddw r4, 0xd3198133b7c1776c                     r4 load str located at -3235412798162765972
    xor64 r2, r4                                    r2 ^= r4   ///  r2 = r2.xor(r4)
    ldxdw r4, [r7+0x90]                     
    lddw r5, 0x504156a22548f8dd                     r5 load str located at 5782998650930657501
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r5, [r7+0x40]                     
    ldxdw r1, [r10-0x880]                   
    stxdw [r8+0x2c0], r1                    
    ldxdw r1, [r10-0x890]                   
    stxdw [r8+0x2b8], r1                    
    ldxdw r1, [r10-0x8a8]                   
    stxdw [r8+0x2b0], r1                    
    ldxdw r1, [r10-0x898]                   
    stxdw [r8+0x2a0], r1                    
    ldxdw r1, [r10-0x8a0]                   
    stxdw [r8+0x298], r1                    
    stxdw [r8+0x378], r4                    
    stxdw [r8+0x320], r2                    
    xor64 r5, r9                                    r5 ^= r9   ///  r5 = r5.xor(r9)
    stxdw [r8+0x2a8], r5                    
    ldxdw r2, [r10-0x878]                   
    jeq r2, 40, lbb_12684                           if r2 == (40 as i32 as i64 as u64) { pc += -932 }
    ldxdw r8, [r10-0x7f8]                   
    ldxdw r2, [r8+0x2b0]                    
    xor64 r2, r9                                    r2 ^= r9   ///  r2 = r2.xor(r9)
    jge r2, r3, lbb_13683                           if r2 >= r3 { pc += 63 }
    ldxdw r2, [r7+0x70]                     
    lddw r4, 0xd3198133b7c1776c                     r4 load str located at -3235412798162765972
    xor64 r2, r4                                    r2 ^= r4   ///  r2 = r2.xor(r4)
    ldxdw r4, [r7+0x98]                     
    lddw r5, 0x504156a22548f8dd                     r5 load str located at 5782998650930657501
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r5, [r7+0x48]                     
    ldxdw r1, [r10-0x880]                   
    stxdw [r8+0x2c0], r1                    
    ldxdw r1, [r10-0x890]                   
    stxdw [r8+0x2b8], r1                    
    ldxdw r1, [r10-0x8a8]                   
    stxdw [r8+0x2b0], r1                    
    ldxdw r1, [r10-0x898]                   
    stxdw [r8+0x2a0], r1                    
    ldxdw r1, [r10-0x8a0]                   
    stxdw [r8+0x298], r1                    
    stxdw [r8+0x378], r4                    
    stxdw [r8+0x320], r2                    
    xor64 r5, r9                                    r5 ^= r9   ///  r5 = r5.xor(r9)
    stxdw [r8+0x2a8], r5                    
    ldxdw r2, [r10-0x878]                   
    jeq r2, 48, lbb_12684                           if r2 == (48 as i32 as i64 as u64) { pc += -961 }
    ldxdw r8, [r10-0x7f0]                   
    ldxdw r2, [r8+0x2b0]                    
    xor64 r2, r9                                    r2 ^= r9   ///  r2 = r2.xor(r9)
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jge r2, r3, lbb_13684                           if r2 >= r3 { pc += 34 }
    ldxdw r2, [r7+0x78]                     
    lddw r3, 0xd3198133b7c1776c                     r3 load str located at -3235412798162765972
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    ldxdw r3, [r7+0xa0]                     
    lddw r4, 0x504156a22548f8dd                     r4 load str located at 5782998650930657501
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    ldxdw r4, [r7+0x50]                     
    ldxdw r1, [r10-0x880]                   
    stxdw [r8+0x2c0], r1                    
    ldxdw r1, [r10-0x890]                   
    stxdw [r8+0x2b8], r1                    
    ldxdw r1, [r10-0x8a8]                   
    stxdw [r8+0x2b0], r1                    
    ldxdw r1, [r10-0x898]                   
    stxdw [r8+0x2a0], r1                    
    ldxdw r1, [r10-0x8a0]                   
    stxdw [r8+0x298], r1                    
    stxdw [r8+0x378], r3                    
    stxdw [r8+0x320], r2                    
    lddw r1, 0x6e9de2b30b19f9ea                     r1 load str located at 7970776174128921066
    xor64 r4, r1                                    r4 ^= r1   ///  r4 = r4.xor(r1)
    stxdw [r8+0x2a8], r4                    
    mov64 r5, 26                                    r5 = 26 as i32 as i64 as u64
    ldxdw r1, [r10-0x878]                   
    jeq r1, 56, lbb_13684                           if r1 == (56 as i32 as i64 as u64) { pc += 6 }
    mov64 r1, 5                                     r1 = 5 as i32 as i64 as u64
    mov64 r2, 5                                     r2 = 5 as i32 as i64 as u64
    lddw r3, 0x1000239f8 --> b"\x00\x00\x00\x00\x804\x02\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x18\x08\x00…        r3 load str located at 4295113208
    call function_15494                     
lbb_13683:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_13684:
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    jsle r5, 12, lbb_13694                          if (r5 as i64) <= (12 as i32 as i64) { pc += 7 }
    jsgt r5, 19, lbb_13701                          if (r5 as i64) > (19 as i32 as i64) { pc += 13 }
    jsle r5, 15, lbb_13713                          if (r5 as i64) <= (15 as i32 as i64) { pc += 24 }
    jsgt r5, 17, lbb_13733                          if (r5 as i64) > (17 as i32 as i64) { pc += 43 }
    jne r5, 16, lbb_13761                           if r5 != (16 as i32 as i64 as u64) { pc += 70 }
    lddw r0, 0x1100000000                           r0 load str located at 73014444032
    ja lbb_13776                                    if true { pc += 82 }
lbb_13694:
    jsle r5, 5, lbb_13707                           if (r5 as i64) <= (5 as i32 as i64) { pc += 12 }
    jsle r5, 8, lbb_13723                           if (r5 as i64) <= (8 as i32 as i64) { pc += 27 }
    jsgt r5, 10, lbb_13742                          if (r5 as i64) > (10 as i32 as i64) { pc += 45 }
    jne r5, 9, lbb_13783                            if r5 != (9 as i32 as i64 as u64) { pc += 85 }
    lddw r0, 0xa00000000                            r0 load str located at 42949672960
    ja lbb_13776                                    if true { pc += 75 }
lbb_13701:
    jsle r5, 22, lbb_13718                          if (r5 as i64) <= (22 as i32 as i64) { pc += 16 }
    jsgt r5, 24, lbb_13737                          if (r5 as i64) > (24 as i32 as i64) { pc += 34 }
    jne r5, 23, lbb_13764                           if r5 != (23 as i32 as i64 as u64) { pc += 60 }
    lddw r0, 0x1800000000                           r0 load str located at 103079215104
    ja lbb_13776                                    if true { pc += 69 }
lbb_13707:
    jsgt r5, 2, lbb_13728                           if (r5 as i64) > (2 as i32 as i64) { pc += 20 }
    jeq r5, 0, lbb_13770                            if r5 == (0 as i32 as i64 as u64) { pc += 61 }
    jne r5, 1, lbb_13789                            if r5 != (1 as i32 as i64 as u64) { pc += 79 }
    lddw r0, 0x200000000                            r0 load str located at 8589934592
    ja lbb_13776                                    if true { pc += 63 }
lbb_13713:
    jeq r5, 13, lbb_13746                           if r5 == (13 as i32 as i64 as u64) { pc += 32 }
    jne r5, 14, lbb_13755                           if r5 != (14 as i32 as i64 as u64) { pc += 40 }
    lddw r0, 0xf00000000                            r0 load str located at 64424509440
    ja lbb_13776                                    if true { pc += 58 }
lbb_13718:
    jeq r5, 20, lbb_13749                           if r5 == (20 as i32 as i64 as u64) { pc += 30 }
    jne r5, 21, lbb_13758                           if r5 != (21 as i32 as i64 as u64) { pc += 38 }
    lddw r0, 0x1600000000                           r0 load str located at 94489280512
    ja lbb_13776                                    if true { pc += 53 }
lbb_13723:
    jeq r5, 6, lbb_13752                            if r5 == (6 as i32 as i64 as u64) { pc += 28 }
    jne r5, 7, lbb_13780                            if r5 != (7 as i32 as i64 as u64) { pc += 55 }
    lddw r0, 0x800000000                            r0 load str located at 34359738368
    ja lbb_13776                                    if true { pc += 48 }
lbb_13728:
    jeq r5, 3, lbb_13777                            if r5 == (3 as i32 as i64 as u64) { pc += 48 }
    jne r5, 4, lbb_13792                            if r5 != (4 as i32 as i64 as u64) { pc += 62 }
    lddw r0, 0x500000000                            r0 load str located at 21474836480
    ja lbb_13776                                    if true { pc += 43 }
lbb_13733:
    jne r5, 18, lbb_13767                           if r5 != (18 as i32 as i64 as u64) { pc += 33 }
    lddw r0, 0x1300000000                           r0 load str located at 81604378624
    ja lbb_13776                                    if true { pc += 39 }
lbb_13737:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jne r5, 25, lbb_13776                           if r5 != (25 as i32 as i64 as u64) { pc += 37 }
    lddw r0, 0x1a00000000                           r0 load str located at 111669149696
    ja lbb_13776                                    if true { pc += 34 }
lbb_13742:
    jne r5, 11, lbb_13786                           if r5 != (11 as i32 as i64 as u64) { pc += 43 }
    lddw r0, 0xc00000000                            r0 load str located at 51539607552
    ja lbb_13776                                    if true { pc += 30 }
lbb_13746:
    lddw r0, 0xe00000000                            r0 load str located at 60129542144
    ja lbb_13776                                    if true { pc += 27 }
lbb_13749:
    lddw r0, 0x1500000000                           r0 load str located at 90194313216
    ja lbb_13776                                    if true { pc += 24 }
lbb_13752:
    lddw r0, 0x700000000                            r0 load str located at 30064771072
    ja lbb_13776                                    if true { pc += 21 }
lbb_13755:
    lddw r0, 0x1000000000                           r0 load str located at 68719476736
    ja lbb_13776                                    if true { pc += 18 }
lbb_13758:
    lddw r0, 0x1700000000                           r0 load str located at 98784247808
    ja lbb_13776                                    if true { pc += 15 }
lbb_13761:
    lddw r0, 0x1200000000                           r0 load str located at 77309411328
    ja lbb_13776                                    if true { pc += 12 }
lbb_13764:
    lddw r0, 0x1900000000                           r0 load str located at 107374182400
    ja lbb_13776                                    if true { pc += 9 }
lbb_13767:
    lddw r0, 0x1400000000                           r0 load str located at 85899345920
    ja lbb_13776                                    if true { pc += 6 }
lbb_13770:
    mov64 r1, r0                                    r1 = r0
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jeq r1, 0, lbb_13795                            if r1 == (0 as i32 as i64 as u64) { pc += 21 }
lbb_13774:
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
lbb_13776:
    exit                                    
lbb_13777:
    lddw r0, 0x400000000                            r0 load str located at 17179869184
    ja lbb_13776                                    if true { pc += -4 }
lbb_13780:
    lddw r0, 0x900000000                            r0 load str located at 38654705664
    ja lbb_13776                                    if true { pc += -7 }
lbb_13783:
    lddw r0, 0xb00000000                            r0 load str located at 47244640256
    ja lbb_13776                                    if true { pc += -10 }
lbb_13786:
    lddw r0, 0xd00000000                            r0 load str located at 55834574848
    ja lbb_13776                                    if true { pc += -13 }
lbb_13789:
    lddw r0, 0x300000000                            r0 load str located at 12884901888
    ja lbb_13776                                    if true { pc += -16 }
lbb_13792:
    lddw r0, 0x600000000                            r0 load str located at 25769803776
    ja lbb_13776                                    if true { pc += -19 }
lbb_13795:
    lddw r0, 0x100000000                            r0 load str located at 4294967296
    ja lbb_13776                                    if true { pc += -22 }
lbb_13798:
    lddw r1, 0x1000230d3 --> b"from_bytes"          r1 load str located at 4295110867
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    call function_581                       
lbb_13803:
    lddw r1, 0x1000230d3 --> b"from_bytes"          r1 load str located at 4295110867
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    call function_581                       

function_13808:
    mov64 r4, r3                                    r4 = r3
    mov64 r8, -64                                   r8 = -64 as i32 as i64 as u64
    ldxdw r6, [r2+0x18]                     
    mov64 r5, r6                                    r5 = r6
    jne r6, 0, lbb_13819                            if r6 != (0 as i32 as i64 as u64) { pc += 6 }
    mov64 r8, -128                                  r8 = -128 as i32 as i64 as u64
    ldxdw r5, [r2+0x10]                     
    jne r5, 0, lbb_13819                            if r5 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r8, -192                                  r8 = -192 as i32 as i64 as u64
    ldxdw r5, [r2+0x8]                      
    jeq r5, 0, lbb_14528                            if r5 == (0 as i32 as i64 as u64) { pc += 709 }
lbb_13819:
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 16                                    r3 >>= 16   ///  r3 = r3.wrapping_shr(16)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    lddw r3, 0x5555555555555555                     r3 load str located at 6148914691236517205
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r3                                    r0 &= r3   ///  r0 = r0.and(r3)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    lddw r0, 0x3333333333333333                     r0 load str located at 3689348814741910323
    mov64 r3, r5                                    r3 = r5
    and64 r3, r0                                    r3 &= r0   ///  r3 = r3.and(r0)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, r0                                    r5 &= r0   ///  r5 = r5.and(r0)
    add64 r3, r5                                    r3 += r5   ///  r3 = r3.wrapping_add(r5)
    mov64 r5, r3                                    r5 = r3
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    add64 r3, r5                                    r3 += r5   ///  r3 = r3.wrapping_add(r5)
    lddw r5, 0xf0f0f0f0f0f0f0f                      r5 load str located at 1085102592571150095
    and64 r3, r5                                    r3 &= r5   ///  r3 = r3.and(r5)
    lddw r5, 0x101010101010101                      r5 load str located at 72340172838076673
    mul64 r3, r5                                    r3 *= r5   ///  r3 = r3.wrapping_mul(r5)
    rsh64 r3, 56                                    r3 >>= 56   ///  r3 = r3.wrapping_shr(56)
    sub64 r8, r3                                    r8 -= r3   ///  r8 = r8.wrapping_sub(r3)
    add64 r8, 320                                   r8 += 320   ///  r8 = r8.wrapping_add(320 as i32 as i64 as u64)
    mov64 r7, -64                                   r7 = -64 as i32 as i64 as u64
    ldxdw r5, [r4+0x18]                     
    jne r5, 0, lbb_14578                            if r5 != (0 as i32 as i64 as u64) { pc += 712 }
lbb_13866:
    mov64 r7, -128                                  r7 = -128 as i32 as i64 as u64
    ldxdw r5, [r4+0x10]                     
    jne r5, 0, lbb_14578                            if r5 != (0 as i32 as i64 as u64) { pc += 709 }
    mov64 r7, -192                                  r7 = -192 as i32 as i64 as u64
    ldxdw r5, [r4+0x8]                      
    jne r5, 0, lbb_14578                            if r5 != (0 as i32 as i64 as u64) { pc += 706 }
    mov64 r9, r6                                    r9 = r6
    mov64 r7, 64                                    r7 = 64 as i32 as i64 as u64
    ldxdw r5, [r4+0x0]                      
    mov64 r6, 64                                    r6 = 64 as i32 as i64 as u64
    jeq r5, 0, lbb_13920                            if r5 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    mov64 r0, r5                                    r0 = r5
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    mov64 r3, r0                                    r3 = r0
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    mov64 r3, r0                                    r3 = r0
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    mov64 r3, r0                                    r3 = r0
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    mov64 r3, r0                                    r3 = r0
    rsh64 r3, 16                                    r3 >>= 16   ///  r3 = r3.wrapping_shr(16)
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    mov64 r3, r0                                    r3 = r0
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    xor64 r0, -1                                    r0 ^= -1   ///  r0 = r0.xor(-1)
    lddw r3, 0x5555555555555555                     r3 load str located at 6148914691236517205
    mov64 r6, r0                                    r6 = r0
    rsh64 r6, 1                                     r6 >>= 1   ///  r6 = r6.wrapping_shr(1)
    and64 r6, r3                                    r6 &= r3   ///  r6 = r6.and(r3)
    sub64 r0, r6                                    r0 -= r6   ///  r0 = r0.wrapping_sub(r6)
    lddw r3, 0x3333333333333333                     r3 load str located at 3689348814741910323
    mov64 r6, r0                                    r6 = r0
    and64 r6, r3                                    r6 &= r3   ///  r6 = r6.and(r3)
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    and64 r0, r3                                    r0 &= r3   ///  r0 = r0.and(r3)
    add64 r6, r0                                    r6 += r0   ///  r6 = r6.wrapping_add(r0)
    mov64 r3, r6                                    r3 = r6
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r6, r3                                    r6 += r3   ///  r6 = r6.wrapping_add(r3)
    lddw r3, 0xf0f0f0f0f0f0f0f                      r3 load str located at 1085102592571150095
    and64 r6, r3                                    r6 &= r3   ///  r6 = r6.and(r3)
    lddw r3, 0x101010101010101                      r3 load str located at 72340172838076673
    mul64 r6, r3                                    r6 *= r3   ///  r6 = r6.wrapping_mul(r3)
    rsh64 r6, 56                                    r6 >>= 56   ///  r6 = r6.wrapping_shr(56)
lbb_13920:
    jeq r5, 0, lbb_14640                            if r5 == (0 as i32 as i64 as u64) { pc += 719 }
    sub64 r7, r6                                    r7 -= r6   ///  r7 = r7.wrapping_sub(r6)
    mov64 r6, r9                                    r6 = r9
    jlt r8, r7, lbb_14623                           if r8 < r7 { pc += 699 }
lbb_13924:
    stxdw [r10-0x208], r1                   
    jge r7, 65, lbb_14000                           if r7 >= (65 as i32 as i64 as u64) { pc += 74 }
    ldxdw r7, [r4+0x0]                      
    jeq r7, 0, lbb_14667                            if r7 == (0 as i32 as i64 as u64) { pc += 739 }
    ldxdw r9, [r2+0x10]                     
    ldxdw r8, [r2+0x8]                      
    ldxdw r1, [r2+0x0]                      
    stxdw [r10-0x190], r1                   
    mov64 r3, r6                                    r3 = r6
    div64 r6, r7                                    r6 /= r7   ///  r6 = r6 / r7
    mov64 r1, r6                                    r1 = r6
    mul64 r1, r7                                    r1 *= r7   ///  r1 = r1.wrapping_mul(r7)
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -312                                  r1 += -312   ///  r1 = r1.wrapping_add(-312 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    mov64 r4, r7                                    r4 = r7
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17794                     
    ldxdw r2, [r10-0x138]                   
    stxdw [r10-0x198], r2                   
    ldxdw r3, [r10-0x130]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -328                                  r1 += -328   ///  r1 = r1.wrapping_add(-328 as i32 as i64 as u64)
    mov64 r4, r7                                    r4 = r7
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    ldxdw r1, [r10-0x148]                   
    sub64 r9, r1                                    r9 -= r1   ///  r9 = r9.wrapping_sub(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -344                                  r1 += -344   ///  r1 = r1.wrapping_add(-344 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r9                                    r3 = r9
    mov64 r4, r7                                    r4 = r7
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17794                     
    ldxdw r2, [r10-0x158]                   
    stxdw [r10-0x1a0], r2                   
    ldxdw r3, [r10-0x150]                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -360                                  r1 += -360   ///  r1 = r1.wrapping_add(-360 as i32 as i64 as u64)
    mov64 r4, r7                                    r4 = r7
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    ldxdw r1, [r10-0x168]                   
    sub64 r8, r1                                    r8 -= r1   ///  r8 = r8.wrapping_sub(r1)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -376                                  r1 += -376   ///  r1 = r1.wrapping_add(-376 as i32 as i64 as u64)
    ldxdw r9, [r10-0x190]                   
    mov64 r2, r9                                    r2 = r9
    mov64 r3, r8                                    r3 = r8
    mov64 r4, r7                                    r4 = r7
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17794                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -392                                  r1 += -392   ///  r1 = r1.wrapping_add(-392 as i32 as i64 as u64)
    ldxdw r8, [r10-0x178]                   
    ldxdw r3, [r10-0x170]                   
    mov64 r2, r8                                    r2 = r8
    mov64 r4, r7                                    r4 = r7
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    ldxdw r2, [r10-0x208]                   
    stxdw [r2+0x18], r6                     
    ldxdw r1, [r10-0x198]                   
    stxdw [r2+0x10], r1                     
    ldxdw r1, [r10-0x1a0]                   
    stxdw [r2+0x8], r1                      
    stxdw [r2+0x0], r8                      
    ldxdw r1, [r10-0x188]                   
    sub64 r9, r1                                    r9 -= r1   ///  r9 = r9.wrapping_sub(r1)
    stxdw [r2+0x20], r9                     
    stdw [r2+0x28], 0                       
    stdw [r2+0x30], 0                       
    stdw [r2+0x38], 0                       
    ja lbb_14635                                    if true { pc += 635 }
lbb_14000:
    ldxdw r3, [r4+0x18]                     
    stxdw [r10-0x70], r3                    
    ldxdw r3, [r4+0x10]                     
    stxdw [r10-0x78], r3                    
    ldxdw r3, [r4+0x8]                      
    stxdw [r10-0x80], r3                    
    ldxdw r3, [r4+0x0]                      
    stxdw [r10-0x88], r3                    
    add64 r7, -1                                    r7 += -1   ///  r7 = r7.wrapping_add(-1 as i32 as i64 as u64)
    rsh64 r7, 6                                     r7 >>= 6   ///  r7 = r7.wrapping_shr(6)
    mov64 r3, r7                                    r3 = r7
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    mov64 r4, r10                                   r4 = r10
    add64 r4, -136                                  r4 += -136   ///  r4 = r4.wrapping_add(-136 as i32 as i64 as u64)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    stxdw [r10-0x198], r4                   
    ldxdw r5, [r4+0x0]                      
    jeq r5, 0, lbb_14062                            if r5 == (0 as i32 as i64 as u64) { pc += 44 }
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    mov64 r0, r5                                    r0 = r5
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    mov64 r3, r0                                    r3 = r0
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    mov64 r3, r0                                    r3 = r0
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    mov64 r3, r0                                    r3 = r0
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    mov64 r3, r0                                    r3 = r0
    rsh64 r3, 16                                    r3 >>= 16   ///  r3 = r3.wrapping_shr(16)
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    mov64 r3, r0                                    r3 = r0
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    or64 r0, r3                                     r0 |= r3   ///  r0 = r0.or(r3)
    xor64 r0, -1                                    r0 ^= -1   ///  r0 = r0.xor(-1)
    lddw r3, 0x5555555555555555                     r3 load str located at 6148914691236517205
    mov64 r4, r0                                    r4 = r0
    rsh64 r4, 1                                     r4 >>= 1   ///  r4 = r4.wrapping_shr(1)
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    sub64 r0, r4                                    r0 -= r4   ///  r0 = r0.wrapping_sub(r4)
    lddw r3, 0x3333333333333333                     r3 load str located at 3689348814741910323
    mov64 r4, r0                                    r4 = r0
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    and64 r0, r3                                    r0 &= r3   ///  r0 = r0.and(r3)
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    mov64 r3, r4                                    r3 = r4
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    lddw r3, 0xf0f0f0f0f0f0f0f                      r3 load str located at 1085102592571150095
    and64 r4, r3                                    r4 &= r3   ///  r4 = r4.and(r3)
    lddw r3, 0x101010101010101                      r3 load str located at 72340172838076673
    mul64 r4, r3                                    r4 *= r3   ///  r4 = r4.wrapping_mul(r3)
    rsh64 r4, 56                                    r4 >>= 56   ///  r4 = r4.wrapping_shr(56)
    ja lbb_14063                                    if true { pc += 1 }
lbb_14062:
    mov64 r4, 64                                    r4 = 64 as i32 as i64 as u64
lbb_14063:
    stxdw [r10-0x1a0], r6                   
    stxdw [r10-0x190], r8                   
    stxdw [r10-0x200], r4                   
    stxdw [r10-0x1f8], r7                   
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jeq r5, 0, lbb_14070                            if r5 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_14070:
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    ldxdw r4, [r10-0x70]                    
    ldxdw r0, [r10-0x78]                    
    ldxdw r6, [r10-0x80]                    
    ldxdw r9, [r10-0x88]                    
    mov64 r8, r10                                   r8 = r10
    add64 r8, -184                                  r8 += -184   ///  r8 = r8.wrapping_add(-184 as i32 as i64 as u64)
    mov64 r7, r8                                    r7 = r8
    add64 r7, r3                                    r7 += r3   ///  r7 = r7.wrapping_add(r3)
    stdw [r10-0xa0], 0                      
    stdw [r10-0xa8], 0                      
    stdw [r10-0xb0], 0                      
    stdw [r10-0xb8], 0                      
    ldxdw r1, [r10-0x200]                   
    and64 r1, 63                                    r1 &= 63   ///  r1 = r1.and(63)
    mov64 r3, r9                                    r3 = r9
    lsh64 r3, r1                                    r3 <<= r1   ///  r3 = r3.wrapping_shl(r1 as u32)
    stxdw [r7+0x0], r3                      
    mov64 r3, 16                                    r3 = 16 as i32 as i64 as u64
    jeq r5, 0, lbb_14091                            if r5 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 8                                     r3 = 8 as i32 as i64 as u64
lbb_14091:
    add64 r8, r3                                    r8 += r3   ///  r8 = r8.wrapping_add(r3)
    mov64 r3, r6                                    r3 = r6
    lsh64 r3, r1                                    r3 <<= r1   ///  r3 = r3.wrapping_shl(r1 as u32)
    stxdw [r8+0x0], r3                      
    mov64 r3, r0                                    r3 = r0
    lsh64 r3, r1                                    r3 <<= r1   ///  r3 = r3.wrapping_shl(r1 as u32)
    stxdw [r7+0x10], r3                     
    jeq r5, 0, lbb_14101                            if r5 == (0 as i32 as i64 as u64) { pc += 2 }
    lsh64 r4, r1                                    r4 <<= r1   ///  r4 = r4.wrapping_shl(r1 as u32)
    stxdw [r10-0xa0], r4                    
lbb_14101:
    mov64 r3, r1                                    r3 = r1
    ldxdw r1, [r10-0x190]                   
    add64 r1, -1                                    r1 += -1   ///  r1 = r1.wrapping_add(-1 as i32 as i64 as u64)
    stxdw [r10-0x190], r1                   
    stxdw [r10-0x210], r3                   
    jeq r3, 0, lbb_14124                            if r3 == (0 as i32 as i64 as u64) { pc += 17 }
    add64 r7, 16                                    r7 += 16   ///  r7 = r7.wrapping_add(16 as i32 as i64 as u64)
    ldxdw r3, [r10-0x200]                   
    neg64 r3                                        r3 = -r3   ///  r3 = (r3 as i64).wrapping_neg() as u64
    and64 r3, 63                                    r3 &= 63   ///  r3 = r3.and(63)
    rsh64 r9, r3                                    r9 >>= r3   ///  r9 = r9.wrapping_shr(r3 as u32)
    ldxdw r4, [r8+0x0]                      
    add64 r4, r9                                    r4 += r9   ///  r4 = r4.wrapping_add(r9)
    stxdw [r8+0x0], r4                      
    rsh64 r6, r3                                    r6 >>= r3   ///  r6 = r6.wrapping_shr(r3 as u32)
    ldxdw r4, [r7+0x0]                      
    add64 r4, r6                                    r4 += r6   ///  r4 = r4.wrapping_add(r6)
    stxdw [r7+0x0], r4                      
    jeq r5, 0, lbb_14124                            if r5 == (0 as i32 as i64 as u64) { pc += 4 }
    rsh64 r0, r3                                    r0 >>= r3   ///  r0 = r0.wrapping_shr(r3 as u32)
    ldxdw r3, [r10-0xa0]                    
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
    stxdw [r10-0xa0], r3                    
lbb_14124:
    ldxdw r1, [r10-0x190]                   
    rsh64 r1, 6                                     r1 >>= 6   ///  r1 = r1.wrapping_shr(6)
    stxdw [r10-0x190], r1                   
    ldxdw r3, [r10-0xa0]                    
    stxdw [r10-0x70], r3                    
    ldxdw r3, [r10-0xa8]                    
    stxdw [r10-0x78], r3                    
    ldxdw r3, [r10-0xb0]                    
    stxdw [r10-0x80], r3                    
    ldxdw r3, [r10-0xb8]                    
    stxdw [r10-0x88], r3                    
    mov64 r4, 64                                    r4 = 64 as i32 as i64 as u64
    ldxdw r1, [r10-0x200]                   
    sub64 r4, r1                                    r4 -= r1   ///  r4 = r4.wrapping_sub(r1)
    mov64 r5, r4                                    r5 = r4
    rsh64 r5, 6                                     r5 >>= 6   ///  r5 = r5.wrapping_shr(6)
    lsh64 r5, 3                                     r5 <<= 3   ///  r5 = r5.wrapping_shl(3)
    mov64 r6, r2                                    r6 = r2
    add64 r6, r5                                    r6 += r5   ///  r6 = r6.wrapping_add(r5)
    ldxdw r3, [r2+0x0]                      
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    ldxdw r0, [r6+0x10]                     
    mov64 r7, r0                                    r7 = r0
    rsh64 r7, r4                                    r7 >>= r4   ///  r7 = r7.wrapping_shr(r4 as u32)
    ldxdw r8, [r6+0x0]                      
    rsh64 r8, r4                                    r8 >>= r4   ///  r8 = r8.wrapping_shr(r4 as u32)
    ldxdw r6, [r6+0x8]                      
    mov64 r9, r6                                    r9 = r6
    rsh64 r9, r4                                    r9 >>= r4   ///  r9 = r9.wrapping_shr(r4 as u32)
    jeq r1, 0, lbb_14168                            if r1 == (0 as i32 as i64 as u64) { pc += 14 }
    add64 r5, r2                                    r5 += r2   ///  r5 = r5.wrapping_add(r2)
    ldxdw r2, [r5+0x18]                     
    rsh64 r2, r4                                    r2 >>= r4   ///  r2 = r2.wrapping_shr(r4 as u32)
    jeq r4, 0, lbb_14173                            if r4 == (0 as i32 as i64 as u64) { pc += 15 }
    ldxdw r5, [r10-0x210]                   
    ldxdw r1, [r10-0x1a0]                   
    lsh64 r1, r5                                    r1 <<= r5   ///  r1 = r1.wrapping_shl(r5 as u32)
    add64 r1, r7                                    r1 += r7   ///  r1 = r1.wrapping_add(r7)
    lsh64 r0, r5                                    r0 <<= r5   ///  r0 = r0.wrapping_shl(r5 as u32)
    add64 r0, r9                                    r0 += r9   ///  r0 = r0.wrapping_add(r9)
    lsh64 r6, r5                                    r6 <<= r5   ///  r6 = r6.wrapping_shl(r5 as u32)
    add64 r6, r8                                    r6 += r8   ///  r6 = r6.wrapping_add(r8)
    mov64 r7, r1                                    r7 = r1
    ja lbb_14176                                    if true { pc += 8 }
lbb_14168:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jeq r4, 0, lbb_14173                            if r4 == (0 as i32 as i64 as u64) { pc += 3 }
    add64 r0, r9                                    r0 += r9   ///  r0 = r0.wrapping_add(r9)
    add64 r6, r8                                    r6 += r8   ///  r6 = r6.wrapping_add(r8)
    ja lbb_14175                                    if true { pc += 2 }
lbb_14173:
    mov64 r6, r8                                    r6 = r8
    mov64 r0, r9                                    r0 = r9
lbb_14175:
    ldxdw r5, [r10-0x210]                   
lbb_14176:
    ldxdw r1, [r10-0x190]                   
    mov64 r8, r1                                    r8 = r1
    ldxdw r4, [r10-0x1f8]                   
    sub64 r8, r4                                    r8 -= r4   ///  r8 = r8.wrapping_sub(r4)
    mov64 r9, r4                                    r9 = r4
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x1c0], r9                   
    lsh64 r3, r5                                    r3 <<= r5   ///  r3 = r3.wrapping_shl(r5 as u32)
    stxdw [r10-0x48], r2                    
    stxdw [r10-0x50], r7                    
    stxdw [r10-0x58], r0                    
    stxdw [r10-0x60], r6                    
    mov64 r5, r8                                    r5 = r8
    stxdw [r10-0x68], r3                    
    lsh64 r1, 3                                     r1 <<= 3   ///  r1 = r1.wrapping_shl(3)
    mov64 r2, r4                                    r2 = r4
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    sub64 r1, r2                                    r1 -= r2   ///  r1 = r1.wrapping_sub(r2)
    mov64 r3, r10                                   r3 = r10
    add64 r3, -104                                  r3 += -104   ///  r3 = r3.wrapping_add(-104 as i32 as i64 as u64)
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    stxdw [r10-0x190], r3                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -136                                  r1 += -136   ///  r1 = r1.wrapping_add(-136 as i32 as i64 as u64)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxdw r1, [r10-0x70]                    
    stxdw [r10-0x1d8], r1                   
    ldxdw r1, [r10-0x78]                    
    stxdw [r10-0x1e0], r1                   
    ldxdw r1, [r10-0x80]                    
    stxdw [r10-0x1e8], r1                   
    ldxdw r1, [r10-0x88]                    
    stxdw [r10-0x1f0], r1                   
    add64 r4, 2                                     r4 += 2   ///  r4 = r4.wrapping_add(2 as i32 as i64 as u64)
    stxdw [r10-0x1c8], r4                   
    ldxdw r1, [r2-0x8]                      
    stxdw [r10-0x1a0], r1                   
    stdw [r10-0x28], 0                      
    stdw [r10-0x30], 0                      
    stdw [r10-0x38], 0                      
    stdw [r10-0x40], 0                      
    ldxdw r1, [r10-0x198]                   
    ldxdw r1, [r1+0x0]                      
    stxdw [r10-0x198], r1                   
    mov64 r6, r5                                    r6 = r5
    stxdw [r10-0x1b0], r5                   
lbb_14222:
    mov64 r9, r6                                    r9 = r6
    ldxdw r1, [r10-0x1c0]                   
    add64 r9, r1                                    r9 += r1   ///  r9 = r9.wrapping_add(r1)
    jge r9, 5, lbb_14652                            if r9 >= (5 as i32 as i64 as u64) { pc += 426 }
    mov64 r1, r9                                    r1 = r9
    lsh64 r1, 3                                     r1 <<= 3   ///  r1 = r1.wrapping_shl(3)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -104                                  r2 += -104   ///  r2 = r2.wrapping_add(-104 as i32 as i64 as u64)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r8, -1                                    r8 = -1 as i32 as i64 as u64
    stxdw [r10-0x1d0], r2                   
    ldxdw r3, [r2+0x0]                      
    ldxdw r1, [r10-0x198]                   
    stxdw [r10-0x1a8], r6                   
    jge r3, r1, lbb_14308                           if r3 >= r1 { pc += 71 }
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x1f8]                   
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    jge r1, 5, lbb_14662                            if r1 >= (5 as i32 as i64 as u64) { pc += 421 }
    add64 r9, -2                                    r9 += -2   ///  r9 = r9.wrapping_add(-2 as i32 as i64 as u64)
    jgt r9, 4, lbb_14652                            if r9 > (4 as i32 as i64 as u64) { pc += 409 }
    lsh64 r1, 3                                     r1 <<= 3   ///  r1 = r1.wrapping_shl(3)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -104                                  r2 += -104   ///  r2 = r2.wrapping_add(-104 as i32 as i64 as u64)
    stxdw [r10-0x1b8], r2                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxdw r7, [r2+0x0]                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -200                                  r1 += -200   ///  r1 = r1.wrapping_add(-200 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    ldxdw r6, [r10-0x198]                   
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17794                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -216                                  r1 += -216   ///  r1 = r1.wrapping_add(-216 as i32 as i64 as u64)
    ldxdw r8, [r10-0xc8]                    
    ldxdw r3, [r10-0xc0]                    
    mov64 r2, r8                                    r2 = r8
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    ldxdw r1, [r10-0xd8]                    
    sub64 r7, r1                                    r7 -= r1   ///  r7 = r7.wrapping_sub(r1)
    lsh64 r9, 3                                     r9 <<= 3   ///  r9 = r9.wrapping_shl(3)
    ldxdw r1, [r10-0x1b8]                   
    add64 r1, r9                                    r1 += r9   ///  r1 = r1.wrapping_add(r9)
    ldxdw r9, [r1+0x0]                      
    ldxdw r6, [r10-0x1a8]                   
    ja lbb_14276                                    if true { pc += 4 }
lbb_14272:
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    mov64 r7, r1                                    r7 = r1
    jne r2, 0, lbb_14308                            if r2 != (0 as i32 as i64 as u64) { pc += 32 }
lbb_14276:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -232                                  r1 += -232   ///  r1 = r1.wrapping_add(-232 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0x1a0]                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0xe8]                    
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jge r9, r3, lbb_14295                           if r9 >= r3 { pc += 8 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r3, [r10-0xe0]                    
    jle r7, r3, lbb_14297                           if r7 <= r3 { pc += 7 }
lbb_14290:
    jeq r7, r3, lbb_14299                           if r7 == r3 { pc += 8 }
lbb_14291:
    mov64 r1, r2                                    r1 = r2
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jeq r1, 0, lbb_14301                            if r1 == (0 as i32 as i64 as u64) { pc += 7 }
    ja lbb_14308                                    if true { pc += 13 }
lbb_14295:
    ldxdw r3, [r10-0xe0]                    
    jgt r7, r3, lbb_14290                           if r7 > r3 { pc += -7 }
lbb_14297:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jne r7, r3, lbb_14291                           if r7 != r3 { pc += -8 }
lbb_14299:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_14308                            if r1 != (0 as i32 as i64 as u64) { pc += 7 }
lbb_14301:
    mov64 r1, r7                                    r1 = r7
    ldxdw r2, [r10-0x198]                   
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r1, r7, lbb_14272                           if r1 < r7 { pc += -34 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ja lbb_14272                                    if true { pc += -36 }
lbb_14308:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -248                                  r1 += -248   ///  r1 = r1.wrapping_add(-248 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0x1f0]                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -264                                  r1 += -264   ///  r1 = r1.wrapping_add(-264 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0x1e8]                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -280                                  r1 += -280   ///  r1 = r1.wrapping_add(-280 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0x1e0]                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -296                                  r1 += -296   ///  r1 = r1.wrapping_add(-296 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0x1d8]                   
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    ldxdw r1, [r10-0x108]                   
    ldxdw r4, [r10-0xf0]                    
    mov64 r2, r4                                    r2 = r4
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r2, r4, lbb_14344                           if r2 < r4 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_14344:
    ldxdw r5, [r10-0x100]                   
    add64 r5, r3                                    r5 += r3   ///  r5 = r5.wrapping_add(r3)
    ldxdw r4, [r10-0x118]                   
    mov64 r3, r5                                    r3 = r5
    add64 r3, r4                                    r3 += r4   ///  r3 = r3.wrapping_add(r4)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r3, r5, lbb_14352                           if r3 < r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_14352:
    ldxdw r5, [r10-0x110]                   
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxdw r0, [r10-0x128]                   
    mov64 r4, r5                                    r4 = r5
    add64 r4, r0                                    r4 += r0   ///  r4 = r4.wrapping_add(r0)
    jlt r4, r5, lbb_14359                           if r4 < r5 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_14359:
    ldxdw r5, [r10-0xf8]                    
    stxdw [r10-0xb8], r5                    
    stxdw [r10-0xb0], r2                    
    stxdw [r10-0xa8], r3                    
    stxdw [r10-0xa0], r4                    
    ldxdw r2, [r10-0x120]                   
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    stxdw [r10-0x98], r2                    
    ldxdw r1, [r10-0x1b0]                   
    jgt r1, 5, lbb_14636                            if r1 > (5 as i32 as i64 as u64) { pc += 267 }
    mov64 r1, 5                                     r1 = 5 as i32 as i64 as u64
    mov64 r2, 5                                     r2 = 5 as i32 as i64 as u64
    sub64 r2, r6                                    r2 -= r6   ///  r2 = r2.wrapping_sub(r6)
    mov64 r3, r2                                    r3 = r2
    ldxdw r4, [r10-0x1c8]                   
    jlt r2, r4, lbb_14376                           if r2 < r4 { pc += 1 }
    ldxdw r3, [r10-0x1c8]                   
lbb_14376:
    jeq r6, 5, lbb_14658                            if r6 == (5 as i32 as i64 as u64) { pc += 281 }
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    stxdw [r10-0x1b8], r6                   
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_14389                                    if true { pc += 6 }
lbb_14383:
    sub64 r1, r7                                    r1 -= r7   ///  r1 = r1.wrapping_sub(r7)
    stxdw [r6+0x0], r1                      
    or64 r0, r9                                     r0 |= r9   ///  r0 = r0.or(r9)
    add64 r4, 8                                     r4 += 8   ///  r4 = r4.wrapping_add(8 as i32 as i64 as u64)
    add64 r5, 1                                     r5 += 1   ///  r5 = r5.wrapping_add(1 as i32 as i64 as u64)
    jge r5, r3, lbb_14405                           if r5 >= r3 { pc += 16 }
lbb_14389:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -184                                  r1 += -184   ///  r1 = r1.wrapping_add(-184 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    ldxdw r1, [r1+0x0]                      
    mov64 r7, r1                                    r7 = r1
    add64 r7, r0                                    r7 += r0   ///  r7 = r7.wrapping_add(r0)
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jlt r7, r1, lbb_14399                           if r7 < r1 { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_14399:
    ldxdw r6, [r10-0x190]                   
    add64 r6, r4                                    r6 += r4   ///  r6 = r6.wrapping_add(r4)
    ldxdw r1, [r6+0x0]                      
    jlt r1, r7, lbb_14383                           if r1 < r7 { pc += -20 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ja lbb_14383                                    if true { pc += -22 }
lbb_14405:
    and64 r0, 1                                     r0 &= 1   ///  r0 = r0.and(1)
    jeq r0, 0, lbb_14442                            if r0 == (0 as i32 as i64 as u64) { pc += 35 }
    ldxdw r1, [r10-0x1c0]                   
    jlt r2, r1, lbb_14410                           if r2 < r1 { pc += 1 }
    ldxdw r2, [r10-0x1c0]                   
lbb_14410:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ja lbb_14419                                    if true { pc += 5 }
lbb_14414:
    stxdw [r6+0x0], r1                      
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    add64 r4, 1                                     r4 += 1   ///  r4 = r4.wrapping_add(1 as i32 as i64 as u64)
    jge r4, r2, lbb_14437                           if r4 >= r2 { pc += 18 }
lbb_14419:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -136                                  r1 += -136   ///  r1 = r1.wrapping_add(-136 as i32 as i64 as u64)
    add64 r1, r3                                    r1 += r3   ///  r1 = r1.wrapping_add(r3)
    ldxdw r1, [r1+0x0]                      
    mov64 r7, r1                                    r7 = r1
    add64 r7, r5                                    r7 += r5   ///  r7 = r7.wrapping_add(r5)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r7, r1, lbb_14429                           if r7 < r1 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_14429:
    ldxdw r6, [r10-0x190]                   
    add64 r6, r3                                    r6 += r3   ///  r6 = r6.wrapping_add(r3)
    ldxdw r9, [r6+0x0]                      
    mov64 r1, r9                                    r1 = r9
    add64 r1, r7                                    r1 += r7   ///  r1 = r1.wrapping_add(r7)
    jlt r1, r9, lbb_14414                           if r1 < r9 { pc += -21 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_14414                                    if true { pc += -23 }
lbb_14437:
    ldxdw r2, [r10-0x1d0]                   
    ldxdw r1, [r2+0x0]                      
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    stxdw [r2+0x0], r1                      
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
lbb_14442:
    ldxdw r5, [r10-0x1b0]                   
    ldxdw r3, [r10-0x1a8]                   
    jge r5, 4, lbb_14657                            if r5 >= (4 as i32 as i64 as u64) { pc += 212 }
    mov64 r1, r3                                    r1 = r3
    lsh64 r1, 3                                     r1 <<= 3   ///  r1 = r1.wrapping_shl(3)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    stxdw [r2+0x0], r8                      
    ldxdw r1, [r10-0x190]                   
    add64 r1, -8                                    r1 += -8   ///  r1 = r1.wrapping_add(-8 as i32 as i64 as u64)
    stxdw [r10-0x190], r1                   
    ldxdw r6, [r10-0x1b8]                   
    jne r3, 0, lbb_14222                            if r3 != (0 as i32 as i64 as u64) { pc += -234 }
    ldxdw r1, [r10-0x48]                    
    stxdw [r10-0x98], r1                    
    ldxdw r1, [r10-0x50]                    
    stxdw [r10-0xa0], r1                    
    ldxdw r2, [r10-0x58]                    
    stxdw [r10-0xa8], r2                    
    ldxdw r3, [r10-0x60]                    
    stxdw [r10-0xb0], r3                    
    ldxdw r4, [r10-0x68]                    
    stxdw [r10-0xb8], r4                    
    ldxdw r5, [r10-0x210]                   
    rsh64 r4, r5                                    r4 >>= r5   ///  r4 = r4.wrapping_shr(r5 as u32)
    stxdw [r10-0x20], r4                    
    rsh64 r3, r5                                    r3 >>= r5   ///  r3 = r3.wrapping_shr(r5 as u32)
    stxdw [r10-0x18], r3                    
    rsh64 r2, r5                                    r2 >>= r5   ///  r2 = r2.wrapping_shr(r5 as u32)
    stxdw [r10-0x10], r2                    
    rsh64 r1, r5                                    r1 >>= r5   ///  r1 = r1.wrapping_shr(r5 as u32)
    stxdw [r10-0x8], r1                     
    ldxdw r0, [r10-0x208]                   
    ldxdw r6, [r10-0x200]                   
    jeq r6, 0, lbb_14511                            if r6 == (0 as i32 as i64 as u64) { pc += 33 }
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    neg64 r6                                        r6 = -r6   ///  r6 = (r6 as i64).wrapping_neg() as u64
    and64 r6, 63                                    r6 &= 63   ///  r6 = r6.and(63)
    ja lbb_14499                                    if true { pc += 17 }
lbb_14482:
    mov64 r1, 4                                     r1 = 4 as i32 as i64 as u64
    mov64 r3, 3                                     r3 = 3 as i32 as i64 as u64
lbb_14484:
    mov64 r4, r2                                    r4 = r2
    lsh64 r4, 3                                     r4 <<= 3   ///  r4 = r4.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -184                                  r5 += -184   ///  r5 = r5.wrapping_add(-184 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxdw r4, [r5+0x0]                      
    lsh64 r4, r6                                    r4 <<= r6   ///  r4 = r4.wrapping_shl(r6 as u32)
    lsh64 r3, 3                                     r3 <<= 3   ///  r3 = r3.wrapping_shl(3)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -32                                   r5 += -32   ///  r5 = r5.wrapping_add(-32 as i32 as i64 as u64)
    add64 r5, r3                                    r5 += r3   ///  r5 = r5.wrapping_add(r3)
    ldxdw r3, [r5+0x0]                      
    or64 r3, r4                                     r3 |= r4   ///  r3 = r3.or(r4)
    stxdw [r5+0x0], r3                      
    jgt r2, 3, lbb_14511                            if r2 > (3 as i32 as i64 as u64) { pc += 12 }
lbb_14499:
    mov64 r2, r1                                    r2 = r1
    jgt r2, 3, lbb_14482                            if r2 > (3 as i32 as i64 as u64) { pc += -19 }
    mov64 r1, r2                                    r1 = r2
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    mov64 r3, r2                                    r3 = r2
    add64 r3, -1                                    r3 += -1   ///  r3 = r3.wrapping_add(-1 as i32 as i64 as u64)
    jle r3, 3, lbb_14484                            if r3 <= (3 as i32 as i64 as u64) { pc += -22 }
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
    mov64 r2, 4                                     r2 = 4 as i32 as i64 as u64
    lddw r3, 0x1000239c8 --> b"\x00\x00\x00\x00\xfb0\x02\x00\x15\x00\x00\x00\x00\x00\x00\x00\x07\x00\x00…        r3 load str located at 4295113160
    call function_15494                     
lbb_14511:
    ldxdw r1, [r10-0x8]                     
    stxdw [r0+0x38], r1                     
    ldxdw r1, [r10-0x10]                    
    stxdw [r0+0x30], r1                     
    ldxdw r1, [r10-0x18]                    
    stxdw [r0+0x28], r1                     
    ldxdw r1, [r10-0x20]                    
    stxdw [r0+0x20], r1                     
    ldxdw r1, [r10-0x28]                    
    stxdw [r0+0x18], r1                     
    ldxdw r1, [r10-0x30]                    
    stxdw [r0+0x10], r1                     
    ldxdw r1, [r10-0x38]                    
    stxdw [r0+0x8], r1                      
    ldxdw r1, [r10-0x40]                    
    stxdw [r0+0x0], r1                      
    ja lbb_14635                                    if true { pc += 107 }
lbb_14528:
    mov64 r8, 64                                    r8 = 64 as i32 as i64 as u64
    ldxdw r5, [r2+0x0]                      
    mov64 r0, 64                                    r0 = 64 as i32 as i64 as u64
    jeq r5, 0, lbb_14574                            if r5 == (0 as i32 as i64 as u64) { pc += 42 }
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 16                                    r3 >>= 16   ///  r3 = r3.wrapping_shr(16)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    lddw r3, 0x5555555555555555                     r3 load str located at 6148914691236517205
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r3                                    r0 &= r3   ///  r0 = r0.and(r3)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    lddw r3, 0x3333333333333333                     r3 load str located at 3689348814741910323
    mov64 r0, r5                                    r0 = r5
    and64 r0, r3                                    r0 &= r3   ///  r0 = r0.and(r3)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, r3                                    r5 &= r3   ///  r5 = r5.and(r3)
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    mov64 r3, r0                                    r3 = r0
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    lddw r3, 0xf0f0f0f0f0f0f0f                      r3 load str located at 1085102592571150095
    and64 r0, r3                                    r0 &= r3   ///  r0 = r0.and(r3)
    lddw r3, 0x101010101010101                      r3 load str located at 72340172838076673
    mul64 r0, r3                                    r0 *= r3   ///  r0 = r0.wrapping_mul(r3)
    rsh64 r0, 56                                    r0 >>= 56   ///  r0 = r0.wrapping_shr(56)
lbb_14574:
    sub64 r8, r0                                    r8 -= r0   ///  r8 = r8.wrapping_sub(r0)
    mov64 r7, -64                                   r7 = -64 as i32 as i64 as u64
    ldxdw r5, [r4+0x18]                     
    jeq r5, 0, lbb_13866                            if r5 == (0 as i32 as i64 as u64) { pc += -712 }
lbb_14578:
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 2                                     r3 >>= 2   ///  r3 = r3.wrapping_shr(2)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 4                                     r3 >>= 4   ///  r3 = r3.wrapping_shr(4)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 8                                     r3 >>= 8   ///  r3 = r3.wrapping_shr(8)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 16                                    r3 >>= 16   ///  r3 = r3.wrapping_shr(16)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    mov64 r3, r5                                    r3 = r5
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    or64 r5, r3                                     r5 |= r3   ///  r5 = r5.or(r3)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    lddw r3, 0x5555555555555555                     r3 load str located at 6148914691236517205
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r3                                    r0 &= r3   ///  r0 = r0.and(r3)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    lddw r0, 0x3333333333333333                     r0 load str located at 3689348814741910323
    mov64 r3, r5                                    r3 = r5
    and64 r3, r0                                    r3 &= r0   ///  r3 = r3.and(r0)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, r0                                    r5 &= r0   ///  r5 = r5.and(r0)
    add64 r3, r5                                    r3 += r5   ///  r3 = r3.wrapping_add(r5)
    mov64 r5, r3                                    r5 = r3
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    add64 r3, r5                                    r3 += r5   ///  r3 = r3.wrapping_add(r5)
    lddw r5, 0xf0f0f0f0f0f0f0f                      r5 load str located at 1085102592571150095
    and64 r3, r5                                    r3 &= r5   ///  r3 = r3.and(r5)
    lddw r5, 0x101010101010101                      r5 load str located at 72340172838076673
    mul64 r3, r5                                    r3 *= r5   ///  r3 = r3.wrapping_mul(r5)
    rsh64 r3, 56                                    r3 >>= 56   ///  r3 = r3.wrapping_shr(56)
    sub64 r7, r3                                    r7 -= r3   ///  r7 = r7.wrapping_sub(r3)
    add64 r7, 320                                   r7 += 320   ///  r7 = r7.wrapping_add(320 as i32 as i64 as u64)
    jge r8, r7, lbb_13924                           if r8 >= r7 { pc += -699 }
lbb_14623:
    stdw [r1+0x18], 0                       
    stdw [r1+0x10], 0                       
    stdw [r1+0x8], 0                        
    stdw [r1+0x0], 0                        
    ldxdw r3, [r2+0x0]                      
    stxdw [r1+0x20], r3                     
    ldxdw r3, [r2+0x8]                      
    stxdw [r1+0x28], r3                     
    ldxdw r3, [r2+0x10]                     
    stxdw [r1+0x30], r3                     
    ldxdw r2, [r2+0x18]                     
    stxdw [r1+0x38], r2                     
lbb_14635:
    exit                                    
lbb_14636:
    mov64 r2, 5                                     r2 = 5 as i32 as i64 as u64
    lddw r3, 0x1000239c8 --> b"\x00\x00\x00\x00\xfb0\x02\x00\x15\x00\x00\x00\x00\x00\x00\x00\x07\x00\x00…        r3 load str located at 4295113160
    call function_16487                     
lbb_14640:
    lddw r1, 0x100023ae8 --> b"\x00\x00\x00\x00p1\x02\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295113448
    stxdw [r10-0xb8], r1                    
    stdw [r10-0x98], 0                      
    stdw [r10-0xb0], 1                      
    stdw [r10-0xa0], 0                      
    stdw [r10-0xa8], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -184                                  r1 += -184   ///  r1 = r1.wrapping_add(-184 as i32 as i64 as u64)
    lddw r2, 0x1000239c8 --> b"\x00\x00\x00\x00\xfb0\x02\x00\x15\x00\x00\x00\x00\x00\x00\x00\x07\x00\x00…        r2 load str located at 4295113160
    call function_15475                     
lbb_14652:
    mov64 r1, r9                                    r1 = r9
    mov64 r2, 5                                     r2 = 5 as i32 as i64 as u64
    lddw r3, 0x1000239c8 --> b"\x00\x00\x00\x00\xfb0\x02\x00\x15\x00\x00\x00\x00\x00\x00\x00\x07\x00\x00…        r3 load str located at 4295113160
    call function_15494                     
lbb_14657:
    mov64 r1, r5                                    r1 = r5
lbb_14658:
    mov64 r2, 4                                     r2 = 4 as i32 as i64 as u64
    lddw r3, 0x1000239c8 --> b"\x00\x00\x00\x00\xfb0\x02\x00\x15\x00\x00\x00\x00\x00\x00\x00\x07\x00\x00…        r3 load str located at 4295113160
    call function_15494                     
lbb_14662:
    mov64 r1, -1                                    r1 = -1 as i32 as i64 as u64
    mov64 r2, 5                                     r2 = 5 as i32 as i64 as u64
    lddw r3, 0x1000239c8 --> b"\x00\x00\x00\x00\xfb0\x02\x00\x15\x00\x00\x00\x00\x00\x00\x00\x07\x00\x00…        r3 load str located at 4295113160
    call function_15494                     
lbb_14667:
    lddw r1, 0x1000239c8 --> b"\x00\x00\x00\x00\xfb0\x02\x00\x15\x00\x00\x00\x00\x00\x00\x00\x07\x00\x00…        r1 load str located at 4295113160
    call function_16647                     

function_14670:
    mov64 r6, r1                                    r6 = r1
    ldxdw r1, [r6+0x38]                     
    lddw r2, 0x96296f3e7c155a2a                     r2 load str located at -7626442179814794710
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    ldxdw r2, [r6+0x30]                     
    lddw r3, 0x96286f3f7c145a29                     r3 load str located at -7626723650496603607
    xor64 r2, r3                                    r2 ^= r3   ///  r2 = r2.xor(r3)
    ldxdw r3, [r6+0x28]                     
    lddw r4, 0x962b6f3c7c175a28                     r4 load str located at -7625879238451176920
    xor64 r3, r4                                    r3 ^= r4   ///  r3 = r3.xor(r4)
    ldxdw r4, [r6+0x20]                     
    lddw r5, 0x962a6f3d7c165a2f                     r5 load str located at -7626160709132985809
    xor64 r4, r5                                    r4 ^= r5   ///  r4 = r4.xor(r5)
    ldxdw r5, [r6+0x18]                     
    lddw r0, 0x962d6f3a7c115a2e                     r0 load str located at -7625316297088083410
    xor64 r5, r0                                    r5 ^= r0   ///  r5 = r5.xor(r0)
    ldxdw r0, [r6+0x10]                     
    lddw r7, 0x962c6f3b7c105a2d                     r7 load str located at -7625597767769892307
    xor64 r0, r7                                    r0 ^= r7   ///  r0 = r0.xor(r7)
    ldxdw r7, [r6+0x8]                      
    lddw r8, 0x962f6f387c135a2c                     r8 load str located at -7624753355724465620
    xor64 r7, r8                                    r7 ^= r8   ///  r7 = r7.xor(r8)
    ldxdw r8, [r6+0x0]                      
    lddw r9, 0x69d190c683eda5d3                     r9 load str located at 7625034826406274515
    xor64 r8, r9                                    r8 ^= r9   ///  r8 = r8.xor(r9)
    stxdw [r6+0x0], r8                      
    stxdw [r6+0x8], r7                      
    stxdw [r6+0x10], r0                     
    stxdw [r6+0x18], r5                     
    stxdw [r6+0x20], r4                     
    stxdw [r6+0x28], r3                     
    stxdw [r6+0x30], r2                     
    stxdw [r6+0x38], r1                     
    ldxdw r1, [r6+0x40]                     
    lddw r2, 0x96266f317c1a5a2b                     r2 load str located at -7627286660579173845
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    stxdw [r6+0x40], r1                     
    ldxdw r1, [r6+0x48]                     
    lddw r2, 0x96276f307c1b5a24                     r2 load str located at -7627005189897364956
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    stxdw [r6+0x48], r1                     
    ldxdw r1, [r6+0x50]                     
    lddw r2, 0x96246f337c185a25                     r2 load str located at -7627849601942791643
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    stxdw [r6+0x50], r1                     
    lddw r1, 0x96256f327c195a26                     r1 load str located at -7627568131260982746
    ldxdw r2, [r6+0x58]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x58], r2                     
    lddw r1, 0x96226f357c1e5a27                     r1 load str located at -7628412543305885145
    ldxdw r2, [r6+0x60]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x60], r2                     
    lddw r1, 0x96236f347c1f5a20                     r1 load str located at -7628131072624076256
    ldxdw r2, [r6+0x68]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x68], r2                     
    lddw r1, 0x96206f377c1c5a21                     r1 load str located at -7628975484669502943
    ldxdw r2, [r6+0x70]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x70], r2                     
    lddw r1, 0x96216f367c1d5a22                     r1 load str located at -7628694013987694046
    ldxdw r2, [r6+0x78]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x78], r2                     
    lddw r1, 0x963e6f297c025a23                     r1 load str located at -7620531295499429341
    ldxdw r2, [r6+0x80]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x80], r2                     
    lddw r1, 0x963f6f287c035a3c                     r1 load str located at -7620249824817620420
    ldxdw r2, [r6+0x88]                     
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x88], r2                     
    mov64 r1, r6                                    r1 = r6
    add64 r1, 144                                   r1 += 144   ///  r1 = r1.wrapping_add(144 as i32 as i64 as u64)
    call function_1369                      
    lddw r1, 0xfb5ce87aae443c38                     r1 load str located at -334136658724897736
    ldxdw r2, [r6+0x180]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x180], r2                    
    lddw r2, 0x4a2178451bac3c7                      r2 load str located at 333855179453154247
    ldxdw r3, [r6+0x188]                    
    xor64 r3, r2                                    r3 ^= r2   ///  r3 = r3.xor(r2)
    stxdw [r6+0x188], r3                    
    lddw r3, 0x4a1178751b9c3c6                      r3 load str located at 333573717361279942
    ldxdw r4, [r6+0x190]                    
    xor64 r4, r3                                    r4 ^= r3   ///  r4 = r4.xor(r3)
    stxdw [r6+0x190], r4                    
    lddw r4, 0x4a0178651b8c3c5                      r4 load str located at 333292238089536453
    ldxdw r5, [r6+0x198]                    
    xor64 r5, r4                                    r5 ^= r4   ///  r5 = r5.xor(r4)
    stxdw [r6+0x198], r5                    
    ldxdw r5, [r6+0x1a0]                    
    xor64 r5, r1                                    r5 ^= r1   ///  r5 = r5.xor(r1)
    stxdw [r6+0x1a0], r5                    
    ldxdw r5, [r6+0x1a8]                    
    xor64 r5, r2                                    r5 ^= r2   ///  r5 = r5.xor(r2)
    stxdw [r6+0x1a8], r5                    
    ldxdw r5, [r6+0x1b0]                    
    xor64 r5, r3                                    r5 ^= r3   ///  r5 = r5.xor(r3)
    stxdw [r6+0x1b0], r5                    
    ldxdw r5, [r6+0x1b8]                    
    xor64 r5, r4                                    r5 ^= r4   ///  r5 = r5.xor(r4)
    stxdw [r6+0x1b8], r5                    
    ldxdw r5, [r6+0x1c0]                    
    xor64 r5, r1                                    r5 ^= r1   ///  r5 = r5.xor(r1)
    stxdw [r6+0x1c0], r5                    
    ldxdw r5, [r6+0x1c8]                    
    xor64 r5, r2                                    r5 ^= r2   ///  r5 = r5.xor(r2)
    stxdw [r6+0x1c8], r5                    
    ldxdw r5, [r6+0x1d0]                    
    xor64 r5, r3                                    r5 ^= r3   ///  r5 = r5.xor(r3)
    stxdw [r6+0x1d0], r5                    
    ldxdw r5, [r6+0x1d8]                    
    xor64 r5, r4                                    r5 ^= r4   ///  r5 = r5.xor(r4)
    stxdw [r6+0x1d8], r5                    
    ldxdw r5, [r6+0x1e0]                    
    xor64 r5, r1                                    r5 ^= r1   ///  r5 = r5.xor(r1)
    stxdw [r6+0x1e0], r5                    
    ldxdw r5, [r6+0x1e8]                    
    xor64 r5, r2                                    r5 ^= r2   ///  r5 = r5.xor(r2)
    stxdw [r6+0x1e8], r5                    
    ldxdw r5, [r6+0x1f0]                    
    xor64 r5, r3                                    r5 ^= r3   ///  r5 = r5.xor(r3)
    stxdw [r6+0x1f0], r5                    
    ldxdw r5, [r6+0x1f8]                    
    xor64 r5, r4                                    r5 ^= r4   ///  r5 = r5.xor(r4)
    stxdw [r6+0x1f8], r5                    
    ldxdw r5, [r6+0x200]                    
    xor64 r5, r1                                    r5 ^= r1   ///  r5 = r5.xor(r1)
    stxdw [r6+0x200], r5                    
    ldxdw r1, [r6+0x208]                    
    xor64 r1, r2                                    r1 ^= r2   ///  r1 = r1.xor(r2)
    stxdw [r6+0x208], r1                    
    ldxdw r1, [r6+0x210]                    
    xor64 r1, r3                                    r1 ^= r3   ///  r1 = r1.xor(r3)
    stxdw [r6+0x210], r1                    
    ldxdw r1, [r6+0x218]                    
    xor64 r1, r4                                    r1 ^= r4   ///  r1 = r1.xor(r4)
    stxdw [r6+0x218], r1                    
    lddw r1, 0xb957ed15dc877c26                     r1 load str located at -5091340175569093594
    ldxdw r2, [r6+0x220]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x220], r2                    
    lddw r1, 0x46a912eb237873d9                     r1 load str located at 5091621654840767449
    ldxdw r2, [r6+0x228]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x228], r2                    
    lddw r1, 0xf539f2cf9513d4a1                     r1 load str located at -776322487371443039
    ldxdw r2, [r6+0x230]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x230], r2                    
    lddw r1, 0xadcf8e5743314562                     r1 load str located at -5922358479528311454
    ldxdw r2, [r6+0x238]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x238], r2                    
    lddw r1, 0xb957ed15dc877426                     r1 load str located at -5091340175569095642
    ldxdw r2, [r6+0x240]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x240], r2                    
    lddw r1, 0x46a912eb23798bd9                     r1 load str located at 5091621654840839129
    ldxdw r2, [r6+0x248]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x248], r2                    
    lddw r1, 0x6e9de2b30b19f9ea                     r1 load str located at 7970776174128921066
    ldxdw r2, [r6+0x250]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x250], r2                    
    ldxdw r2, [r6+0x258]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x258], r2                    
    lddw r1, 0x6e9de2b30b19f1ea                     r1 load str located at 7970776174128919018
    ldxdw r2, [r6+0x260]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x260], r2                    
    ldxdw r2, [r6+0x268]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x268], r2                    
    lddw r1, 0xdbf169454ad22fa                      r1 load str located at 990535269376402170
    ldxdw r2, [r6+0x270]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x270], r2                    
    lddw r1, 0xf241e96aab522d05                     r1 load str located at -990253798694703867
    ldxdw r2, [r6+0x278]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x278], r2                    
    lddw r1, 0xf242e969ab532d04                     r1 load str located at -989972328012894972
    ldxdw r2, [r6+0x280]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x280], r2                    
    lddw r1, 0xf243e968ab502d07                     r1 load str located at -989690857331348217
    ldxdw r2, [r6+0x288]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x288], r2                    
    lddw r1, 0xf244e96fab512d06                     r1 load str located at -989409352289800954
    ldxdw r2, [r6+0x290]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x290], r2                    
    lddw r1, 0xf245e96eab562d01                     r1 load str located at -989127881607729919
    ldxdw r2, [r6+0x298]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x298], r2                    
    lddw r1, 0xf246e96dab572d00                     r1 load str located at -988846410925921024
    ldxdw r2, [r6+0x2a0]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x2a0], r2                    
    lddw r1, 0xf247e96cab542d03                     r1 load str located at -988564940244374269
    ldxdw r2, [r6+0x2a8]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x2a8], r2                    
    lddw r1, 0xf248e963ab552d02                     r1 load str located at -988283503922303742
    ldxdw r2, [r6+0x2b0]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x2b0], r2                    
    lddw r1, 0xf249e962ab5a2d0d                     r1 load str located at -988002033240232691
    ldxdw r2, [r6+0x2b8]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x2b8], r2                    
    lddw r1, 0xed5f563e78eee80b                     r1 load str located at -1342259337616234485
    ldxdw r2, [r6+0x2c0]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x2c0], r2                    
    lddw r1, 0xd3198133b7c1776c                     r1 load str located at -3235412798162765972
    ldxdw r2, [r6+0x2c8]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x2c8], r2                    
    lddw r1, 0xb82c93d08854ebff                     r1 load str located at -5175599347905795073
    ldxdw r2, [r6+0x2d0]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x2d0], r2                    
    lddw r1, 0x47d26c2e77aa1400                     r1 load str located at 5175317868634051584
    ldxdw r2, [r6+0x2d8]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x2d8], r2                    
    lddw r1, 0x47d16c2d77a91401                     r1 load str located at 5175036389362308097
    ldxdw r2, [r6+0x2e0]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x2e0], r2                    
    lddw r1, 0x47d06c2c77a81402                     r1 load str located at 5174754910090564610
    ldxdw r2, [r6+0x2e8]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x2e8], r2                    
    lddw r1, 0x47d76c2b77af1403                     r1 load str located at 5176725230633030659
    ldxdw r2, [r6+0x2f0]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x2f0], r2                    
    lddw r1, 0x47d66c2a77ae1404                     r1 load str located at 5176443751361287172
    ldxdw r2, [r6+0x2f8]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x2f8], r2                    
    lddw r1, 0x47d56c2977ad1405                     r1 load str located at 5176162272089543685
    ldxdw r2, [r6+0x300]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x300], r2                    
    lddw r1, 0x47d46c2877ac1406                     r1 load str located at 5175880792817800198
    ldxdw r2, [r6+0x308]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x308], r2                    
    lddw r1, 0x47db6c2777a31407                     r1 load str located at 5177851113359217671
    ldxdw r2, [r6+0x310]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x310], r2                    
    lddw r1, 0x47da6c2677a21408                     r1 load str located at 5177569634087474184
    ldxdw r2, [r6+0x318]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x318], r2                    
    lddw r1, 0x504156a22548f8dd                     r1 load str located at 5782998650930657501
    ldxdw r2, [r6+0x320]                    
    xor64 r2, r1                                    r2 ^= r1   ///  r2 = r2.xor(r1)
    stxdw [r6+0x320], r2                    
    exit                                    

custom_panic:
    ldxdw r1, [r1+0x8]                      
    ldxdw r2, [r1+0x8]                      
    ldxdw r1, [r1+0x0]                      
    syscall [invalid]                       
    lddw r1, 0x100023720 --> b"** PANICKED **"        r1 load str located at 4295112480
    mov64 r2, 14                                    r2 = 14 as i32 as i64 as u64
    syscall [invalid]                       
    exit                                    

function_15003:
    mov64 r9, r3                                    r9 = r3
    ldxdw r3, [r2+0x8]                      
    ldxdw r6, [r3+0x0]                      
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r0, [r2+0x0]                      
    ldxdw r7, [r0+0x0]                      
    stxdw [r10-0xd0], r6                    
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xe0], r7                    
    sth [r10-0xc8], 0                       
    sth [r10-0xd8], 1                       
    stb [r10-0xb9], 18                      
    ldxdw r2, [r2+0x10]                     
    ldxb r6, [r2+0x0]                       
    stxb [r10-0xb8], r6                     
    ldxb r6, [r2+0x1]                       
    stxb [r10-0xb7], r6                     
    ldxb r6, [r2+0x2]                       
    stxb [r10-0xb6], r6                     
    ldxb r6, [r2+0x3]                       
    stxb [r10-0xb5], r6                     
    ldxb r6, [r2+0x4]                       
    stxb [r10-0xb4], r6                     
    ldxb r6, [r2+0x5]                       
    stxb [r10-0xb3], r6                     
    ldxb r6, [r2+0x6]                       
    stxb [r10-0xb2], r6                     
    ldxb r6, [r2+0x7]                       
    stxb [r10-0xb1], r6                     
    ldxb r6, [r2+0x8]                       
    stxb [r10-0xb0], r6                     
    ldxb r6, [r2+0x9]                       
    stxb [r10-0xaf], r6                     
    ldxb r6, [r2+0xa]                       
    stxb [r10-0xae], r6                     
    ldxb r6, [r2+0xb]                       
    stxb [r10-0xad], r6                     
    ldxb r6, [r2+0xc]                       
    stxb [r10-0xac], r6                     
    ldxb r6, [r2+0xd]                       
    stxb [r10-0xab], r6                     
    ldxb r6, [r2+0xe]                       
    stxb [r10-0xaa], r6                     
    ldxb r6, [r2+0xf]                       
    stxb [r10-0xa9], r6                     
    ldxb r6, [r2+0x10]                      
    stxb [r10-0xa8], r6                     
    ldxb r6, [r2+0x11]                      
    stxb [r10-0xa7], r6                     
    ldxb r6, [r2+0x12]                      
    stxb [r10-0xa6], r6                     
    ldxb r6, [r2+0x13]                      
    stxb [r10-0xa5], r6                     
    ldxb r6, [r2+0x14]                      
    stxb [r10-0xa4], r6                     
    ldxb r6, [r2+0x15]                      
    stxb [r10-0xa3], r6                     
    ldxb r6, [r2+0x16]                      
    stxb [r10-0xa2], r6                     
    ldxb r6, [r2+0x17]                      
    stxb [r10-0xa1], r6                     
    ldxb r6, [r2+0x18]                      
    stxb [r10-0xa0], r6                     
    ldxb r6, [r2+0x19]                      
    stxb [r10-0x9f], r6                     
    ldxb r6, [r2+0x1a]                      
    stxb [r10-0x9e], r6                     
    ldxb r6, [r2+0x1b]                      
    stxb [r10-0x9d], r6                     
    ldxb r6, [r2+0x1c]                      
    stxb [r10-0x9c], r6                     
    ldxb r6, [r2+0x1d]                      
    stxb [r10-0x9b], r6                     
    ldxb r6, [r2+0x1e]                      
    stxb [r10-0x9a], r6                     
    ldxb r2, [r2+0x1f]                      
    stxb [r10-0x99], r2                     
    ldxdw r2, [r0+0x0]                      
    mov64 r5, r2                                    r5 = r2
    add64 r5, 8                                     r5 += 8   ///  r5 = r5.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r6, [r10-0xe0]                    
    ldxdw r7, [r6+0x0]                      
    ldxdw r8, [r2+0x8]                      
    jne r8, r7, lbb_15097                           if r8 != r7 { pc += 10 }
    ldxdw r7, [r6+0x8]                      
    ldxdw r8, [r5+0x8]                      
    jne r8, r7, lbb_15097                           if r8 != r7 { pc += 7 }
    ldxdw r7, [r6+0x10]                     
    ldxdw r8, [r5+0x10]                     
    jne r8, r7, lbb_15097                           if r8 != r7 { pc += 4 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxdw r6, [r6+0x18]                     
    ldxdw r7, [r5+0x18]                     
    jeq r7, r6, lbb_15098                           if r7 == r6 { pc += 1 }
lbb_15097:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
lbb_15098:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jne r8, 0, lbb_15211                            if r8 != (0 as i32 as i64 as u64) { pc += 111 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxb r7, [r10-0xd8]                     
    jne r7, 0, lbb_15104                            if r7 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, 119                                   r6 = 119 as i32 as i64 as u64
lbb_15104:
    ldxb r7, [r2+0x0]                       
    or64 r6, r7                                     r6 |= r7   ///  r6 = r6.or(r7)
    mov64 r7, 11                                    r7 = 11 as i32 as i64 as u64
    and64 r6, 255                                   r6 &= 255   ///  r6 = r6.and(255)
    jne r6, 255, lbb_15211                          if r6 != (255 as i32 as i64 as u64) { pc += 102 }
    stxdw [r10-0xe8], r9                    
    ldxb r8, [r2+0x1]                       
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jeq r8, 0, lbb_15213                            if r8 == (0 as i32 as i64 as u64) { pc += 99 }
    ldxb r9, [r2+0x2]                       
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jeq r9, 0, lbb_15217                            if r9 == (0 as i32 as i64 as u64) { pc += 100 }
lbb_15117:
    ldxb r9, [r2+0x3]                       
    jne r9, 0, lbb_15120                            if r9 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_15119:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_15120:
    ldxdw r9, [r2+0x50]                     
    mov64 r0, r2                                    r0 = r2
    add64 r0, 40                                    r0 += 40   ///  r0 = r0.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r0                    
    mov64 r0, r2                                    r0 = r2
    add64 r0, 88                                    r0 += 88   ///  r0 = r0.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r0                    
    stxdw [r10-0x88], r9                    
    add64 r2, 72                                    r2 += 72   ///  r2 = r2.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r2                    
    stxdw [r10-0x98], r5                    
    stxb [r10-0x66], r7                     
    stxb [r10-0x67], r8                     
    stxb [r10-0x68], r6                     
    stdw [r10-0x70], 0                      
    ldxdw r3, [r3+0x0]                      
    mov64 r2, r3                                    r2 = r3
    add64 r2, 8                                     r2 += 8   ///  r2 = r2.wrapping_add(8 as i32 as i64 as u64)
    ldxdw r0, [r10-0xd0]                    
    ldxdw r5, [r0+0x0]                      
    ldxdw r6, [r3+0x8]                      
    jne r6, r5, lbb_15152                           if r6 != r5 { pc += 10 }
    ldxdw r5, [r0+0x8]                      
    ldxdw r6, [r2+0x8]                      
    jne r6, r5, lbb_15152                           if r6 != r5 { pc += 7 }
    ldxdw r5, [r0+0x10]                     
    ldxdw r6, [r2+0x10]                     
    jne r6, r5, lbb_15152                           if r6 != r5 { pc += 4 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxdw r5, [r0+0x18]                     
    ldxdw r0, [r2+0x18]                     
    jeq r0, r5, lbb_15153                           if r0 == r5 { pc += 1 }
lbb_15152:
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
lbb_15153:
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r5, r4                                    r5 = r4
    ldxdw r4, [r10-0xe8]                    
    jne r6, 0, lbb_15211                            if r6 != (0 as i32 as i64 as u64) { pc += 54 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxb r6, [r10-0xc8]                     
    jne r6, 0, lbb_15161                            if r6 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r0, 119                                   r0 = 119 as i32 as i64 as u64
lbb_15161:
    ldxb r6, [r3+0x0]                       
    or64 r0, r6                                     r0 |= r6   ///  r0 = r0.or(r6)
    and64 r0, 255                                   r0 &= 255   ///  r0 = r0.and(255)
    mov64 r7, 11                                    r7 = 11 as i32 as i64 as u64
    jne r0, 255, lbb_15211                          if r0 != (255 as i32 as i64 as u64) { pc += 45 }
    ldxb r6, [r3+0x1]                       
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jeq r6, 0, lbb_15221                            if r6 == (0 as i32 as i64 as u64) { pc += 51 }
    ldxb r6, [r3+0x2]                       
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jeq r6, 0, lbb_15225                            if r6 == (0 as i32 as i64 as u64) { pc += 52 }
lbb_15173:
    mov64 r6, r1                                    r6 = r1
    ldxb r1, [r3+0x3]                       
    jne r1, 0, lbb_15177                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_15176:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_15177:
    ldxdw r1, [r3+0x50]                     
    mov64 r9, r3                                    r9 = r3
    add64 r9, 40                                    r9 += 40   ///  r9 = r9.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r9                    
    mov64 r9, r3                                    r9 = r3
    add64 r9, 88                                    r9 += 88   ///  r9 = r9.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r9                    
    stxdw [r10-0x50], r1                    
    add64 r3, 72                                    r3 += 72   ///  r3 = r3.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r3                    
    stxdw [r10-0x60], r2                    
    stxb [r10-0x2e], r7                     
    stxb [r10-0x2f], r8                     
    stxb [r10-0x30], r0                     
    stdw [r10-0x38], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -185                                  r1 += -185   ///  r1 = r1.wrapping_add(-185 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -224                                  r1 += -224   ///  r1 = r1.wrapping_add(-224 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100022de8 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r1 load str located at 4295110120
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 33                      
    stdw [r10-0x18], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -152                                  r2 += -152   ///  r2 = r2.wrapping_add(-152 as i32 as i64 as u64)
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    syscall [invalid]                       
    mov64 r7, 26                                    r7 = 26 as i32 as i64 as u64
    mov64 r1, r6                                    r1 = r6
lbb_15211:
    stxw [r1+0x0], r7                       
    exit                                    
lbb_15213:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxb r9, [r2+0x2]                       
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jne r9, 0, lbb_15117                            if r9 != (0 as i32 as i64 as u64) { pc += -100 }
lbb_15217:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    ldxb r9, [r2+0x3]                       
    jeq r9, 0, lbb_15119                            if r9 == (0 as i32 as i64 as u64) { pc += -101 }
    ja lbb_15120                                    if true { pc += -101 }
lbb_15221:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxb r6, [r3+0x2]                       
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jne r6, 0, lbb_15173                            if r6 != (0 as i32 as i64 as u64) { pc += -52 }
lbb_15225:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r6, r1                                    r6 = r1
    ldxb r1, [r3+0x3]                       
    jeq r1, 0, lbb_15176                            if r1 == (0 as i32 as i64 as u64) { pc += -53 }
    ja lbb_15177                                    if true { pc += -53 }

function_15230:
    stxdw [r10-0x120], r3                   
    ldxdw r3, [r2+0x0]                      
    ldxdw r9, [r3+0x0]                      
    ldxdw r3, [r2+0x8]                      
    ldxdw r6, [r3+0x0]                      
    ldxdw r3, [r2+0x10]                     
    ldxdw r3, [r3+0x0]                      
    stxdw [r10-0x118], r3                   
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0xf0], r3                    
    mov64 r7, r6                                    r7 = r6
    add64 r7, 8                                     r7 += 8   ///  r7 = r7.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x100], r7                   
    mov64 r0, r9                                    r0 = r9
    add64 r0, 8                                     r0 += 8   ///  r0 = r0.wrapping_add(8 as i32 as i64 as u64)
    stxdw [r10-0x110], r0                   
    sth [r10-0xe8], 256                     
    sth [r10-0xf8], 1                       
    sth [r10-0x108], 1                      
    stb [r10-0xd9], 3                       
    ldxdw r2, [r2+0x18]                     
    stxdw [r10-0xd8], r2                    
    ldxb r2, [r9+0x0]                       
    jne r2, 255, lbb_15367                          if r2 != (255 as i32 as i64 as u64) { pc += 113 }
    ldxb r2, [r9+0x1]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_15258                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_15258:
    stxdw [r10-0x130], r5                   
    ldxb r2, [r9+0x2]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_15263                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_15263:
    stxdw [r10-0x128], r1                   
    stxdw [r10-0x138], r4                   
    ldxb r2, [r9+0x3]                       
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jne r2, 0, lbb_15269                            if r2 != (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_15269:
    ldxdw r2, [r9+0x50]                     
    mov64 r8, r9                                    r8 = r9
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0xb0], r8                    
    mov64 r8, r9                                    r8 = r9
    add64 r8, 88                                    r8 += 88   ///  r8 = r8.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0xb8], r8                    
    stxdw [r10-0xc0], r2                    
    add64 r9, 72                                    r9 += 72   ///  r9 = r9.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0xc8], r9                    
    stxdw [r10-0xd0], r0                    
    stxb [r10-0x9e], r1                     
    stxb [r10-0x9f], r5                     
    ldxdw r1, [r10-0x130]                   
    stxb [r10-0xa0], r1                     
    stdw [r10-0xa8], 0                      
    ldxb r2, [r6+0x0]                       
    ldxdw r1, [r10-0x128]                   
    jne r2, 255, lbb_15367                          if r2 != (255 as i32 as i64 as u64) { pc += 79 }
    ldxb r1, [r6+0x1]                       
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_15370                            if r1 == (0 as i32 as i64 as u64) { pc += 78 }
    ldxb r1, [r6+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r0, [r10-0x118]                   
    jeq r1, 0, lbb_15375                            if r1 == (0 as i32 as i64 as u64) { pc += 79 }
lbb_15296:
    ldxb r1, [r6+0x3]                       
    jne r1, 0, lbb_15299                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_15298:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_15299:
    ldxdw r1, [r6+0x50]                     
    mov64 r8, r6                                    r8 = r6
    add64 r8, 40                                    r8 += 40   ///  r8 = r8.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x78], r8                    
    mov64 r8, r6                                    r8 = r6
    add64 r8, 88                                    r8 += 88   ///  r8 = r8.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x80], r8                    
    stxdw [r10-0x88], r1                    
    add64 r6, 72                                    r6 += 72   ///  r6 = r6.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x90], r6                    
    stxdw [r10-0x98], r7                    
    stxb [r10-0x66], r5                     
    stxb [r10-0x67], r4                     
    stxb [r10-0x68], r2                     
    stdw [r10-0x70], 0                      
    ldxb r2, [r0+0x0]                       
    and64 r2, 136                                   r2 &= 136   ///  r2 = r2.and(136)
    ldxdw r5, [r10-0x138]                   
    ldxdw r1, [r10-0x128]                   
    jne r2, 136, lbb_15367                          if r2 != (136 as i32 as i64 as u64) { pc += 48 }
    ldxdw r0, [r10-0x118]                   
    ldxb r1, [r0+0x1]                       
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_15379                            if r1 == (0 as i32 as i64 as u64) { pc += 55 }
    ldxb r1, [r0+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jeq r1, 0, lbb_15383                            if r1 == (0 as i32 as i64 as u64) { pc += 56 }
lbb_15327:
    ldxdw r6, [r10-0x128]                   
    ldxb r1, [r0+0x3]                       
    jne r1, 0, lbb_15331                            if r1 != (0 as i32 as i64 as u64) { pc += 1 }
lbb_15330:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_15331:
    ldxdw r1, [r0+0x50]                     
    mov64 r7, r0                                    r7 = r0
    add64 r7, 40                                    r7 += 40   ///  r7 = r7.wrapping_add(40 as i32 as i64 as u64)
    stxdw [r10-0x40], r7                    
    mov64 r7, r0                                    r7 = r0
    add64 r7, 88                                    r7 += 88   ///  r7 = r7.wrapping_add(88 as i32 as i64 as u64)
    stxdw [r10-0x48], r7                    
    stxdw [r10-0x50], r1                    
    add64 r0, 72                                    r0 += 72   ///  r0 = r0.wrapping_add(72 as i32 as i64 as u64)
    stxdw [r10-0x58], r0                    
    stxdw [r10-0x60], r3                    
    stxb [r10-0x2e], r8                     
    stxb [r10-0x2f], r4                     
    stxb [r10-0x30], r2                     
    stdw [r10-0x38], 0                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -217                                  r1 += -217   ///  r1 = r1.wrapping_add(-217 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -272                                  r1 += -272   ///  r1 = r1.wrapping_add(-272 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    lddw r1, 0x100022de8 --> b"\x06\xdd\xf6\xe1\xd7e\xa1\x93\xd9\xcb\xe1F\xce\xeby\xac\x1c\xb4\x85\xed_[…        r1 load str located at 4295110120
    stxdw [r10-0x28], r1                    
    stdw [r10-0x8], 9                       
    stdw [r10-0x18], 3                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -40                                   r1 += -40   ///  r1 = r1.wrapping_add(-40 as i32 as i64 as u64)
    mov64 r2, r10                                   r2 = r10
    add64 r2, -208                                  r2 += -208   ///  r2 = r2.wrapping_add(-208 as i32 as i64 as u64)
    mov64 r3, 3                                     r3 = 3 as i32 as i64 as u64
    ldxdw r4, [r10-0x120]                   
    syscall [invalid]                       
    mov64 r2, 26                                    r2 = 26 as i32 as i64 as u64
    mov64 r1, r6                                    r1 = r6
    ja lbb_15368                                    if true { pc += 1 }
lbb_15367:
    mov64 r2, 11                                    r2 = 11 as i32 as i64 as u64
lbb_15368:
    stxw [r1+0x0], r2                       
    exit                                    
lbb_15370:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxb r1, [r6+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r0, [r10-0x118]                   
    jne r1, 0, lbb_15296                            if r1 != (0 as i32 as i64 as u64) { pc += -79 }
lbb_15375:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxb r1, [r6+0x3]                       
    jeq r1, 0, lbb_15298                            if r1 == (0 as i32 as i64 as u64) { pc += -80 }
    ja lbb_15299                                    if true { pc += -80 }
lbb_15379:
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxb r1, [r0+0x2]                       
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_15327                            if r1 != (0 as i32 as i64 as u64) { pc += -56 }
lbb_15383:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ldxdw r6, [r10-0x128]                   
    ldxb r1, [r0+0x3]                       
    jeq r1, 0, lbb_15330                            if r1 == (0 as i32 as i64 as u64) { pc += -57 }
    ja lbb_15331                                    if true { pc += -57 }

function_15388:
    mov64 r4, r2                                    r4 = r2
    ldxdw r1, [r1+0x0]                      
    ldxb r1, [r1+0x0]                       
    lsh64 r1, 3                                     r1 <<= 3   ///  r1 = r1.wrapping_shl(3)
    lddw r3, 0x100022da8 --> b"(\x00\x00\x00\x00\x00\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\x0c\x00\x00…        r3 load str located at 4295110056
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    lddw r2, 0x100023ba0 --> b"\x00\x00\x00\x00.7\x02\x00\x00\x00\x00\x00V7\x02\x00\x00\x00\x00\x00n7\x0…        r2 load str located at 4295113632
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxdw r2, [r2+0x0]                      
    ldxdw r3, [r3+0x0]                      
    mov64 r1, r4                                    r1 = r4
    call function_16380                     
    exit                                    

function_15403:
    stxdw [r10-0x48], r1                    
    lddw r1, 0x100023160 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00division …        r1 load str located at 4295111008
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x30], r1                    
    lddw r1, 0x10001e200 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x…        r1 load str located at 4295090688
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -72                                   r1 += -72   ///  r1 = r1.wrapping_add(-72 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 1                      
    ldxdw r4, [r2+0x28]                     
    ldxdw r1, [r2+0x20]                     
    mov64 r3, r10                                   r3 = r10
    add64 r3, -64                                   r3 += -64   ///  r3 = r3.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r4                                    r2 = r4
    call function_15878                     
    exit                                    

function_15426:
    mov64 r0, r4                                    r0 = r4
    mov64 r6, r1                                    r6 = r1
    stdw [r10-0x20], 0                      
    stdw [r10-0x28], 0                      
    stdw [r10-0x30], 0                      
    stdw [r10-0x38], 0                      
    stb [r10-0x1], 255                      
    mov64 r4, r10                                   r4 = r10
    add64 r4, -56                                   r4 += -56   ///  r4 = r4.wrapping_add(-56 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -1                                    r5 += -1   ///  r5 = r5.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r2                                    r1 = r2
    mov64 r2, r3                                    r2 = r3
    mov64 r3, r0                                    r3 = r0
    syscall [invalid]                       
    jne r0, 0, lbb_15453                            if r0 != (0 as i32 as i64 as u64) { pc += 11 }
    ldxdw r1, [r10-0x20]                    
    stxdw [r6+0x18], r1                     
    ldxdw r1, [r10-0x28]                    
    stxdw [r6+0x10], r1                     
    ldxdw r1, [r10-0x30]                    
    stxdw [r6+0x8], r1                      
    ldxdw r1, [r10-0x38]                    
    stxdw [r6+0x0], r1                      
    ldxb r1, [r10-0x1]                      
    stxb [r6+0x20], r1                      
    exit                                    
lbb_15453:
    lddw r1, 0x100023bc0 --> b"\x00\x00\x00\x00\x957\x02\x001\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295113664
    stxdw [r10-0x38], r1                    
    stdw [r10-0x18], 0                      
    stdw [r10-0x30], 1                      
    stdw [r10-0x20], 0                      
    stdw [r10-0x28], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -56                                   r1 += -56   ///  r1 = r1.wrapping_add(-56 as i32 as i64 as u64)
    lddw r2, 0x100023bd0 --> b"\x00\x00\x00\x00\x8b7\x02\x00\x0a\x00\x00\x00\x00\x00\x00\x00d\x02\x00\x0…        r2 load str located at 4295113680
    call function_15475                     

function_15465:
    syscall [invalid]                       
    exit                                    

function_15467:
    call function_15468                     

function_15468:
    call custom_panic                       
    syscall [invalid]                       

function_15470:
    mov64 r3, r1                                    r3 = r1
    lddw r1, 0x1000237c7 --> b"called `Option::unwrap()` on a `None` value"        r1 load str located at 4295112647
    mov64 r2, 43                                    r2 = 43 as i32 as i64 as u64
    call function_15481                     

function_15475:
    stxdw [r10-0x10], r2                    
    stxdw [r10-0x18], r1                    
    sth [r10-0x8], 1                        
    mov64 r1, r10                                   r1 = r10
    add64 r1, -24                                   r1 += -24   ///  r1 = r1.wrapping_add(-24 as i32 as i64 as u64)
    call function_15467                     

function_15481:
    mov64 r4, r10                                   r4 = r10
    add64 r4, -16                                   r4 += -16   ///  r4 = r4.wrapping_add(-16 as i32 as i64 as u64)
    stxdw [r10-0x40], r4                    
    stxdw [r10-0x8], r2                     
    stxdw [r10-0x10], r1                    
    stdw [r10-0x20], 0                      
    stdw [r10-0x38], 1                      
    stdw [r10-0x28], 0                      
    stdw [r10-0x30], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_15475                     

function_15494:
    stxdw [r10-0x58], r2                    
    stxdw [r10-0x60], r1                    
    lddw r1, 0x100023be8 --> b"\x00\x00\x00\x00\xc8-\x02\x00 \x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295113704
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x1000210c8 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295102664
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_15475                     

function_15519:
    stxdw [r10-0x68], r2                    
    stxdw [r10-0x70], r1                    
    stxdw [r10-0x58], r4                    
    stxdw [r10-0x60], r3                    
    lddw r1, 0x100023c08 --> b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\…        r1 load str located at 4295113736
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    lddw r1, 0x100021398 --> b"y\x13\x00\x00\x00\x00\x00\x00y\x11\x08\x00\x00\x00\x00\x00y\x14\x18\x00\x…        r1 load str located at 4295103384
    stxdw [r10-0x8], r1                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x1000213c8 --> b"\xbf$\x00\x00\x00\x00\x00\x00y\x13\x08\x00\x00\x00\x00\x00y\x12\x00\x00\x…        r1 load str located at 4295103432
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r5                                    r2 = r5
    call function_15475                     
    mov64 r5, r3                                    r5 = r3
    stxdw [r10-0x18], r2                    
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    stxdw [r10-0x48], r2                    
    ldxdw r2, [r1+0x8]                      
    stxdw [r10-0x38], r2                    
    ldxdw r2, [r1+0x0]                      
    stxdw [r10-0x40], r2                    
    ldxdw r1, [r1+0x10]                     
    stxdw [r10-0x20], r1                    
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    stxdw [r10-0x30], r5                    
lbb_15562:
    and64 r8, 1                                     r8 &= 1   ///  r8 = r8.and(1)
    jne r8, 0, lbb_15651                            if r8 != (0 as i32 as i64 as u64) { pc += 87 }
    jgt r6, r5, lbb_15602                           if r6 > r5 { pc += 37 }
    mov64 r7, r6                                    r7 = r6
lbb_15566:
    ldxdw r3, [r10-0x18]                    
    add64 r3, r7                                    r3 += r7   ///  r3 = r3.wrapping_add(r7)
    mov64 r4, r5                                    r4 = r5
    sub64 r4, r7                                    r4 -= r7   ///  r4 = r4.wrapping_sub(r7)
    jgt r4, 15, lbb_15580                           if r4 > (15 as i32 as i64 as u64) { pc += 9 }
    jeq r5, r7, lbb_15601                           if r5 == r7 { pc += 29 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_15573:
    mov64 r2, r3                                    r2 = r3
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxb r2, [r2+0x0]                       
    jeq r2, 10, lbb_15588                           if r2 == (10 as i32 as i64 as u64) { pc += 11 }
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    jlt r1, r4, lbb_15573                           if r1 < r4 { pc += -6 }
    ja lbb_15601                                    if true { pc += 21 }
lbb_15580:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, 10                                    r2 = 10 as i32 as i64 as u64
    call function_16416                     
    ldxdw r1, [r10-0x10]                    
    jne r1, 1, lbb_15641                            if r1 != (1 as i32 as i64 as u64) { pc += 55 }
    ldxdw r1, [r10-0x8]                     
    ldxdw r5, [r10-0x30]                    
lbb_15588:
    mov64 r2, r7                                    r2 = r7
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r6, r2                                    r6 = r2
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    jge r2, r5, lbb_15598                           if r2 >= r5 { pc += 5 }
    ldxdw r2, [r10-0x18]                    
    add64 r2, r7                                    r2 += r7   ///  r2 = r2.wrapping_add(r7)
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxb r1, [r2+0x0]                       
    jeq r1, 10, lbb_15647                           if r1 == (10 as i32 as i64 as u64) { pc += 49 }
lbb_15598:
    mov64 r7, r6                                    r7 = r6
    jle r6, r5, lbb_15566                           if r6 <= r5 { pc += -34 }
    ja lbb_15602                                    if true { pc += 1 }
lbb_15601:
    mov64 r6, r5                                    r6 = r5
lbb_15602:
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    mov64 r2, r9                                    r2 = r9
    mov64 r7, r5                                    r7 = r5
    jeq r9, r5, lbb_15651                           if r9 == r5 { pc += 45 }
lbb_15606:
    stxdw [r10-0x28], r2                    
    ldxdw r1, [r10-0x20]                    
    ldxb r1, [r1+0x0]                       
    jeq r1, 0, lbb_15618                            if r1 == (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r1, [r10-0x38]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r10-0x40]                    
    lddw r2, 0x100023134 --> b"    "                r2 load str located at 4295110964
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
    callx r4                                
    jne r0, 0, lbb_15653                            if r0 != (0 as i32 as i64 as u64) { pc += 35 }
lbb_15618:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    jeq r7, r9, lbb_15626                           if r7 == r9 { pc += 6 }
    ldxdw r1, [r10-0x48]                    
    add64 r1, r7                                    r1 += r7   ///  r1 = r1.wrapping_add(r7)
    ldxb r2, [r1+0x0]                       
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r2, 10, lbb_15626                           if r2 == (10 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_15626:
    ldxdw r2, [r10-0x18]                    
    add64 r2, r9                                    r2 += r9   ///  r2 = r2.wrapping_add(r9)
    sub64 r7, r9                                    r7 -= r9   ///  r7 = r7.wrapping_sub(r9)
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    ldxdw r3, [r10-0x20]                    
    stxb [r3+0x0], r1                       
    ldxdw r1, [r10-0x38]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r10-0x40]                    
    mov64 r3, r7                                    r3 = r7
    callx r4                                
    ldxdw r9, [r10-0x28]                    
    ldxdw r5, [r10-0x30]                    
    jeq r0, 0, lbb_15562                            if r0 == (0 as i32 as i64 as u64) { pc += -78 }
    ja lbb_15653                                    if true { pc += 12 }
lbb_15641:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    ldxdw r5, [r10-0x30]                    
    mov64 r6, r5                                    r6 = r5
    jeq r1, 0, lbb_15602                            if r1 == (0 as i32 as i64 as u64) { pc += -44 }
    ja lbb_15606                                    if true { pc += -41 }
lbb_15647:
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    mov64 r2, r6                                    r2 = r6
    mov64 r7, r6                                    r7 = r6
    ja lbb_15606                                    if true { pc += -45 }
lbb_15651:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ja lbb_15654                                    if true { pc += 1 }
lbb_15653:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
lbb_15654:
    exit                                    

function_15655:
    mov64 r6, r2                                    r6 = r2
    ldxdw r8, [r1+0x8]                      
    ldxdw r7, [r1+0x0]                      
    ldxdw r9, [r1+0x10]                     
    ldxb r1, [r9+0x0]                       
    jeq r1, 0, lbb_15670                            if r1 == (0 as i32 as i64 as u64) { pc += 9 }
    ldxdw r4, [r8+0x18]                     
    mov64 r1, r7                                    r1 = r7
    lddw r2, 0x100023134 --> b"    "                r2 load str located at 4295110964
    mov64 r3, 4                                     r3 = 4 as i32 as i64 as u64
    callx r4                                
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_15681                            if r1 != (0 as i32 as i64 as u64) { pc += 11 }
lbb_15670:
    mov64 r2, r6                                    r2 = r6
    lsh64 r2, 32                                    r2 <<= 32   ///  r2 = r2.wrapping_shl(32)
    rsh64 r2, 32                                    r2 >>= 32   ///  r2 = r2.wrapping_shr(32)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r2, 10, lbb_15676                           if r2 == (10 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_15676:
    stxb [r9+0x0], r1                       
    ldxdw r3, [r8+0x20]                     
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r6                                    r2 = r6
    callx r3                                
lbb_15681:
    exit                                    

function_15682:
    mov64 r6, r1                                    r6 = r1
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    ldxdw r9, [r6+0x0]                      
    ldxb r1, [r6+0x10]                      
    jne r1, 0, lbb_15765                            if r1 != (0 as i32 as i64 as u64) { pc += 78 }
    stxdw [r10-0x68], r2                    
    ldxdw r8, [r6+0x8]                      
    ldxw r1, [r8+0x34]                      
    mov64 r2, r1                                    r2 = r1
    and64 r2, 4                                     r2 &= 4   ///  r2 = r2.and(4)
    stxdw [r10-0x70], r3                    
    jne r2, 0, lbb_15713                            if r2 != (0 as i32 as i64 as u64) { pc += 19 }
    lddw r2, 0x10002380a --> b"((\x0a,core/src/fmt/num.rs0x0001020304050607080910111"        r2 load str located at 4295112714
    jeq r9, 0, lbb_15699                            if r9 == (0 as i32 as i64 as u64) { pc += 2 }
    lddw r2, 0x100023806 --> b","                   r2 load str located at 4295112710
lbb_15699:
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jeq r9, 0, lbb_15702                            if r9 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
lbb_15702:
    ldxdw r1, [r8+0x20]                     
    ldxdw r4, [r8+0x28]                     
    ldxdw r4, [r4+0x18]                     
    callx r4                                
    jne r0, 0, lbb_15765                            if r0 != (0 as i32 as i64 as u64) { pc += 58 }
    ldxdw r1, [r10-0x70]                    
    ldxdw r3, [r1+0x18]                     
    ldxdw r1, [r10-0x68]                    
    mov64 r2, r8                                    r2 = r8
    callx r3                                
    ja lbb_15764                                    if true { pc += 51 }
lbb_15713:
    jne r9, 0, lbb_15723                            if r9 != (0 as i32 as i64 as u64) { pc += 9 }
    ldxdw r1, [r8+0x20]                     
    ldxdw r2, [r8+0x28]                     
    ldxdw r4, [r2+0x18]                     
    lddw r2, 0x10002380b --> b"(\x0a"               r2 load str located at 4295112715
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    callx r4                                
    jne r0, 0, lbb_15765                            if r0 != (0 as i32 as i64 as u64) { pc += 43 }
    ldxw r1, [r8+0x34]                      
lbb_15723:
    stb [r10-0x41], 1                       
    ldxdw r2, [r8+0x20]                     
    ldxdw r3, [r8+0x28]                     
    mov64 r4, r10                                   r4 = r10
    add64 r4, -65                                   r4 += -65   ///  r4 = r4.wrapping_add(-65 as i32 as i64 as u64)
    stxdw [r10-0x50], r4                    
    stxdw [r10-0x58], r3                    
    stxdw [r10-0x60], r2                    
    ldxdw r2, [r8+0x0]                      
    ldxdw r3, [r8+0x8]                      
    ldxdw r4, [r8+0x10]                     
    ldxdw r5, [r8+0x18]                     
    ldxw r0, [r8+0x30]                      
    ldxb r8, [r8+0x38]                      
    stxb [r10-0x8], r8                      
    stxw [r10-0x10], r0                     
    stxw [r10-0xc], r1                      
    lddw r1, 0x100023c28 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r1 load str located at 4295113768
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stxdw [r10-0x28], r5                    
    stxdw [r10-0x30], r4                    
    stxdw [r10-0x38], r3                    
    stxdw [r10-0x40], r2                    
    ldxdw r1, [r10-0x70]                    
    ldxdw r3, [r1+0x18]                     
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    ldxdw r1, [r10-0x68]                    
    callx r3                                
    jne r0, 0, lbb_15765                            if r0 != (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r1, [r10-0x18]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r10-0x20]                    
    lddw r2, 0x100023808 --> b",\x0a"               r2 load str located at 4295112712
    mov64 r3, 2                                     r3 = 2 as i32 as i64 as u64
    callx r4                                
lbb_15764:
    mov64 r7, r0                                    r7 = r0
lbb_15765:
    stxb [r6+0x10], r7                      
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r6+0x0], r9                      
    mov64 r0, r6                                    r0 = r6
    exit                                    

function_15770:
    mov64 r6, r1                                    r6 = r1
    ldxb r1, [r6+0x10]                      
    ldxdw r2, [r6+0x0]                      
    jeq r2, 0, lbb_15803                            if r2 == (0 as i32 as i64 as u64) { pc += 29 }
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jne r1, 0, lbb_15801                            if r1 != (0 as i32 as i64 as u64) { pc += 24 }
    jne r2, 1, lbb_15792                            if r2 != (1 as i32 as i64 as u64) { pc += 14 }
    ldxb r1, [r6+0x11]                      
    jeq r1, 0, lbb_15792                            if r1 == (0 as i32 as i64 as u64) { pc += 12 }
    ldxdw r2, [r6+0x8]                      
    ldxw r1, [r2+0x34]                      
    and64 r1, 4                                     r1 &= 4   ///  r1 = r1.and(4)
    jne r1, 0, lbb_15792                            if r1 != (0 as i32 as i64 as u64) { pc += 8 }
    ldxdw r1, [r2+0x20]                     
    ldxdw r2, [r2+0x28]                     
    ldxdw r4, [r2+0x18]                     
    lddw r2, 0x10002380d --> b","                   r2 load str located at 4295112717
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    callx r4                                
    jne r0, 0, lbb_15801                            if r0 != (0 as i32 as i64 as u64) { pc += 9 }
lbb_15792:
    ldxdw r2, [r6+0x8]                      
    ldxdw r1, [r2+0x20]                     
    ldxdw r2, [r2+0x28]                     
    ldxdw r4, [r2+0x18]                     
    lddw r2, 0x1000237c6 --> b")"                   r2 load str located at 4295112646
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    callx r4                                
    mov64 r7, r0                                    r7 = r0
lbb_15801:
    stxb [r6+0x10], r7                      
    ja lbb_15804                                    if true { pc += 1 }
lbb_15803:
    mov64 r7, r1                                    r7 = r1
lbb_15804:
    and64 r7, 1                                     r7 &= 1   ///  r7 = r7.and(1)
    mov64 r0, r7                                    r0 = r7
    exit                                    

function_15807:
    mov64 r6, r3                                    r6 = r3
    mov64 r7, r2                                    r7 = r2
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64

function_15810:
    mov64 r9, r10                                   r9 = r10
    add64 r9, -128                                  r9 += -128   ///  r9 = r9.wrapping_add(-128 as i32 as i64 as u64)
    add64 r9, r8                                    r9 += r8   ///  r9 = r9.wrapping_add(r8)
    mov64 r1, r7                                    r1 = r7
    and64 r1, 15                                    r1 &= 15   ///  r1 = r1.and(15)
    call function_16658                     
    stxb [r9+0x7f], r0                      
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    mov64 r1, r7                                    r1 = r7
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r7, r1                                    r7 = r1
    rsh64 r7, 4                                     r7 >>= 4   ///  r7 = r7.wrapping_shr(4)
    jgt r1, 15, function_15810                      if r1 > (15 as i32 as i64 as u64) { pc += -14 }
    mov64 r1, r8                                    r1 = r8
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    add64 r1, 128                                   r1 += 128   ///  r1 = r1.wrapping_add(128 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x100023821 --> b"0x"                  r3 load str located at 4295112737
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    call function_16014                     
    exit                                    

function_15840:
    mov64 r6, r3                                    r6 = r3
    mov64 r7, r2                                    r7 = r2
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64

function_15843:
    mov64 r9, r10                                   r9 = r10
    add64 r9, -128                                  r9 += -128   ///  r9 = r9.wrapping_add(-128 as i32 as i64 as u64)
    add64 r9, r8                                    r9 += r8   ///  r9 = r9.wrapping_add(r8)
    mov64 r1, r7                                    r1 = r7
    and64 r1, 15                                    r1 &= 15   ///  r1 = r1.and(15)
    call function_16692                     
    stxb [r9+0x7f], r0                      
    add64 r8, -1                                    r8 += -1   ///  r8 = r8.wrapping_add(-1 as i32 as i64 as u64)
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    mov64 r1, r7                                    r1 = r7
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r7, r1                                    r7 = r1
    rsh64 r7, 4                                     r7 >>= 4   ///  r7 = r7.wrapping_shr(4)
    jgt r1, 15, function_15843                      if r1 > (15 as i32 as i64 as u64) { pc += -14 }
    mov64 r1, r8                                    r1 = r8
    neg64 r1                                        r1 = -r1   ///  r1 = (r1 as i64).wrapping_neg() as u64
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    add64 r1, r8                                    r1 += r8   ///  r1 = r1.wrapping_add(r8)
    add64 r1, 128                                   r1 += 128   ///  r1 = r1.wrapping_add(128 as i32 as i64 as u64)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r6                                    r1 = r6
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    lddw r3, 0x100023821 --> b"0x"                  r3 load str located at 4295112737
    mov64 r4, 2                                     r4 = 2 as i32 as i64 as u64
    call function_16014                     
    exit                                    

function_15873:
    mov64 r3, r2                                    r3 = r2
    lddw r2, 0x100023c28 --> b"\x00\x00\x00\x00\x00\x00\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\x08\x00\…        r2 load str located at 4295113768
    call function_15878                     
    exit                                    

function_15878:
    stxdw [r10-0x18], r2                    
    stxdw [r10-0x20], r1                    
    stb [r10-0x8], 3                        
    stdw [r10-0x10], 32                     
    stdw [r10-0x30], 0                      
    stdw [r10-0x40], 0                      
    ldxdw r8, [r3+0x20]                     
    stxdw [r10-0x58], r3                    
    jeq r8, 0, lbb_15967                            if r8 == (0 as i32 as i64 as u64) { pc += 80 }
    ldxdw r1, [r3+0x28]                     
    jeq r1, 0, lbb_15995                            if r1 == (0 as i32 as i64 as u64) { pc += 106 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mul64 r1, 56                                    r1 *= 56   ///  r1 = r1.wrapping_mul(56 as u64)
    ldxdw r2, [r3+0x10]                     
    stxdw [r10-0x48], r2                    
    ldxdw r9, [r3+0x0]                      
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    stxdw [r10-0x50], r1                    
lbb_15897:
    ldxdw r3, [r9+0x0]                      
    jeq r3, 0, lbb_15905                            if r3 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r1, [r10-0x18]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r2, [r9-0x8]                      
    ldxdw r1, [r10-0x20]                    
    callx r4                                
    jne r0, 0, lbb_16010                            if r0 != (0 as i32 as i64 as u64) { pc += 105 }
lbb_15905:
    mov64 r2, r8                                    r2 = r8
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    ldxw r1, [r2+0x28]                      
    stxw [r10-0x10], r1                     
    ldxb r1, [r2+0x30]                      
    stxb [r10-0x8], r1                      
    ldxw r1, [r2+0x2c]                      
    stxw [r10-0xc], r1                      
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    ldxdw r5, [r2+0x10]                     
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jeq r5, 2, lbb_15929                            if r5 == (2 as i32 as i64 as u64) { pc += 12 }
    add64 r2, 16                                    r2 += 16   ///  r2 = r2.wrapping_add(16 as i32 as i64 as u64)
    jne r5, 1, lbb_15927                            if r5 != (1 as i32 as i64 as u64) { pc += 8 }
    mov64 r2, r8                                    r2 = r8
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    ldxdw r3, [r2+0x18]                     
    lsh64 r3, 4                                     r3 <<= 4   ///  r3 = r3.wrapping_shl(4)
    ldxdw r2, [r10-0x48]                    
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    ldxdw r5, [r2+0x0]                      
    jne r5, 0, lbb_15929                            if r5 != (0 as i32 as i64 as u64) { pc += 2 }
lbb_15927:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r3, [r2+0x8]                      
lbb_15929:
    mov64 r2, r8                                    r2 = r8
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    stxdw [r10-0x38], r3                    
    stxdw [r10-0x40], r4                    
    ldxdw r3, [r2+0x0]                      
    jeq r3, 2, lbb_15947                            if r3 == (2 as i32 as i64 as u64) { pc += 12 }
    jne r3, 1, lbb_15944                            if r3 != (1 as i32 as i64 as u64) { pc += 8 }
    mov64 r2, r8                                    r2 = r8
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    ldxdw r3, [r2+0x8]                      
    lsh64 r3, 4                                     r3 <<= 4   ///  r3 = r3.wrapping_shl(4)
    ldxdw r2, [r10-0x48]                    
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    ldxdw r4, [r2+0x0]                      
    jne r4, 0, lbb_15947                            if r4 != (0 as i32 as i64 as u64) { pc += 3 }
lbb_15944:
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r3, [r2+0x8]                      
    ja lbb_15947                                    if true { pc += 0 }
lbb_15947:
    stxdw [r10-0x28], r3                    
    stxdw [r10-0x30], r1                    
    mov64 r1, r8                                    r1 = r8
    add64 r1, r6                                    r1 += r6   ///  r1 = r1.wrapping_add(r6)
    ldxdw r1, [r1+0x20]                     
    lsh64 r1, 4                                     r1 <<= 4   ///  r1 = r1.wrapping_shl(4)
    ldxdw r2, [r10-0x48]                    
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    ldxdw r3, [r2+0x8]                      
    ldxdw r1, [r2+0x0]                      
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    callx r3                                
    jne r0, 0, lbb_16010                            if r0 != (0 as i32 as i64 as u64) { pc += 49 }
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    add64 r9, 16                                    r9 += 16   ///  r9 = r9.wrapping_add(16 as i32 as i64 as u64)
    add64 r6, 56                                    r6 += 56   ///  r6 = r6.wrapping_add(56 as i32 as i64 as u64)
    ldxdw r1, [r10-0x50]                    
    jne r1, r6, lbb_15897                           if r1 != r6 { pc += -69 }
    ja lbb_15996                                    if true { pc += 29 }
lbb_15967:
    ldxdw r1, [r3+0x18]                     
    jeq r1, 0, lbb_15995                            if r1 == (0 as i32 as i64 as u64) { pc += 26 }
    ldxdw r6, [r3+0x10]                     
    lsh64 r1, 4                                     r1 <<= 4   ///  r1 = r1.wrapping_shl(4)
    mov64 r8, r6                                    r8 = r6
    add64 r8, r1                                    r8 += r1   ///  r8 = r8.wrapping_add(r1)
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ldxdw r9, [r3+0x0]                      
    add64 r9, 8                                     r9 += 8   ///  r9 = r9.wrapping_add(8 as i32 as i64 as u64)
lbb_15976:
    ldxdw r3, [r9+0x0]                      
    jeq r3, 0, lbb_15984                            if r3 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r1, [r10-0x18]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r2, [r9-0x8]                      
    ldxdw r1, [r10-0x20]                    
    callx r4                                
    jne r0, 0, lbb_16010                            if r0 != (0 as i32 as i64 as u64) { pc += 26 }
lbb_15984:
    ldxdw r3, [r6+0x8]                      
    ldxdw r1, [r6+0x0]                      
    mov64 r2, r10                                   r2 = r10
    add64 r2, -64                                   r2 += -64   ///  r2 = r2.wrapping_add(-64 as i32 as i64 as u64)
    callx r3                                
    jne r0, 0, lbb_16010                            if r0 != (0 as i32 as i64 as u64) { pc += 20 }
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    add64 r9, 16                                    r9 += 16   ///  r9 = r9.wrapping_add(16 as i32 as i64 as u64)
    add64 r6, 16                                    r6 += 16   ///  r6 = r6.wrapping_add(16 as i32 as i64 as u64)
    jne r6, r8, lbb_15976                           if r6 != r8 { pc += -18 }
    ja lbb_15996                                    if true { pc += 1 }
lbb_15995:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
lbb_15996:
    ldxdw r1, [r10-0x58]                    
    ldxdw r1, [r1+0x8]                      
    jge r7, r1, lbb_16012                           if r7 >= r1 { pc += 13 }
    lsh64 r7, 4                                     r7 <<= 4   ///  r7 = r7.wrapping_shl(4)
    ldxdw r1, [r10-0x58]                    
    ldxdw r1, [r1+0x0]                      
    add64 r1, r7                                    r1 += r7   ///  r1 = r1.wrapping_add(r7)
    ldxdw r3, [r1+0x8]                      
    ldxdw r2, [r1+0x0]                      
    ldxdw r1, [r10-0x18]                    
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r10-0x20]                    
    callx r4                                
    jeq r0, 0, lbb_16012                            if r0 == (0 as i32 as i64 as u64) { pc += 2 }
lbb_16010:
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    ja lbb_16013                                    if true { pc += 1 }
lbb_16012:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_16013:
    exit                                    

function_16014:
    mov64 r6, r1                                    r6 = r1
    ldxdw r9, [r5-0xff8]                    
    stxdw [r10-0x28], r9                    
    jeq r2, 0, lbb_16025                            if r2 == (0 as i32 as i64 as u64) { pc += 7 }
    mov64 r2, 1114112                               r2 = 1114112 as i32 as i64 as u64
    ldxw r7, [r6+0x34]                      
    mov64 r1, r7                                    r1 = r7
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jeq r1, 0, lbb_16028                            if r1 == (0 as i32 as i64 as u64) { pc += 5 }
    mov64 r2, 43                                    r2 = 43 as i32 as i64 as u64
    ja lbb_16027                                    if true { pc += 2 }
lbb_16025:
    mov64 r2, 45                                    r2 = 45 as i32 as i64 as u64
    ldxw r7, [r6+0x34]                      
lbb_16027:
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
lbb_16028:
    ldxdw r1, [r5-0x1000]                   
    stxdw [r10-0x30], r1                    
    mov64 r1, r7                                    r1 = r7
    and64 r1, 4                                     r1 &= 4   ///  r1 = r1.and(4)
    jne r1, 0, lbb_16036                            if r1 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r8, r9                                    r8 = r9
    ja lbb_16063                                    if true { pc += 27 }
lbb_16036:
    stxdw [r10-0x38], r2                    
    jge r4, 32, lbb_16053                           if r4 >= (32 as i32 as i64 as u64) { pc += 15 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
    jeq r4, 0, lbb_16061                            if r4 == (0 as i32 as i64 as u64) { pc += 21 }
    mov64 r1, r3                                    r1 = r3
    mov64 r2, r4                                    r2 = r4
lbb_16042:
    ldxb r5, [r1+0x0]                       
    lsh64 r5, 56                                    r5 <<= 56   ///  r5 = r5.wrapping_shl(56)
    arsh64 r5, 56                                   r5 >>= 56 (signed)   ///  r5 = (r5 as i64).wrapping_shr(56)
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jsgt r5, -65, lbb_16048                         if (r5 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_16048:
    add64 r8, r0                                    r8 += r0   ///  r8 = r8.wrapping_add(r0)
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    jne r2, 0, lbb_16042                            if r2 != (0 as i32 as i64 as u64) { pc += -10 }
    ja lbb_16061                                    if true { pc += 8 }
lbb_16053:
    mov64 r1, r3                                    r1 = r3
    mov64 r2, r4                                    r2 = r4
    stxdw [r10-0x40], r4                    
    stxdw [r10-0x48], r3                    
    call function_16488                     
    mov64 r8, r0                                    r8 = r0
    ldxdw r3, [r10-0x48]                    
    ldxdw r4, [r10-0x40]                    
lbb_16061:
    add64 r8, r9                                    r8 += r9   ///  r8 = r8.wrapping_add(r9)
    ldxdw r2, [r10-0x38]                    
lbb_16063:
    ldxdw r1, [r6+0x0]                      
    jeq r1, 0, lbb_16109                            if r1 == (0 as i32 as i64 as u64) { pc += 44 }
    ldxdw r9, [r6+0x8]                      
    jle r9, r8, lbb_16109                           if r9 <= r8 { pc += 42 }
    and64 r7, 8                                     r7 &= 8   ///  r7 = r7.and(8)
    jne r7, 0, lbb_16123                            if r7 != (0 as i32 as i64 as u64) { pc += 54 }
    stxdw [r10-0x38], r2                    
    stxdw [r10-0x48], r3                    
    stxdw [r10-0x40], r4                    
    sub64 r9, r8                                    r9 -= r8   ///  r9 = r9.wrapping_sub(r8)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r9                                    r3 = r9
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_16344                     
    ldxw r8, [r10-0x18]                     
    jeq r8, 1114112, lbb_16120                      if r8 == (1114112 as i32 as i64 as u64) { pc += 38 }
    ldxdw r1, [r10-0x20]                    
    stxdw [r10-0x50], r1                    
    mov64 r1, r6                                    r1 = r6
    ldxdw r2, [r10-0x38]                    
    ldxdw r3, [r10-0x48]                    
    ldxdw r4, [r10-0x40]                    
    call function_16177                     
    jne r0, 0, lbb_16120                            if r0 != (0 as i32 as i64 as u64) { pc += 30 }
    ldxdw r1, [r6+0x20]                     
    ldxdw r6, [r6+0x28]                     
    ldxdw r4, [r6+0x18]                     
    stxdw [r10-0x38], r1                    
    ldxdw r2, [r10-0x30]                    
    ldxdw r3, [r10-0x28]                    
    callx r4                                
    jne r0, 0, lbb_16120                            if r0 != (0 as i32 as i64 as u64) { pc += 22 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ldxdw r7, [r10-0x50]                    
lbb_16100:
    jeq r7, r9, lbb_16171                           if r7 == r9 { pc += 70 }
    ldxdw r3, [r6+0x20]                     
    ldxdw r1, [r10-0x38]                    
    mov64 r2, r8                                    r2 = r8
    callx r3                                
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    jeq r0, 0, lbb_16100                            if r0 == (0 as i32 as i64 as u64) { pc += -7 }
    add64 r9, -1                                    r9 += -1   ///  r9 = r9.wrapping_add(-1 as i32 as i64 as u64)
    ja lbb_16172                                    if true { pc += 63 }
lbb_16109:
    mov64 r1, r6                                    r1 = r6
    call function_16177                     
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_16120                            if r0 != (0 as i32 as i64 as u64) { pc += 7 }
    ldxdw r1, [r6+0x20]                     
    ldxdw r2, [r6+0x28]                     
    ldxdw r4, [r2+0x18]                     
    ldxdw r2, [r10-0x30]                    
    ldxdw r3, [r10-0x28]                    
    callx r4                                
    mov64 r7, r0                                    r7 = r0
lbb_16120:
    and64 r7, 1                                     r7 &= 1   ///  r7 = r7.and(1)
    mov64 r0, r7                                    r0 = r7
    exit                                    
lbb_16123:
    ldxw r1, [r6+0x30]                      
    stxdw [r10-0x50], r1                    
    stw [r6+0x30], 48                       
    ldxb r1, [r6+0x38]                      
    stxdw [r10-0x58], r1                    
    stb [r6+0x38], 1                        
    mov64 r1, r6                                    r1 = r6
    call function_16177                     
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jne r0, 0, lbb_16120                            if r0 != (0 as i32 as i64 as u64) { pc += -13 }
    sub64 r9, r8                                    r9 -= r8   ///  r9 = r9.wrapping_sub(r8)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, r6                                    r2 = r6
    mov64 r3, r9                                    r3 = r9
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    call function_16344                     
    ldxw r8, [r10-0x8]                      
    jeq r8, 1114112, lbb_16120                      if r8 == (1114112 as i32 as i64 as u64) { pc += -22 }
    ldxdw r1, [r10-0x10]                    
    stxdw [r10-0x38], r1                    
    ldxdw r1, [r6+0x20]                     
    ldxdw r2, [r6+0x28]                     
    stxdw [r10-0x48], r2                    
    ldxdw r4, [r2+0x18]                     
    stxdw [r10-0x40], r1                    
    ldxdw r2, [r10-0x30]                    
    ldxdw r3, [r10-0x28]                    
    callx r4                                
    jne r0, 0, lbb_16120                            if r0 != (0 as i32 as i64 as u64) { pc += -33 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_16154:
    ldxdw r1, [r10-0x38]                    
    jeq r1, r9, lbb_16166                           if r1 == r9 { pc += 10 }
    ldxdw r1, [r10-0x48]                    
    ldxdw r3, [r1+0x20]                     
    ldxdw r1, [r10-0x40]                    
    mov64 r2, r8                                    r2 = r8
    callx r3                                
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    jeq r0, 0, lbb_16154                            if r0 == (0 as i32 as i64 as u64) { pc += -9 }
    add64 r9, -1                                    r9 += -1   ///  r9 = r9.wrapping_add(-1 as i32 as i64 as u64)
    ldxdw r1, [r10-0x38]                    
    jlt r9, r1, lbb_16120                           if r9 < r1 { pc += -46 }
lbb_16166:
    ldxdw r1, [r10-0x58]                    
    stxb [r6+0x38], r1                      
    ldxdw r1, [r10-0x50]                    
    stxw [r6+0x30], r1                      
    ja lbb_16175                                    if true { pc += 4 }
lbb_16171:
    mov64 r9, r7                                    r9 = r7
lbb_16172:
    mov64 r1, r7                                    r1 = r7
    mov64 r7, 1                                     r7 = 1 as i32 as i64 as u64
    jlt r9, r1, lbb_16120                           if r9 < r1 { pc += -55 }
lbb_16175:
    mov64 r7, 0                                     r7 = 0 as i32 as i64 as u64
    ja lbb_16120                                    if true { pc += -57 }

function_16177:
    mov64 r6, r4                                    r6 = r4
    mov64 r7, r3                                    r7 = r3
    mov64 r8, r1                                    r8 = r1
    mov64 r1, r2                                    r1 = r2
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jeq r1, 1114112, lbb_16191                      if r1 == (1114112 as i32 as i64 as u64) { pc += 7 }
    ldxdw r1, [r8+0x20]                     
    ldxdw r3, [r8+0x28]                     
    ldxdw r3, [r3+0x20]                     
    callx r3                                
    mov64 r1, r0                                    r1 = r0
    mov64 r0, 1                                     r0 = 1 as i32 as i64 as u64
    jne r1, 0, lbb_16200                            if r1 != (0 as i32 as i64 as u64) { pc += 9 }
lbb_16191:
    jeq r7, 0, lbb_16199                            if r7 == (0 as i32 as i64 as u64) { pc += 7 }
    ldxdw r1, [r8+0x20]                     
    ldxdw r2, [r8+0x28]                     
    ldxdw r4, [r2+0x18]                     
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r6                                    r3 = r6
    callx r4                                
    ja lbb_16200                                    if true { pc += 1 }
lbb_16199:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_16200:
    exit                                    

function_16201:
    mov64 r6, r3                                    r6 = r3
    mov64 r7, r2                                    r7 = r2
    mov64 r8, r1                                    r8 = r1
    ldxdw r2, [r8+0x10]                     
    ldxdw r1, [r8+0x0]                      
    jne r1, 0, lbb_16210                            if r1 != (0 as i32 as i64 as u64) { pc += 3 }
    mov64 r3, r2                                    r3 = r2
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jeq r3, 0, lbb_16328                            if r3 == (0 as i32 as i64 as u64) { pc += 118 }
lbb_16210:
    and64 r2, 1                                     r2 &= 1   ///  r2 = r2.and(1)
    jeq r2, 0, lbb_16274                            if r2 == (0 as i32 as i64 as u64) { pc += 62 }
    mov64 r3, r7                                    r3 = r7
    stxdw [r10-0x20], r6                    
    add64 r3, r6                                    r3 += r6   ///  r3 = r3.wrapping_add(r6)
    ldxdw r4, [r8+0x18]                     
    stxdw [r10-0x18], r7                    
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    jeq r4, 0, lbb_16262                            if r4 == (0 as i32 as i64 as u64) { pc += 43 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r0, r7                                    r0 = r7
lbb_16221:
    mov64 r7, r0                                    r7 = r0
    mov64 r6, r2                                    r6 = r2
    jeq r7, r3, lbb_16264                           if r7 == r3 { pc += 40 }
    mov64 r0, r7                                    r0 = r7
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    ldxb r2, [r7+0x0]                       
    mov64 r9, r2                                    r9 = r2
    lsh64 r9, 56                                    r9 <<= 56   ///  r9 = r9.wrapping_shl(56)
    arsh64 r9, 56                                   r9 >>= 56 (signed)   ///  r9 = (r9 as i64).wrapping_shr(56)
    jsgt r9, -1, lbb_16239                          if (r9 as i64) > (-1 as i32 as i64) { pc += 8 }
    mov64 r0, r7                                    r0 = r7
    add64 r0, 2                                     r0 += 2   ///  r0 = r0.wrapping_add(2 as i32 as i64 as u64)
    jlt r2, 224, lbb_16239                          if r2 < (224 as i32 as i64 as u64) { pc += 5 }
    mov64 r0, r7                                    r0 = r7
    add64 r0, 3                                     r0 += 3   ///  r0 = r0.wrapping_add(3 as i32 as i64 as u64)
    jlt r2, 240, lbb_16239                          if r2 < (240 as i32 as i64 as u64) { pc += 2 }
    mov64 r0, r7                                    r0 = r7
    add64 r0, 4                                     r0 += 4   ///  r0 = r0.wrapping_add(4 as i32 as i64 as u64)
lbb_16239:
    add64 r5, 1                                     r5 += 1   ///  r5 = r5.wrapping_add(1 as i32 as i64 as u64)
    mov64 r2, r0                                    r2 = r0
    sub64 r2, r7                                    r2 -= r7   ///  r2 = r2.wrapping_sub(r7)
    add64 r2, r6                                    r2 += r6   ///  r2 = r2.wrapping_add(r6)
    jlt r5, r4, lbb_16221                           if r5 < r4 { pc += -23 }
    jeq r0, r3, lbb_16264                           if r0 == r3 { pc += 19 }
lbb_16245:
    ldxb r3, [r0+0x0]                       
    mov64 r4, r3                                    r4 = r3
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    ldxdw r6, [r10-0x20]                    
    jsgt r4, -1, lbb_16251                          if (r4 as i64) > (-1 as i32 as i64) { pc += 0 }
lbb_16251:
    ldxdw r7, [r10-0x18]                    
    jeq r2, 0, lbb_16269                            if r2 == (0 as i32 as i64 as u64) { pc += 16 }
    jge r2, r6, lbb_16267                           if r2 >= r6 { pc += 13 }
    mov64 r4, r7                                    r4 = r7
    add64 r4, r2                                    r4 += r2   ///  r4 = r4.wrapping_add(r2)
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxb r4, [r4+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    jsge r4, -64, lbb_16269                         if (r4 as i64) >= (-64 as i32 as i64) { pc += 8 }
    ja lbb_16270                                    if true { pc += 8 }
lbb_16262:
    mov64 r0, r7                                    r0 = r7
    jne r0, r3, lbb_16245                           if r0 != r3 { pc += -19 }
lbb_16264:
    ldxdw r6, [r10-0x20]                    
    ldxdw r7, [r10-0x18]                    
    ja lbb_16274                                    if true { pc += 7 }
lbb_16267:
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jne r2, r6, lbb_16270                           if r2 != r6 { pc += 1 }
lbb_16269:
    mov64 r3, r7                                    r3 = r7
lbb_16270:
    jeq r3, 0, lbb_16272                            if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r6, r2                                    r6 = r2
lbb_16272:
    jeq r3, 0, lbb_16274                            if r3 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, r3                                    r7 = r3
lbb_16274:
    jeq r1, 0, lbb_16328                            if r1 == (0 as i32 as i64 as u64) { pc += 53 }
    ldxdw r9, [r8+0x8]                      
    jge r6, 32, lbb_16324                           if r6 >= (32 as i32 as i64 as u64) { pc += 47 }
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r6, 0, lbb_16291                            if r6 == (0 as i32 as i64 as u64) { pc += 12 }
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r6                                    r2 = r6
lbb_16281:
    ldxb r4, [r1+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jsgt r4, -65, lbb_16287                         if (r4 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_16287:
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    jne r2, 0, lbb_16281                            if r2 != (0 as i32 as i64 as u64) { pc += -10 }
lbb_16291:
    jle r9, r0, lbb_16328                           if r9 <= r0 { pc += 36 }
lbb_16292:
    stxdw [r10-0x18], r7                    
    sub64 r9, r0                                    r9 -= r0   ///  r9 = r9.wrapping_sub(r0)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, r9                                    r3 = r9
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_16344                     
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    ldxw r7, [r10-0x8]                      
    jeq r7, 1114112, lbb_16335                      if r7 == (1114112 as i32 as i64 as u64) { pc += 32 }
    ldxdw r1, [r10-0x10]                    
    stxdw [r10-0x28], r1                    
    ldxdw r1, [r8+0x20]                     
    ldxdw r8, [r8+0x28]                     
    ldxdw r4, [r8+0x18]                     
    stxdw [r10-0x20], r1                    
    ldxdw r2, [r10-0x18]                    
    mov64 r3, r6                                    r3 = r6
    callx r4                                
    jne r0, 0, lbb_16335                            if r0 != (0 as i32 as i64 as u64) { pc += 22 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    ldxdw r9, [r10-0x28]                    
lbb_16315:
    jeq r9, r6, lbb_16338                           if r9 == r6 { pc += 22 }
    ldxdw r3, [r8+0x20]                     
    ldxdw r1, [r10-0x20]                    
    mov64 r2, r7                                    r2 = r7
    callx r3                                
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    jeq r0, 0, lbb_16315                            if r0 == (0 as i32 as i64 as u64) { pc += -7 }
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    ja lbb_16339                                    if true { pc += 15 }
lbb_16324:
    mov64 r1, r7                                    r1 = r7
    mov64 r2, r6                                    r2 = r6
    call function_16488                     
    jgt r9, r0, lbb_16292                           if r9 > r0 { pc += -36 }
lbb_16328:
    ldxdw r1, [r8+0x20]                     
    ldxdw r2, [r8+0x28]                     
    ldxdw r4, [r2+0x18]                     
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r6                                    r3 = r6
    callx r4                                
    mov64 r9, r0                                    r9 = r0
lbb_16335:
    and64 r9, 1                                     r9 &= 1   ///  r9 = r9.and(1)
    mov64 r0, r9                                    r0 = r9
    exit                                    
lbb_16338:
    mov64 r6, r9                                    r6 = r9
lbb_16339:
    mov64 r1, r9                                    r1 = r9
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jlt r6, r1, lbb_16335                           if r6 < r1 { pc += -7 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
    ja lbb_16335                                    if true { pc += -9 }

function_16344:
    stxdw [r10-0x8], r1                     
    ldxb r9, [r2+0x38]                      
    jsgt r9, 1, lbb_16349                           if (r9 as i64) > (1 as i32 as i64) { pc += 2 }
    jne r9, 0, lbb_16357                            if r9 != (0 as i32 as i64 as u64) { pc += 9 }
    ja lbb_16362                                    if true { pc += 13 }
lbb_16349:
    jne r9, 2, lbb_16355                            if r9 != (2 as i32 as i64 as u64) { pc += 5 }
    mov64 r9, r3                                    r9 = r3
    rsh64 r9, 1                                     r9 >>= 1   ///  r9 = r9.wrapping_shr(1)
    add64 r3, 1                                     r3 += 1   ///  r3 = r3.wrapping_add(1 as i32 as i64 as u64)
    rsh64 r3, 1                                     r3 >>= 1   ///  r3 = r3.wrapping_shr(1)
    ja lbb_16362                                    if true { pc += 7 }
lbb_16355:
    and64 r4, 255                                   r4 &= 255   ///  r4 = r4.and(255)
    jeq r4, 0, lbb_16361                            if r4 == (0 as i32 as i64 as u64) { pc += 4 }
lbb_16357:
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
    stxdw [r10-0x10], r1                    
    mov64 r9, r3                                    r9 = r3
    ja lbb_16363                                    if true { pc += 2 }
lbb_16361:
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_16362:
    stxdw [r10-0x10], r3                    
lbb_16363:
    add64 r9, 1                                     r9 += 1   ///  r9 = r9.wrapping_add(1 as i32 as i64 as u64)
    ldxw r7, [r2+0x30]                      
    ldxdw r6, [r2+0x28]                     
    ldxdw r8, [r2+0x20]                     
lbb_16367:
    add64 r9, -1                                    r9 += -1   ///  r9 = r9.wrapping_add(-1 as i32 as i64 as u64)
    jeq r9, 0, lbb_16375                            if r9 == (0 as i32 as i64 as u64) { pc += 6 }
    ldxdw r3, [r6+0x20]                     
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r7                                    r2 = r7
    callx r3                                
    jeq r0, 0, lbb_16367                            if r0 == (0 as i32 as i64 as u64) { pc += -7 }
    mov64 r7, 1114112                               r7 = 1114112 as i32 as i64 as u64
lbb_16375:
    ldxdw r1, [r10-0x8]                     
    stxw [r1+0x8], r7                       
    ldxdw r2, [r10-0x10]                    
    stxdw [r1+0x0], r2                      
    exit                                    

function_16380:
    ldxdw r4, [r1+0x20]                     
    ldxdw r1, [r1+0x28]                     
    ldxdw r5, [r1+0x18]                     
    mov64 r1, r4                                    r1 = r4
    callx r5                                
    exit                                    

function_16386:
    mov64 r6, r5                                    r6 = r5
    mov64 r7, r4                                    r7 = r4
    mov64 r9, r3                                    r9 = r3
    mov64 r8, r1                                    r8 = r1
    ldxdw r1, [r8+0x28]                     
    ldxdw r4, [r1+0x18]                     
    ldxdw r1, [r8+0x20]                     
    callx r4                                
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jeq r9, 0, lbb_16397                            if r9 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_16397:
    stxb [r10-0x8], r0                      
    stxdw [r10-0x10], r8                    
    stxb [r10-0x7], r1                      
    stdw [r10-0x18], 0                      
    mov64 r8, r10                                   r8 = r10
    add64 r8, -24                                   r8 += -24   ///  r8 = r8.wrapping_add(-24 as i32 as i64 as u64)
    mov64 r1, r8                                    r1 = r8
    mov64 r2, r7                                    r2 = r7
    mov64 r3, r6                                    r3 = r6
    call function_15682                     
    mov64 r1, r8                                    r1 = r8
    call function_15770                     
    exit                                    

function_16410:
    mov64 r4, r2                                    r4 = r2
    mov64 r2, r1                                    r2 = r1
    mov64 r1, r3                                    r1 = r3
    mov64 r3, r4                                    r3 = r4
    call function_16201                     
    exit                                    

function_16416:
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    mov64 r0, r3                                    r0 = r3
    add64 r0, 7                                     r0 += 7   ///  r0 = r0.wrapping_add(7 as i32 as i64 as u64)
    and64 r0, -8                                    r0 &= -8   ///  r0 = r0.and(-8)
    jeq r0, r3, lbb_16434                           if r0 == r3 { pc += 13 }
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
    jlt r0, r4, lbb_16424                           if r0 < r4 { pc += 1 }
    mov64 r0, r4                                    r0 = r4
lbb_16424:
    jeq r0, 0, lbb_16434                            if r0 == (0 as i32 as i64 as u64) { pc += 9 }
lbb_16425:
    mov64 r6, r3                                    r6 = r3
    add64 r6, r5                                    r6 += r5   ///  r6 = r6.wrapping_add(r5)
    ldxb r7, [r6+0x0]                       
    mov64 r6, r2                                    r6 = r2
    and64 r6, 255                                   r6 &= 255   ///  r6 = r6.and(255)
    jeq r7, r6, lbb_16483                           if r7 == r6 { pc += 52 }
    add64 r5, 1                                     r5 += 1   ///  r5 = r5.wrapping_add(1 as i32 as i64 as u64)
    jlt r5, r0, lbb_16425                           if r5 < r0 { pc += -8 }
    mov64 r5, r0                                    r5 = r0
lbb_16434:
    stxdw [r10-0x8], r1                     
    mov64 r0, r4                                    r0 = r4
    add64 r0, -16                                   r0 += -16   ///  r0 = r0.wrapping_add(-16 as i32 as i64 as u64)
    jgt r5, r0, lbb_16464                           if r5 > r0 { pc += 26 }
    mov64 r6, r2                                    r6 = r2
    and64 r6, 255                                   r6 &= 255   ///  r6 = r6.and(255)
    lddw r7, 0x101010101010101                      r7 load str located at 72340172838076673
    mul64 r6, r7                                    r6 *= r7   ///  r6 = r6.wrapping_mul(r7)
    lddw r1, 0x8080808080808080                     r1 load str located at -9187201950435737472
lbb_16445:
    mov64 r8, r3                                    r8 = r3
    add64 r8, r5                                    r8 += r5   ///  r8 = r8.wrapping_add(r5)
    ldxdw r9, [r8+0x8]                      
    xor64 r9, r6                                    r9 ^= r6   ///  r9 = r9.xor(r6)
    lddw r7, 0x101010101010100                      r7 load str located at 72340172838076672
    sub64 r7, r9                                    r7 -= r9   ///  r7 = r7.wrapping_sub(r9)
    or64 r7, r9                                     r7 |= r9   ///  r7 = r7.or(r9)
    ldxdw r8, [r8+0x0]                      
    xor64 r8, r6                                    r8 ^= r6   ///  r8 = r8.xor(r6)
    lddw r9, 0x101010101010100                      r9 load str located at 72340172838076672
    sub64 r9, r8                                    r9 -= r8   ///  r9 = r9.wrapping_sub(r8)
    or64 r9, r8                                     r9 |= r8   ///  r9 = r9.or(r8)
    and64 r9, r7                                    r9 &= r7   ///  r9 = r9.and(r7)
    and64 r9, r1                                    r9 &= r1   ///  r9 = r9.and(r1)
    jne r9, r1, lbb_16464                           if r9 != r1 { pc += 2 }
    add64 r5, 16                                    r5 += 16   ///  r5 = r5.wrapping_add(16 as i32 as i64 as u64)
    jle r5, r0, lbb_16445                           if r5 <= r0 { pc += -19 }
lbb_16464:
    jne r4, r5, lbb_16468                           if r4 != r5 { pc += 3 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x8]                     
    ja lbb_16484                                    if true { pc += 16 }
lbb_16468:
    sub64 r4, r5                                    r4 -= r5   ///  r4 = r4.wrapping_sub(r5)
    add64 r3, r5                                    r3 += r5   ///  r3 = r3.wrapping_add(r5)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x8]                     
lbb_16472:
    mov64 r6, r3                                    r6 = r3
    add64 r6, r0                                    r6 += r0   ///  r6 = r6.wrapping_add(r0)
    ldxb r7, [r6+0x0]                       
    mov64 r6, r2                                    r6 = r2
    and64 r6, 255                                   r6 &= 255   ///  r6 = r6.and(255)
    jeq r7, r6, lbb_16482                           if r7 == r6 { pc += 4 }
    add64 r0, 1                                     r0 += 1   ///  r0 = r0.wrapping_add(1 as i32 as i64 as u64)
    jlt r0, r4, lbb_16472                           if r0 < r4 { pc += -8 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    ja lbb_16484                                    if true { pc += 2 }
lbb_16482:
    add64 r5, r0                                    r5 += r0   ///  r5 = r5.wrapping_add(r0)
lbb_16483:
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
lbb_16484:
    stxdw [r1+0x8], r5                      
    stxdw [r1+0x0], r2                      
    exit                                    

function_16487:
    call function_16987                     

function_16488:
    mov64 r7, r1                                    r7 = r1
    add64 r7, 7                                     r7 += 7   ///  r7 = r7.wrapping_add(7 as i32 as i64 as u64)
    and64 r7, -8                                    r7 &= -8   ///  r7 = r7.and(-8)
    mov64 r3, r7                                    r3 = r7
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    jge r2, r3, lbb_16507                           if r2 >= r3 { pc += 13 }
lbb_16494:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    jeq r2, 0, lbb_16646                            if r2 == (0 as i32 as i64 as u64) { pc += 150 }
lbb_16496:
    ldxb r4, [r1+0x0]                       
    lsh64 r4, 56                                    r4 <<= 56   ///  r4 = r4.wrapping_shl(56)
    arsh64 r4, 56                                   r4 >>= 56 (signed)   ///  r4 = (r4 as i64).wrapping_shr(56)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jsgt r4, -65, lbb_16502                         if (r4 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_16502:
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    jne r2, 0, lbb_16496                            if r2 != (0 as i32 as i64 as u64) { pc += -10 }
    ja lbb_16646                                    if true { pc += 139 }
lbb_16507:
    mov64 r8, r2                                    r8 = r2
    sub64 r8, r3                                    r8 -= r3   ///  r8 = r8.wrapping_sub(r3)
    jlt r8, 8, lbb_16494                            if r8 < (8 as i32 as i64 as u64) { pc += -16 }
    stxdw [r10-0x8], r3                     
    mov64 r2, r8                                    r2 = r8
    and64 r2, 7                                     r2 &= 7   ///  r2 = r2.and(7)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jeq r7, r1, lbb_16532                           if r7 == r1 { pc += 16 }
    mov64 r6, r1                                    r6 = r1
    sub64 r6, r7                                    r6 -= r7   ///  r6 = r6.wrapping_sub(r7)
    mov64 r7, r1                                    r7 = r1
lbb_16519:
    ldxb r5, [r7+0x0]                       
    lsh64 r5, 56                                    r5 <<= 56   ///  r5 = r5.wrapping_shl(56)
    arsh64 r5, 56                                   r5 >>= 56 (signed)   ///  r5 = (r5 as i64).wrapping_shr(56)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jsgt r5, -65, lbb_16526                         if (r5 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_16526:
    add64 r6, 1                                     r6 += 1   ///  r6 = r6.wrapping_add(1 as i32 as i64 as u64)
    jeq r6, 0, lbb_16529                            if r6 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_16529:
    add64 r3, r9                                    r3 += r9   ///  r3 = r3.wrapping_add(r9)
    add64 r7, 1                                     r7 += 1   ///  r7 = r7.wrapping_add(1 as i32 as i64 as u64)
    jne r4, 1, lbb_16519                            if r4 != (1 as i32 as i64 as u64) { pc += -13 }
lbb_16532:
    ldxdw r4, [r10-0x8]                     
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    jeq r2, 0, lbb_16550                            if r2 == (0 as i32 as i64 as u64) { pc += 15 }
    mov64 r0, r8                                    r0 = r8
    and64 r0, -8                                    r0 &= -8   ///  r0 = r0.and(-8)
    mov64 r5, r1                                    r5 = r1
    add64 r5, r0                                    r5 += r0   ///  r5 = r5.wrapping_add(r0)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
lbb_16540:
    ldxb r7, [r5+0x0]                       
    lsh64 r7, 56                                    r7 <<= 56   ///  r7 = r7.wrapping_shl(56)
    arsh64 r7, 56                                   r7 >>= 56 (signed)   ///  r7 = (r7 as i64).wrapping_shr(56)
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    jsgt r7, -65, lbb_16546                         if (r7 as i64) > (-65 as i32 as i64) { pc += 1 }
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_16546:
    add64 r0, r6                                    r0 += r6   ///  r0 = r0.wrapping_add(r6)
    add64 r5, 1                                     r5 += 1   ///  r5 = r5.wrapping_add(1 as i32 as i64 as u64)
    add64 r2, -1                                    r2 += -1   ///  r2 = r2.wrapping_add(-1 as i32 as i64 as u64)
    jne r2, 0, lbb_16540                            if r2 != (0 as i32 as i64 as u64) { pc += -10 }
lbb_16550:
    rsh64 r8, 3                                     r8 >>= 3   ///  r8 = r8.wrapping_shr(3)
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    lddw r9, 0x101010101010101                      r9 load str located at 72340172838076673
lbb_16554:
    mov64 r7, r8                                    r7 = r8
    mov64 r3, r1                                    r3 = r1
    jeq r7, 0, lbb_16646                            if r7 == (0 as i32 as i64 as u64) { pc += 89 }
    mov64 r1, r7                                    r1 = r7
    jlt r7, 192, lbb_16560                          if r7 < (192 as i32 as i64 as u64) { pc += 1 }
    mov64 r1, 192                                   r1 = 192 as i32 as i64 as u64
lbb_16560:
    stxdw [r10-0x10], r1                    
    lsh64 r1, 3                                     r1 <<= 3   ///  r1 = r1.wrapping_shl(3)
    stxdw [r10-0x8], r1                     
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    jlt r7, 4, lbb_16587                            if r7 < (4 as i32 as i64 as u64) { pc += 22 }
    ldxdw r2, [r10-0x8]                     
    and64 r2, 2016                                  r2 &= 2016   ///  r2 = r2.and(2016)
    mov64 r1, r3                                    r1 = r3
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, r3                                    r2 = r3
lbb_16570:
    mov64 r8, r5                                    r8 = r5
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
lbb_16572:
    mov64 r5, r2                                    r5 = r2
    add64 r5, r6                                    r5 += r6   ///  r5 = r5.wrapping_add(r6)
    ldxdw r5, [r5+0x0]                      
    mov64 r4, r5                                    r4 = r5
    rsh64 r4, 6                                     r4 >>= 6   ///  r4 = r4.wrapping_shr(6)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    rsh64 r5, 7                                     r5 >>= 7   ///  r5 = r5.wrapping_shr(7)
    or64 r5, r4                                     r5 |= r4   ///  r5 = r5.or(r4)
    and64 r5, r9                                    r5 &= r9   ///  r5 = r5.and(r9)
    add64 r5, r8                                    r5 += r8   ///  r5 = r5.wrapping_add(r8)
    add64 r6, 8                                     r6 += 8   ///  r6 = r6.wrapping_add(8 as i32 as i64 as u64)
    mov64 r8, r5                                    r8 = r5
    jne r6, 32, lbb_16572                           if r6 != (32 as i32 as i64 as u64) { pc += -13 }
    add64 r2, 32                                    r2 += 32   ///  r2 = r2.wrapping_add(32 as i32 as i64 as u64)
    jne r2, r1, lbb_16570                           if r2 != r1 { pc += -17 }
lbb_16587:
    mov64 r1, r3                                    r1 = r3
    ldxdw r2, [r10-0x8]                     
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    ldxdw r2, [r10-0x10]                    
    mov64 r4, r2                                    r4 = r2
    and64 r4, 3                                     r4 &= 3   ///  r4 = r4.and(3)
    stxdw [r10-0x8], r3                     
    mov64 r3, r7                                    r3 = r7
    sub64 r3, r2                                    r3 -= r2   ///  r3 = r3.wrapping_sub(r2)
    mov64 r6, r5                                    r6 = r5
    lddw r8, 0xff00ff00ff00ff                       r8 load str located at 71777214294589695
    and64 r6, r8                                    r6 &= r8   ///  r6 = r6.and(r8)
    rsh64 r5, 8                                     r5 >>= 8   ///  r5 = r5.wrapping_shr(8)
    and64 r5, r8                                    r5 &= r8   ///  r5 = r5.and(r8)
    mov64 r8, r3                                    r8 = r3
    add64 r5, r6                                    r5 += r6   ///  r5 = r5.wrapping_add(r6)
    lddw r6, 0x1000100010001                        r6 load str located at 281479271743489
    mul64 r5, r6                                    r5 *= r6   ///  r5 = r5.wrapping_mul(r6)
    rsh64 r5, 48                                    r5 >>= 48   ///  r5 = r5.wrapping_shr(48)
    add64 r5, r0                                    r5 += r0   ///  r5 = r5.wrapping_add(r0)
    mov64 r0, r5                                    r0 = r5
    jeq r4, 0, lbb_16554                            if r4 == (0 as i32 as i64 as u64) { pc += -57 }
    ldxdw r3, [r10-0x8]                     
    and64 r2, 252                                   r2 &= 252   ///  r2 = r2.and(252)
    lsh64 r2, 3                                     r2 <<= 3   ///  r2 = r2.wrapping_shl(3)
    jlt r7, 192, lbb_16616                          if r7 < (192 as i32 as i64 as u64) { pc += 1 }
    mov64 r7, 192                                   r7 = 192 as i32 as i64 as u64
lbb_16616:
    add64 r3, r2                                    r3 += r2   ///  r3 = r3.wrapping_add(r2)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
    and64 r7, 3                                     r7 &= 3   ///  r7 = r7.and(3)
    lsh64 r7, 3                                     r7 <<= 3   ///  r7 = r7.wrapping_shl(3)
    lddw r1, 0x101010101010101                      r1 load str located at 72340172838076673
lbb_16622:
    ldxdw r0, [r3+0x0]                      
    mov64 r4, r0                                    r4 = r0
    rsh64 r4, 6                                     r4 >>= 6   ///  r4 = r4.wrapping_shr(6)
    xor64 r0, -1                                    r0 ^= -1   ///  r0 = r0.xor(-1)
    rsh64 r0, 7                                     r0 >>= 7   ///  r0 = r0.wrapping_shr(7)
    or64 r0, r4                                     r0 |= r4   ///  r0 = r0.or(r4)
    and64 r0, r1                                    r0 &= r1   ///  r0 = r0.and(r1)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    add64 r3, 8                                     r3 += 8   ///  r3 = r3.wrapping_add(8 as i32 as i64 as u64)
    add64 r7, -8                                    r7 += -8   ///  r7 = r7.wrapping_add(-8 as i32 as i64 as u64)
    mov64 r2, r0                                    r2 = r0
    jne r7, 0, lbb_16622                            if r7 != (0 as i32 as i64 as u64) { pc += -12 }
    lddw r1, 0xff00ff00ff00ff                       r1 load str located at 71777214294589695
    mov64 r2, r0                                    r2 = r0
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    and64 r0, r1                                    r0 &= r1   ///  r0 = r0.and(r1)
    add64 r0, r2                                    r0 += r2   ///  r0 = r0.wrapping_add(r2)
    lddw r1, 0x1000100010001                        r1 load str located at 281479271743489
    mul64 r0, r1                                    r0 *= r1   ///  r0 = r0.wrapping_mul(r1)
    rsh64 r0, 48                                    r0 >>= 48   ///  r0 = r0.wrapping_shr(48)
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
lbb_16646:
    exit                                    

function_16647:
    mov64 r2, r1                                    r2 = r1
    lddw r1, 0x100023c58 --> b"\x00\x00\x00\x00\xeb8\x02\x00\x19\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295113816
    stxdw [r10-0x30], r1                    
    stdw [r10-0x10], 0                      
    stdw [r10-0x28], 1                      
    stdw [r10-0x18], 0                      
    stdw [r10-0x20], 8                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    call function_15475                     

function_16658:
    mov64 r2, r1                                    r2 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jge r2, 10, lbb_16663                           if r2 >= (10 as i32 as i64 as u64) { pc += 2 }
    or64 r1, 48                                     r1 |= 48   ///  r1 = r1.or(48)
    ja lbb_16665                                    if true { pc += 2 }
lbb_16663:
    jge r2, 16, lbb_16667                           if r2 >= (16 as i32 as i64 as u64) { pc += 3 }
    add64 r1, 87                                    r1 += 87   ///  r1 = r1.wrapping_add(87 as i32 as i64 as u64)
lbb_16665:
    mov64 r0, r1                                    r0 = r1
    exit                                    
lbb_16667:
    stxb [r10-0x51], r1                     
    lddw r1, 0x100023c68 --> b"\x00\x00\x00\x00\x049\x02\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295113832
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -81                                   r1 += -81   ///  r1 = r1.wrapping_add(-81 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x100020c10 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295101456
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    lddw r1, 0x100023920 --> b"\x0frange start index  out of range for slice of leng"        r1 load str located at 4295112992
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    lddw r2, 0x100023c88 --> b"\x00\x00\x00\x00\x0e8\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\x8c\x00\x00…        r2 load str located at 4295113864
    call function_15475                     

function_16692:
    mov64 r2, r1                                    r2 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    jge r2, 10, lbb_16697                           if r2 >= (10 as i32 as i64 as u64) { pc += 2 }
    or64 r1, 48                                     r1 |= 48   ///  r1 = r1.or(48)
    ja lbb_16699                                    if true { pc += 2 }
lbb_16697:
    jge r2, 16, lbb_16701                           if r2 >= (16 as i32 as i64 as u64) { pc += 3 }
    add64 r1, 55                                    r1 += 55   ///  r1 = r1.wrapping_add(55 as i32 as i64 as u64)
lbb_16699:
    mov64 r0, r1                                    r0 = r1
    exit                                    
lbb_16701:
    stxb [r10-0x51], r1                     
    lddw r1, 0x100023c68 --> b"\x00\x00\x00\x00\x049\x02\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00…        r1 load str located at 4295113832
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -81                                   r1 += -81   ///  r1 = r1.wrapping_add(-81 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x100020c10 --> b"\xbf#\x00\x00\x00\x00\x00\x00q\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295101456
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    lddw r1, 0x100023920 --> b"\x0frange start index  out of range for slice of leng"        r1 load str located at 4295112992
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    lddw r2, 0x100023ca0 --> b"\x00\x00\x00\x00\x0e8\x02\x00\x13\x00\x00\x00\x00\x00\x00\x00\x8d\x00\x00…        r2 load str located at 4295113888
    call function_15475                     

function_16726:
    mov64 r3, r2                                    r3 = r2
    ldxw r2, [r1+0x0]                       
    call function_15807                     
    exit                                    

function_16730:
    mov64 r3, r2                                    r3 = r2
    ldxw r2, [r1+0x0]                       
    call function_15840                     
    exit                                    

function_16734:
    mov64 r3, r2                                    r3 = r2
    ldxb r1, [r1+0x0]                       
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_16739                     
    exit                                    

function_16739:
    mov64 r4, r1                                    r4 = r1
    and64 r4, 255                                   r4 &= 255   ///  r4 = r4.and(255)
    jlt r4, 100, lbb_16755                          if r4 < (100 as i32 as i64 as u64) { pc += 13 }
    div64 r4, 100                                   r4 /= 100   ///  r4 = r4 / (100 as u64)
    mov64 r5, r4                                    r5 = r4
    mul64 r5, 100                                   r5 *= 100   ///  r5 = r5.wrapping_mul(100 as u64)
    sub64 r1, r5                                    r1 -= r5   ///  r1 = r1.wrapping_sub(r5)
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    and64 r1, 254                                   r1 &= 254   ///  r1 = r1.and(254)
    lddw r5, 0x100023823 --> b"00010203040506070809101112131415161718192021222324"        r5 load str located at 4295112739
    add64 r5, r1                                    r5 += r1   ///  r5 = r5.wrapping_add(r1)
    ldxh r1, [r5+0x0]                       
    stxh [r10-0x2], r1                      
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ja lbb_16758                                    if true { pc += 3 }
lbb_16755:
    jge r4, 10, lbb_16764                           if r4 >= (10 as i32 as i64 as u64) { pc += 8 }
    mov64 r5, 2                                     r5 = 2 as i32 as i64 as u64
    mov64 r4, r1                                    r4 = r1
lbb_16758:
    mov64 r1, r10                                   r1 = r10
    add64 r1, -3                                    r1 += -3   ///  r1 = r1.wrapping_add(-3 as i32 as i64 as u64)
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    or64 r4, 48                                     r4 |= 48   ///  r4 = r4.or(48)
    stxb [r1+0x0], r4                       
    ja lbb_16772                                    if true { pc += 8 }
lbb_16764:
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    and64 r1, 254                                   r1 &= 254   ///  r1 = r1.and(254)
    lddw r4, 0x100023823 --> b"00010203040506070809101112131415161718192021222324"        r4 load str located at 4295112739
    add64 r4, r1                                    r4 += r1   ///  r4 = r4.wrapping_add(r1)
    ldxh r1, [r4+0x0]                       
    stxh [r10-0x2], r1                      
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
lbb_16772:
    mov64 r1, r5                                    r1 = r5
    xor64 r1, 3                                     r1 ^= 3   ///  r1 = r1.xor(3)
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -3                                    r1 += -3   ///  r1 = r1.wrapping_add(-3 as i32 as i64 as u64)
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r3                                    r1 = r3
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_16014                     
    exit                                    

function_16785:
    mov64 r3, r2                                    r3 = r2
    ldxw r1, [r1+0x0]                       
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_16790                     
    exit                                    

function_16790:
    mov64 r4, 10                                    r4 = 10 as i32 as i64 as u64
    mov64 r5, r1                                    r5 = r1
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    jlt r5, 10000, lbb_16831                        if r5 < (10000 as i32 as i64 as u64) { pc += 36 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_16796:
    mov64 r0, r1                                    r0 = r1
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r5, r0                                    r5 = r0
    div64 r5, 10000                                 r5 /= 10000   ///  r5 = r5 / (10000 as u64)
    mov64 r6, r5                                    r6 = r5
    mul64 r6, 10000                                 r6 *= 10000   ///  r6 = r6.wrapping_mul(10000 as u64)
    sub64 r1, r6                                    r1 -= r6   ///  r1 = r1.wrapping_sub(r6)
    mov64 r6, r1                                    r6 = r1
    and64 r6, 65535                                 r6 &= 65535   ///  r6 = r6.and(65535)
    div64 r6, 100                                   r6 /= 100   ///  r6 = r6 / (100 as u64)
    mov64 r7, r6                                    r7 = r6
    mul64 r7, 100                                   r7 *= 100   ///  r7 = r7.wrapping_mul(100 as u64)
    sub64 r1, r7                                    r1 -= r7   ///  r1 = r1.wrapping_sub(r7)
    mov64 r7, r10                                   r7 = r10
    add64 r7, -10                                   r7 += -10   ///  r7 = r7.wrapping_add(-10 as i32 as i64 as u64)
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    lsh64 r6, 1                                     r6 <<= 1   ///  r6 = r6.wrapping_shl(1)
    lddw r8, 0x100023823 --> b"00010203040506070809101112131415161718192021222324"        r8 load str located at 4295112739
    add64 r8, r6                                    r8 += r6   ///  r8 = r8.wrapping_add(r6)
    ldxh r6, [r8+0x0]                       
    stxh [r7+0x6], r6                       
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    and64 r1, 65534                                 r1 &= 65534   ///  r1 = r1.and(65534)
    lddw r6, 0x100023823 --> b"00010203040506070809101112131415161718192021222324"        r6 load str located at 4295112739
    add64 r6, r1                                    r6 += r1   ///  r6 = r6.wrapping_add(r1)
    ldxh r1, [r6+0x0]                       
    stxh [r7+0x8], r1                       
    add64 r4, -4                                    r4 += -4   ///  r4 = r4.wrapping_add(-4 as i32 as i64 as u64)
    mov64 r1, r5                                    r1 = r5
    jgt r0, 99999999, lbb_16796                     if r0 > (99999999 as i32 as i64 as u64) { pc += -33 }
    add64 r4, 10                                    r4 += 10   ///  r4 = r4.wrapping_add(10 as i32 as i64 as u64)
    ja lbb_16832                                    if true { pc += 1 }
lbb_16831:
    mov64 r5, r1                                    r5 = r1
lbb_16832:
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    jle r5, 99, lbb_16864                           if r5 <= (99 as i32 as i64 as u64) { pc += 29 }
    mov64 r1, r5                                    r1 = r5
    and64 r1, 65535                                 r1 &= 65535   ///  r1 = r1.and(65535)
    div64 r1, 100                                   r1 /= 100   ///  r1 = r1 / (100 as u64)
    mov64 r0, r1                                    r0 = r1
    mul64 r0, 100                                   r0 *= 100   ///  r0 = r0.wrapping_mul(100 as u64)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    lsh64 r5, 1                                     r5 <<= 1   ///  r5 = r5.wrapping_shl(1)
    and64 r5, 65534                                 r5 &= 65534   ///  r5 = r5.and(65534)
    lddw r0, 0x100023823 --> b"00010203040506070809101112131415161718192021222324"        r0 load str located at 4295112739
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -10                                   r5 += -10   ///  r5 = r5.wrapping_add(-10 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxh r0, [r0+0x0]                       
    stxh [r5+0x0], r0                       
    jlt r1, 10, lbb_16866                           if r1 < (10 as i32 as i64 as u64) { pc += 13 }
lbb_16853:
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    lddw r5, 0x100023823 --> b"00010203040506070809101112131415161718192021222324"        r5 load str located at 4295112739
    add64 r5, r1                                    r5 += r1   ///  r5 = r5.wrapping_add(r1)
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -10                                   r1 += -10   ///  r1 = r1.wrapping_add(-10 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    ldxh r5, [r5+0x0]                       
    stxh [r1+0x0], r5                       
    ja lbb_16872                                    if true { pc += 8 }
lbb_16864:
    mov64 r1, r5                                    r1 = r5
    jge r1, 10, lbb_16853                           if r1 >= (10 as i32 as i64 as u64) { pc += -13 }
lbb_16866:
    add64 r4, -1                                    r4 += -1   ///  r4 = r4.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -10                                   r5 += -10   ///  r5 = r5.wrapping_add(-10 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    or64 r1, 48                                     r1 |= 48   ///  r1 = r1.or(48)
    stxb [r5+0x0], r1                       
lbb_16872:
    mov64 r1, 10                                    r1 = 10 as i32 as i64 as u64
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -10                                   r1 += -10   ///  r1 = r1.wrapping_add(-10 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r3                                    r1 = r3
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_16014                     
    exit                                    

function_16885:
    mov64 r3, r2                                    r3 = r2
    ldxdw r1, [r1+0x0]                      
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    call function_16890                     
    exit                                    

function_16890:
    mov64 r4, 20                                    r4 = 20 as i32 as i64 as u64
    jlt r1, 10000, lbb_16924                        if r1 < (10000 as i32 as i64 as u64) { pc += 32 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_16893:
    mov64 r5, r1                                    r5 = r1
    div64 r1, 10000                                 r1 /= 10000   ///  r1 = r1 / (10000 as u64)
    mov64 r6, r1                                    r6 = r1
    mul64 r6, 10000                                 r6 *= 10000   ///  r6 = r6.wrapping_mul(10000 as u64)
    mov64 r0, r5                                    r0 = r5
    sub64 r0, r6                                    r0 -= r6   ///  r0 = r0.wrapping_sub(r6)
    mov64 r6, r0                                    r6 = r0
    and64 r6, 65535                                 r6 &= 65535   ///  r6 = r6.and(65535)
    div64 r6, 100                                   r6 /= 100   ///  r6 = r6 / (100 as u64)
    mov64 r7, r6                                    r7 = r6
    mul64 r7, 100                                   r7 *= 100   ///  r7 = r7.wrapping_mul(100 as u64)
    sub64 r0, r7                                    r0 -= r7   ///  r0 = r0.wrapping_sub(r7)
    mov64 r7, r10                                   r7 = r10
    add64 r7, -20                                   r7 += -20   ///  r7 = r7.wrapping_add(-20 as i32 as i64 as u64)
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    lsh64 r6, 1                                     r6 <<= 1   ///  r6 = r6.wrapping_shl(1)
    lddw r8, 0x100023823 --> b"00010203040506070809101112131415161718192021222324"        r8 load str located at 4295112739
    add64 r8, r6                                    r8 += r6   ///  r8 = r8.wrapping_add(r6)
    ldxh r6, [r8+0x0]                       
    stxh [r7+0x10], r6                      
    lsh64 r0, 1                                     r0 <<= 1   ///  r0 = r0.wrapping_shl(1)
    and64 r0, 65534                                 r0 &= 65534   ///  r0 = r0.and(65534)
    lddw r6, 0x100023823 --> b"00010203040506070809101112131415161718192021222324"        r6 load str located at 4295112739
    add64 r6, r0                                    r6 += r0   ///  r6 = r6.wrapping_add(r0)
    ldxh r0, [r6+0x0]                       
    stxh [r7+0x12], r0                      
    add64 r4, -4                                    r4 += -4   ///  r4 = r4.wrapping_add(-4 as i32 as i64 as u64)
    jgt r5, 99999999, lbb_16893                     if r5 > (99999999 as i32 as i64 as u64) { pc += -30 }
    add64 r4, 20                                    r4 += 20   ///  r4 = r4.wrapping_add(20 as i32 as i64 as u64)
lbb_16924:
    jle r1, 99, lbb_16954                           if r1 <= (99 as i32 as i64 as u64) { pc += 29 }
    mov64 r5, r1                                    r5 = r1
    and64 r5, 65535                                 r5 &= 65535   ///  r5 = r5.and(65535)
    div64 r5, 100                                   r5 /= 100   ///  r5 = r5 / (100 as u64)
    mov64 r0, r5                                    r0 = r5
    mul64 r0, 100                                   r0 *= 100   ///  r0 = r0.wrapping_mul(100 as u64)
    sub64 r1, r0                                    r1 -= r0   ///  r1 = r1.wrapping_sub(r0)
    lsh64 r1, 1                                     r1 <<= 1   ///  r1 = r1.wrapping_shl(1)
    and64 r1, 65534                                 r1 &= 65534   ///  r1 = r1.and(65534)
    lddw r0, 0x100023823 --> b"00010203040506070809101112131415161718192021222324"        r0 load str located at 4295112739
    add64 r0, r1                                    r0 += r1   ///  r0 = r0.wrapping_add(r1)
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -20                                   r1 += -20   ///  r1 = r1.wrapping_add(-20 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    ldxh r0, [r0+0x0]                       
    stxh [r1+0x0], r0                       
    jlt r5, 10, lbb_16956                           if r5 < (10 as i32 as i64 as u64) { pc += 13 }
lbb_16943:
    lsh64 r5, 1                                     r5 <<= 1   ///  r5 = r5.wrapping_shl(1)
    lddw r1, 0x100023823 --> b"00010203040506070809101112131415161718192021222324"        r1 load str located at 4295112739
    add64 r1, r5                                    r1 += r5   ///  r1 = r1.wrapping_add(r5)
    add64 r4, -2                                    r4 += -2   ///  r4 = r4.wrapping_add(-2 as i32 as i64 as u64)
    mov64 r5, r10                                   r5 = r10
    add64 r5, -20                                   r5 += -20   ///  r5 = r5.wrapping_add(-20 as i32 as i64 as u64)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    ldxh r1, [r1+0x0]                       
    stxh [r5+0x0], r1                       
    ja lbb_16962                                    if true { pc += 8 }
lbb_16954:
    mov64 r5, r1                                    r5 = r1
    jge r5, 10, lbb_16943                           if r5 >= (10 as i32 as i64 as u64) { pc += -13 }
lbb_16956:
    add64 r4, -1                                    r4 += -1   ///  r4 = r4.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -20                                   r1 += -20   ///  r1 = r1.wrapping_add(-20 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    or64 r5, 48                                     r5 |= 48   ///  r5 = r5.or(48)
    stxb [r1+0x0], r5                       
lbb_16962:
    mov64 r1, 20                                    r1 = 20 as i32 as i64 as u64
    sub64 r1, r4                                    r1 -= r4   ///  r1 = r1.wrapping_sub(r4)
    stxdw [r10-0xff8], r1                   
    mov64 r1, r10                                   r1 = r10
    add64 r1, -20                                   r1 += -20   ///  r1 = r1.wrapping_add(-20 as i32 as i64 as u64)
    add64 r1, r4                                    r1 += r4   ///  r1 = r1.wrapping_add(r4)
    stxdw [r10-0x1000], r1                  
    mov64 r5, r10                                   r5 = r10
    mov64 r1, r3                                    r1 = r3
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    call function_16014                     
    exit                                    

function_16975:
    ldxdw r3, [r1+0x0]                      
    ldxdw r1, [r1+0x8]                      
    ldxdw r4, [r1+0x18]                     
    mov64 r1, r3                                    r1 = r3
    callx r4                                
    exit                                    

function_16981:
    mov64 r4, r2                                    r4 = r2
    ldxdw r3, [r1+0x8]                      
    ldxdw r2, [r1+0x0]                      
    mov64 r1, r4                                    r1 = r4
    call function_16201                     
    exit                                    

function_16987:
    stxdw [r10-0x58], r2                    
    stxdw [r10-0x60], r1                    
    lddw r1, 0x100023cb8 --> b"\x00\x00\x00\x00!9\x02\x00\x12\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0…        r1 load str located at 4295113912
    stxdw [r10-0x50], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    stxdw [r10-0x40], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -88                                   r1 += -88   ///  r1 = r1.wrapping_add(-88 as i32 as i64 as u64)
    stxdw [r10-0x10], r1                    
    lddw r1, 0x1000210c8 --> b"\xbf#\x00\x00\x00\x00\x00\x00y\x11\x00\x00\x00\x00\x00\x00\xb7\x02\x00\x0…        r1 load str located at 4295102664
    stxdw [r10-0x8], r1                     
    stxdw [r10-0x18], r1                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    stxdw [r10-0x20], r1                    
    stdw [r10-0x30], 0                      
    stdw [r10-0x48], 2                      
    stdw [r10-0x38], 2                      
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    mov64 r2, r3                                    r2 = r3
    call function_15475                     

function_17012:
    mov64 r6, r1                                    r6 = r1
    syscall [invalid]                       
    mov64 r0, r6                                    r0 = r6
    exit                                    

function_17016:
    mov64 r6, r1                                    r6 = r1
    syscall [invalid]                       
    mov64 r0, r6                                    r0 = r6
    exit                                    

function_17020:
    mov64 r6, r1                                    r6 = r1
    and64 r2, 255                                   r2 &= 255   ///  r2 = r2.and(255)
    syscall [invalid]                       
    mov64 r0, r6                                    r0 = r6
    exit                                    

function_17025:
    mov64 r5, r4                                    r5 = r4
    and64 r5, 64                                    r5 &= 64   ///  r5 = r5.and(64)
    jne r5, 0, lbb_17042                            if r5 != (0 as i32 as i64 as u64) { pc += 14 }
    mov64 r5, r4                                    r5 = r4
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    jeq r5, 0, lbb_17046                            if r5 == (0 as i32 as i64 as u64) { pc += 14 }
    mov64 r5, r4                                    r5 = r4
    and64 r5, 63                                    r5 &= 63   ///  r5 = r5.and(63)
    lsh64 r3, r5                                    r3 <<= r5   ///  r3 = r3.wrapping_shl(r5 as u32)
    neg64 r4                                        r4 = -r4   ///  r4 = (r4 as i64).wrapping_neg() as u64
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    mov64 r0, r2                                    r0 = r2
    rsh64 r0, r4                                    r0 >>= r4   ///  r0 = r0.wrapping_shr(r4 as u32)
    or64 r3, r0                                     r3 |= r0   ///  r3 = r3.or(r0)
    lsh64 r2, r5                                    r2 <<= r5   ///  r2 = r2.wrapping_shl(r5 as u32)
    ja lbb_17046                                    if true { pc += 4 }
lbb_17042:
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    mov64 r3, r2                                    r3 = r2
    lsh64 r3, r4                                    r3 <<= r4   ///  r3 = r3.wrapping_shl(r4 as u32)
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_17046:
    stxdw [r1+0x0], r2                      
    stxdw [r1+0x8], r3                      
    exit                                    

function_17049:
    mov64 r5, r4                                    r5 = r4
    and64 r5, 64                                    r5 &= 64   ///  r5 = r5.and(64)
    jne r5, 0, lbb_17067                            if r5 != (0 as i32 as i64 as u64) { pc += 15 }
    mov64 r5, r4                                    r5 = r4
    lsh64 r5, 32                                    r5 <<= 32   ///  r5 = r5.wrapping_shl(32)
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    jeq r5, 0, lbb_17071                            if r5 == (0 as i32 as i64 as u64) { pc += 15 }
    mov64 r5, r4                                    r5 = r4
    and64 r5, 63                                    r5 &= 63   ///  r5 = r5.and(63)
    rsh64 r2, r5                                    r2 >>= r5   ///  r2 = r2.wrapping_shr(r5 as u32)
    neg64 r4                                        r4 = -r4   ///  r4 = (r4 as i64).wrapping_neg() as u64
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    mov64 r0, r3                                    r0 = r3
    lsh64 r0, r4                                    r0 <<= r4   ///  r0 = r0.wrapping_shl(r4 as u32)
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    rsh64 r3, r5                                    r3 >>= r5   ///  r3 = r3.wrapping_shr(r5 as u32)
    mov64 r2, r0                                    r2 = r0
    ja lbb_17071                                    if true { pc += 4 }
lbb_17067:
    and64 r4, 63                                    r4 &= 63   ///  r4 = r4.and(63)
    rsh64 r3, r4                                    r3 >>= r4   ///  r3 = r3.wrapping_shr(r4 as u32)
    mov64 r2, r3                                    r2 = r3
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_17071:
    stxdw [r1+0x0], r2                      
    stxdw [r1+0x8], r3                      
    exit                                    

function_17074:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_17025                     
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x8]                     
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_17083:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_17092                     
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x8]                     
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_17092:
    stxdw [r10-0x10], r3                    
    stxdw [r10-0x8], r1                     
    mov64 r1, r2                                    r1 = r2
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r7, r4                                    r7 = r4
    lsh64 r7, 32                                    r7 <<= 32   ///  r7 = r7.wrapping_shl(32)
    rsh64 r7, 32                                    r7 >>= 32   ///  r7 = r7.wrapping_shr(32)
    mov64 r6, r2                                    r6 = r2
    rsh64 r6, 32                                    r6 >>= 32   ///  r6 = r6.wrapping_shr(32)
    mov64 r3, r7                                    r3 = r7
    mul64 r3, r1                                    r3 *= r1   ///  r3 = r3.wrapping_mul(r1)
    mul64 r7, r6                                    r7 *= r6   ///  r7 = r7.wrapping_mul(r6)
    mov64 r0, r4                                    r0 = r4
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    mov64 r9, r0                                    r9 = r0
    mul64 r9, r1                                    r9 *= r1   ///  r9 = r9.wrapping_mul(r1)
    mov64 r1, r9                                    r1 = r9
    add64 r1, r7                                    r1 += r7   ///  r1 = r1.wrapping_add(r7)
    mov64 r8, 1                                     r8 = 1 as i32 as i64 as u64
    jlt r1, r9, lbb_17114                           if r1 < r9 { pc += 1 }
    mov64 r8, 0                                     r8 = 0 as i32 as i64 as u64
lbb_17114:
    mov64 r9, r1                                    r9 = r1
    lsh64 r9, 32                                    r9 <<= 32   ///  r9 = r9.wrapping_shl(32)
    mov64 r7, r3                                    r7 = r3
    add64 r7, r9                                    r7 += r9   ///  r7 = r7.wrapping_add(r9)
    mov64 r9, 1                                     r9 = 1 as i32 as i64 as u64
    jlt r7, r3, lbb_17121                           if r7 < r3 { pc += 1 }
    mov64 r9, 0                                     r9 = 0 as i32 as i64 as u64
lbb_17121:
    ldxdw r3, [r10-0x8]                     
    stxdw [r3+0x0], r7                      
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    or64 r8, r1                                     r8 |= r1   ///  r8 = r8.or(r1)
    ldxdw r1, [r10-0x10]                    
    mul64 r4, r1                                    r4 *= r1   ///  r4 = r4.wrapping_mul(r1)
    mul64 r5, r2                                    r5 *= r2   ///  r5 = r5.wrapping_mul(r2)
    mul64 r0, r6                                    r0 *= r6   ///  r0 = r0.wrapping_mul(r6)
    add64 r0, r8                                    r0 += r8   ///  r0 = r0.wrapping_add(r8)
    add64 r5, r4                                    r5 += r4   ///  r5 = r5.wrapping_add(r4)
    add64 r0, r9                                    r0 += r9   ///  r0 = r0.wrapping_add(r9)
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    stxdw [r3+0x8], r0                      
    exit                                    

function_17136:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    call function_17049                     
    ldxdw r1, [r10-0x10]                    
    ldxdw r2, [r10-0x8]                     
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    

function_17145:
    stxdw [r10-0xc0], r4                    
    mov64 r9, r2                                    r9 = r2
    stxdw [r10-0xd0], r1                    
    mov64 r1, r5                                    r1 = r5
    rsh64 r1, 1                                     r1 >>= 1   ///  r1 = r1.wrapping_shr(1)
    stxdw [r10-0xb8], r5                    
    mov64 r4, r5                                    r4 = r5
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 8                                     r1 >>= 8   ///  r1 = r1.wrapping_shr(8)
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 16                                    r1 >>= 16   ///  r1 = r1.wrapping_shr(16)
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    mov64 r1, r4                                    r1 = r4
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    or64 r4, r1                                     r4 |= r1   ///  r4 = r4.or(r1)
    lddw r1, 0x5555555555555555                     r1 load str located at 6148914691236517205
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    mov64 r2, r4                                    r2 = r4
    rsh64 r2, 1                                     r2 >>= 1   ///  r2 = r2.wrapping_shr(1)
    and64 r2, r1                                    r2 &= r1   ///  r2 = r2.and(r1)
    sub64 r4, r2                                    r4 -= r2   ///  r4 = r4.wrapping_sub(r2)
    lddw r2, 0x3333333333333333                     r2 load str located at 3689348814741910323
    mov64 r7, r4                                    r7 = r4
    and64 r7, r2                                    r7 &= r2   ///  r7 = r7.and(r2)
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    and64 r4, r2                                    r4 &= r2   ///  r4 = r4.and(r2)
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    mov64 r4, r7                                    r4 = r7
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    add64 r7, r4                                    r7 += r4   ///  r7 = r7.wrapping_add(r4)
    lddw r6, 0xf0f0f0f0f0f0f0f                      r6 load str located at 1085102592571150095
    and64 r7, r6                                    r7 &= r6   ///  r7 = r7.and(r6)
    lddw r4, 0x101010101010101                      r4 load str located at 72340172838076673
    mul64 r7, r4                                    r7 *= r4   ///  r7 = r7.wrapping_mul(r4)
    jne r3, 0, lbb_17229                            if r3 != (0 as i32 as i64 as u64) { pc += 37 }
    mov64 r0, r9                                    r0 = r9
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    mov64 r5, r9                                    r5 = r9
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r1                                    r0 &= r1   ///  r0 = r0.and(r1)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    mov64 r8, r5                                    r8 = r5
    and64 r8, r2                                    r8 &= r2   ///  r8 = r8.and(r2)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, r2                                    r5 &= r2   ///  r5 = r5.and(r2)
    add64 r8, r5                                    r8 += r5   ///  r8 = r8.wrapping_add(r5)
    mov64 r5, r8                                    r5 = r8
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    add64 r8, r5                                    r8 += r5   ///  r8 = r8.wrapping_add(r5)
    and64 r8, r6                                    r8 &= r6   ///  r8 = r8.and(r6)
    mul64 r8, r4                                    r8 *= r4   ///  r8 = r8.wrapping_mul(r4)
    rsh64 r8, 56                                    r8 >>= 56   ///  r8 = r8.wrapping_shr(56)
    add64 r8, 64                                    r8 += 64   ///  r8 = r8.wrapping_add(64 as i32 as i64 as u64)
    ja lbb_17264                                    if true { pc += 35 }
lbb_17229:
    mov64 r5, r3                                    r5 = r3
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 2                                     r0 >>= 2   ///  r0 = r0.wrapping_shr(2)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 4                                     r0 >>= 4   ///  r0 = r0.wrapping_shr(4)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 8                                     r0 >>= 8   ///  r0 = r0.wrapping_shr(8)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 16                                    r0 >>= 16   ///  r0 = r0.wrapping_shr(16)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 32                                    r0 >>= 32   ///  r0 = r0.wrapping_shr(32)
    or64 r5, r0                                     r5 |= r0   ///  r5 = r5.or(r0)
    xor64 r5, -1                                    r5 ^= -1   ///  r5 = r5.xor(-1)
    mov64 r0, r5                                    r0 = r5
    rsh64 r0, 1                                     r0 >>= 1   ///  r0 = r0.wrapping_shr(1)
    and64 r0, r1                                    r0 &= r1   ///  r0 = r0.and(r1)
    sub64 r5, r0                                    r5 -= r0   ///  r5 = r5.wrapping_sub(r0)
    mov64 r8, r5                                    r8 = r5
    and64 r8, r2                                    r8 &= r2   ///  r8 = r8.and(r2)
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    and64 r5, r2                                    r5 &= r2   ///  r5 = r5.and(r2)
    add64 r8, r5                                    r8 += r5   ///  r8 = r8.wrapping_add(r5)
    mov64 r5, r8                                    r5 = r8
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    add64 r8, r5                                    r8 += r5   ///  r8 = r8.wrapping_add(r5)
    and64 r8, r6                                    r8 &= r6   ///  r8 = r8.and(r6)
    mul64 r8, r4                                    r8 *= r4   ///  r8 = r8.wrapping_mul(r4)
    rsh64 r8, 56                                    r8 >>= 56   ///  r8 = r8.wrapping_shr(56)
lbb_17264:
    rsh64 r7, 56                                    r7 >>= 56   ///  r7 = r7.wrapping_shr(56)
    ldxdw r5, [r10-0xb8]                    
    jne r5, 0, lbb_17303                            if r5 != (0 as i32 as i64 as u64) { pc += 36 }
    ldxdw r7, [r10-0xc0]                    
    mov64 r5, r7                                    r5 = r7
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    or64 r7, r5                                     r7 |= r5   ///  r7 = r7.or(r5)
    mov64 r5, r7                                    r5 = r7
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    or64 r7, r5                                     r7 |= r5   ///  r7 = r7.or(r5)
    mov64 r5, r7                                    r5 = r7
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    or64 r7, r5                                     r7 |= r5   ///  r7 = r7.or(r5)
    mov64 r5, r7                                    r5 = r7
    rsh64 r5, 8                                     r5 >>= 8   ///  r5 = r5.wrapping_shr(8)
    or64 r7, r5                                     r7 |= r5   ///  r7 = r7.or(r5)
    mov64 r5, r7                                    r5 = r7
    rsh64 r5, 16                                    r5 >>= 16   ///  r5 = r5.wrapping_shr(16)
    or64 r7, r5                                     r7 |= r5   ///  r7 = r7.or(r5)
    mov64 r5, r7                                    r5 = r7
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    or64 r7, r5                                     r7 |= r5   ///  r7 = r7.or(r5)
    xor64 r7, -1                                    r7 ^= -1   ///  r7 = r7.xor(-1)
    mov64 r5, r7                                    r5 = r7
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    and64 r5, r1                                    r5 &= r1   ///  r5 = r5.and(r1)
    sub64 r7, r5                                    r7 -= r5   ///  r7 = r7.wrapping_sub(r5)
    mov64 r1, r7                                    r1 = r7
    rsh64 r1, 2                                     r1 >>= 2   ///  r1 = r1.wrapping_shr(2)
    and64 r7, r2                                    r7 &= r2   ///  r7 = r7.and(r2)
    and64 r1, r2                                    r1 &= r2   ///  r1 = r1.and(r2)
    add64 r7, r1                                    r7 += r1   ///  r7 = r7.wrapping_add(r1)
    mov64 r1, r7                                    r1 = r7
    rsh64 r1, 4                                     r1 >>= 4   ///  r1 = r1.wrapping_shr(4)
    add64 r7, r1                                    r7 += r1   ///  r7 = r7.wrapping_add(r1)
    and64 r7, r6                                    r7 &= r6   ///  r7 = r7.and(r6)
    mul64 r7, r4                                    r7 *= r4   ///  r7 = r7.wrapping_mul(r4)
    rsh64 r7, 56                                    r7 >>= 56   ///  r7 = r7.wrapping_shr(56)
    add64 r7, 64                                    r7 += 64   ///  r7 = r7.wrapping_add(64 as i32 as i64 as u64)
lbb_17303:
    mov64 r0, r3                                    r0 = r3
    jle r7, r8, lbb_17318                           if r7 <= r8 { pc += 13 }
    mov64 r1, r8                                    r1 = r8
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jle r1, 63, lbb_17335                           if r1 <= (63 as i32 as i64 as u64) { pc += 26 }
    mov64 r6, r9                                    r6 = r9
    ldxdw r2, [r10-0xc0]                    
    div64 r6, r2                                    r6 /= r2   ///  r6 = r6 / r2
    mov64 r1, r6                                    r1 = r6
    mul64 r1, r2                                    r1 *= r2   ///  r1 = r1.wrapping_mul(r2)
    sub64 r9, r1                                    r9 -= r1   ///  r9 = r9.wrapping_sub(r1)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r8, r9                                    r8 = r9
    ja lbb_17370                                    if true { pc += 52 }
lbb_17318:
    mov64 r6, 0                                     r6 = 0 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    ldxdw r3, [r10-0xc0]                    
    jlt r9, r3, lbb_17324                           if r9 < r3 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_17324:
    ldxdw r7, [r10-0xd0]                    
    ldxdw r4, [r10-0xb8]                    
    jlt r0, r4, lbb_17328                           if r0 < r4 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_17328:
    ldxdw r4, [r10-0xb8]                    
    jeq r0, r4, lbb_17331                           if r0 == r4 { pc += 1 }
    mov64 r1, r2                                    r1 = r2
lbb_17331:
    and64 r1, 1                                     r1 &= 1   ///  r1 = r1.and(1)
    jeq r1, 0, lbb_17373                            if r1 == (0 as i32 as i64 as u64) { pc += 40 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ja lbb_17382                                    if true { pc += 47 }
lbb_17335:
    mov64 r1, r7                                    r1 = r7
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jle r1, 95, lbb_17388                           if r1 <= (95 as i32 as i64 as u64) { pc += 49 }
    ldxdw r4, [r10-0xc0]                    
    mov64 r1, r4                                    r1 = r4
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    mov64 r2, r0                                    r2 = r0
    div64 r2, r1                                    r2 /= r1   ///  r2 = r2 / r1
    mov64 r3, r2                                    r3 = r2
    mul64 r3, r4                                    r3 *= r4   ///  r3 = r3.wrapping_mul(r4)
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    mov64 r8, r9                                    r8 = r9
    rsh64 r8, 32                                    r8 >>= 32   ///  r8 = r8.wrapping_shr(32)
    or64 r0, r8                                     r0 |= r8   ///  r0 = r0.or(r8)
    div64 r0, r1                                    r0 /= r1   ///  r0 = r0 / r1
    mov64 r3, r0                                    r3 = r0
    mul64 r3, r4                                    r3 *= r4   ///  r3 = r3.wrapping_mul(r4)
    sub64 r8, r3                                    r8 -= r3   ///  r8 = r8.wrapping_sub(r3)
    mov64 r4, r0                                    r4 = r0
    rsh64 r4, 32                                    r4 >>= 32   ///  r4 = r4.wrapping_shr(32)
    or64 r4, r2                                     r4 |= r2   ///  r4 = r4.or(r2)
    lsh64 r9, 32                                    r9 <<= 32   ///  r9 = r9.wrapping_shl(32)
    rsh64 r9, 32                                    r9 >>= 32   ///  r9 = r9.wrapping_shr(32)
    lsh64 r8, 32                                    r8 <<= 32   ///  r8 = r8.wrapping_shl(32)
    or64 r8, r9                                     r8 |= r9   ///  r8 = r8.or(r9)
    lsh64 r0, 32                                    r0 <<= 32   ///  r0 = r0.wrapping_shl(32)
    mov64 r2, r8                                    r2 = r8
    div64 r2, r1                                    r2 /= r1   ///  r2 = r2 / r1
    or64 r0, r2                                     r0 |= r2   ///  r0 = r0.or(r2)
    mul64 r2, r1                                    r2 *= r1   ///  r2 = r2.wrapping_mul(r1)
    sub64 r8, r2                                    r8 -= r2   ///  r8 = r8.wrapping_sub(r2)
    mov64 r6, r0                                    r6 = r0
lbb_17370:
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    ldxdw r7, [r10-0xd0]                    
    ja lbb_17383                                    if true { pc += 10 }
lbb_17373:
    ldxdw r1, [r10-0xb8]                    
    sub64 r0, r1                                    r0 -= r1   ///  r0 = r0.wrapping_sub(r1)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r6, 1                                     r6 = 1 as i32 as i64 as u64
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r9, r3, lbb_17380                           if r9 < r3 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_17380:
    sub64 r0, r1                                    r0 -= r1   ///  r0 = r0.wrapping_sub(r1)
    sub64 r9, r3                                    r9 -= r3   ///  r9 = r9.wrapping_sub(r3)
lbb_17382:
    mov64 r8, r9                                    r8 = r9
lbb_17383:
    stxdw [r7+0x10], r8                     
    stxdw [r7+0x0], r6                      
    stxdw [r7+0x18], r0                     
    stxdw [r7+0x8], r4                      
    exit                                    
lbb_17388:
    stxdw [r10-0xd8], r0                    
    mov64 r1, r7                                    r1 = r7
    sub64 r1, r8                                    r1 -= r8   ///  r1 = r1.wrapping_sub(r8)
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jge r1, 32, lbb_17459                           if r1 >= (32 as i32 as i64 as u64) { pc += 65 }
    mov64 r6, 64                                    r6 = 64 as i32 as i64 as u64
    sub64 r6, r8                                    r6 -= r8   ///  r6 = r6.wrapping_sub(r8)
    lsh64 r6, 32                                    r6 <<= 32   ///  r6 = r6.wrapping_shl(32)
    arsh64 r6, 32                                   r6 >>= 32 (signed)   ///  r6 = (r6 as i64).wrapping_shr(32)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -128                                  r1 += -128   ///  r1 = r1.wrapping_add(-128 as i32 as i64 as u64)
    ldxdw r8, [r10-0xc0]                    
    mov64 r2, r8                                    r2 = r8
    ldxdw r7, [r10-0xb8]                    
    mov64 r3, r7                                    r3 = r7
    mov64 r4, r6                                    r4 = r6
    call function_17136                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -144                                  r1 += -144   ///  r1 = r1.wrapping_add(-144 as i32 as i64 as u64)
    mov64 r2, r9                                    r2 = r9
    ldxdw r3, [r10-0xd8]                    
    mov64 r4, r6                                    r4 = r6
    call function_17136                     
    ldxdw r1, [r10-0x80]                    
    ldxdw r6, [r10-0x90]                    
    div64 r6, r1                                    r6 /= r1   ///  r6 = r6 / r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -160                                  r1 += -160   ///  r1 = r1.wrapping_add(-160 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -176                                  r1 += -176   ///  r1 = r1.wrapping_add(-176 as i32 as i64 as u64)
    mov64 r2, r7                                    r2 = r7
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    call function_17083                     
    ldxdw r1, [r10-0xb0]                    
    ldxdw r3, [r10-0x98]                    
    mov64 r2, r3                                    r2 = r3
    add64 r2, r1                                    r2 += r1   ///  r2 = r2.wrapping_add(r1)
    mov64 r1, 1                                     r1 = 1 as i32 as i64 as u64
    jlt r2, r3, lbb_17436                           if r2 < r3 { pc += 1 }
    mov64 r1, 0                                     r1 = 0 as i32 as i64 as u64
lbb_17436:
    ldxdw r3, [r10-0xa8]                    
    add64 r3, r1                                    r3 += r1   ///  r3 = r3.wrapping_add(r1)
    ldxdw r1, [r10-0xa0]                    
    ldxdw r7, [r10-0xd0]                    
    ldxdw r0, [r10-0xd8]                    
    jne r3, 0, lbb_17664                            if r3 != (0 as i32 as i64 as u64) { pc += 222 }
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r9, r1, lbb_17660                           if r9 < r1 { pc += 215 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    jge r0, r2, lbb_17661                           if r0 >= r2 { pc += 214 }
lbb_17447:
    jeq r0, r2, lbb_17449                           if r0 == r2 { pc += 1 }
lbb_17448:
    mov64 r3, r4                                    r3 = r4
lbb_17449:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_17664                            if r3 != (0 as i32 as i64 as u64) { pc += 213 }
    sub64 r0, r2                                    r0 -= r2   ///  r0 = r0.wrapping_sub(r2)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    jlt r9, r1, lbb_17456                           if r9 < r1 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_17456:
    sub64 r0, r2                                    r0 -= r2   ///  r0 = r0.wrapping_sub(r2)
    sub64 r9, r1                                    r9 -= r1   ///  r9 = r9.wrapping_sub(r1)
    ja lbb_17382                                    if true { pc += -77 }
lbb_17459:
    mov64 r1, 96                                    r1 = 96 as i32 as i64 as u64
    sub64 r1, r7                                    r1 -= r7   ///  r1 = r1.wrapping_sub(r7)
    stxdw [r10-0xf8], r1                    
    mov64 r4, r1                                    r4 = r1
    lsh64 r4, 32                                    r4 <<= 32   ///  r4 = r4.wrapping_shl(32)
    stxdw [r10-0xf0], r4                    
    arsh64 r4, 32                                   r4 >>= 32 (signed)   ///  r4 = (r4 as i64).wrapping_shr(32)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -16                                   r1 += -16   ///  r1 = r1.wrapping_add(-16 as i32 as i64 as u64)
    ldxdw r2, [r10-0xc0]                    
    ldxdw r3, [r10-0xb8]                    
    call function_17136                     
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
    ldxdw r1, [r10-0x10]                    
    lsh64 r1, 32                                    r1 <<= 32   ///  r1 = r1.wrapping_shl(32)
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    stxdw [r10-0x100], r1                   
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    stxdw [r10-0xc8], r9                    
    ldxdw r0, [r10-0xd8]                    
lbb_17480:
    stxdw [r10-0xe8], r5                    
    stxdw [r10-0xe0], r4                    
    mov64 r6, 64                                    r6 = 64 as i32 as i64 as u64
    sub64 r6, r8                                    r6 -= r8   ///  r6 = r6.wrapping_sub(r8)
    mov64 r9, r6                                    r9 = r6
    lsh64 r9, 32                                    r9 <<= 32   ///  r9 = r9.wrapping_shl(32)
    mov64 r8, r9                                    r8 = r9
    arsh64 r8, 32                                   r8 >>= 32 (signed)   ///  r8 = (r8 as i64).wrapping_shr(32)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    ldxdw r2, [r10-0xc8]                    
    stxdw [r10-0xd8], r0                    
    mov64 r3, r0                                    r3 = r0
    mov64 r4, r8                                    r4 = r8
    call function_17136                     
    ldxdw r2, [r10-0x20]                    
    rsh64 r9, 32                                    r9 >>= 32   ///  r9 = r9.wrapping_shr(32)
    ldxdw r1, [r10-0xf0]                    
    rsh64 r1, 32                                    r1 >>= 32   ///  r1 = r1.wrapping_shr(32)
    jlt r9, r1, lbb_17685                           if r9 < r1 { pc += 185 }
    ldxdw r1, [r10-0x100]                   
    div64 r2, r1                                    r2 /= r1   ///  r2 = r2 / r1
    ldxdw r1, [r10-0xf8]                    
    sub64 r6, r1                                    r6 -= r1   ///  r6 = r6.wrapping_sub(r1)
    and64 r6, 127                                   r6 &= 127   ///  r6 = r6.and(127)
    mov64 r1, r10                                   r1 = r10
    add64 r1, -48                                   r1 += -48   ///  r1 = r1.wrapping_add(-48 as i32 as i64 as u64)
    mov64 r8, r2                                    r8 = r2
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    mov64 r4, r6                                    r4 = r6
    call function_17074                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -64                                   r1 += -64   ///  r1 = r1.wrapping_add(-64 as i32 as i64 as u64)
    mov64 r2, r8                                    r2 = r8
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r4, [r10-0xc0]                    
    ldxdw r5, [r10-0xb8]                    
    call function_17083                     
    mov64 r1, r10                                   r1 = r10
    add64 r1, -80                                   r1 += -80   ///  r1 = r1.wrapping_add(-80 as i32 as i64 as u64)
    ldxdw r2, [r10-0x40]                    
    ldxdw r3, [r10-0x38]                    
    mov64 r4, r6                                    r4 = r6
    call function_17074                     
    ldxdw r4, [r10-0x30]                    
    mov64 r1, r4                                    r1 = r4
    ldxdw r2, [r10-0xe8]                    
    add64 r1, r2                                    r1 += r2   ///  r1 = r1.wrapping_add(r2)
    mov64 r2, 1                                     r2 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r1, r4, lbb_17532                           if r1 < r4 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_17532:
    ldxdw r4, [r10-0x50]                    
    ldxdw r0, [r10-0xd8]                    
    ldxdw r6, [r10-0xc8]                    
    jlt r6, r4, lbb_17537                           if r6 < r4 { pc += 1 }
    mov64 r2, 0                                     r2 = 0 as i32 as i64 as u64
lbb_17537:
    ldxdw r5, [r10-0x48]                    
    sub64 r0, r5                                    r0 -= r5   ///  r0 = r0.wrapping_sub(r5)
    sub64 r0, r2                                    r0 -= r2   ///  r0 = r0.wrapping_sub(r2)
    ldxdw r2, [r10-0x28]                    
    sub64 r6, r4                                    r6 -= r4   ///  r6 = r6.wrapping_sub(r4)
    stxdw [r10-0xc8], r6                    
    jne r0, 0, lbb_17589                            if r0 != (0 as i32 as i64 as u64) { pc += 45 }
    mov64 r5, r6                                    r5 = r6
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    mov64 r4, r6                                    r4 = r6
    or64 r4, r5                                     r4 |= r5   ///  r4 = r4.or(r5)
    mov64 r5, r4                                    r5 = r4
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    or64 r4, r5                                     r4 |= r5   ///  r4 = r4.or(r5)
    mov64 r5, r4                                    r5 = r4
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    or64 r4, r5                                     r4 |= r5   ///  r4 = r4.or(r5)
    mov64 r5, r4                                    r5 = r4
    rsh64 r5, 8                                     r5 >>= 8   ///  r5 = r5.wrapping_shr(8)
    or64 r4, r5                                     r4 |= r5   ///  r4 = r4.or(r5)
    mov64 r5, r4                                    r5 = r4
    rsh64 r5, 16                                    r5 >>= 16   ///  r5 = r5.wrapping_shr(16)
    or64 r4, r5                                     r4 |= r5   ///  r4 = r4.or(r5)
    mov64 r5, r4                                    r5 = r4
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    or64 r4, r5                                     r4 |= r5   ///  r4 = r4.or(r5)
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    mov64 r5, r4                                    r5 = r4
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    lddw r6, 0x5555555555555555                     r6 load str located at 6148914691236517205
    and64 r5, r6                                    r5 &= r6   ///  r5 = r5.and(r6)
    sub64 r4, r5                                    r4 -= r5   ///  r4 = r4.wrapping_sub(r5)
    mov64 r8, r4                                    r8 = r4
    lddw r5, 0x3333333333333333                     r5 load str located at 3689348814741910323
    and64 r8, r5                                    r8 &= r5   ///  r8 = r8.and(r5)
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    add64 r8, r4                                    r8 += r4   ///  r8 = r8.wrapping_add(r4)
    mov64 r4, r8                                    r4 = r8
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    add64 r8, r4                                    r8 += r4   ///  r8 = r8.wrapping_add(r4)
    lddw r4, 0xf0f0f0f0f0f0f0f                      r4 load str located at 1085102592571150095
    and64 r8, r4                                    r8 &= r4   ///  r8 = r8.and(r4)
    lddw r4, 0x101010101010101                      r4 load str located at 72340172838076673
    mul64 r8, r4                                    r8 *= r4   ///  r8 = r8.wrapping_mul(r4)
    rsh64 r8, 56                                    r8 >>= 56   ///  r8 = r8.wrapping_shr(56)
    add64 r8, 64                                    r8 += 64   ///  r8 = r8.wrapping_add(64 as i32 as i64 as u64)
    ja lbb_17632                                    if true { pc += 43 }
lbb_17589:
    mov64 r5, r0                                    r5 = r0
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    mov64 r4, r0                                    r4 = r0
    or64 r4, r5                                     r4 |= r5   ///  r4 = r4.or(r5)
    mov64 r5, r4                                    r5 = r4
    rsh64 r5, 2                                     r5 >>= 2   ///  r5 = r5.wrapping_shr(2)
    or64 r4, r5                                     r4 |= r5   ///  r4 = r4.or(r5)
    mov64 r5, r4                                    r5 = r4
    rsh64 r5, 4                                     r5 >>= 4   ///  r5 = r5.wrapping_shr(4)
    or64 r4, r5                                     r4 |= r5   ///  r4 = r4.or(r5)
    mov64 r5, r4                                    r5 = r4
    rsh64 r5, 8                                     r5 >>= 8   ///  r5 = r5.wrapping_shr(8)
    or64 r4, r5                                     r4 |= r5   ///  r4 = r4.or(r5)
    mov64 r5, r4                                    r5 = r4
    rsh64 r5, 16                                    r5 >>= 16   ///  r5 = r5.wrapping_shr(16)
    or64 r4, r5                                     r4 |= r5   ///  r4 = r4.or(r5)
    mov64 r5, r4                                    r5 = r4
    rsh64 r5, 32                                    r5 >>= 32   ///  r5 = r5.wrapping_shr(32)
    or64 r4, r5                                     r4 |= r5   ///  r4 = r4.or(r5)
    xor64 r4, -1                                    r4 ^= -1   ///  r4 = r4.xor(-1)
    mov64 r5, r4                                    r5 = r4
    rsh64 r5, 1                                     r5 >>= 1   ///  r5 = r5.wrapping_shr(1)
    lddw r6, 0x5555555555555555                     r6 load str located at 6148914691236517205
    and64 r5, r6                                    r5 &= r6   ///  r5 = r5.and(r6)
    sub64 r4, r5                                    r4 -= r5   ///  r4 = r4.wrapping_sub(r5)
    mov64 r8, r4                                    r8 = r4
    lddw r5, 0x3333333333333333                     r5 load str located at 3689348814741910323
    and64 r8, r5                                    r8 &= r5   ///  r8 = r8.and(r5)
    rsh64 r4, 2                                     r4 >>= 2   ///  r4 = r4.wrapping_shr(2)
    and64 r4, r5                                    r4 &= r5   ///  r4 = r4.and(r5)
    add64 r8, r4                                    r8 += r4   ///  r8 = r8.wrapping_add(r4)
    mov64 r4, r8                                    r4 = r8
    rsh64 r4, 4                                     r4 >>= 4   ///  r4 = r4.wrapping_shr(4)
    add64 r8, r4                                    r8 += r4   ///  r8 = r8.wrapping_add(r4)
    lddw r4, 0xf0f0f0f0f0f0f0f                      r4 load str located at 1085102592571150095
    and64 r8, r4                                    r8 &= r4   ///  r8 = r8.and(r4)
    lddw r4, 0x101010101010101                      r4 load str located at 72340172838076673
    mul64 r8, r4                                    r8 *= r4   ///  r8 = r8.wrapping_mul(r4)
    rsh64 r8, 56                                    r8 >>= 56   ///  r8 = r8.wrapping_shr(56)
lbb_17632:
    ldxdw r4, [r10-0xe0]                    
    add64 r2, r4                                    r2 += r4   ///  r2 = r2.wrapping_add(r4)
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    mov64 r3, r7                                    r3 = r7
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    jle r3, r8, lbb_17745                           if r3 <= r8 { pc += 106 }
    mov64 r3, r8                                    r3 = r8
    lsh64 r3, 32                                    r3 <<= 32   ///  r3 = r3.wrapping_shl(32)
    rsh64 r3, 32                                    r3 >>= 32   ///  r3 = r3.wrapping_shr(32)
    mov64 r5, r1                                    r5 = r1
    mov64 r4, r2                                    r4 = r2
    jle r3, 63, lbb_17480                           if r3 <= (63 as i32 as i64 as u64) { pc += -165 }
    ldxdw r4, [r10-0xc0]                    
    jeq r4, 0, lbb_17791                            if r4 == (0 as i32 as i64 as u64) { pc += 144 }
    ldxdw r8, [r10-0xc8]                    
    mov64 r3, r8                                    r3 = r8
    div64 r3, r4                                    r3 /= r4   ///  r3 = r3 / r4
    ldxdw r7, [r10-0xd0]                    
lbb_17651:
    mov64 r6, r1                                    r6 = r1
    add64 r6, r3                                    r6 += r3   ///  r6 = r6.wrapping_add(r3)
    mov64 r0, 0                                     r0 = 0 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r6, r1, lbb_17657                           if r6 < r1 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_17657:
    mod64 r8, r4                                    r8 %= r4   ///  r8 = r8 % r4
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    ja lbb_17773                                    if true { pc += 113 }
lbb_17660:
    jlt r0, r2, lbb_17447                           if r0 < r2 { pc += -214 }
lbb_17661:
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    jne r0, r2, lbb_17448                           if r0 != r2 { pc += -215 }
    ja lbb_17449                                    if true { pc += -215 }
lbb_17664:
    ldxdw r3, [r10-0xb8]                    
    add64 r3, r0                                    r3 += r0   ///  r3 = r3.wrapping_add(r0)
    stxdw [r10-0xb8], r3                    
    mov64 r5, r8                                    r5 = r8
    add64 r5, r9                                    r5 += r9   ///  r5 = r5.wrapping_add(r9)
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r9, r5                                    r9 = r5
    jlt r5, r8, lbb_17674                           if r5 < r8 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_17674:
    ldxdw r0, [r10-0xb8]                    
    add64 r0, r4                                    r0 += r4   ///  r0 = r0.wrapping_add(r4)
    sub64 r0, r2                                    r0 -= r2   ///  r0 = r0.wrapping_sub(r2)
    mov64 r8, r9                                    r8 = r9
    jlt r8, r1, lbb_17680                           if r8 < r1 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_17680:
    sub64 r0, r3                                    r0 -= r3   ///  r0 = r0.wrapping_sub(r3)
    sub64 r8, r1                                    r8 -= r1   ///  r8 = r8.wrapping_sub(r1)
    add64 r6, -1                                    r6 += -1   ///  r6 = r6.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
    ja lbb_17383                                    if true { pc += -302 }
lbb_17685:
    mov64 r6, r2                                    r6 = r2
    mov64 r1, r10                                   r1 = r10
    add64 r1, -96                                   r1 += -96   ///  r1 = r1.wrapping_add(-96 as i32 as i64 as u64)
    ldxdw r2, [r10-0xc0]                    
    ldxdw r3, [r10-0xb8]                    
    mov64 r4, r8                                    r4 = r8
    call function_17136                     
    ldxdw r1, [r10-0x60]                    
    jeq r1, 0, lbb_17790                            if r1 == (0 as i32 as i64 as u64) { pc += 96 }
    mov64 r2, r6                                    r2 = r6
    div64 r2, r1                                    r2 /= r1   ///  r2 = r2 / r1
lbb_17696:
    ldxdw r7, [r10-0xd0]                    
    ldxdw r8, [r10-0xc8]                    
    mov64 r1, r10                                   r1 = r10
    add64 r1, -112                                  r1 += -112   ///  r1 = r1.wrapping_add(-112 as i32 as i64 as u64)
    mov64 r9, r2                                    r9 = r2
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
    ldxdw r6, [r10-0xc0]                    
    mov64 r4, r6                                    r4 = r6
    ldxdw r5, [r10-0xb8]                    
    call function_17083                     
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    ldxdw r1, [r10-0x70]                    
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r8, r1, lbb_17711                           if r8 < r1 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_17711:
    ldxdw r2, [r10-0x68]                    
    ldxdw r0, [r10-0xd8]                    
    jlt r0, r2, lbb_17715                           if r0 < r2 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_17715:
    jeq r0, r2, lbb_17717                           if r0 == r2 { pc += 1 }
    mov64 r3, r4                                    r3 = r4
lbb_17717:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jeq r3, 0, lbb_17775                            if r3 == (0 as i32 as i64 as u64) { pc += 56 }
    ldxdw r3, [r10-0xb8]                    
    add64 r0, r3                                    r0 += r3   ///  r0 = r0.wrapping_add(r3)
    mov64 r3, r8                                    r3 = r8
    add64 r3, r6                                    r3 += r6   ///  r3 = r3.wrapping_add(r6)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    jlt r3, r8, lbb_17727                           if r3 < r8 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_17727:
    add64 r0, r5                                    r0 += r5   ///  r0 = r0.wrapping_add(r5)
    ldxdw r6, [r10-0xe8]                    
    add64 r9, r6                                    r9 += r6   ///  r9 = r9.wrapping_add(r6)
    add64 r9, -1                                    r9 += -1   ///  r9 = r9.wrapping_add(-1 as i32 as i64 as u64)
    mov64 r5, 1                                     r5 = 1 as i32 as i64 as u64
    mov64 r8, r9                                    r8 = r9
    jlt r9, r6, lbb_17735                           if r9 < r6 { pc += 1 }
    mov64 r5, 0                                     r5 = 0 as i32 as i64 as u64
lbb_17735:
    sub64 r0, r2                                    r0 -= r2   ///  r0 = r0.wrapping_sub(r2)
    jlt r3, r1, lbb_17738                           if r3 < r1 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_17738:
    sub64 r0, r4                                    r0 -= r4   ///  r0 = r0.wrapping_sub(r4)
    sub64 r3, r1                                    r3 -= r1   ///  r3 = r3.wrapping_sub(r1)
    ldxdw r4, [r10-0xe0]                    
    add64 r4, r5                                    r4 += r5   ///  r4 = r4.wrapping_add(r5)
    mov64 r6, r8                                    r6 = r8
    mov64 r8, r3                                    r8 = r3
    ja lbb_17383                                    if true { pc += -362 }
lbb_17745:
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    ldxdw r7, [r10-0xd0]                    
    ldxdw r5, [r10-0xc0]                    
    ldxdw r8, [r10-0xc8]                    
    jlt r8, r5, lbb_17752                           if r8 < r5 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_17752:
    ldxdw r6, [r10-0xb8]                    
    jlt r0, r6, lbb_17755                           if r0 < r6 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_17755:
    ldxdw r6, [r10-0xb8]                    
    jeq r0, r6, lbb_17758                           if r0 == r6 { pc += 1 }
    mov64 r3, r4                                    r3 = r4
lbb_17758:
    and64 r3, 1                                     r3 &= 1   ///  r3 = r3.and(1)
    jne r3, 0, lbb_17772                            if r3 != (0 as i32 as i64 as u64) { pc += 12 }
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    jlt r8, r5, lbb_17764                           if r8 < r5 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_17764:
    ldxdw r6, [r10-0xb8]                    
    sub64 r0, r6                                    r0 -= r6   ///  r0 = r0.wrapping_sub(r6)
    add64 r1, 1                                     r1 += 1   ///  r1 = r1.wrapping_add(1 as i32 as i64 as u64)
    jeq r1, 0, lbb_17769                            if r1 == (0 as i32 as i64 as u64) { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_17769:
    sub64 r0, r4                                    r0 -= r4   ///  r0 = r0.wrapping_sub(r4)
    add64 r2, r3                                    r2 += r3   ///  r2 = r2.wrapping_add(r3)
    sub64 r8, r5                                    r8 -= r5   ///  r8 = r8.wrapping_sub(r5)
lbb_17772:
    mov64 r6, r1                                    r6 = r1
lbb_17773:
    mov64 r4, r2                                    r4 = r2
    ja lbb_17383                                    if true { pc += -392 }
lbb_17775:
    ldxdw r5, [r10-0xe8]                    
    mov64 r6, r5                                    r6 = r5
    add64 r6, r9                                    r6 += r9   ///  r6 = r6.wrapping_add(r9)
    mov64 r4, 1                                     r4 = 1 as i32 as i64 as u64
    mov64 r3, 1                                     r3 = 1 as i32 as i64 as u64
    jlt r6, r5, lbb_17782                           if r6 < r5 { pc += 1 }
    mov64 r3, 0                                     r3 = 0 as i32 as i64 as u64
lbb_17782:
    jlt r8, r1, lbb_17784                           if r8 < r1 { pc += 1 }
    mov64 r4, 0                                     r4 = 0 as i32 as i64 as u64
lbb_17784:
    sub64 r0, r2                                    r0 -= r2   ///  r0 = r0.wrapping_sub(r2)
    sub64 r0, r4                                    r0 -= r4   ///  r0 = r0.wrapping_sub(r4)
    sub64 r8, r1                                    r8 -= r1   ///  r8 = r8.wrapping_sub(r1)
    ldxdw r4, [r10-0xe0]                    
    add64 r4, r3                                    r4 += r3   ///  r4 = r4.wrapping_add(r3)
    ja lbb_17383                                    if true { pc += -407 }
lbb_17790:
    ja lbb_17696                                    if true { pc += -95 }
lbb_17791:
    ldxdw r7, [r10-0xd0]                    
    ldxdw r8, [r10-0xc8]                    
    ja lbb_17651                                    if true { pc += -143 }

function_17794:
    mov64 r6, r1                                    r6 = r1
    mov64 r1, r10                                   r1 = r10
    add64 r1, -32                                   r1 += -32   ///  r1 = r1.wrapping_add(-32 as i32 as i64 as u64)
    call function_17145                     
    ldxdw r1, [r10-0x20]                    
    ldxdw r2, [r10-0x18]                    
    stxdw [r6+0x8], r2                      
    stxdw [r6+0x0], r1                      
    exit                                    
